{"2025-03-21T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2410.05554v2","updated":"2025-03-21T17:46:46Z","published":"2024-10-07T23:27:51Z","title":"MultiNash-PF: A Particle Filtering Approach for Computing Multiple Local\n  Generalized Nash Equilibria in Trajectory Games","summary":"  Modern robotic systems frequently engage in complex multi-agent interactions,\nmany of which are inherently multi-modal, meaning they can lead to multiple\ndistinct outcomes. To interact effectively, robots must recognize the possible\ninteraction modes and adapt to the one preferred by other agents. In this work,\nwe propose an efficient algorithm for capturing the multimodality in\nmulti-agent interactions. We leverage a game-theoretic planner to model\ninteraction outcomes as equilibria where \\emph{each equilibrium} corresponds to\na distinct interaction \\emph{mode}. We then develop an efficient algorithm to\nidentify all the equilibria, allowing robots to reason about multiple\ninteraction modes. More specifically, we formulate interactive planning as\nConstrained Potential Trajectory Games (CPTGs) and model interaction outcomes\nby local Generalized Nash equilibria (GNEs) of the game. CPTGs are a class of\ngames for which a local GNE can be found by solving a single constrained\noptimal control problem where a potential function is minimized. We propose to\nintegrate the potential game approach with implicit particle filtering, a\nsample-efficient method for non-convex trajectory optimization. We utilize\nimplicit particle filtering to identify the coarse estimates of multiple local\nminimizers of the game's potential function. MultiNash-PF then refines these\nestimates with optimization solvers, obtaining different local GNEs. We show\nthrough numerical simulations that MultiNash-PF reduces computation time by up\nto 50\\% compared to a baseline. We further demonstrate the effectiveness of our\nalgorithm in real-world human-robot interaction scenarios, where it\nsuccessfully accounts for the multi-modal nature of interactions and resolves\npotential conflicts in real-time.\n","authors":["Maulik Bhatt","Iman Askari","Yue Yu","Ufuk Topcu","Huazhen Fang","Negar Mehr"],"pdf_url":"https://arxiv.org/pdf/2410.05554v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05876v3","updated":"2025-03-21T17:36:59Z","published":"2023-08-10T23:10:13Z","title":"Strategic Decision-Making in Multi-Agent Domains: A Weighted Constrained\n  Potential Dynamic Game Approach","summary":"  In interactive multi-agent settings, decision-making and planning are\nchallenging mainly due to the agents' interconnected objectives. Dynamic game\ntheory offers a formal framework for analyzing such intricacies. Yet, solving\nconstrained dynamic games and determining the interaction outcome in the form\nof generalized Nash Equilibria (GNE) pose computational challenges due to the\nneed for solving constrained coupled optimal control problems. In this paper,\nwe address this challenge by proposing to leverage the special structure of\nmany real-world multi-agent interactions. More specifically, our key idea is to\nleverage constrained dynamic potential games, which are games for which GNE can\nbe found by solving a single constrained optimal control problem associated\nwith minimizing the potential function. We argue that constrained dynamic\npotential games can effectively facilitate interactive decision-making in many\nmulti-agent interactions. We will identify structures in realistic multi-agent\ninteractive scenarios that can be transformed into weighted constrained\npotential dynamic games (WCPDGs). We will show that the GNE of the resulting\nWCPDG can be obtained by solving a single constrained optimal control problem.\nWe will demonstrate the effectiveness of the proposed method through various\nsimulation studies and show that we achieve significant improvements in solve\ntime compared to state-of-the-art game solvers. We further provide experimental\nvalidation of our proposed method in a navigation setup involving two\nquadrotors carrying a rigid object while avoiding collisions with two humans.\n","authors":["Maulik Bhatt","Yixuan Jia","Negar Mehr"],"pdf_url":"https://arxiv.org/pdf/2308.05876v3.pdf","comment":"in IEEE Transactions on Robotics 2025"},{"id":"http://arxiv.org/abs/2412.16346v2","updated":"2025-03-21T17:22:28Z","published":"2024-12-20T21:13:11Z","title":"SOUS VIDE: Cooking Visual Drone Navigation Policies in a Gaussian\n  Splatting Vacuum","summary":"  We propose a new simulator, training approach, and policy architecture,\ncollectively called SOUS VIDE, for end-to-end visual drone navigation. Our\ntrained policies exhibit zero-shot sim-to-real transfer with robust real-world\nperformance using only onboard perception and computation. Our simulator,\ncalled FiGS, couples a computationally simple drone dynamics model with a high\nvisual fidelity Gaussian Splatting scene reconstruction. FiGS can quickly\nsimulate drone flights producing photorealistic images at up to 130 fps. We use\nFiGS to collect 100k-300k image/state-action pairs from an expert MPC with\nprivileged state and dynamics information, randomized over dynamics parameters\nand spatial disturbances. We then distill this expert MPC into an end-to-end\nvisuomotor policy with a lightweight neural architecture, called SV-Net. SV-Net\nprocesses color image, optical flow and IMU data streams into low-level thrust\nand body rate commands at 20 Hz onboard a drone. Crucially, SV-Net includes a\nlearned module for low-level control that adapts at runtime to variations in\ndrone dynamics. In a campaign of 105 hardware experiments, we show SOUS VIDE\npolicies to be robust to 30% mass variations, 40 m/s wind gusts, 60% changes in\nambient brightness, shifting or removing objects from the scene, and people\nmoving aggressively through the drone's visual field. Code, data, and\nexperiment videos can be found on our project page:\nhttps://stanfordmsl.github.io/SousVide/.\n","authors":["JunEn Low","Maximilian Adang","Javier Yu","Keiko Nagami","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2412.16346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17309v1","updated":"2025-03-21T17:04:01Z","published":"2025-03-21T17:04:01Z","title":"LLM+MAP: Bimanual Robot Task Planning using Large Language Models and\n  Planning Domain Definition Language","summary":"  Bimanual robotic manipulation provides significant versatility, but also\npresents an inherent challenge due to the complexity involved in the spatial\nand temporal coordination between two hands. Existing works predominantly focus\non attaining human-level manipulation skills for robotic hands, yet little\nattention has been paid to task planning on long-horizon timescales. With their\noutstanding in-context learning and zero-shot generation abilities, Large\nLanguage Models (LLMs) have been applied and grounded in diverse robotic\nembodiments to facilitate task planning. However, LLMs still suffer from errors\nin long-horizon reasoning and from hallucinations in complex robotic tasks,\nlacking a guarantee of logical correctness when generating the plan. Previous\nworks, such as LLM+P, extended LLMs with symbolic planners. However, none have\nbeen successfully applied to bimanual robots. New challenges inevitably arise\nin bimanual manipulation, necessitating not only effective task decomposition\nbut also efficient task allocation. To address these challenges, this paper\nintroduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning\nand multi-agent planning, automating effective and efficient bimanual task\nplanning. We conduct simulated experiments on various long-horizon manipulation\ntasks of differing complexity. Our method is built using GPT-4o as the backend,\nand we compare its performance against plans generated directly by LLMs,\nincluding GPT-4o, V3 and also recent strong reasoning models o1 and R1. By\nanalyzing metrics such as planning time, success rate, group debits, and\nplanning-step reduction rate, we demonstrate the superior performance of\nLLM+MAP, while also providing insights into robotic reasoning. Code is\navailable at https://github.com/Kchu/LLM-MAP.\n","authors":["Kun Chu","Xufeng Zhao","Cornelius Weber","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2503.17309v1.pdf","comment":"Code and video are available at https://github.com/Kchu/LLM-MAP"},{"id":"http://arxiv.org/abs/2503.03234v3","updated":"2025-03-21T16:50:42Z","published":"2025-03-05T07:24:00Z","title":"Social Gesture Recognition in spHRI: Leveraging Fabric-Based Tactile\n  Sensing on Humanoid Robots","summary":"  Humans are able to convey different messages using only touch. Equipping\nrobots with the ability to understand social touch adds another modality in\nwhich humans and robots can communicate. In this paper, we present a social\ngesture recognition system using a fabric-based, large-scale tactile sensor\nplaced onto the arms of a humanoid robot. We built a social gesture dataset\nusing multiple participants and extracted temporal features for classification.\nBy collecting tactile data on a humanoid robot, our system provides insights\ninto human-robot social touch, and displays that the use of fabric based\nsensors could be a potential way of advancing the development of spHRI systems\nfor more natural and effective communication.\n","authors":["Dakarai Crowder","Kojo Vandyck","Xiping Sun","James McCann","Wenzhen Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.03234v3.pdf","comment":"Accepted to ICRA 25. 8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.18707v4","updated":"2025-03-21T16:30:58Z","published":"2024-09-27T12:50:52Z","title":"Discrete Policy: Learning Disentangled Action Space for Multi-Task\n  Robotic Manipulation","summary":"  Learning visuomotor policy for multi-task robotic manipulation has been a\nlong-standing challenge for the robotics community. The difficulty lies in the\ndiversity of action space: typically, a goal can be accomplished in multiple\nways, resulting in a multimodal action distribution for a single task. The\ncomplexity of action distribution escalates as the number of tasks increases.\nIn this work, we propose \\textbf{Discrete Policy}, a robot learning method for\ntraining universal agents capable of multi-task manipulation skills. Discrete\nPolicy employs vector quantization to map action sequences into a discrete\nlatent space, facilitating the learning of task-specific codes. These codes are\nthen reconstructed into the action space conditioned on observations and\nlanguage instruction. We evaluate our method on both simulation and multiple\nreal-world embodiments, including both single-arm and bimanual robot settings.\nWe demonstrate that our proposed Discrete Policy outperforms a well-established\nDiffusion Policy baseline and many state-of-the-art approaches, including ACT,\nOcto, and OpenVLA. For example, in a real-world multi-task training setting\nwith five tasks, Discrete Policy achieves an average success rate that is 26\\%\nhigher than Diffusion Policy and 15\\% higher than OpenVLA. As the number of\ntasks increases to 12, the performance gap between Discrete Policy and\nDiffusion Policy widens to 32.5\\%, further showcasing the advantages of our\napproach. Our work empirically demonstrates that learning multi-task policies\nwithin the latent space is a vital step toward achieving general-purpose\nagents.\n","authors":["Kun Wu","Yichen Zhu","Jinming Li","Junjie Wen","Ning Liu","Zhiyuan Xu","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2409.18707v4.pdf","comment":"Accept to ICRA 2025"},{"id":"http://arxiv.org/abs/2503.14558v2","updated":"2025-03-21T16:23:42Z","published":"2025-03-18T03:21:58Z","title":"SuperPC: A Single Diffusion Model for Point Cloud Completion,\n  Upsampling, Denoising, and Colorization","summary":"  Point cloud (PC) processing tasks-such as completion, upsampling, denoising,\nand colorization-are crucial in applications like autonomous driving and 3D\nreconstruction. Despite substantial advancements, prior approaches often\naddress each of these tasks independently, with separate models focused on\nindividual issues. However, this isolated approach fails to account for the\nfact that defects like incompleteness, low resolution, noise, and lack of color\nfrequently coexist, with each defect influencing and correlating with the\nothers. Simply applying these models sequentially can lead to error\naccumulation from each model, along with increased computational costs. To\naddress these challenges, we introduce SuperPC, the first unified diffusion\nmodel capable of concurrently handling all four tasks. Our approach employs a\nthree-level-conditioned diffusion framework, enhanced by a novel\nspatial-mix-fusion strategy, to leverage the correlations among these four\ndefects for simultaneous, efficient processing. We show that SuperPC\noutperforms the state-of-the-art specialized models as well as their\ncombination on all four individual tasks.\n","authors":["Yi Du","Zhipeng Zhao","Shaoshu Su","Sharath Golluri","Haoze Zheng","Runmao Yao","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2503.14558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12262v4","updated":"2025-03-21T15:32:07Z","published":"2024-09-18T18:47:58Z","title":"Bootstrapping Object-level Planning with Large Language Models","summary":"  We introduce a new method that extracts knowledge from a large language model\n(LLM) to produce object-level plans, which describe high-level changes to\nobject state, and uses them to bootstrap task and motion planning (TAMP).\nExisting work uses LLMs to directly output task plans or generate goals in\nrepresentations like PDDL. However, these methods fall short because they rely\non the LLM to do the actual planning or output a hard-to-satisfy goal. Our\napproach instead extracts knowledge from an LLM in the form of plan schemas as\nan object-level representation called functional object-oriented networks\n(FOON), from which we automatically generate PDDL subgoals. Our method markedly\noutperforms alternative planning strategies in completing several\npick-and-place tasks in simulation.\n","authors":["David Paulius","Alejandro Agostini","Benedict Quartey","George Konidaris"],"pdf_url":"https://arxiv.org/pdf/2409.12262v4.pdf","comment":"Accepted to ICRA 2025; 11 pages (6 pages + 1 page references + 4\n  pages appendix); for demo videos, please see\n  https://davidpaulius.github.io/olp_llm/"},{"id":"http://arxiv.org/abs/2503.17227v1","updated":"2025-03-21T15:30:09Z","published":"2025-03-21T15:30:09Z","title":"Control the Soft Robot Arm with its Physical Twin","summary":"  To exploit the compliant capabilities of soft robot arms we require\ncontroller which can exploit their physical capabilities. Teleoperation,\nleveraging a human in the loop, is a key step towards achieving more complex\ncontrol strategies. Whilst teleoperation is widely used for rigid robots, for\nsoft robots we require teleoperation methods where the configuration of the\nwhole body is considered. We propose a method of using an identical 'physical\ntwin', or demonstrator of the robot. This tendon robot can be back-driven, with\nthe tendon lengths providing configuration perception, and enabling a direct\nmapping of tendon lengths for the execture. We demonstrate how this\nteleoperation across the entire configuration of the robot enables complex\ninteractions with exploit the envrionment, such as squeezing into gaps. We also\nshow how this method can generalize to robots which are a larger scale that the\nphysical twin, and how, tuneability of the stiffness properties of the physical\ntwin simplify its use.\n","authors":["Qinghua Guan","Hung Hon Cheng","Benhui Dai","Josie Hughes"],"pdf_url":"https://arxiv.org/pdf/2503.17227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04170v2","updated":"2025-03-21T15:26:12Z","published":"2025-01-07T22:40:37Z","title":"A Bayesian Modeling Framework for Estimation and Ground Segmentation of\n  Cluttered Staircases","summary":"  Autonomous robot navigation in complex environments requires robust\nperception as well as high-level scene understanding due to perceptual\nchallenges, such as occlusions, and uncertainty introduced by robot movement.\nFor example, a robot climbing a cluttered staircase can misinterpret clutter as\na step, misrepresenting the state and compromising safety. This requires robust\nstate estimation methods capable of inferring the underlying structure of the\nenvironment even from incomplete sensor data. In this paper, we introduce a\nnovel method for robust state estimation of staircases. To address the\nchallenge of perceiving occluded staircases extending beyond the robot's\nfield-of-view, our approach combines an infinite-width staircase representation\nwith a finite endpoint state to capture the overall staircase structure. This\nrepresentation is integrated into a Bayesian inference framework to fuse noisy\nmeasurements enabling accurate estimation of staircase location even with\npartial observations and occlusions. Additionally, we present a segmentation\nalgorithm that works in conjunction with the staircase estimation pipeline to\naccurately identify clutter-free regions on a staircase. Our method is\nextensively evaluated on real robot across diverse staircases, demonstrating\nsignificant improvements in estimation accuracy and segmentation performance\ncompared to baseline approaches.\n","authors":["Prasanna Sriganesh","Burhanuddin Shirose","Matthew Travers"],"pdf_url":"https://arxiv.org/pdf/2501.04170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.00675v5","updated":"2025-03-21T15:00:27Z","published":"2021-10-01T23:03:21Z","title":"Contraction Theory for Nonlinear Stability Analysis and Learning-based\n  Control: A Tutorial Overview","summary":"  Contraction theory is an analytical tool to study differential dynamics of a\nnon-autonomous (i.e., time-varying) nonlinear system under a contraction metric\ndefined with a uniformly positive definite matrix, the existence of which\nresults in a necessary and sufficient characterization of incremental\nexponential stability of multiple solution trajectories with respect to each\nother. By using a squared differential length as a Lyapunov-like function, its\nnonlinear stability analysis boils down to finding a suitable contraction\nmetric that satisfies a stability condition expressed as a linear matrix\ninequality, indicating that many parallels can be drawn between well-known\nlinear systems theory and contraction theory for nonlinear systems.\nFurthermore, contraction theory takes advantage of a superior robustness\nproperty of exponential stability used in conjunction with the comparison\nlemma. This yields much-needed safety and stability guarantees for neural\nnetwork-based control and estimation schemes, without resorting to a more\ninvolved method of using uniform asymptotic stability for input-to-state\nstability. Such distinctive features permit the systematic construction of a\ncontraction metric via convex optimization, thereby obtaining an explicit\nexponential bound on the distance between a time-varying target trajectory and\nsolution trajectories perturbed externally due to disturbances and learning\nerrors. The objective of this paper is, therefore, to present a tutorial\noverview of contraction theory and its advantages in nonlinear stability\nanalysis of deterministic and stochastic systems, with an emphasis on deriving\nformal robustness and stability guarantees for various learning-based and\ndata-driven automatic control methods. In particular, we provide a detailed\nreview of techniques for finding contraction metrics and associated control and\nestimation laws using deep neural networks.\n","authors":["Hiroyasu Tsukamoto","Soon-Jo Chung","Jean-Jacques E. Slotine"],"pdf_url":"https://arxiv.org/pdf/2110.00675v5.pdf","comment":"Annual Reviews in Control, Preprint Version, Accepted, Oct. 1st"},{"id":"http://arxiv.org/abs/2412.04455v3","updated":"2025-03-21T14:54:29Z","published":"2024-12-05T18:58:27Z","title":"Code-as-Monitor: Constraint-aware Visual Programming for Reactive and\n  Proactive Robotic Failure Detection","summary":"  Automatic detection and prevention of open-set failures are crucial in\nclosed-loop robotic systems. Recent studies often struggle to simultaneously\nidentify unexpected failures reactively after they occur and prevent\nforeseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a\nnovel paradigm leveraging the vision-language model (VLM) for both open-set\nreactive and proactive failure detection. The core of our method is to\nformulate both tasks as a unified set of spatio-temporal constraint\nsatisfaction problems and use VLM-generated code to evaluate them for real-time\nmonitoring. To enhance the accuracy and efficiency of monitoring, we further\nintroduce constraint elements that abstract constraint-related entities or\ntheir parts into compact geometric elements. This approach offers greater\ngenerality, simplifies tracking, and facilitates constraint-aware visual\nprogramming by leveraging these elements as visual prompts. Experiments show\nthat CaM achieves a 28.7% higher success rate and reduces execution time by\n31.8% under severe disturbances compared to baselines across three simulators\nand a real-world setting. Moreover, CaM can be integrated with open-loop\ncontrol policies to form closed-loop systems, enabling long-horizon tasks in\ncluttered scenes with dynamic environments.\n","authors":["Enshen Zhou","Qi Su","Cheng Chi","Zhizheng Zhang","Zhongyuan Wang","Tiejun Huang","Lu Sheng","He Wang"],"pdf_url":"https://arxiv.org/pdf/2412.04455v3.pdf","comment":"Accepted by CVPR 2025. Project page:\n  https://zhoues.github.io/Code-as-Monitor/"},{"id":"http://arxiv.org/abs/2402.10079v2","updated":"2025-03-21T14:13:38Z","published":"2024-01-29T16:56:17Z","title":"Data-driven Camera and Lidar Simulation Models for Autonomous Driving: A\n  Review from Generative Models to Volume Renderers","summary":"  Perception sensors, particularly camera and Lidar, are key elements of\nAutonomous Driving Systems (ADS) that enable them to comprehend their\nsurroundings to informed driving and control decisions. Therefore, developing\nrealistic simulation models for these sensors is essential for conducting\neffective simulation-based testing of ADS. Moreover, the rise of deep\nlearning-based perception models has increased the utility of sensor simulation\nmodels for synthesising diverse training datasets. The traditional sensor\nsimulation models rely on computationally expensive physics-based algorithms,\nspecifically in complex systems such as ADS. Hence, the current potential\nresides in data-driven approaches, fuelled by the exceptional performance of\ndeep generative models in capturing high-dimensional data distribution and\nvolume renderers in accurately representing scenes. This paper reviews the\ncurrent state-of-the-art data-driven camera and Lidar simulation models and\ntheir evaluation methods. It explores a spectrum of models from the novel\nperspective of generative models and volume renderers. Generative models are\ndiscussed in terms of their input-output types, while volume renderers are\ncategorised based on their input encoding. Finally, the paper illustrates\ncommonly used evaluation techniques for assessing sensor simulation models and\nhighlights the existing research gaps in the area.\n","authors":["Hamed Haghighi","Xiaomeng Wang","Hao Jing","Mehrdad Dianati"],"pdf_url":"https://arxiv.org/pdf/2402.10079v2.pdf","comment":"To be published in IEEE Transactions on Intelligent Vehicles"},{"id":"http://arxiv.org/abs/2412.12406v5","updated":"2025-03-21T13:48:46Z","published":"2024-12-16T23:17:40Z","title":"Global SLAM Using 5G ToA Integration: Performance Analysis with Unknown\n  Base Stations and Loop Closure Alternatives","summary":"  This paper presents a novel approach that integrates 5G Time of Arrival (ToA)\nmeasurements into ORB-SLAM3 to enable global localization and enhance mapping\ncapabilities for indoor drone navigation. We extend ORB-SLAM3's optimization\npipeline to jointly process ToA data from 5G base stations alongside visual and\ninertial measurements while estimating system biases. This integration\ntransforms the inherently local SLAM estimates into globally referenced\ntrajectories and effectively resolves scale ambiguity in monocular\nconfigurations. Our method is evaluated using both Aerolab indoor datasets with\nRGB-D cameras and the EuRoC MAV benchmark, complemented by simulated 5G ToA\nmeasurements at 28 GHz and 78 GHz frequencies using MATLAB and QuaDRiGa.\nExtensive experiments across multiple SLAM configurations demonstrate that ToA\nintegration enables consistent global positioning across all modes while\nmaintaining local accuracy. For monocular configurations, ToA integration\nsuccessfully resolves scale ambiguity and improves consistency. We further\ninvestigate scenarios with unknown base station positions and demonstrate that\nToA measurements can effectively serve as an alternative to loop closure for\ndrift correction. We also analyze how different geometric arrangements of base\nstations impact SLAM performance. Comparative analysis with state-of-the-art\nmethods, including UWB-VO, confirms our approach's robustness even with lower\nmeasurement frequencies and sequential base station operation. The results\nvalidate that 5G ToA integration provides substantial benefits for global SLAM\napplications, particularly in challenging indoor environments where accurate\npositioning is critical.\n","authors":["Meisam Kabiri","Holger Voos"],"pdf_url":"https://arxiv.org/pdf/2412.12406v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17125v1","updated":"2025-03-21T13:20:39Z","published":"2025-03-21T13:20:39Z","title":"Leveraging Language Models for Out-of-Distribution Recovery in\n  Reinforcement Learning","summary":"  Deep Reinforcement Learning (DRL) has demonstrated strong performance in\nrobotic control but remains susceptible to out-of-distribution (OOD) states,\noften resulting in unreliable actions and task failure. While previous methods\nhave focused on minimizing or preventing OOD occurrences, they largely neglect\nrecovery once an agent encounters such states. Although the latest research has\nattempted to address this by guiding agents back to in-distribution states,\ntheir reliance on uncertainty estimation hinders scalability in complex\nenvironments. To overcome this limitation, we introduce Language Models for\nOut-of-Distribution Recovery (LaMOuR), which enables recovery learning without\nrelying on uncertainty estimation. LaMOuR generates dense reward codes that\nguide the agent back to a state where it can successfully perform its original\ntask, leveraging the capabilities of LVLMs in image description, logical\nreasoning, and code generation. Experimental results show that LaMOuR\nsubstantially enhances recovery efficiency across diverse locomotion tasks and\neven generalizes effectively to complex environments, including humanoid\nlocomotion and mobile manipulation, where existing methods struggle. The code\nand supplementary materials are available at\n\\href{https://lamour-rl.github.io/}{https://lamour-rl.github.io/}.\n","authors":["Chan Kim","Seung-Woo Seo","Seong-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2503.17125v1.pdf","comment":"14 pages, 17 figures"},{"id":"http://arxiv.org/abs/2503.17106v1","updated":"2025-03-21T12:46:38Z","published":"2025-03-21T12:46:38Z","title":"GAA-TSO: Geometry-Aware Assisted Depth Completion for Transparent and\n  Specular Objects","summary":"  Transparent and specular objects are frequently encountered in daily life,\nfactories, and laboratories. However, due to the unique optical properties, the\ndepth information on these objects is usually incomplete and inaccurate, which\nposes significant challenges for downstream robotics tasks. Therefore, it is\ncrucial to accurately restore the depth information of transparent and specular\nobjects. Previous depth completion methods for these objects usually use RGB\ninformation as an additional channel of the depth image to perform depth\nprediction. Due to the poor-texture characteristics of transparent and specular\nobjects, these methods that rely heavily on color information tend to generate\nstructure-less depth predictions. Moreover, these 2D methods cannot effectively\nexplore the 3D structure hidden in the depth channel, resulting in depth\nambiguity. To this end, we propose a geometry-aware assisted depth completion\nmethod for transparent and specular objects, which focuses on exploring the 3D\nstructural cues of the scene. Specifically, besides extracting 2D features from\nRGB-D input, we back-project the input depth to a point cloud and build the 3D\nbranch to extract hierarchical scene-level 3D structural features. To exploit\n3D geometric information, we design several gated cross-modal fusion modules to\neffectively propagate multi-level 3D geometric features to the image branch. In\naddition, we propose an adaptive correlation aggregation strategy to\nappropriately assign 3D features to the corresponding 2D features. Extensive\nexperiments on ClearGrasp, OOD, TransCG, and STD datasets show that our method\noutperforms other state-of-the-art methods. We further demonstrate that our\nmethod significantly enhances the performance of downstream robotic grasping\ntasks.\n","authors":["Yizhe Liu","Tong Jia","Da Cai","Hao Wang","Dongyue Chen"],"pdf_url":"https://arxiv.org/pdf/2503.17106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17078v1","updated":"2025-03-21T12:03:49Z","published":"2025-03-21T12:03:49Z","title":"Exploring psychophysiological methods for human-robot collaboration in\n  construction","summary":"  Psychophysiological methods present a promising approach to fostering\nenhanced mutual communication and collaboration between human workers and\nrobots. Despite their potential, there is still limited understanding of how to\neffectively integrate psychophysiological methods to improve human-robot\ncollaboration (HRC) in construction. This paper addresses this gap by\ncritically reviewing the use of psychophysiological methods for HRC within\nconstruction environments, employing a concept-methodology-value philosophical\nframework. The analysis reveals that measuring brain activity using\nelectroencephalography is the most widely used method, while most of the works\nare still at the proof of concept stage and lack empirical evidence. Three\npotential research directions were proposed: the integration of multi-modal\npsychophysiological signals, enriching the existing experimental settings for\nbetter generalizability, and leveraging advanced biocompatible or contactless\ntechnologies for effective signal detection. The findings should benefit\nsubsequent exploration and practical applications of psychophysiological\nmethods to enable better implementation of robots and support HRC in\nconstruction.\n","authors":["Saika Wong","Zhentao Chen","Mi Pan","Miroslaw J. Skibniewski"],"pdf_url":"https://arxiv.org/pdf/2503.17078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17046v1","updated":"2025-03-21T11:04:01Z","published":"2025-03-21T11:04:01Z","title":"HAPI: A Model for Learning Robot Facial Expressions from Human\n  Preferences","summary":"  Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses.\n","authors":["Dongsheng Yang","Qianying Liu","Wataru Sato","Takashi Minato","Chaoran Liu","Shin'ya Nishida"],"pdf_url":"https://arxiv.org/pdf/2503.17046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17014v1","updated":"2025-03-21T10:23:06Z","published":"2025-03-21T10:23:06Z","title":"Behavioral Conflict Avoidance Between Humans and Quadruped Robots in\n  Shared Environments","summary":"  Nowadays, robots are increasingly operated in environments shared with\nhumans, where conflicts between human and robot behaviors may compromise\nsafety. This paper presents a proactive behavioral conflict avoidance framework\nbased on the principle of adaptation to trends for quadruped robots that not\nonly ensures the robot's safety but also minimizes interference with human\nactivities. It can proactively avoid potential conflicts with approaching\nhumans or other dynamic objects, whether the robot is stationary or in motion,\nthen swiftly resume its tasks once the conflict subsides. An enhanced approach\nis proposed to achieve precise human detection and tracking on vibratory robot\nplatform equipped with low-cost hybrid solid-state LiDAR. When potential\nconflict detected, the robot selects an avoidance point and executes an evasion\nmaneuver before resuming its task. This approach contrasts with conventional\nmethods that remain goal-driven, often resulting in aggressive behaviors, such\nas forcibly bypassing obstacles and causing conflicts or becoming stuck in\ndeadlock scenarios. The selection of avoidance points is achieved by\nintegrating static and dynamic obstacle to generate a potential field map. The\nrobot then searches for feasible regions within this map and determines the\noptimal avoidance point using an evaluation function. Experimental results\ndemonstrate that the framework significantly reduces interference with human\nactivities, enhances the safety of both robots and persons.\n","authors":["Shuang Wei","Muhua Zhang","Yun Gan","Deqing Huang","Lei Ma","Chenguang Yang"],"pdf_url":"https://arxiv.org/pdf/2503.17014v1.pdf","comment":"7 pages, 9 figures. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2503.17005v1","updated":"2025-03-21T10:10:04Z","published":"2025-03-21T10:10:04Z","title":"Autonomous Exploration-Based Precise Mapping for Mobile Robots through\n  Stepwise and Consistent Motions","summary":"  This paper presents an autonomous exploration framework. It is designed for\nindoor ground mobile robots that utilize laser Simultaneous Localization and\nMapping (SLAM), ensuring process completeness and precise mapping results. For\nfrontier search, the local-global sampling architecture based on multiple\nRapidly Exploring Random Trees (RRTs) is employed. Traversability checks during\nRRT expansion and global RRT pruning upon map updates eliminate unreachable\nfrontiers, reducing potential collisions and deadlocks. Adaptive sampling\ndensity adjustments, informed by obstacle distribution, enhance exploration\ncoverage potential. For frontier point navigation, a stepwise consistent motion\nstrategy is adopted, wherein the robot strictly drives straight on\napproximately equidistant line segments in the polyline path and rotates in\nplace at segment junctions. This simplified, decoupled motion pattern improves\nscan-matching stability and mitigates map drift. For process control, the\nframework serializes frontier point selection and navigation, avoiding\noscillation caused by frequent goal changes in conventional parallelized\nprocesses. The waypoint retracing mechanism is introduced to generate repeated\nobservations, triggering loop closure detection and backend optimization in\ngraph-based SLAM, thereby improving map consistency and precision. Experiments\nin both simulation and real-world scenarios validate the effectiveness of the\nframework. It achieves improved mapping coverage and precision in more\nchallenging environments compared to baseline 2D exploration algorithms. It\nalso shows robustness in supporting resource-constrained robot platforms and\nmaintaining mapping consistency across various LiDAR field-of-view (FoV)\nconfigurations.\n","authors":["Muhua Zhang","Lei Ma","Ying Wu","Kai Shen","Yongkui Sun","Henry Leung"],"pdf_url":"https://arxiv.org/pdf/2503.17005v1.pdf","comment":"8 pages, 11 figures. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2503.17002v1","updated":"2025-03-21T10:09:04Z","published":"2025-03-21T10:09:04Z","title":"Targetless 6DoF Calibration of LiDAR and 2D Scanning Radar Based on\n  Cylindrical Occupancy","summary":"  Owing to the capability for reliable and all-weather long-range sensing, the\nfusion of LiDAR and Radar has been widely applied to autonomous vehicles for\nrobust perception. In practical operation, well manually calibrated extrinsic\nparameters, which are crucial for the fusion of multi-modal sensors, may drift\ndue to the vibration. To address this issue, we present a novel targetless\ncalibration approach, termed LiRaCo, for the extrinsic 6DoF calibration of\nLiDAR and Radar sensors. Although both types of sensors can obtain geometric\ninformation, bridging the geometric correspondences between multi-modal data\nwithout any clues of explicit artificial markers is nontrivial, mainly due to\nthe low vertical resolution of scanning Radar. To achieve the targetless\ncalibration, LiRaCo leverages a spatial occupancy consistency between LiDAR\npoint clouds and Radar scans in a common cylindrical representation,\nconsidering the increasing data sparsity with distance for both sensors.\nSpecifically, LiRaCo expands the valid Radar scanned pixels into 3D occupancy\ngrids to constrain LiDAR point clouds based on spatial consistency.\nConsequently, a cost function involving extrinsic calibration parameters is\nformulated based on the spatial overlap of 3D grids and LiDAR points. Extrinsic\nparameters are finally estimated by optimizing the cost function. Comprehensive\nquantitative and qualitative experiments on two real outdoor datasets with\ndifferent LiDAR sensors demonstrate the feasibility and accuracy of the\nproposed method. The source code will be publicly available.\n","authors":["Weimin Wang","Yu Du","Ting Yang","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2503.17002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11623v2","updated":"2025-03-21T09:59:58Z","published":"2024-03-18T09:55:22Z","title":"Synthesizing multi-log grasp poses in cluttered environments","summary":"  Multi-object grasping is a challenging task. It is important for energy and\ncost-efficient operation of industrial crane manipulators, such as those used\nto collect tree logs from the forest floor and on forest machines. In this\nwork, we used synthetic data from physics simulations to explore how\ndata-driven modeling can be used to infer multi-object grasp poses from images.\nWe showed that convolutional neural networks can be trained specifically for\nsynthesizing multi-object grasps. Using RGB-Depth images and instance\nsegmentation masks as input, a U-Net model outputs grasp maps with the\ncorresponding grapple orientation and opening width. Given an observation of a\npile of logs, the model can be used to synthesize and rate the possible grasp\nposes and select the most suitable one, with the possibility to respect\nchanging operational constraints such as lift capacity and reach. When tested\non previously unseen data, the proposed model found successful grasp poses with\nan accuracy up to 96%.\n","authors":["Arvid Fälldin","Tommy Löfstedt","Tobias Semberg","Erik Wallin","Martin Servin"],"pdf_url":"https://arxiv.org/pdf/2403.11623v2.pdf","comment":"21 pages, 14 figures"},{"id":"http://arxiv.org/abs/2503.16969v1","updated":"2025-03-21T09:32:25Z","published":"2025-03-21T09:32:25Z","title":"Extending Behavior Trees for Robotic Missions with Quality Requirements","summary":"  Context and motivation: In recent years, behavior trees have gained growing\ninterest within the robotics community as a specification and control switching\nmechanism for the different tasks that form a robotics mission. Problem: Given\nthe rising complexity and prevalence of robotic systems, it is increasingly\nchallenging and important for practitioners to design high-quality missions\nthat meet certain qualities, for instance, to consider potential failures or\nmitigate safety risks. In software requirements engineering, quality or\nnon-functional requirements have long been recognized as a key factor in system\nsuccess. Currently, qualities are not represented in behavior tree models,\nwhich capture a robotic mission, making it difficult to assess the extent to\nwhich different mission components comply with those qualities. Principal\nideas: In this paper, we propose an extension for behavior trees to have\nqualities and quality requirements explicitly represented in robotics missions.\nWe provide a meta-model for the extension, develop a domain-specific language\n(DSL), and describe how we integrated our DSL in one of the most used languages\nin robotics for developing behavior trees, BehaviorTree.CPP. A preliminary\nevaluation of the implemented DSL shows promising results for the feasibility\nof our approach and the need for similar DSLs. Contribution: Our approach paves\nthe way for incorporating qualities into the behavior model of robotics\nmissions. This promotes early expression of qualities in robotics missions, and\na better overview of missions components and their contribution to the\nsatisfaction of quality concerns.\n","authors":["Razan Ghzouli","Rebekka Wohlrab","Jennifer Horkoff"],"pdf_url":"https://arxiv.org/pdf/2503.16969v1.pdf","comment":"17 pages, 6 figures, Requirements Engineering: Foundation for\n  Software Quality (REFSQ) 2025"},{"id":"http://arxiv.org/abs/2403.15113v3","updated":"2025-03-21T09:21:16Z","published":"2024-03-22T11:08:56Z","title":"Set-membership target search and tracking within an unknown cluttered\n  area using cooperating UAVs equipped with vision systems","summary":"  This paper addresses the problem of target search and tracking using a fleet\nof cooperating UAVs evolving in some unknown region of interest containing an a\npriori unknown number of moving ground targets. Each drone is equipped with an\nembedded Computer Vision System (CVS), providing an image with labeled pixels\nand a depth map of the observed part of its environment. Moreover, a box\ncontaining the corresponding pixels in the image frame is available when a UAV\nidentifies a target. Hypotheses regarding information provided by the pixel\nclassification, depth map construction, and target identification algorithms\nare proposed to allow its exploitation by set-membership approaches. A\nset-membership target location estimator is developed using the information\nprovided by the CVS. Each UAV evaluates sets guaranteed to contain the location\nof the identified targets and a set possibly containing the locations of\ntargets still to be identified. Then, each UAV uses these sets to search and\ntrack targets cooperatively.\n","authors":["Maxime Zagar","Luc Meyer","Michel Kieffer","Hélène Piet-Lahanier"],"pdf_url":"https://arxiv.org/pdf/2403.15113v3.pdf","comment":"This work has been submitted to Elsevier / ScienceDirect for possible\n  publication"},{"id":"http://arxiv.org/abs/2503.16960v1","updated":"2025-03-21T09:09:58Z","published":"2025-03-21T09:09:58Z","title":"Somatic Safety: An Embodied Approach Towards Safe Human-Robot\n  Interaction","summary":"  As robots enter the messy human world so the vital matter of safety takes on\na fresh complexion with physical contact becoming inevitable and even\ndesirable. We report on an artistic-exploration of how dancers, working as part\nof a multidisciplinary team, engaged in contact improvisation exercises to\nexplore the opportunities and challenges of dancing with cobots. We reveal how\nthey employed their honed bodily senses and physical skills to engage with the\nrobots aesthetically and yet safely, interleaving improvised physical\nmanipulations with reflections to grow their knowledge of how the robots\nbehaved and felt. We introduce somatic safety, a holistic mind-body approach in\nwhich safety is learned, felt and enacted through bodily contact with robots in\naddition to being reasoned about. We conclude that robots need to be better\ndesigned for people to hold them and might recognise tacit safety cues among\npeople.We propose that safety should be learned through iterative bodily\nexperience interleaved with reflection.\n","authors":["Steve Benford","Eike Schneiders","Juan Pablo Martinez Avila","Praminda Caleb-Solly","Patrick Robert Brundell","Simon Castle-Green","Feng Zhou","Rachael Garrett","Kristina Höök","Sarah Whatley","Kate Marsh","Paul Tennent"],"pdf_url":"https://arxiv.org/pdf/2503.16960v1.pdf","comment":"ACM/IEEE International Conference on Human-Robot Interaction (HRI'25)"},{"id":"http://arxiv.org/abs/2503.16935v1","updated":"2025-03-21T08:21:10Z","published":"2025-03-21T08:21:10Z","title":"Reachability-Guaranteed Optimal Control for the Interception of Dynamic\n  Targets under Uncertainty","summary":"  Intercepting dynamic objects in uncertain environments involves a significant\nunresolved challenge in modern robotic systems. Current control approaches rely\nsolely on estimated information, and results lack guarantees of robustness and\nfeasibility. In this work, we introduce a novel method to tackle the\ninterception of targets whose motion is affected by known and bounded\nuncertainty. Our approach introduces new techniques of reachability analysis\nfor rigid bodies, leveraged to guarantee feasibility of interception under\nuncertain conditions. We then propose a Reachability-Guaranteed Optimal Control\nProblem, ensuring robustness and guaranteed reachability to a target set of\nconfigurations. We demonstrate the methodology in the case study of an\ninterception maneuver of a tumbling target in space.\n","authors":["Tommaso Faraci","Roberto Lampariello"],"pdf_url":"https://arxiv.org/pdf/2503.16935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09258v2","updated":"2025-03-21T08:19:55Z","published":"2024-01-17T15:05:00Z","title":"Efficient Training of Generalizable Visuomotor Policies via\n  Control-Aware Augmentation","summary":"  Improving generalization is one key challenge in embodied AI, where obtaining\nlarge-scale datasets across diverse scenarios is costly. Traditional weak\naugmentations, such as cropping and flipping, are insufficient for improving a\nmodel's performance in new environments. Existing data augmentation methods\noften disrupt task-relevant information in images, potentially degrading\nperformance. To overcome these challenges, we introduce EAGLE, an efficient\ntraining framework for generalizable visuomotor policies that improves upon\nexisting methods by (1) enhancing generalization by applying augmentation only\nto control-related regions identified through a self-supervised control-aware\nmask and (2) improving training stability and efficiency by distilling\nknowledge from an expert to a visuomotor student policy, which is then deployed\nto unseen environments without further fine-tuning. Comprehensive experiments\non three domains, including the DMControl Generalization Benchmark, the\nenhanced Robot Manipulation Distraction Benchmark, and a long-sequential\ndrawer-opening task, validate the effectiveness of our method.\n","authors":["Yinuo Zhao","Kun Wu","Tianjiao Yi","Zhiyuan Xu","Xiaozhu Ju","Zhengping Che","Chi Harold Liu","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2401.09258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16932v1","updated":"2025-03-21T08:12:40Z","published":"2025-03-21T08:12:40Z","title":"Rude Humans and Vengeful Robots: Examining Human Perceptions of Robot\n  Retaliatory Intentions in Professional Settings","summary":"  Humans and robots are increasingly working in personal and professional\nsettings. In workplace settings, humans and robots may work together as\ncolleagues, potentially leading to social expectations, or violation thereof.\nExtant research has primarily sought to understand social interactions and\nexpectations in personal rather than professional settings, and none of these\nstudies have examined negative outcomes arising from violations of social\nexpectations. This paper reports the results of a 2x3 online experiment that\nused a unique first-person perspective video to immerse participants in a\ncollaborative workplace setting. The results are nuanced and reveal that while\nrobots are expected to act in accordance with social expectations despite human\nbehavior, there are benefits for robots perceived as being the bigger person in\nthe face of human rudeness. Theoretical and practical implications are provided\nwhich discuss the import of these findings for the design of social robots.\n","authors":["Kate Letheren","Nicole Robinson"],"pdf_url":"https://arxiv.org/pdf/2503.16932v1.pdf","comment":"This is the author version of the manuscript submitted to ACM\n  Transactions on Human-Robot Interaction. The final version, if accepted, will\n  be published by ACM and available via the ACM Digital Library. 12 pages, 1\n  figure, 2 tables"},{"id":"http://arxiv.org/abs/2503.15202v2","updated":"2025-03-21T08:10:48Z","published":"2025-03-19T13:40:56Z","title":"A Unified Framework for Real-Time Failure Handling in Robotics Using\n  Vision-Language Models, Reactive Planner and Behavior Trees","summary":"  Robotic systems often face execution failures due to unexpected obstacles,\nsensor errors, or environmental changes. Traditional failure recovery methods\nrely on predefined strategies or human intervention, making them less\nadaptable. This paper presents a unified failure recovery framework that\ncombines Vision-Language Models (VLMs), a reactive planner, and Behavior Trees\n(BTs) to enable real-time failure handling. Our approach includes pre-execution\nverification, which checks for potential failures before execution, and\nreactive failure handling, which detects and corrects failures during execution\nby verifying existing BT conditions, adding missing preconditions and, when\nnecessary, generating new skills. The framework uses a scene graph for\nstructured environmental perception and an execution history for continuous\nmonitoring, enabling context-aware and adaptive failure handling. We evaluate\nour framework through real-world experiments with an ABB YuMi robot on tasks\nlike peg insertion, object sorting, and drawer placement, as well as in\nAI2-THOR simulator. Compared to using pre-execution and reactive methods\nseparately, our approach achieves higher task success rates and greater\nadaptability. Ablation studies highlight the importance of VLM-based reasoning,\nstructured scene representation, and execution history tracking for effective\nfailure recovery in robotics.\n","authors":["Faseeh Ahmad","Hashim Ismail","Jonathan Styrud","Maj Stenmark","Volker Krueger"],"pdf_url":"https://arxiv.org/pdf/2503.15202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13945v2","updated":"2025-03-21T07:57:38Z","published":"2024-11-21T08:54:45Z","title":"Neuromorphic Attitude Estimation and Control","summary":"  The real-world application of small drones is mostly hampered by energy\nlimitations. Neuromorphic computing promises extremely energy-efficient AI for\nautonomous flight but is still challenging to train and deploy on real robots.\nTo reap the maximal benefits from neuromorphic computing, it is necessary to\nperform all autonomy functions end-to-end on a single neuromorphic chip, from\nlow-level attitude control to high-level navigation. This research presents the\nfirst neuromorphic control system using a spiking neural network (SNN) to\neffectively map a drone's raw sensory input directly to motor commands. We\napply this method to low-level attitude estimation and control for a quadrotor,\ndeploying the SNN on a tiny Crazyflie. We propose a modular SNN, separately\ntraining and then merging estimation and control sub-networks. The SNN is\ntrained with imitation learning, using a flight dataset of sensory-motor pairs.\nPost-training, the network is deployed on the Crazyflie, issuing control\ncommands from sensor inputs at 500Hz. Furthermore, for the training procedure\nwe augmented training data by flying a controller with additional excitation\nand time-shifting the target data to enhance the predictive capabilities of the\nSNN. On the real drone, the perception-to-control SNN tracks attitude commands\nwith an average error of 3.0 degrees, compared to 2.7 degrees for the regular\nflight stack. We also show the benefits of the proposed learning modifications\nfor reducing the average tracking error and reducing oscillations. Our work\nshows the feasibility of performing neuromorphic end-to-end control, laying the\nbasis for highly energy-efficient and low-latency neuromorphic autopilots.\n","authors":["Stein Stroobants","Christophe de Wagter","Guido C. H. E. De Croon"],"pdf_url":"https://arxiv.org/pdf/2411.13945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18276v2","updated":"2025-03-21T07:52:16Z","published":"2024-11-27T12:11:23Z","title":"GAPartManip: A Large-scale Part-centric Dataset for Material-Agnostic\n  Articulated Object Manipulation","summary":"  Effectively manipulating articulated objects in household scenarios is a\ncrucial step toward achieving general embodied artificial intelligence.\nMainstream research in 3D vision has primarily focused on manipulation through\ndepth perception and pose detection. However, in real-world environments, these\nmethods often face challenges due to imperfect depth perception, such as with\ntransparent lids and reflective handles. Moreover, they generally lack the\ndiversity in part-based interactions required for flexible and adaptable\nmanipulation. To address these challenges, we introduced a large-scale\npart-centric dataset for articulated object manipulation that features both\nphoto-realistic material randomization and detailed annotations of\npart-oriented, scene-level actionable interaction poses. We evaluated the\neffectiveness of our dataset by integrating it with several state-of-the-art\nmethods for depth estimation and interaction pose prediction. Additionally, we\nproposed a novel modular framework that delivers superior and robust\nperformance for generalizable articulated object manipulation. Our extensive\nexperiments demonstrate that our dataset significantly improves the performance\nof depth perception and actionable interaction pose prediction in both\nsimulation and real-world scenarios. More information and demos can be found\nat: https://pku-epic.github.io/GAPartManip/.\n","authors":["Wenbo Cui","Chengyang Zhao","Songlin Wei","Jiazhao Zhang","Haoran Geng","Yaran Chen","Haoran Li","He Wang"],"pdf_url":"https://arxiv.org/pdf/2411.18276v2.pdf","comment":"Accepted by ICRA 2025. Project page:\n  https://pku-epic.github.io/GAPartManip/"},{"id":"http://arxiv.org/abs/2503.16904v1","updated":"2025-03-21T07:12:44Z","published":"2025-03-21T07:12:44Z","title":"Deep Learning for Human Locomotion Analysis in Lower-Limb Exoskeletons:\n  A Comparative Study","summary":"  Wearable robotics for lower-limb assistance have become a pivotal area of\nresearch, aiming to enhance mobility for individuals with physical impairments\nor augment the performance of able-bodied users. Accurate and adaptive control\nsystems are essential to ensure seamless interaction between the wearer and the\nrobotic device, particularly when navigating diverse and dynamic terrains.\nDespite the recent advances in neural networks for time series analysis, no\nattempts have been directed towards the classification of ground conditions,\ncategorized into five classes and subsequently determining the ramp's slope and\nstair's height. In this respect, this paper presents an experimental comparison\nbetween eight deep neural network backbones to predict high-level locomotion\nparameters across diverse terrains.\n  All the models are trained on the publicly available CAMARGO 2021 dataset.\nIMU-only data equally or outperformed IMU+EMG inputs, promoting a\ncost-effective and efficient design. Indeeds, using three IMU sensors, the LSTM\nachieved high terrain classification accuracy (0.94 +- 0.04) and precise ramp\nslope (1.95 +- 0.58{\\deg}) and the CNN-LSTM a stair height (15.65 +- 7.40 mm)\nestimations. As a further contribution, SHAP analysis justified sensor\nreduction without performance loss, ensuring a lightweight setup. The system\noperates with ~2 ms inference time, supporting real-time applications. The code\nis code available at\nhttps://github.com/cosbidev/Human-Locomotion-Identification.\n","authors":["Omar Coser","Christian Tamantini","Matteo Tortora","Leonardo Furia","Rosa Sicilia","Loredana Zollo","Paolo Soda"],"pdf_url":"https://arxiv.org/pdf/2503.16904v1.pdf","comment":"26 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.06315v3","updated":"2025-03-21T07:05:12Z","published":"2024-10-08T19:46:05Z","title":"Incremental Learning for Robot Shared Autonomy","summary":"  Shared autonomy holds promise for improving the usability and accessibility\nof assistive robotic arms, but current methods often rely on costly expert\ndemonstrations and remain static after pretraining, limiting their ability to\nhandle real-world variations. Even with extensive training data, unforeseen\nchallenges--especially those that fundamentally alter task dynamics, such as\nunexpected obstacles or spatial constraints--can cause assistive policies to\nbreak down, leading to ineffective or unreliable assistance. To address this,\nwe propose ILSA, an Incrementally Learned Shared Autonomy framework that\ncontinuously refines its assistive policy through user interactions, adapting\nto real-world challenges beyond the scope of pre-collected data. At the core of\nILSA is a structured fine-tuning mechanism that enables continual improvement\nwith each interaction by effectively integrating limited new interaction data\nwhile preserving prior knowledge, ensuring a balance between adaptation and\ngeneralization. A user study with 20 participants demonstrates ILSA's\neffectiveness, showing faster task completion and improved user experience\ncompared to static alternatives. Code and videos are available at\nhttps://ilsa-robo.github.io/.\n","authors":["Yiran Tao","Guixiu Qiao","Dan Ding","Zackory Erickson"],"pdf_url":"https://arxiv.org/pdf/2410.06315v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08140v2","updated":"2025-03-21T07:00:11Z","published":"2025-03-11T07:59:45Z","title":"HOTFormerLoc: Hierarchical Octree Transformer for Versatile Lidar Place\n  Recognition Across Ground and Aerial Views","summary":"  We present HOTFormerLoc, a novel and versatile Hierarchical Octree-based\nTransFormer, for large-scale 3D place recognition in both ground-to-ground and\nground-to-aerial scenarios across urban and forest environments. We propose an\noctree-based multi-scale attention mechanism that captures spatial and semantic\nfeatures across granularities. To address the variable density of point\ndistributions from spinning lidar, we present cylindrical octree attention\nwindows to reflect the underlying distribution during attention. We introduce\nrelay tokens to enable efficient global-local interactions and multi-scale\nrepresentation learning at reduced computational cost. Our pyramid attentional\npooling then synthesises a robust global descriptor for end-to-end place\nrecognition in challenging environments. In addition, we introduce\nCS-Wild-Places, a novel 3D cross-source dataset featuring point cloud data from\naerial and ground lidar scans captured in dense forests. Point clouds in\nCS-Wild-Places contain representational gaps and distinctive attributes such as\nvarying point densities and noise patterns, making it a challenging benchmark\nfor cross-view localisation in the wild. HOTFormerLoc achieves a top-1 average\nrecall improvement of 5.5% - 11.5% on the CS-Wild-Places benchmark.\nFurthermore, it consistently outperforms SOTA 3D place recognition methods,\nwith an average performance gain of 4.9% on well-established urban and forest\ndatasets. The code and CS-Wild-Places benchmark is available at\nhttps://csiro-robotics.github.io/HOTFormerLoc.\n","authors":["Ethan Griffiths","Maryam Haghighat","Simon Denman","Clinton Fookes","Milad Ramezani"],"pdf_url":"https://arxiv.org/pdf/2503.08140v2.pdf","comment":"16 pages, 13 figures, 10 tables, Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2409.11905v2","updated":"2025-03-21T04:40:24Z","published":"2024-09-18T12:05:30Z","title":"AlignBot: Aligning VLM-powered Customized Task Planning with User\n  Reminders Through Fine-Tuning for Household Robots","summary":"  This paper presents AlignBot, a novel framework designed to optimize\nVLM-powered customized task planning for household robots by effectively\naligning with user reminders. In domestic settings, aligning task planning with\nuser reminders poses significant challenges due to the limited quantity,\ndiversity, and multimodal nature of the reminders. To address these challenges,\nAlignBot employs a fine-tuned LLaVA-7B model, functioning as an adapter for\nGPT-4o. This adapter model internalizes diverse forms of user reminders-such as\npersonalized preferences, corrective guidance, and contextual assistance-into\nstructured instruction-formatted cues that prompt GPT-4o in generating\ncustomized task plans. Additionally, AlignBot integrates a dynamic retrieval\nmechanism that selects task-relevant historical successes as prompts for\nGPT-4o, further enhancing task planning accuracy. To validate the effectiveness\nof AlignBot, experiments are conducted in real-world household environments,\nwhich are constructed within the laboratory to replicate typical household\nsettings. A multimodal dataset with over 1,500 entries derived from volunteer\nreminders is used for training and evaluation. The results demonstrate that\nAlignBot significantly improves customized task planning, outperforming\nexisting LLM- and VLM-powered planners by interpreting and aligning with user\nreminders, achieving 86.8% success rate compared to the vanilla GPT-4o baseline\nat 21.6%, reflecting a 65% improvement and over four times greater\neffectiveness. Supplementary materials are available at:\nhttps://yding25.com/AlignBot/\n","authors":["Zhaxizhuoma Zhaxizhuoma","Pengan Chen","Ziniu Wu","Jiawei Sun","Dong Wang","Peng Zhou","Nieqing Cao","Yan Ding","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2409.11905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16849v1","updated":"2025-03-21T04:40:04Z","published":"2025-03-21T04:40:04Z","title":"Safe On-Orbit Dislodging of Deployable Structures via Robust Adaptive\n  MPC","summary":"  This paper proposes a novel robust adaptive model predictive controller for\non-orbit dislodging. We consider the scenario where a servicer, equipped with a\nrobot arm, must dislodge a client, a time-varying system composed of an\nunderpowered jammed solar panel with a hybrid hinge system on a space station.\nOur approach leverages online set-membership identification to reduce the\nuncertainty to provide robust safety guarantees during dislodging despite\nbounded disturbances while balancing exploration and exploitation effectively\nin the parameter space. The feasibility of the developed robust adaptive MPC\nmethod is also examined through dislodging simulations and hardware experiments\nin zero-gravity and gravity environments, respectively. In addition, the\nadvantages of our method are shown through comparison experiments with several\nstate-of-the-art control schemes for both accuracy of parameter estimation and\ncontrol performance.\n","authors":["Longsen Gao","Claus Danielson","Andrew Kwas","Rafael Fierro"],"pdf_url":"https://arxiv.org/pdf/2503.16849v1.pdf","comment":"This paper has been submitted to IEEE Transactions on Control Systems\n  Technology and is being reviewed"},{"id":"http://arxiv.org/abs/2411.13205v2","updated":"2025-03-21T04:00:22Z","published":"2024-11-20T11:07:37Z","title":"An Integrated Approach to Robotic Object Grasping and Manipulation","summary":"  In response to the growing challenges of manual labor and efficiency in\nwarehouse operations, Amazon has embarked on a significant transformation by\nincorporating robotics to assist with various tasks. While a substantial number\nof robots have been successfully deployed for tasks such as item transportation\nwithin warehouses, the complex process of object picking from shelves remains a\nsignificant challenge. This project addresses the issue by developing an\ninnovative robotic system capable of autonomously fulfilling a simulated order\nby efficiently selecting specific items from shelves. A distinguishing feature\nof the proposed robotic system is its capacity to navigate the challenge of\nuncertain object positions within each bin of the shelf. The system is\nengineered to autonomously adapt its approach, employing strategies that enable\nit to efficiently locate and retrieve the desired items, even in the absence of\npre-established knowledge about their placements.\n","authors":["Owais Ahmed","M Huzaifa","M Areeb","Hamza Ali Khan"],"pdf_url":"https://arxiv.org/pdf/2411.13205v2.pdf","comment":"I am making big changes in the paper and continuing its further\n  development with other instituition"},{"id":"http://arxiv.org/abs/2503.16825v1","updated":"2025-03-21T03:37:08Z","published":"2025-03-21T03:37:08Z","title":"SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion","summary":"  Recently, camera-based solutions have been extensively explored for scene\nsemantic completion (SSC). Despite their success in visible areas, existing\nmethods struggle to capture complete scene semantics due to frequent visual\nocclusions. To address this limitation, this paper presents the first\nsatellite-ground cooperative SSC framework, i.e., SGFormer, exploring the\npotential of satellite-ground image pairs in the SSC task. Specifically, we\npropose a dual-branch architecture that encodes orthogonal satellite and ground\nviews in parallel, unifying them into a common domain. Additionally, we design\na ground-view guidance strategy that corrects satellite image biases during\nfeature encoding, addressing misalignment between satellite and ground views.\nMoreover, we develop an adaptive weighting strategy that balances contributions\nfrom satellite and ground views. Experiments demonstrate that SGFormer\noutperforms the state of the art on SemanticKITTI and SSCBench-KITTI-360\ndatasets. Our code is available on https://github.com/gxytcrc/SGFormer.\n","authors":["Xiyue Guo","Jiarui Hu","Junjie Hu","Hujun Bao","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16806v1","updated":"2025-03-21T02:29:52Z","published":"2025-03-21T02:29:52Z","title":"DyWA: Dynamics-adaptive World Action Model for Generalizable\n  Non-prehensile Manipulation","summary":"  Nonprehensile manipulation is crucial for handling objects that are too thin,\nlarge, or otherwise ungraspable in unstructured environments. While\nconventional planning-based approaches struggle with complex contact modeling,\nlearning-based methods have recently emerged as a promising alternative.\nHowever, existing learning-based approaches face two major limitations: they\nheavily rely on multi-view cameras and precise pose tracking, and they fail to\ngeneralize across varying physical conditions, such as changes in object mass\nand table friction. To address these challenges, we propose the\nDynamics-Adaptive World Action Model (DyWA), a novel framework that enhances\naction learning by jointly predicting future states while adapting to dynamics\nvariations based on historical trajectories. By unifying the modeling of\ngeometry, state, physics, and robot actions, DyWA enables more robust policy\nlearning under partial observability. Compared to baselines, our method\nimproves the success rate by 31.5% using only single-view point cloud\nobservations in the simulation. Furthermore, DyWA achieves an average success\nrate of 68% in real-world experiments, demonstrating its ability to generalize\nacross diverse object geometries, adapt to varying table friction, and\nrobustness in challenging scenarios such as half-filled water bottles and\nslippery surfaces.\n","authors":["Jiangran Lyu","Ziming Li","Xuesong Shi","Chaoyi Xu","Yizhou Wang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16806v1.pdf","comment":"Project Page:https://pku-epic.github.io/DyWA/"},{"id":"http://arxiv.org/abs/2503.16803v1","updated":"2025-03-21T02:26:14Z","published":"2025-03-21T02:26:14Z","title":"BEAC: Imitating Complex Exploration and Task-oriented Behaviors for\n  Invisible Object Nonprehensile Manipulation","summary":"  Applying imitation learning (IL) is challenging to nonprehensile manipulation\ntasks of invisible objects with partial observations, such as excavating buried\nrocks. The demonstrator must make such complex action decisions as exploring to\nfind the object and task-oriented actions to complete the task while estimating\nits hidden state, perhaps causing inconsistent action demonstration and high\ncognitive load problems. For these problems, work in human cognitive science\nsuggests that promoting the use of pre-designed, simple exploration rules for\nthe demonstrator may alleviate the problems of action inconsistency and high\ncognitive load. Therefore, when performing imitation learning from\ndemonstrations using such exploration rules, it is important to accurately\nimitate not only the demonstrator's task-oriented behavior but also his/her\nmode-switching behavior (exploratory or task-oriented behavior) under partial\nobservation. Based on the above considerations, this paper proposes a novel\nimitation learning framework called Belief Exploration-Action Cloning (BEAC),\nwhich has a switching policy structure between a pre-designed exploration\npolicy and a task-oriented action policy trained on the estimated belief states\nbased on past history. In simulation and real robot experiments, we confirmed\nthat our proposed method achieved the best task performance, higher mode and\naction prediction accuracies, while reducing the cognitive load in the\ndemonstration indicated by a user study.\n","authors":["Hirotaka Tahara","Takamitsu Matsubara"],"pdf_url":"https://arxiv.org/pdf/2503.16803v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2409.14908v2","updated":"2025-03-21T01:58:00Z","published":"2024-09-23T11:02:46Z","title":"KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory\n  Systems","summary":"  Embodied AI agents responsible for executing interconnected, long-sequence\nhousehold tasks often face difficulties with in-context memory, leading to\ninefficiencies and errors in task execution. To address this issue, we\nintroduce KARMA, an innovative memory system that integrates long-term and\nshort-term memory modules, enhancing large language models (LLMs) for planning\nin embodied agents through memory-augmented prompting. KARMA distinguishes\nbetween long-term and short-term memory, with long-term memory capturing\ncomprehensive 3D scene graphs as representations of the environment, while\nshort-term memory dynamically records changes in objects' positions and states.\nThis dual-memory structure allows agents to retrieve relevant past scene\nexperiences, thereby improving the accuracy and efficiency of task planning.\nShort-term memory employs strategies for effective and adaptive memory\nreplacement, ensuring the retention of critical information while discarding\nless pertinent data. Compared to state-of-the-art embodied agents enhanced with\nmemory, our memory-augmented embodied AI agent improves success rates by 1.3x\nand 2.3x in Composite Tasks and Complex Tasks within the AI2-THOR simulator,\nrespectively, and enhances task execution efficiency by 3.4x and 62.7x.\nFurthermore, we demonstrate that KARMA's plug-and-play capability allows for\nseamless deployment on real-world robotic systems, such as mobile manipulation\nplatforms.Through this plug-and-play memory system, KARMA significantly\nenhances the ability of embodied agents to generate coherent and contextually\nappropriate plans, making the execution of complex household tasks more\nefficient. The experimental videos from the work can be found at\nhttps://youtu.be/4BT7fnw9ehs. Our code is available at\nhttps://github.com/WZX0Swarm0Robotics/KARMA/tree/master.\n","authors":["Zixuan Wang","Bo Yu","Junzhe Zhao","Wenhao Sun","Sai Hou","Shuai Liang","Xing Hu","Yinhe Han","Yiming Gan"],"pdf_url":"https://arxiv.org/pdf/2409.14908v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04445v3","updated":"2025-03-21T01:45:21Z","published":"2024-12-05T18:57:04Z","title":"Moto: Latent Motion Token as the Bridging Language for Learning Robot\n  Manipulation from Videos","summary":"  Recent developments in Large Language Models pre-trained on extensive corpora\nhave shown significant success in various natural language processing tasks\nwith minimal fine-tuning. This success offers new promise for robotics, which\nhas long been constrained by the high cost of action-labeled data. We ask:\ngiven the abundant video data containing interaction-related knowledge\navailable as a rich \"corpus\", can a similar generative pre-training approach be\neffectively applied to enhance robot learning? The key challenge is to identify\nan effective representation for autoregressive pre-training that benefits robot\nmanipulation tasks. Inspired by the way humans learn new skills through\nobserving dynamic environments, we propose that effective robotic learning\nshould emphasize motion-related knowledge, which is closely tied to low-level\nactions and is hardware-agnostic, facilitating the transfer of learned motions\nto actual robot actions. To this end, we introduce Moto, which converts video\ncontent into latent Motion Token sequences by a Latent Motion Tokenizer,\nlearning a bridging \"language\" of motion from videos in an unsupervised manner.\nWe pre-train Moto-GPT through motion token autoregression, enabling it to\ncapture diverse visual motion knowledge. After pre-training, Moto-GPT\ndemonstrates the promising ability to produce semantically interpretable motion\ntokens, predict plausible motion trajectories, and assess trajectory\nrationality through output likelihood. To transfer learned motion priors to\nreal robot actions, we implement a co-fine-tuning strategy that seamlessly\nbridges latent motion token prediction and real robot control. Extensive\nexperiments show that the fine-tuned Moto-GPT exhibits superior robustness and\nefficiency on robot manipulation benchmarks, underscoring its effectiveness in\ntransferring knowledge from video data to downstream visual manipulation tasks.\n","authors":["Yi Chen","Yuying Ge","Weiliang Tang","Yizhuo Li","Yixiao Ge","Mingyu Ding","Ying Shan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2412.04445v3.pdf","comment":"Project released at: https://chenyi99.github.io/moto/ Code released\n  at: https://github.com/TencentARC/Moto Update: Added content related to\n  real-world robot experiments and learning from human videos; Modified author\n  information"},{"id":"http://arxiv.org/abs/2410.03035v3","updated":"2025-03-21T01:34:48Z","published":"2024-10-03T22:41:47Z","title":"SPINE: Online Semantic Planning for Missions with Incomplete Natural\n  Language Specifications in Unstructured Environments","summary":"  As robots become increasingly capable, users will want to describe high-level\nmissions and have robots infer the relevant details. Because pre-built maps are\ndifficult to obtain in many realistic settings, accomplishing such missions\nwill require the robot to map and plan online. While many semantic planning\nmethods operate online, they are typically designed for well specified missions\nsuch as object search or exploration. Recently, Large Language Models (LLMs)\nhave demonstrated powerful contextual reasoning abilities over a range of\nrobotic tasks described in natural language. However, existing LLM-enabled\nplanners typically do not consider online planning or complex missions; rather,\nrelevant subtasks and semantics are provided by a pre-built map or a user. We\naddress these limitations via SPINE, an online planner for missions with\nincomplete mission specifications provided in natural language. The planner\nuses an LLM to reason about subtasks implied by the mission specification and\nthen realizes these subtasks in a receding horizon framework. Tasks are\nautomatically validated for safety and refined online with new map\nobservations. We evaluate SPINE in simulation and real-world settings with\nmissions that require multiple steps of semantic reasoning and exploration in\ncluttered outdoor environments of over 20,000m$^2$. Compared to baselines that\nuse existing LLM-enabled planning approaches, our method is over twice as\nefficient in terms of time and distance, requires less user interactions, and\ndoes not require a full map. Additional resources are provided at\nhttps://zacravichandran.github.io/SPINE.\n","authors":["Zachary Ravichandran","Varun Murali","Mariliza Tzes","George J. Pappas","Vijay Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.03035v3.pdf","comment":"Accepted to the International Conference on Robotics and Automation\n  (ICRA) 2025"},{"id":"http://arxiv.org/abs/2502.07472v2","updated":"2025-03-21T01:17:29Z","published":"2025-02-11T11:26:24Z","title":"Robotic In-Hand Manipulation for Large-Range Precise Object Movement:\n  The RGMC Champion Solution","summary":"  In-hand manipulation using multiple dexterous fingers is a critical robotic\nskill that can reduce the reliance on large arm motions, thereby saving space\nand energy. This letter focuses on in-grasp object movement, which refers to\nmanipulating an object to a desired pose through only finger motions within a\nstable grasp. The key challenge lies in simultaneously achieving high precision\nand large-range movements while maintaining a constant stable grasp. To address\nthis problem, we propose a simple and practical approach based on kinematic\ntrajectory optimization with no need for pretraining or object geometries,\nwhich can be easily applied to novel objects in real-world scenarios. Adopting\nthis approach, we won the championship for the in-hand manipulation track at\nthe 9th Robotic Grasping and Manipulation Competition (RGMC) held at ICRA 2024.\nImplementation details, discussion, and further quantitative experimental\nresults are presented in this letter, which aims to comprehensively evaluate\nour approach and share our key takeaways from the competition. Supplementary\nmaterials including video and code are available at\nhttps://rgmc-xl-team.github.io/ingrasp_manipulation .\n","authors":["Mingrui Yu","Yongpeng Jiang","Chen Chen","Yongyi Jia","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2502.07472v2.pdf","comment":"Accepted by RA-L. Project website:\n  https://rgmc-xl-team.github.io/ingrasp_manipulation"},{"id":"http://arxiv.org/abs/2503.16778v1","updated":"2025-03-21T01:16:27Z","published":"2025-03-21T01:16:27Z","title":"Displacement-Actuated Continuum Robots: A Joint Space Abstraction","summary":"  The displacement-actuated continuum robot as an abstraction has been shown as\na key abstraction to significantly simplify and improve approaches due to its\nrelation to the Clarke transform. To highlight further potentials, we revisit\nand extend this abstraction that features an increasingly popular length\nextension and an underutilized twisting. For each extension, the corresponding\nmapping from the joint values to the local coordinates of the manifold embedded\nin the joint spaces is provided. Each mapping is characterized by its\ncompactness and linearity.\n","authors":["Reinhard M. Grassmann","Jessica Burgner-Kahrs"],"pdf_url":"https://arxiv.org/pdf/2503.16778v1.pdf","comment":"11 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.17566v1","updated":"2025-03-21T22:54:41Z","published":"2025-03-21T22:54:41Z","title":"LLM-Drone: Aerial Additive Manufacturing with Drones Planned Using Large\n  Language Models","summary":"  Additive manufacturing (AM) has transformed the production landscape by\nenabling the precision creation of complex geometries. However, AM faces\nlimitations when applied to challenging environments, such as elevated surfaces\nand remote locations. Aerial additive manufacturing, facilitated by drones,\npresents a solution to these challenges. However, despite advances in methods\nfor the planning, control, and localization of drones, the accuracy of these\nmethods is insufficient to run traditional feedforward extrusion-based additive\nmanufacturing processes (such as Fused Deposition Manufacturing). Recently, the\nemergence of LLMs has revolutionized various fields by introducing advanced\nsemantic reasoning and real-time planning capabilities. This paper proposes the\nintegration of LLMs with aerial additive manufacturing to assist with the\nplanning and execution of construction tasks, granting greater flexibility and\nenabling a feed-back based design and construction system. Using the semantic\nunderstanding and adaptability of LLMs, we can overcome the limitations of\ndrone based systems by dynamically generating and adapting building plans on\nsite, ensuring efficient and accurate construction even in constrained\nenvironments. Our system is able to design and build structures given only a\nsemantic prompt and has shown success in understanding the spatial environment\ndespite tight planning constraints. Our method's feedback system enables\nreplanning using the LLM if the manufacturing process encounters unforeseen\nerrors, without requiring complicated heuristics or evaluation functions.\nCombining the semantic planning with automatic error correction, our system\nachieved a 90% build accuracy, converting simple text prompts to build\nstructures.\n","authors":["Akshay Raman","Chad Merrill","Abraham George","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2503.17566v1.pdf","comment":"26 Pages, 6 Figures"},{"id":"http://arxiv.org/abs/2410.06380v2","updated":"2025-03-21T20:59:38Z","published":"2024-10-08T21:26:22Z","title":"Adver-City: Open-Source Multi-Modal Dataset for Collaborative Perception\n  Under Adverse Weather Conditions","summary":"  Adverse weather conditions pose a significant challenge to the widespread\nadoption of Autonomous Vehicles (AVs) by impacting sensors like LiDARs and\ncameras. Even though Collaborative Perception (CP) improves AV perception in\ndifficult conditions, existing CP datasets lack adverse weather conditions. To\naddress this, we introduce Adver-City, the first open-source synthetic CP\ndataset focused on adverse weather conditions. Simulated in CARLA with OpenCDA,\nit contains over 24 thousand frames, over 890 thousand annotations, and 110\nunique scenarios across six different weather conditions: clear weather, soft\nrain, heavy rain, fog, foggy heavy rain and, for the first time in a synthetic\nCP dataset, glare. It has six object categories including pedestrians and\ncyclists, and uses data from vehicles and roadside units featuring LiDARs, RGB\nand semantic segmentation cameras, GNSS, and IMUs. Its scenarios, based on real\ncrash reports, depict the most relevant road configurations for adverse weather\nand poor visibility conditions, varying in object density, with both dense and\nsparse scenes, allowing for novel testing conditions of CP models. Benchmarks\nrun on the dataset show that weather conditions created challenging conditions\nfor perception models, with CoBEVT scoring 58.30/52.44/38.90 (AP@30/50/70). The\ndataset, code and documentation are available at\nhttps://labs.cs.queensu.ca/quarrg/datasets/adver-city/.\n","authors":["Mateus Karvat","Sidney Givigi"],"pdf_url":"https://arxiv.org/pdf/2410.06380v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2501.08469v2","updated":"2025-03-21T20:38:25Z","published":"2025-01-14T22:30:38Z","title":"Electrostatic Clutches Enable Simultaneous Mechanical Multiplexing","summary":"  Actuating robotic systems with multiple degrees of freedom (DoF)\ntraditionally requires numerous motors, leading to increased size, weight,\ncost, and power consumption. Mechanical multiplexing offers a solution by\nenabling a single actuator to control multiple DoF. However, existing\nmultiplexers have either been limited to electrically controlled time-based\nmultiplexing that control one DoF at a time or have relied on mechanical\nswitching to control multiple DoF simultaneously. There is a strong need for a\nsystem that can perform electrically controlled multiplexing for both\ntime-based and simultaneous control of multiple DoF. This study introduces a\nnovel electrostatic capstan clutch-based mechanical multiplexer that enables\nhigh-force, single-motor control of multiple DoF. Here, we show that our system\nachieves both single-input-single-output (SISO) and single-input-multipleoutput\n(SIMO) actuation, allowing bidirectional control and position holding with\nminimal power consumption. Each output can actuate a 22.24 N load, limited by\nclutch performance, up to 5 cm. The number of outputs and actuation length is\ncurrently limited by the length of the drive shaft. We demonstrate the\nintegration of our system into a 4-DoF commercial robotic hand using a single\nmotor. These findings show that electrostatic clutchbased multiplexing provides\na scalable and energy-efficient design solution for high-DoF robotic platforms,\nopening new possibilities for lightweight and power-efficient actuation in\nrobotics.\n","authors":["Timothy E. Amish","Jeffrey T. Auletta","Chad C. Kessens","Joshua R. Smith","Jeffrey I. Lipton"],"pdf_url":"https://arxiv.org/pdf/2501.08469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17501v1","updated":"2025-03-21T19:23:31Z","published":"2025-03-21T19:23:31Z","title":"Shear-based Grasp Control for Multi-fingered Underactuated Tactile\n  Robotic Hands","summary":"  This paper presents a shear-based control scheme for grasping and\nmanipulating delicate objects with a Pisa/IIT anthropomorphic SoftHand equipped\nwith soft biomimetic tactile sensors on all five fingertips. These `microTac'\ntactile sensors are miniature versions of the TacTip vision-based tactile\nsensor, and can extract precise contact geometry and force information at each\nfingertip for use as feedback into a controller to modulate the grasp while a\nheld object is manipulated. Using a parallel processing pipeline, we\nasynchronously capture tactile images and predict contact pose and force from\nmultiple tactile sensors. Consistent pose and force models across all sensors\nare developed using supervised deep learning with transfer learning techniques.\nWe then develop a grasp control framework that uses contact force feedback from\nall fingertip sensors simultaneously, allowing the hand to safely handle\ndelicate objects even under external disturbances. This control framework is\napplied to several grasp-manipulation experiments: first, retaining a flexible\ncup in a grasp without crushing it under changes in object weight; second, a\npouring task where the center of mass of the cup changes dynamically; and\nthird, a tactile-driven leader-follower task where a human guides a held\nobject. These manipulation tasks demonstrate more human-like dexterity with\nunderactuated robotic hands by using fast reflexive control from tactile\nsensing.\n","authors":["Christopher J. Ford","Haoran Li","Manuel G. Catalano","Matteo Bianchi","Efi Psomopoulou","Nathan F. Lepora"],"pdf_url":"https://arxiv.org/pdf/2503.17501v1.pdf","comment":"16 pages. 10 figures. Accepted in IEEE Transactions on Robotics,\n  Special Section on Tactile Robotics"},{"id":"http://arxiv.org/abs/2503.17491v1","updated":"2025-03-21T19:00:30Z","published":"2025-03-21T19:00:30Z","title":"Splat-LOAM: Gaussian Splatting LiDAR Odometry and Mapping","summary":"  LiDARs provide accurate geometric measurements, making them valuable for\nego-motion estimation and reconstruction tasks. Although its success, managing\nan accurate and lightweight representation of the environment still poses\nchallenges. Both classic and NeRF-based solutions have to trade off accuracy\nover memory and processing times. In this work, we build on recent advancements\nin Gaussian Splatting methods to develop a novel LiDAR odometry and mapping\npipeline that exclusively relies on Gaussian primitives for its scene\nrepresentation. Leveraging spherical projection, we drive the refinement of the\nprimitives uniquely from LiDAR measurements. Experiments show that our approach\nmatches the current registration performance, while achieving SOTA results for\nmapping tasks with minimal GPU requirements. This efficiency makes it a strong\ncandidate for further exploration and potential adoption in real-time robotics\nestimation tasks.\n","authors":["Emanuele Giacomini","Luca Di Giammarino","Lorenzo De Rebotti","Giorgio Grisetti","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2503.17491v1.pdf","comment":"submitted to ICCV 2025"},{"id":"http://arxiv.org/abs/2503.03071v2","updated":"2025-03-21T18:09:27Z","published":"2025-03-05T00:21:23Z","title":"Physically-Feasible Reactive Synthesis for Terrain-Adaptive Locomotion\n  via Trajectory Optimization and Symbolic Repair","summary":"  We propose an integrated planning framework for quadrupedal locomotion over\ndynamically changing, unforeseen terrains. Existing approaches either rely on\nheuristics for instantaneous foothold selection--compromising safety and\nversatility--or solve expensive trajectory optimization problems with complex\nterrain features and long time horizons. In contrast, our framework leverages\nreactive synthesis to generate correct-by-construction controllers at the\nsymbolic level, and mixed-integer convex programming (MICP) for dynamic and\nphysically feasible footstep planning for each symbolic transition. We use a\nhigh-level manager to reduce the large state space in synthesis by\nincorporating local environment information, improving synthesis scalability.\nTo handle specifications that cannot be met due to dynamic infeasibility, and\nto minimize costly MICP solves, we leverage a symbolic repair process to\ngenerate only necessary symbolic transitions. During online execution,\nre-running the MICP with real-world terrain data, along with runtime symbolic\nrepair, bridges the gap between offline synthesis and online execution. We\ndemonstrate, in simulation, our framework's capabilities to discover missing\nlocomotion skills and react promptly in safety-critical environments, such as\nscattered stepping stones and rebars.\n","authors":["Ziyi Zhou","Qian Meng","Hadas Kress-Gazit","Ye Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.03071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05547v2","updated":"2025-03-21T18:00:42Z","published":"2024-10-07T23:19:52Z","title":"Understanding and Imitating Human-Robot Motion with Restricted Visual\n  Fields","summary":"  When working around humans, it is important to model their perception\nlimitations in order to predict their behavior more accurately. In this work,\nwe consider agents with a limited field of view, viewing range, and ability to\nmiss objects within the viewing range (e.g., transparency). By considering the\nobservation model independently from the motion policy, we can better predict\nthe agent's behavior by considering these limitations and approximating them.\nWe perform a user study where human operators navigate a cluttered scene while\nscanning the region for obstacles with a limited field of view and range. Using\nimitation learning, we show that a robot can adopt a human's strategy for\nobserving an environment with limitations on observation and navigate with\nminimal collision with dynamic and static obstacles. We also show that this\nlearned model helps it successfully navigate a physical hardware vehicle in\nreal-time.\n","authors":["Maulik Bhatt","HongHao Zhen","Monroe Kennedy III","Negar Mehr"],"pdf_url":"https://arxiv.org/pdf/2410.05547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17432v1","updated":"2025-03-21T13:13:17Z","published":"2025-03-21T13:13:17Z","title":"TamedPUMA: safe and stable imitation learning with geometric fabrics","summary":"  Using the language of dynamical systems, Imitation learning (IL) provides an\nintuitive and effective way of teaching stable task-space motions to robots\nwith goal convergence. Yet, IL techniques are affected by serious limitations\nwhen it comes to ensuring safety and fulfillment of physical constraints. With\nthis work, we solve this challenge via TamedPUMA, an IL algorithm augmented\nwith a recent development in motion generation called geometric fabrics. As\nboth the IL policy and geometric fabrics describe motions as artificial\nsecond-order dynamical systems, we propose two variations where IL provides a\nnavigation policy for geometric fabrics. The result is a stable imitation\nlearning strategy within which we can seamlessly blend geometrical constraints\nlike collision avoidance and joint limits. Beyond providing a theoretical\nanalysis, we demonstrate TamedPUMA with simulated and real-world tasks,\nincluding a 7-DoF manipulator.\n","authors":["Saray Bakker","Rodrigo Pérez-Dattari","Cosimo Della Santina","Wendelin Böhmer","Javier Alonso-Mora"],"pdf_url":"https://arxiv.org/pdf/2503.17432v1.pdf","comment":"14 pages (10+4), 1+3*5 figures, 1 table, preprint version of accepted\n  paper at L4DC 2025"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.04496v2","updated":"2025-03-21T17:59:54Z","published":"2023-10-06T18:00:02Z","title":"URLOST: Unsupervised Representation Learning without Stationarity or\n  Topology","summary":"  Unsupervised representation learning has seen tremendous progress. However,\nit is constrained by its reliance on domain specific stationarity and topology,\na limitation not found in biological intelligence systems. For instance, unlike\ncomputer vision, human vision can process visual signals sampled from highly\nirregular and non-stationary sensors. We introduce a novel framework that\nlearns from high-dimensional data without prior knowledge of stationarity and\ntopology. Our model, abbreviated as URLOST, combines a learnable\nself-organizing layer, spectral clustering, and a masked autoencoder (MAE). We\nevaluate its effectiveness on three diverse data modalities including simulated\nbiological vision data, neural recordings from the primary visual cortex, and\ngene expressions. Compared to state-of-the-art unsupervised learning methods\nlike SimCLR and MAE, our model excels at learning meaningful representations\nacross diverse modalities without knowing their stationarity or topology. It\nalso outperforms other methods that are not dependent on these factors, setting\na new benchmark in the field. We position this work as a step toward\nunsupervised learning methods capable of generalizing across diverse\nhigh-dimensional data modalities.\n","authors":["Zeyu Yun","Juexiao Zhang","Yann LeCun","Yubei Chen"],"pdf_url":"https://arxiv.org/pdf/2310.04496v2.pdf","comment":"Accepted by ICLR 2025; Code will be available at this\n  https://github.com/zeyuyun1/URLOST"},{"id":"http://arxiv.org/abs/2503.17359v1","updated":"2025-03-21T17:59:22Z","published":"2025-03-21T17:59:22Z","title":"Position: Interactive Generative Video as Next-Generation Game Engine","summary":"  Modern game development faces significant challenges in creativity and cost\ndue to predetermined content in traditional game engines. Recent breakthroughs\nin video generation models, capable of synthesizing realistic and interactive\nvirtual environments, present an opportunity to revolutionize game creation. In\nthis position paper, we propose Interactive Generative Video (IGV) as the\nfoundation for Generative Game Engines (GGE), enabling unlimited novel content\ngeneration in next-generation gaming. GGE leverages IGV's unique strengths in\nunlimited high-quality content synthesis, physics-aware world modeling,\nuser-controlled interactivity, long-term memory capabilities, and causal\nreasoning. We present a comprehensive framework detailing GGE's core modules\nand a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work\ncharts a new course for game development in the AI era, envisioning a future\nwhere AI-powered generative systems fundamentally reshape how games are created\nand experienced.\n","authors":["Jiwen Yu","Yiran Qin","Haoxuan Che","Quande Liu","Xintao Wang","Pengfei Wan","Di Zhang","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2503.17359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17358v1","updated":"2025-03-21T17:58:56Z","published":"2025-03-21T17:58:56Z","title":"Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred\n  Image","summary":"  In many robotics and VR/AR applications, fast camera motions cause a high\nlevel of motion blur, causing existing camera pose estimation methods to fail.\nIn this work, we propose a novel framework that leverages motion blur as a rich\ncue for motion estimation rather than treating it as an unwanted artifact. Our\napproach works by predicting a dense motion flow field and a monocular depth\nmap directly from a single motion-blurred image. We then recover the\ninstantaneous camera velocity by solving a linear least squares problem under\nthe small motion assumption. In essence, our method produces an IMU-like\nmeasurement that robustly captures fast and aggressive camera movements. To\ntrain our model, we construct a large-scale dataset with realistic synthetic\nmotion blur derived from ScanNet++v2 and further refine our model by training\nend-to-end on real data using our fully differentiable pipeline. Extensive\nevaluations on real-world benchmarks demonstrate that our method achieves\nstate-of-the-art angular and translational velocity estimates, outperforming\ncurrent methods like MASt3R and COLMAP.\n","authors":["Jerred Chen","Ronald Clark"],"pdf_url":"https://arxiv.org/pdf/2503.17358v1.pdf","comment":"Project page: https://jerredchen.github.io/image-as-imu/"},{"id":"http://arxiv.org/abs/2410.16646v2","updated":"2025-03-21T17:53:45Z","published":"2024-10-22T02:45:46Z","title":"TopoDiffusionNet: A Topology-aware Diffusion Model","summary":"  Diffusion models excel at creating visually impressive images but often\nstruggle to generate images with a specified topology. The Betti number, which\nrepresents the number of structures in an image, is a fundamental measure in\ntopology. Yet, diffusion models fail to satisfy even this basic constraint.\nThis limitation restricts their utility in applications requiring exact\ncontrol, like robotics and environmental modeling. To address this, we propose\nTopoDiffusionNet (TDN), a novel approach that enforces diffusion models to\nmaintain the desired topology. We leverage tools from topological data\nanalysis, particularly persistent homology, to extract the topological\nstructures within an image. We then design a topology-based objective function\nto guide the denoising process, preserving intended structures while\nsuppressing noisy ones. Our experiments across four datasets demonstrate\nsignificant improvements in topological accuracy. TDN is the first to integrate\ntopology with diffusion models, opening new avenues of research in this area.\nCode available at https://github.com/Saumya-Gupta-26/TopoDiffusionNet\n","authors":["Saumya Gupta","Dimitris Samaras","Chao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16646v2.pdf","comment":"Accepted to ICLR 2025 (Poster)"},{"id":"http://arxiv.org/abs/2503.17352v1","updated":"2025-03-21T17:52:43Z","published":"2025-03-21T17:52:43Z","title":"OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement","summary":"  Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.\n","authors":["Yihe Deng","Hritik Bansal","Fan Yin","Nanyun Peng","Wei Wang","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2503.17352v1.pdf","comment":"23 pages, 11 figures, 8 tables"},{"id":"http://arxiv.org/abs/2503.17351v1","updated":"2025-03-21T17:52:33Z","published":"2025-03-21T17:52:33Z","title":"Time-Series U-Net with Recurrence for Noise-Robust Imaging\n  Photoplethysmography","summary":"  Remote estimation of vital signs enables health monitoring for situations in\nwhich contact-based devices are either not available, too intrusive, or too\nexpensive. In this paper, we present a modular, interpretable pipeline for\npulse signal estimation from video of the face that achieves state-of-the-art\nresults on publicly available datasets.Our imaging photoplethysmography (iPPG)\nsystem consists of three modules: face and landmark detection, time-series\nextraction, and pulse signal/pulse rate estimation. Unlike many deep learning\nmethods that make use of a single black-box model that maps directly from input\nvideo to output signal or heart rate, our modular approach enables each of the\nthree parts of the pipeline to be interpreted individually. The pulse signal\nestimation module, which we call TURNIP (Time-Series U-Net with Recurrence for\nNoise-Robust Imaging Photoplethysmography), allows the system to faithfully\nreconstruct the underlying pulse signal waveform and uses it to measure heart\nrate and pulse rate variability metrics, even in the presence of motion. When\nparts of the face are occluded due to extreme head poses, our system explicitly\ndetects such \"self-occluded\" regions and maintains estimation robustness\ndespite the missing information. Our algorithm provides reliable heart rate\nestimates without the need for specialized sensors or contact with the skin,\noutperforming previous iPPG methods on both color (RGB) and near-infrared (NIR)\ndatasets.\n","authors":["Vineet R. Shenoy","Shaoju Wu","Armand Comas","Tim K. Marks","Suhas Lohit","Hassan Mansour"],"pdf_url":"https://arxiv.org/pdf/2503.17351v1.pdf","comment":"14 Pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.17350v1","updated":"2025-03-21T17:52:05Z","published":"2025-03-21T17:52:05Z","title":"Decouple and Track: Benchmarking and Improving Video Diffusion\n  Transformers for Motion Transfer","summary":"  The motion transfer task involves transferring motion from a source video to\nnewly generated videos, requiring the model to decouple motion from appearance.\nPrevious diffusion-based methods primarily rely on separate spatial and\ntemporal attention mechanisms within 3D U-Net. In contrast, state-of-the-art\nvideo Diffusion Transformers (DiT) models use 3D full attention, which does not\nexplicitly separate temporal and spatial information. Thus, the interaction\nbetween spatial and temporal dimensions makes decoupling motion and appearance\nmore challenging for DiT models. In this paper, we propose DeT, a method that\nadapts DiT models to improve motion transfer ability. Our approach introduces a\nsimple yet effective temporal kernel to smooth DiT features along the temporal\ndimension, facilitating the decoupling of foreground motion from background\nappearance. Meanwhile, the temporal kernel effectively captures temporal\nvariations in DiT features, which are closely related to motion. Moreover, we\nintroduce explicit supervision along dense trajectories in the latent feature\nspace to further enhance motion consistency. Additionally, we present MTBench,\na general and challenging benchmark for motion transfer. We also introduce a\nhybrid motion fidelity metric that considers both the global and local motion\nsimilarity. Therefore, our work provides a more comprehensive evaluation than\nprevious works. Extensive experiments on MTBench demonstrate that DeT achieves\nthe best trade-off between motion fidelity and edit fidelity.\n","authors":["Qingyu Shi","Jianzong Wu","Jinbin Bai","Jiangning Zhang","Lu Qi","Xiangtai Li","Yunhai Tong"],"pdf_url":"https://arxiv.org/pdf/2503.17350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17349v1","updated":"2025-03-21T17:51:14Z","published":"2025-03-21T17:51:14Z","title":"Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language\n  Models","summary":"  Vision-Language Models (VLMs) excel at identifying and describing objects but\nstruggle with spatial reasoning such as accurately understanding the relative\npositions of objects. Inspired by the dual-pathway (ventral-dorsal) model of\nhuman vision, we investigate why VLMs fail spatial tasks despite strong object\nrecognition capabilities. Our interpretability-driven analysis reveals a\ncritical underlying cause: vision embeddings in VLMs are treated primarily as\nsemantic ``bag-of-tokens,\" overshadowing subtle yet crucial positional cues due\nto their disproportionately large embedding norms. We validate this insight\nthrough extensive diagnostic experiments, demonstrating minimal performance\nimpact when token orders or fine-grained spatial details are removed. Guided by\nthese findings, we propose simple, interpretable interventions, including\nnormalizing vision embedding norms and extracting mid-layer spatially rich\nfeatures, to restore spatial awareness. Empirical results on both our synthetic\ndata and standard benchmarks demonstrate improved spatial reasoning\ncapabilities, highlighting the value of interpretability-informed design\nchoices. Our study not only uncovers fundamental limitations in current VLM\narchitectures but also provides actionable insights for enhancing structured\nperception of visual scenes.\n","authors":["Jianing Qi","Jiawei Liu","Hao Tang","Zhigang Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.17349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13693v2","updated":"2025-03-21T17:50:11Z","published":"2025-03-17T20:06:48Z","title":"Adapting to the Unknown: Training-Free Audio-Visual Event Perception\n  with Dynamic Thresholds","summary":"  In the domain of audio-visual event perception, which focuses on the temporal\nlocalization and classification of events across distinct modalities (audio and\nvisual), existing approaches are constrained by the vocabulary available in\ntheir training data. This limitation significantly impedes their capacity to\ngeneralize to novel, unseen event categories. Furthermore, the annotation\nprocess for this task is labor-intensive, requiring extensive manual labeling\nacross modalities and temporal segments, limiting the scalability of current\nmethods. Current state-of-the-art models ignore the shifts in event\ndistributions over time, reducing their ability to adjust to changing video\ndynamics. Additionally, previous methods rely on late fusion to combine audio\nand visual information. While straightforward, this approach results in a\nsignificant loss of multimodal interactions. To address these challenges, we\npropose Audio-Visual Adaptive Video Analysis ($\\text{AV}^2\\text{A}$), a\nmodel-agnostic approach that requires no further training and integrates a\nscore-level fusion technique to retain richer multimodal interactions.\n$\\text{AV}^2\\text{A}$ also includes a within-video label shift algorithm,\nleveraging input video data and predictions from prior frames to dynamically\nadjust event distributions for subsequent frames. Moreover, we present the\nfirst training-free, open-vocabulary baseline for audio-visual event\nperception, demonstrating that $\\text{AV}^2\\text{A}$ achieves substantial\nimprovements over naive training-free baselines. We demonstrate the\neffectiveness of $\\text{AV}^2\\text{A}$ on both zero-shot and weakly-supervised\nstate-of-the-art methods, achieving notable improvements in performance metrics\nover existing approaches.\n","authors":["Eitan Shaar","Ariel Shaulov","Gal Chechik","Lior Wolf"],"pdf_url":"https://arxiv.org/pdf/2503.13693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17347v1","updated":"2025-03-21T17:48:14Z","published":"2025-03-21T17:48:14Z","title":"Dereflection Any Image with Diffusion Priors and Diversified Data","summary":"  Reflection removal of a single image remains a highly challenging task due to\nthe complex entanglement between target scenes and unwanted reflections.\nDespite significant progress, existing methods are hindered by the scarcity of\nhigh-quality, diverse data and insufficient restoration priors, resulting in\nlimited generalization across various real-world scenarios. In this paper, we\npropose Dereflection Any Image, a comprehensive solution with an efficient data\npreparation pipeline and a generalizable model for robust reflection removal.\nFirst, we introduce a dataset named Diverse Reflection Removal (DRR) created by\nrandomly rotating reflective mediums in target scenes, enabling variation of\nreflection angles and intensities, and setting a new benchmark in scale,\nquality, and diversity. Second, we propose a diffusion-based framework with\none-step diffusion for deterministic outputs and fast inference. To ensure\nstable learning, we design a three-stage progressive training strategy,\nincluding reflection-invariant finetuning to encourage consistent outputs\nacross varying reflection patterns that characterize our dataset. Extensive\nexperiments show that our method achieves SOTA performance on both common\nbenchmarks and challenging in-the-wild images, showing superior generalization\nacross diverse real-world scenes.\n","authors":["Jichen Hu","Chen Yang","Zanwei Zhou","Jiemin Fang","Xiaokang Yang","Qi Tian","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2503.17347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17340v1","updated":"2025-03-21T17:42:50Z","published":"2025-03-21T17:42:50Z","title":"Align Your Rhythm: Generating Highly Aligned Dance Poses with\n  Gating-Enhanced Rhythm-Aware Feature Representation","summary":"  Automatically generating natural, diverse and rhythmic human dance movements\ndriven by music is vital for virtual reality and film industries. However,\ngenerating dance that naturally follows music remains a challenge, as existing\nmethods lack proper beat alignment and exhibit unnatural motion dynamics. In\nthis paper, we propose Danceba, a novel framework that leverages gating\nmechanism to enhance rhythm-aware feature representation for music-driven dance\ngeneration, which achieves highly aligned dance poses with enhanced rhythmic\nsensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to\nprecisely extract rhythmic information from musical phase data, capitalizing on\nthe intrinsic periodicity and temporal structures of music. Additionally, we\npropose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic\nfeatures, ensuring that dance movements closely follow the musical rhythm. We\nalso introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately\nmodel upper and lower body motions along with musical features, thereby\nimproving the naturalness and diversity of generated dance movements. Extensive\nexperiments confirm that Danceba outperforms state-of-the-art methods,\nachieving significantly better rhythmic alignment and motion diversity. Project\npage: https://danceba.github.io/ .\n","authors":["Congyi Fan","Jian Guan","Xuanjia Zhao","Dongli Xu","Youtian Lin","Tong Ye","Pengming Feng","Haiwei Pan"],"pdf_url":"https://arxiv.org/pdf/2503.17340v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.17331v1","updated":"2025-03-21T17:32:01Z","published":"2025-03-21T17:32:01Z","title":"A Topological Data Analysis Framework for Quantifying Necrosis in\n  Glioblastomas","summary":"  In this paper, we introduce a shape descriptor that we call \"interior\nfunction\". This is a Topological Data Analysis (TDA) based descriptor that\nrefines previous descriptors for image analysis. Using this concept, we define\nsubcomplex lacunarity, a new index that quantifies geometric characteristics of\nnecrosis in tumors such as conglomeration. Building on this framework, we\npropose a set of indices to analyze necrotic morphology and construct a diagram\nthat captures the distinct structural and geometric properties of necrotic\nregions in tumors. We present an application of this framework in the study of\nMRIs of Glioblastomas (GB). Using cluster analysis, we identify four distinct\nsubtypes of Glioblastomas that reflect geometric properties of necrotic\nregions.\n","authors":["Francisco Tellez","Enrique Torres-Giese"],"pdf_url":"https://arxiv.org/pdf/2503.17331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16346v2","updated":"2025-03-21T17:22:28Z","published":"2024-12-20T21:13:11Z","title":"SOUS VIDE: Cooking Visual Drone Navigation Policies in a Gaussian\n  Splatting Vacuum","summary":"  We propose a new simulator, training approach, and policy architecture,\ncollectively called SOUS VIDE, for end-to-end visual drone navigation. Our\ntrained policies exhibit zero-shot sim-to-real transfer with robust real-world\nperformance using only onboard perception and computation. Our simulator,\ncalled FiGS, couples a computationally simple drone dynamics model with a high\nvisual fidelity Gaussian Splatting scene reconstruction. FiGS can quickly\nsimulate drone flights producing photorealistic images at up to 130 fps. We use\nFiGS to collect 100k-300k image/state-action pairs from an expert MPC with\nprivileged state and dynamics information, randomized over dynamics parameters\nand spatial disturbances. We then distill this expert MPC into an end-to-end\nvisuomotor policy with a lightweight neural architecture, called SV-Net. SV-Net\nprocesses color image, optical flow and IMU data streams into low-level thrust\nand body rate commands at 20 Hz onboard a drone. Crucially, SV-Net includes a\nlearned module for low-level control that adapts at runtime to variations in\ndrone dynamics. In a campaign of 105 hardware experiments, we show SOUS VIDE\npolicies to be robust to 30% mass variations, 40 m/s wind gusts, 60% changes in\nambient brightness, shifting or removing objects from the scene, and people\nmoving aggressively through the drone's visual field. Code, data, and\nexperiment videos can be found on our project page:\nhttps://stanfordmsl.github.io/SousVide/.\n","authors":["JunEn Low","Maximilian Adang","Javier Yu","Keiko Nagami","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2412.16346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17316v1","updated":"2025-03-21T17:12:30Z","published":"2025-03-21T17:12:30Z","title":"Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene\n  Priors","summary":"  We present Pow3r, a novel large 3D vision regression model that is highly\nversatile in the input modalities it accepts. Unlike previous feed-forward\nmodels that lack any mechanism to exploit known camera or scene priors at test\ntime, Pow3r incorporates any combination of auxiliary information such as\nintrinsics, relative pose, dense or sparse depth, alongside input images,\nwithin a single network. Building upon the recent DUSt3R paradigm, a\ntransformer-based architecture that leverages powerful pre-training, our\nlightweight and versatile conditioning acts as additional guidance for the\nnetwork to predict more accurate estimates when auxiliary information is\navailable. During training we feed the model with random subsets of modalities\nat each iteration, which enables the model to operate under different levels of\nknown priors at test time. This in turn opens up new capabilities, such as\nperforming inference in native image resolution, or point-cloud completion. Our\nexperiments on 3D reconstruction, depth completion, multi-view depth\nprediction, multi-view stereo, and multi-view pose estimation tasks yield\nstate-of-the-art results and confirm the effectiveness of Pow3r at exploiting\nall available information. The project webpage is\nhttps://europe.naverlabs.com/pow3r.\n","authors":["Wonbong Jang","Philippe Weinzaepfel","Vincent Leroy","Lourdes Agapito","Jerome Revaud"],"pdf_url":"https://arxiv.org/pdf/2503.17316v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2411.07976v7","updated":"2025-03-21T17:06:08Z","published":"2024-11-12T17:55:39Z","title":"DINO-LG: A Task-Specific DINO Model for Coronary Calcium Scoring","summary":"  Coronary artery disease (CAD), one of the leading causes of mortality\nworldwide, necessitates effective risk assessment strategies, with coronary\nartery calcium (CAC) scoring via computed tomography (CT) being a key method\nfor prevention. Traditional methods, primarily based on UNET architectures\nimplemented on pre-built models, face challenges like the scarcity of annotated\nCT scans containing CAC and imbalanced datasets, leading to reduced performance\nin segmentation and scoring tasks. In this study, we address these limitations\nby incorporating the self-supervised learning (SSL) technique of DINO\n(self-distillation with no labels), which trains without requiring CAC-specific\nannotations, enhancing its robustness in generating distinct features. The\nDINO-LG model, which leverages label guidance to focus on calcified areas,\nachieves significant improvements, with a sensitivity of 89% and specificity of\n90% for detecting CAC-containing CT slices, compared to the standard DINO\nmodel's sensitivity of 79% and specificity of 77%. Additionally, false-negative\nand false-positive rates are reduced by 49% and 59%, respectively, instilling\ngreater confidence in clinicians when ruling out calcification in low-risk\npatients and minimizing unnecessary imaging reviews by radiologists. Further,\nCAC scoring and segmentation tasks are conducted using a basic UNET\narchitecture, applied specifically to CT slices identified by the DINO-LG model\nas containing calcified areas. This targeted approach enhances CAC scoring\naccuracy by feeding the UNET model with relevant slices, significantly\nimproving diagnostic precision, reducing both false positives and false\nnegatives, and ultimately lowering overall healthcare costs by minimizing\nunnecessary tests and treatments, presenting a valuable advancement in CAD risk\nassessment.\n","authors":["Mahmut S. Gokmen","Caner Ozcan","Moneera N. Haque","Steve W. Leung","C. Seth Parker","W. Brent Seales","Cody Bumgardner"],"pdf_url":"https://arxiv.org/pdf/2411.07976v7.pdf","comment":"Developed by Center for Applied Artificial Intelligence (CAAI),\n  University of Kentucky"},{"id":"http://arxiv.org/abs/2501.08837v2","updated":"2025-03-21T17:04:07Z","published":"2025-01-15T14:46:44Z","title":"MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term\n  Dense Anticipation","summary":"  Long-term dense action anticipation is very challenging since it requires\npredicting actions and their durations several minutes into the future based on\nprovided video observations. To model the uncertainty of future outcomes,\nstochastic models predict several potential future action sequences for the\nsame observation. Recent work has further proposed to incorporate uncertainty\nmodelling for observed frames by simultaneously predicting per-frame past and\nfuture actions in a unified manner. While such joint modelling of actions is\nbeneficial, it requires long-range temporal capabilities to connect events\nacross distant past and future time points. However, the previous work\nstruggles to achieve such a long-range understanding due to its limited and/or\nsparse receptive field. To alleviate this issue, we propose a novel MANTA\n(MAmba for ANTicipation) network. Our model enables effective long-term\ntemporal modelling even for very long sequences while maintaining linear\ncomplexity in sequence length. We demonstrate that our approach achieves\nstate-of-the-art results on three datasets - Breakfast, 50Salads, and\nAssembly101 - while also significantly improving computational and memory\nefficiency. Our code is available at https://github.com/olga-zats/DIFF_MANTA .\n","authors":["Olga Zatsarynna","Emad Bahrami","Yazan Abu Farha","Gianpiero Francesca","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2501.08837v2.pdf","comment":"Accepted to CVPR2025"},{"id":"http://arxiv.org/abs/2503.12261v2","updated":"2025-03-21T16:51:33Z","published":"2025-03-15T21:03:20Z","title":"United we stand, Divided we fall: Handling Weak Complementary\n  Relationships for Audio-Visual Emotion Recognition in Valence-Arousal Space","summary":"  Audio and visual modalities are two predominant contact-free channels in\nvideos, which are often expected to carry a complementary relationship with\neach other. However, they may not always complement each other, resulting in\npoor audio-visual feature representations. In this paper, we introduce Gated\nRecursive Joint Cross Attention (GRJCA) using a gating mechanism that can\nadaptively choose the most relevant features to effectively capture the\nsynergic relationships across audio and visual modalities. Specifically, we\nimprove the performance of Recursive Joint Cross-Attention (RJCA) by\nintroducing a gating mechanism to control the flow of information between the\ninput features and the attended features of multiple iterations depending on\nthe strength of their complementary relationship. For instance, if the\nmodalities exhibit strong complementary relationships, the gating mechanism\nemphasizes cross-attended features, otherwise non-attended features. To further\nimprove the performance of the system, we also explored a hierarchical gating\napproach by introducing a gating mechanism at every iteration, followed by\nhigh-level gating across the gated outputs of each iteration. The proposed\napproach improves the performance of RJCA model by adding more flexibility to\ndeal with weak complementary relationships across audio and visual modalities.\nExtensive experiments are conducted on the challenging Affwild2 dataset to\ndemonstrate the robustness of the proposed approach. By effectively handling\nthe weak complementary relationships across the audio and visual modalities,\nthe proposed model achieves a Concordance Correlation Coefficient (CCC) of\n0.561 (0.623) and 0.620 (0.660) for valence and arousal respectively on the\ntest set (validation set).\n","authors":["R. Gnana Praveen","Jahangir Alam","Eric Charton"],"pdf_url":"https://arxiv.org/pdf/2503.12261v2.pdf","comment":"Achieved 2nd place in valence arousal challenge Submission to\n  CVPR2025 Workshop"},{"id":"http://arxiv.org/abs/2503.17288v1","updated":"2025-03-21T16:38:37Z","published":"2025-03-21T16:38:37Z","title":"Exploring a Principled Framework for Deep Subspace Clustering","summary":"  Subspace clustering is a classical unsupervised learning task, built on a\nbasic assumption that high-dimensional data can be approximated by a union of\nsubspaces (UoS). Nevertheless, the real-world data are often deviating from the\nUoS assumption. To address this challenge, state-of-the-art deep subspace\nclustering algorithms attempt to jointly learn UoS representations and\nself-expressive coefficients. However, the general framework of the existing\nalgorithms suffers from a catastrophic feature collapse and lacks a theoretical\nguarantee to learn desired UoS representation. In this paper, we present a\nPrincipled fRamewOrk for Deep Subspace Clustering (PRO-DSC), which is designed\nto learn structured representations and self-expressive coefficients in a\nunified manner. Specifically, in PRO-DSC, we incorporate an effective\nregularization on the learned representations into the self-expressive model,\nprove that the regularized self-expressive model is able to prevent feature\nspace collapse, and demonstrate that the learned optimal representations under\ncertain condition lie on a union of orthogonal subspaces. Moreover, we provide\na scalable and efficient approach to implement our PRO-DSC and conduct\nextensive experiments to verify our theoretical findings and demonstrate the\nsuperior performance of our proposed deep subspace clustering approach. The\ncode is available at https://github.com/mengxianghan123/PRO-DSC.\n","authors":["Xianghan Meng","Zhiyuan Huang","Wei He","Xianbiao Qi","Rong Xiao","Chun-Guang Li"],"pdf_url":"https://arxiv.org/pdf/2503.17288v1.pdf","comment":"The paper is accepted by ICLR 2025. The first two authors are equally\n  contributed"},{"id":"http://arxiv.org/abs/2311.15965v3","updated":"2025-03-21T16:35:08Z","published":"2023-11-27T16:07:39Z","title":"FALCON: Fairness Learning via Contrastive Attention Approach to\n  Continual Semantic Scene Understanding","summary":"  Continual Learning in semantic scene segmentation aims to continually learn\nnew unseen classes in dynamic environments while maintaining previously learned\nknowledge. Prior studies focused on modeling the catastrophic forgetting and\nbackground shift challenges in continual learning. However, fairness, another\nmajor challenge that causes unfair predictions leading to low performance among\nmajor and minor classes, still needs to be well addressed. In addition, prior\nmethods have yet to model the unknown classes well, thus resulting in producing\nnon-discriminative features among unknown classes. This work presents a novel\nFairness Learning via Contrastive Attention Approach to continual learning in\nsemantic scene understanding. In particular, we first introduce a new Fairness\nContrastive Clustering loss to address the problems of catastrophic forgetting\nand fairness. Then, we propose an attention-based visual grammar approach to\neffectively model the background shift problem and unknown classes, producing\nbetter feature representations for different unknown classes. Through our\nexperiments, our proposed approach achieves State-of-the-Art (SoTA) performance\non different continual learning benchmarks, i.e., ADE20K, Cityscapes, and\nPascal VOC. It promotes the fairness of the continual semantic segmentation\nmodel.\n","authors":["Thanh-Dat Truong","Utsav Prabhu","Bhiksha Raj","Jackson Cothren","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2311.15965v3.pdf","comment":"Accepted to CVPR'25"},{"id":"http://arxiv.org/abs/2502.10127v2","updated":"2025-03-21T16:34:23Z","published":"2025-02-14T12:56:10Z","title":"Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph\n  Generation","summary":"  High-Definition (HD) maps play a crucial role in autonomous vehicle\nnavigation, complementing onboard perception sensors for improved accuracy and\nsafety. Traditional HD map generation relies on dedicated mapping vehicles,\nwhich are costly and fail to capture real-time infrastructure changes. This\npaper presents HDMapLaneNet, a novel framework leveraging V2X communication and\nScene Graph Generation to collaboratively construct a localized geometric layer\nof HD maps. The approach extracts lane centerlines from front-facing camera\nimages, represents them as graphs, and transmits the data for global\naggregation to the cloud via V2X. Preliminary results on the nuScenes dataset\ndemonstrate superior association prediction performance compared to a\nstate-of-the-art method.\n","authors":["Gamal Elghazaly","Raphael Frank"],"pdf_url":"https://arxiv.org/pdf/2502.10127v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14312v5","updated":"2025-03-21T16:34:17Z","published":"2022-11-20T04:59:23Z","title":"Karyotype AI for Precision Oncology","summary":"  We present a machine learning method capable of accurately detecting\nchromosome abnormalities that cause blood cancers directly from microscope\nimages of the metaphase stage of cell division. The pipeline is built on a\nseries of fine-tuned Vision Transformers. Current state of the art (and\nstandard clinical practice) requires expensive, manual expert analysis, whereas\nour pipeline takes only 15 seconds per metaphase image. Using a novel\npretraining-finetuning strategy to mitigate the challenge of data scarcity, we\nachieve a high precision-recall score of 94% AUC for the clinically significant\ndel(5q) and t(9;22) anomalies. Our method also unlocks zero-shot detection of\nrare aberrations based on model latent embeddings. The ability to quickly,\naccurately, and scalably diagnose genetic abnormalities directly from metaphase\nimages could transform karyotyping practice and improve patient outcomes. We\nwill make code publicly available.\n","authors":["Zahra Shamsi","Isaac Reid","Drew Bryant","Jacob Wilson","Xiaoyu Qu","Avinava Dubey","Konik Kothari","Mostafa Dehghani","Mariya Chavarha","Valerii Likhosherstov","Brian Williams","Michael Frumkin","Fred Appelbaum","Krzysztof Choromanski","Ali Bashir","Min Fang"],"pdf_url":"https://arxiv.org/pdf/2211.14312v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17285v1","updated":"2025-03-21T16:34:04Z","published":"2025-03-21T16:34:04Z","title":"An Iterative Feedback Mechanism for Improving Natural Language Class\n  Descriptions in Open-Vocabulary Object Detection","summary":"  Recent advances in open-vocabulary object detection models will enable\nAutomatic Target Recognition systems to be sustainable and repurposed by\nnon-technical end-users for a variety of applications or missions. New, and\npotentially nuanced, classes can be defined with natural language text\ndescriptions in the field, immediately before runtime, without needing to\nretrain the model. We present an approach for improving non-technical users'\nnatural language text descriptions of their desired targets of interest, using\na combination of analysis techniques on the text embeddings, and proper\ncombinations of embeddings for contrastive examples. We quantify the\nimprovement that our feedback mechanism provides by demonstrating performance\nwith multiple publicly-available open-vocabulary object detection models.\n","authors":["Louis Y. Kim","Michelle Karker","Victoria Valledor","Seiyoung C. Lee","Karl F. Brzoska","Margaret Duff","Anthony Palladino"],"pdf_url":"https://arxiv.org/pdf/2503.17285v1.pdf","comment":"To appear in the Proceedings of SPIE 13463 Automatic Target\n  Recognition XXXV, Orlando, FL, 2025"},{"id":"http://arxiv.org/abs/2403.10346v2","updated":"2025-03-21T16:26:49Z","published":"2024-03-15T14:31:35Z","title":"End-to-end Adaptive Dynamic Subsampling and Reconstruction for Cardiac\n  MRI","summary":"  $\\textbf{Background:}$ Accelerating dynamic MRI is vital for advancing\nclinical applications and improving patient comfort. Commonly, deep learning\n(DL) methods for accelerated dynamic MRI reconstruction typically rely on\nuniformly applying non-adaptive predetermined or random subsampling patterns\nacross all temporal frames of the dynamic acquisition. This approach fails to\nexploit temporal correlations or optimize subsampling on a case-by-case basis.\n  $\\textbf{Purpose:}$ To develop an end-to-end approach for adaptive dynamic\nMRI subsampling and reconstruction, capable of generating customized sampling\npatterns maximizing at the same time reconstruction quality.\n  $\\textbf{Methods:}$ We introduce the End-to-end Adaptive Dynamic Sampling and\nReconstruction (E2E-ADS-Recon) for MRI framework, which integrates an adaptive\ndynamic sampler (ADS) that adapts the acquisition trajectory to each case for a\ngiven acceleration factor with a state-of-the-art dynamic reconstruction\nnetwork, vSHARP, for reconstructing the adaptively sampled data into a dynamic\nimage. The ADS can produce either frame-specific patterns or unified patterns\napplied to all temporal frames. E2E-ADS-Recon is evaluated under both\nframe-specific and unified 1D or 2D sampling settings, using dynamic cine\ncardiac MRI data and compared with vSHARP models employing standard subsampling\ntrajectories, as well as pipelines where ADS was replaced by parameterized\nsamplers optimized for dataset-specific schemes.\n  $\\textbf{Results:}$ E2E-ADS-Recon exhibited superior reconstruction quality,\nespecially at high accelerations, in terms of standard quantitative metrics\n(SSIM, pSNR, NMSE).\n  $\\textbf{Conclusion:}$ The proposed framework improves reconstruction\nquality, highlighting the importance of case-specific subsampling optimization\nin dynamic MRI applications.\n","authors":["George Yiasemis","Jan-Jakob Sonke","Jonas Teuwen"],"pdf_url":"https://arxiv.org/pdf/2403.10346v2.pdf","comment":"38 pages, 26 figures, 2 tables"},{"id":"http://arxiv.org/abs/2503.17276v1","updated":"2025-03-21T16:24:47Z","published":"2025-03-21T16:24:47Z","title":"HyperNVD: Accelerating Neural Video Decomposition via Hypernetworks","summary":"  Decomposing a video into a layer-based representation is crucial for easy\nvideo editing for the creative industries, as it enables independent editing of\nspecific layers. Existing video-layer decomposition models rely on implicit\nneural representations (INRs) trained independently for each video, making the\nprocess time-consuming when applied to new videos. Noticing this limitation, we\npropose a meta-learning strategy to learn a generic video decomposition model\nto speed up the training on new videos. Our model is based on a hypernetwork\narchitecture which, given a video-encoder embedding, generates the parameters\nfor a compact INR-based neural video decomposition model. Our strategy\nmitigates the problem of single-video overfitting and, importantly, shortens\nthe convergence of video decomposition on new, unseen videos. Our code is\navailable at: https://hypernvd.github.io/\n","authors":["Maria Pilligua","Danna Xue","Javier Vazquez-Corral"],"pdf_url":"https://arxiv.org/pdf/2503.17276v1.pdf","comment":"CVPR 2025, project page: https://hypernvd.github.io/"},{"id":"http://arxiv.org/abs/2503.14558v2","updated":"2025-03-21T16:23:42Z","published":"2025-03-18T03:21:58Z","title":"SuperPC: A Single Diffusion Model for Point Cloud Completion,\n  Upsampling, Denoising, and Colorization","summary":"  Point cloud (PC) processing tasks-such as completion, upsampling, denoising,\nand colorization-are crucial in applications like autonomous driving and 3D\nreconstruction. Despite substantial advancements, prior approaches often\naddress each of these tasks independently, with separate models focused on\nindividual issues. However, this isolated approach fails to account for the\nfact that defects like incompleteness, low resolution, noise, and lack of color\nfrequently coexist, with each defect influencing and correlating with the\nothers. Simply applying these models sequentially can lead to error\naccumulation from each model, along with increased computational costs. To\naddress these challenges, we introduce SuperPC, the first unified diffusion\nmodel capable of concurrently handling all four tasks. Our approach employs a\nthree-level-conditioned diffusion framework, enhanced by a novel\nspatial-mix-fusion strategy, to leverage the correlations among these four\ndefects for simultaneous, efficient processing. We show that SuperPC\noutperforms the state-of-the-art specialized models as well as their\ncombination on all four individual tasks.\n","authors":["Yi Du","Zhipeng Zhao","Shaoshu Su","Sharath Golluri","Haoze Zheng","Runmao Yao","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2503.14558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17275v1","updated":"2025-03-21T16:23:02Z","published":"2025-03-21T16:23:02Z","title":"Vision Transformer Based Semantic Communications for Next Generation\n  Wireless Networks","summary":"  In the evolving landscape of 6G networks, semantic communications are poised\nto revolutionize data transmission by prioritizing the transmission of semantic\nmeaning over raw data accuracy. This paper presents a Vision Transformer\n(ViT)-based semantic communication framework that has been deliberately\ndesigned to achieve high semantic similarity during image transmission while\nsimultaneously minimizing the demand for bandwidth. By equipping ViT as the\nencoder-decoder framework, the proposed architecture can proficiently encode\nimages into a high semantic content at the transmitter and precisely\nreconstruct the images, considering real-world fading and noise consideration\nat the receiver. Building on the attention mechanisms inherent to ViTs, our\nmodel outperforms Convolution Neural Network (CNNs) and Generative Adversarial\nNetworks (GANs) tailored for generating such images. The architecture based on\nthe proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38\ndB, which is higher than other Deep Learning (DL) approaches in maintaining\nsemantic similarity across different communication environments. These findings\nestablish our ViT-based approach as a significant breakthrough in semantic\ncommunications.\n","authors":["Muhammad Ahmed Mohsin","Muhammad Jazib","Zeeshan Alam","Muhmmad Farhan Khan","Muhammad Saad","Muhammad Ali Jamshed"],"pdf_url":"https://arxiv.org/pdf/2503.17275v1.pdf","comment":"Accepted @ ICC 2025"},{"id":"http://arxiv.org/abs/2412.18600v2","updated":"2025-03-21T16:17:28Z","published":"2024-12-24T18:55:38Z","title":"ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation","summary":"  Human-scene interaction (HSI) generation is crucial for applications in\nembodied AI, virtual reality, and robotics. Yet, existing methods cannot\nsynthesize interactions in unseen environments such as in-the-wild scenes or\nreconstructed scenes, as they rely on paired 3D scenes and captured human\nmotion data for training, which are unavailable for unseen environments. We\npresent ZeroHSI, a novel approach that enables zero-shot 4D human-scene\ninteraction synthesis, eliminating the need for training on any MoCap data. Our\nkey insight is to distill human-scene interactions from state-of-the-art video\ngeneration models, which have been trained on vast amounts of natural human\nmovements and interactions, and use differentiable rendering to reconstruct\nhuman-scene interactions. ZeroHSI can synthesize realistic human motions in\nboth static scenes and environments with dynamic objects, without requiring any\nground-truth motion data. We evaluate ZeroHSI on a curated dataset of different\ntypes of various indoor and outdoor scenes with different interaction prompts,\ndemonstrating its ability to generate diverse and contextually appropriate\nhuman-scene interactions.\n","authors":["Hongjie Li","Hong-Xing Yu","Jiaman Li","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2412.18600v2.pdf","comment":"Project website: https://awfuact.github.io/zerohsi/ The first two\n  authors contribute equally"},{"id":"http://arxiv.org/abs/2503.17269v1","updated":"2025-03-21T16:11:21Z","published":"2025-03-21T16:11:21Z","title":"Recovering Pulse Waves from Video Using Deep Unrolling and Deep\n  Equilibrium Models","summary":"  Camera-based monitoring of vital signs, also known as imaging\nphotoplethysmography (iPPG), has seen applications in driver-monitoring,\nperfusion assessment in surgical settings, affective computing, and more. iPPG\ninvolves sensing the underlying cardiac pulse from video of the skin and\nestimating vital signs such as the heart rate or a full pulse waveform. Some\nprevious iPPG methods impose model-based sparse priors on the pulse signals and\nuse iterative optimization for pulse wave recovery, while others use end-to-end\nblack-box deep learning methods. In contrast, we introduce methods that combine\nsignal processing and deep learning methods in an inverse problem framework.\nOur methods estimate the underlying pulse signal and heart rate from facial\nvideo by learning deep-network-based denoising operators that leverage deep\nalgorithm unfolding and deep equilibrium models. Experiments show that our\nmethods can denoise an acquired signal from the face and infer the correct\nunderlying pulse rate, achieving state-of-the-art heart rate estimation\nperformance on well-known benchmarks, all with less than one-fifth the number\nof learnable parameters as the closest competing method.\n","authors":["Vineet R Shenoy","Suhas Lohit","Hassan Mansour","Rama Chellappa","Tim K. Marks"],"pdf_url":"https://arxiv.org/pdf/2503.17269v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.17267v1","updated":"2025-03-21T16:08:25Z","published":"2025-03-21T16:08:25Z","title":"Physical Plausibility-aware Trajectory Prediction via Locomotion\n  Embodiment","summary":"  Humans can predict future human trajectories even from momentary observations\nby using human pose-related cues. However, previous Human Trajectory Prediction\n(HTP) methods leverage the pose cues implicitly, resulting in implausible\npredictions. To address this, we propose Locomotion Embodiment, a framework\nthat explicitly evaluates the physical plausibility of the predicted trajectory\nby locomotion generation under the laws of physics. While the plausibility of\nlocomotion is learned with an indifferentiable physics simulator, it is\nreplaced by our differentiable Locomotion Value function to train an HTP\nnetwork in a data-driven manner. In particular, our proposed Embodied\nLocomotion loss is beneficial for efficiently training a stochastic HTP network\nusing multiple heads. Furthermore, the Locomotion Value filter is proposed to\nfilter out implausible trajectories at inference. Experiments demonstrate that\nour method enhances even the state-of-the-art HTP methods across diverse\ndatasets and problem settings. Our code is available at:\nhttps://github.com/ImIntheMiddle/EmLoco.\n","authors":["Hiromu Taketsugu","Takeru Oba","Takahiro Maeda","Shohei Nobuhara","Norimichi Ukita"],"pdf_url":"https://arxiv.org/pdf/2503.17267v1.pdf","comment":"CVPR2025. Project page: https://iminthemiddle.github.io/EmLoco-Page/"},{"id":"http://arxiv.org/abs/2503.17262v1","updated":"2025-03-21T16:04:13Z","published":"2025-03-21T16:04:13Z","title":"Unsupervised Joint Learning of Optical Flow and Intensity with Event\n  Cameras","summary":"  Event cameras rely on motion to obtain information about scene appearance. In\nother words, for event cameras, motion and appearance are seen both or neither,\nwhich are encoded in the output event stream. Previous works consider\nrecovering these two visual quantities as separate tasks, which does not fit\nwith the nature of event cameras and neglects the inherent relations between\nboth tasks. In this paper, we propose an unsupervised learning framework that\njointly estimates optical flow (motion) and image intensity (appearance), with\na single network. Starting from the event generation model, we newly derive the\nevent-based photometric error as a function of optical flow and image\nintensity, which is further combined with the contrast maximization framework,\nyielding a comprehensive loss function that provides proper constraints for\nboth flow and intensity estimation. Exhaustive experiments show that our model\nachieves state-of-the-art performance for both optical flow (achieves 20% and\n25% improvement in EPE and AE respectively in the unsupervised learning\ncategory) and intensity estimation (produces competitive results with other\nbaselines, particularly in high dynamic range scenarios). Last but not least,\nour model achieves shorter inference time than all the other optical flow\nmodels and many of the image reconstruction models, while they output only one\nquantity. Project page: https://github.com/tub-rip/e2fai\n","authors":["Shuang Guo","Friedhelm Hamann","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2503.17262v1.pdf","comment":"14 page, 8 figures, 9 tables. Project page:\n  https://github.com/tub-rip/e2fai"},{"id":"http://arxiv.org/abs/2503.17261v1","updated":"2025-03-21T16:04:11Z","published":"2025-03-21T16:04:11Z","title":"Cross-Modal Interactive Perception Network with Mamba for Lung Tumor\n  Segmentation in PET-CT Images","summary":"  Lung cancer is a leading cause of cancer-related deaths globally. PET-CT is\ncrucial for imaging lung tumors, providing essential metabolic and anatomical\ninformation, while it faces challenges such as poor image quality, motion\nartifacts, and complex tumor morphology. Deep learning-based models are\nexpected to address these problems, however, existing small-scale and private\ndatasets limit significant performance improvements for these methods. Hence,\nwe introduce a large-scale PET-CT lung tumor segmentation dataset, termed\nPCLT20K, which comprises 21,930 pairs of PET-CT images from 605 patients.\nFurthermore, we propose a cross-modal interactive perception network with Mamba\n(CIPA) for lung tumor segmentation in PET-CT images. Specifically, we design a\nchannel-wise rectification module (CRM) that implements a channel state space\nblock across multi-modal features to learn correlated representations and helps\nfilter out modality-specific noise. A dynamic cross-modality interaction module\n(DCIM) is designed to effectively integrate position and context information,\nwhich employs PET images to learn regional position information and serves as a\nbridge to assist in modeling the relationships between local features of CT\nimages. Extensive experiments on a comprehensive benchmark demonstrate the\neffectiveness of our CIPA compared to the current state-of-the-art segmentation\nmethods. We hope our research can provide more exploration opportunities for\nmedical image segmentation. The dataset and code are available at\nhttps://github.com/mj129/CIPA.\n","authors":["Jie Mei","Chenyu Lin","Yu Qiu","Yaonan Wang","Hui Zhang","Ziyang Wang","Dong Dai"],"pdf_url":"https://arxiv.org/pdf/2503.17261v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.10602v2","updated":"2025-03-21T15:58:26Z","published":"2025-03-13T17:46:06Z","title":"TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention","summary":"  Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt.\n","authors":["Jinhao Duan","Fei Kong","Hao Cheng","James Diffenderfer","Bhavya Kailkhura","Lichao Sun","Xiaofeng Zhu","Xiaoshuang Shi","Kaidi Xu"],"pdf_url":"https://arxiv.org/pdf/2503.10602v2.pdf","comment":"15 pages, 9 figures, the first two authors contributed equally"},{"id":"http://arxiv.org/abs/2503.17244v1","updated":"2025-03-21T15:50:54Z","published":"2025-03-21T15:50:54Z","title":"Deep End-to-End Posterior ENergy (DEEPEN) for image recovery","summary":"  Current end-to-end (E2E) and plug-and-play (PnP) image reconstruction\nalgorithms approximate the maximum a posteriori (MAP) estimate but cannot offer\nsampling from the posterior distribution, like diffusion models. By contrast,\nit is challenging for diffusion models to be trained in an E2E fashion. This\npaper introduces a Deep End-to-End Posterior ENergy (DEEPEN) framework, which\nenables MAP estimation as well as sampling. We learn the parameters of the\nposterior, which is the sum of the data consistency error and the negative\nlog-prior distribution, using maximum likelihood optimization in an E2E\nfashion. The proposed approach does not require algorithm unrolling, and hence\nhas a smaller computational and memory footprint than current E2E methods,\nwhile it does not require contraction constraints typically needed by current\nPnP methods. Our results demonstrate that DEEPEN offers improved performance\nthan current E2E and PnP models in the MAP setting, while it also offers faster\nsampling compared to diffusion models. In addition, the learned energy-based\nmodel is observed to be more robust to changes in image acquisition settings.\n","authors":["Jyothi Rikhab Chand","Mathews Jacob"],"pdf_url":"https://arxiv.org/pdf/2503.17244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07885v3","updated":"2025-03-21T15:47:12Z","published":"2024-11-12T15:47:17Z","title":"RadioActive: 3D Radiological Interactive Segmentation Benchmark","summary":"  Effortless and precise segmentation with minimal clinician effort could\ngreatly streamline clinical workflows. Recent interactive segmentation models,\ninspired by METAs Segment Anything, have made significant progress but face\ncritical limitations in 3D radiology. These include impractical human\ninteraction requirements such as slice-by-slice operations for 2D models on 3D\ndata and a lack of iterative refinement. Prior studies have been hindered by\ninadequate evaluation protocols, resulting in unreliable performance\nassessments and inconsistent findings across studies. The RadioActive benchmark\naddresses these challenges by providing a rigorous and reproducible evaluation\nframework for interactive segmentation methods in clinically relevant\nscenarios. It features diverse datasets, a wide range of target structures, and\nthe most impactful 2D and 3D interactive segmentation methods, all within a\nflexible and extensible codebase. We also introduce advanced prompting\ntechniques that reduce interaction steps, enabling fair comparisons between 2D\nand 3D models. Surprisingly, SAM2 outperforms all specialized medical 2D and 3D\nmodels in a setting requiring only a few interactions to generate prompts for a\n3D volume. This challenges prevailing assumptions and demonstrates that\ngeneral-purpose models surpass specialized medical approaches. By open-sourcing\nRadioActive, we invite researchers to integrate their models and prompting\ntechniques, ensuring continuous and transparent evaluation of 3D medical\ninteractive models.\n","authors":["Constantin Ulrich","Tassilo Wald","Emily Tempus","Maximilian Rokuss","Paul F. Jaeger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2411.07885v3.pdf","comment":"Undergoing Peer-Review"},{"id":"http://arxiv.org/abs/2503.17238v1","updated":"2025-03-21T15:40:37Z","published":"2025-03-21T15:40:37Z","title":"Slide-Level Prompt Learning with Vision Language Models for Few-Shot\n  Multiple Instance Learning in Histopathology","summary":"  In this paper, we address the challenge of few-shot classification in\nhistopathology whole slide images (WSIs) by utilizing foundational\nvision-language models (VLMs) and slide-level prompt learning. Given the\ngigapixel scale of WSIs, conventional multiple instance learning (MIL) methods\nrely on aggregation functions to derive slide-level (bag-level) predictions\nfrom patch representations, which require extensive bag-level labels for\ntraining. In contrast, VLM-based approaches excel at aligning visual embeddings\nof patches with candidate class text prompts but lack essential pathological\nprior knowledge. Our method distinguishes itself by utilizing pathological\nprior knowledge from language models to identify crucial local tissue types\n(patches) for WSI classification, integrating this within a VLM-based MIL\nframework. Our approach effectively aligns patch images with tissue types, and\nwe fine-tune our model via prompt learning using only a few labeled WSIs per\ncategory. Experimentation on real-world pathological WSI datasets and ablation\nstudies highlight our method's superior performance over existing MIL- and\nVLM-based methods in few-shot WSI classification tasks. Our code is publicly\navailable at https://github.com/LTS5/SLIP.\n","authors":["Devavrat Tomar","Guillaume Vray","Dwarikanath Mahapatra","Sudipta Roy","Jean-Philippe Thiran","Behzad Bozorgtabar"],"pdf_url":"https://arxiv.org/pdf/2503.17238v1.pdf","comment":"Accepted to ISBI 2025"},{"id":"http://arxiv.org/abs/2503.17237v1","updated":"2025-03-21T15:40:18Z","published":"2025-03-21T15:40:18Z","title":"Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID","summary":"  Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .\n","authors":["Yu-Hsi Chen"],"pdf_url":"https://arxiv.org/pdf/2503.17237v1.pdf","comment":"10 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.17226v1","updated":"2025-03-21T15:28:22Z","published":"2025-03-21T15:28:22Z","title":"Leveraging Text-to-Image Generation for Handling Spurious Correlation","summary":"  Deep neural networks trained with Empirical Risk Minimization (ERM) perform\nwell when both training and test data come from the same domain, but they often\nfail to generalize to out-of-distribution samples. In image classification,\nthese models may rely on spurious correlations that often exist between labels\nand irrelevant features of images, making predictions unreliable when those\nfeatures do not exist. We propose a technique to generate training samples with\ntext-to-image (T2I) diffusion models for addressing the spurious correlation\nproblem. First, we compute the best describing token for the visual features\npertaining to the causal components of samples by a textual inversion\nmechanism. Then, leveraging a language segmentation method and a diffusion\nmodel, we generate new samples by combining the causal component with the\nelements from other classes. We also meticulously prune the generated samples\nbased on the prediction probabilities and attribution scores of the ERM model\nto ensure their correct composition for our objective. Finally, we retrain the\nERM model on our augmented dataset. This process reduces the model's reliance\non spurious correlations by learning from carefully crafted samples for in\nwhich this correlation does not exist. Our experiments show that across\ndifferent benchmarks, our technique achieves better worst-group accuracy than\nthe existing state-of-the-art methods.\n","authors":["Aryan Yazdan Parast","Basim Azam","Naveed Akhtar"],"pdf_url":"https://arxiv.org/pdf/2503.17226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17224v1","updated":"2025-03-21T15:26:16Z","published":"2025-03-21T15:26:16Z","title":"Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset\n  Generation","summary":"  As machine learning models increase in scale and complexity, obtaining\nsufficient training data has become a critical bottleneck due to acquisition\ncosts, privacy constraints, and data scarcity in specialised domains. While\nsynthetic data generation has emerged as a promising alternative, a notable\nperformance gap remains compared to models trained on real data, particularly\nas task complexity grows. Concurrently, Neuro-Symbolic methods, which combine\nneural networks' learning strengths with symbolic reasoning's structured\nrepresentations, have demonstrated significant potential across various\ncognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning\nfor synthetic image dataset generation, focusing specifically on improving the\nperformance of Scene Graph Generation models. The research investigates whether\nstructured symbolic representations in the form of scene graphs can enhance\nsynthetic data quality through explicit encoding of relational constraints. The\nresults demonstrate that Neuro-Symbolic conditioning yields significant\nimprovements of up to +2.59% in standard Recall metrics and +2.83% in No Graph\nConstraint Recall metrics when used for dataset augmentation. These findings\nestablish that merging Neuro-Symbolic and generative approaches produces\nsynthetic data with complementary structural information that enhances model\nperformance when combined with real data, providing a novel approach to\novercome data scarcity limitations even for complex visual reasoning tasks.\n","authors":["Giacomo Savazzi","Eugenio Lomurno","Cristian Sbrolli","Agnese Chiatti","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2503.17224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17221v1","updated":"2025-03-21T15:25:37Z","published":"2025-03-21T15:25:37Z","title":"UniCon: Unidirectional Information Flow for Effective Control of\n  Large-Scale Diffusion Models","summary":"  We introduce UniCon, a novel architecture designed to enhance control and\nefficiency in training adapters for large-scale diffusion models. Unlike\nexisting methods that rely on bidirectional interaction between the diffusion\nmodel and control adapter, UniCon implements a unidirectional flow from the\ndiffusion network to the adapter, allowing the adapter alone to generate the\nfinal output. UniCon reduces computational demands by eliminating the need for\nthe diffusion model to compute and store gradients during adapter training. Our\nresults indicate that UniCon reduces GPU memory usage by one-third and\nincreases training speed by 2.3 times, while maintaining the same adapter\nparameter size. Additionally, without requiring extra computational resources,\nUniCon enables the training of adapters with double the parameter volume of\nexisting ControlNets. In a series of image conditional generation tasks, UniCon\nhas demonstrated precise responsiveness to control inputs and exceptional\ngeneration capabilities.\n","authors":["Fanghua Yu","Jinjin Gu","Jinfan Hu","Zheyuan Li","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2503.17221v1.pdf","comment":"This work has been accepted for publication at the International\n  Conference on Learning Representations (ICLR) 2025"},{"id":"http://arxiv.org/abs/2503.15868v2","updated":"2025-03-21T15:24:45Z","published":"2025-03-20T05:42:13Z","title":"UniCoRN: Latent Diffusion-based Unified Controllable Image Restoration\n  Network across Multiple Degradations","summary":"  Image restoration is essential for enhancing degraded images across computer\nvision tasks. However, most existing methods address only a single type of\ndegradation (e.g., blur, noise, or haze) at a time, limiting their real-world\napplicability where multiple degradations often occur simultaneously. In this\npaper, we propose UniCoRN, a unified image restoration approach capable of\nhandling multiple degradation types simultaneously using a multi-head diffusion\nmodel. Specifically, we uncover the potential of low-level visual cues\nextracted from images in guiding a controllable diffusion model for real-world\nimage restoration and we design a multi-head control network adaptable via a\nmixture-of-experts strategy. We train our model without any prior assumption of\nspecific degradations, through a smartly designed curriculum learning recipe.\nAdditionally, we also introduce MetaRestore, a metalens imaging benchmark\ncontaining images with multiple degradations and artifacts. Extensive\nevaluations on several challenging datasets, including our benchmark,\ndemonstrate that our method achieves significant performance gains and can\nrobustly restore images with severe degradations. Project page:\nhttps://codejaeger.github.io/unicorn-gh\n","authors":["Debabrata Mandal","Soumitri Chattopadhyay","Guansen Tong","Praneeth Chakravarthula"],"pdf_url":"https://arxiv.org/pdf/2503.15868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17213v1","updated":"2025-03-21T15:20:47Z","published":"2025-03-21T15:20:47Z","title":"PP-DocLayout: A Unified Document Layout Detection Model to Accelerate\n  Large-Scale Data Construction","summary":"  Document layout analysis is a critical preprocessing step in document\nintelligence, enabling the detection and localization of structural elements\nsuch as titles, text blocks, tables, and formulas. Despite its importance,\nexisting layout detection models face significant challenges in generalizing\nacross diverse document types, handling complex layouts, and achieving\nreal-time performance for large-scale data processing. To address these\nlimitations, we present PP-DocLayout, which achieves high precision and\nefficiency in recognizing 23 types of layout regions across diverse document\nformats. To meet different needs, we offer three models of varying scales.\nPP-DocLayout-L is a high-precision model based on the RT-DETR-L detector,\nachieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on\na T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an\ninference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a\nhigh-efficiency model designed for resource-constrained environments and\nreal-time applications, with an inference time of 8.1 ms per page on a T4 GPU\nand 14.5 ms on a CPU. This work not only advances the state of the art in\ndocument layout analysis but also provides a robust solution for constructing\nhigh-quality training data, enabling advancements in document intelligence and\nmultimodal AI systems. Code and models are available at\nhttps://github.com/PaddlePaddle/PaddleX .\n","authors":["Ting Sun","Cheng Cui","Yuning Du","Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2503.17213v1.pdf","comment":"Github Repo: https://github.com/PaddlePaddle/PaddleX"},{"id":"http://arxiv.org/abs/2503.17212v1","updated":"2025-03-21T15:20:29Z","published":"2025-03-21T15:20:29Z","title":"A Deep Learning Framework for Visual Attention Prediction and Analysis\n  of News Interfaces","summary":"  News outlets' competition for attention in news interfaces has highlighted\nthe need for demographically-aware saliency prediction models. Despite recent\nadvancements in saliency detection applied to user interfaces (UI), existing\ndatasets are limited in size and demographic representation. We present a deep\nlearning framework that enhances the SaRa (Saliency Ranking) model with\nDeepGaze IIE, improving Salient Object Ranking (SOR) performance by 10.7%. Our\nframework optimizes three key components: saliency map generation, grid segment\nscoring, and map normalization. Through a two-fold experiment using\neye-tracking (30 participants) and mouse-tracking (375 participants aged\n13--70), we analyze attention patterns across demographic groups. Statistical\nanalysis reveals significant age-based variations (p < 0.05, {\\epsilon^2} =\n0.042), with older users (36--70) engaging more with textual content and\nyounger users (13--35) interacting more with images. Mouse-tracking data\nclosely approximates eye-tracking behavior (sAUC = 0.86) and identifies UI\nelements that immediately stand out, validating its use in large-scale studies.\nWe conclude that saliency studies should prioritize gathering data from a\nlarger, demographically representative sample and report exact demographic\ndistributions.\n","authors":["Matthew Kenely","Dylan Seychell","Carl James Debono","Chris Porter"],"pdf_url":"https://arxiv.org/pdf/2503.17212v1.pdf","comment":"This is a preprint submitted to the 2025 IEEE Conference on\n  Artificial Intelligence (CAI)"},{"id":"http://arxiv.org/abs/2503.17211v1","updated":"2025-03-21T15:20:28Z","published":"2025-03-21T15:20:28Z","title":"A Language Anchor-Guided Method for Robust Noisy Domain Generalization","summary":"  Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels.\n","authors":["Zilin Dai","Lehong Wang","Fangzhou Lin","Yidong Wang","Zhigang Li","Kazunori D Yamada","Ziming Zhang","Wang Lu"],"pdf_url":"https://arxiv.org/pdf/2503.17211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07685v2","updated":"2025-03-21T15:10:32Z","published":"2025-02-11T16:36:55Z","title":"Matrix3D: Large Photogrammetry Model All-in-One","summary":"  We present Matrix3D, a unified model that performs several photogrammetry\nsubtasks, including pose estimation, depth prediction, and novel view synthesis\nusing just the same model. Matrix3D utilizes a multi-modal diffusion\ntransformer (DiT) to integrate transformations across several modalities, such\nas images, camera parameters, and depth maps. The key to Matrix3D's large-scale\nmulti-modal training lies in the incorporation of a mask learning strategy.\nThis enables full-modality model training even with partially complete data,\nsuch as bi-modality data of image-pose and image-depth pairs, thus\nsignificantly increases the pool of available training data. Matrix3D\ndemonstrates state-of-the-art performance in pose estimation and novel view\nsynthesis tasks. Additionally, it offers fine-grained control through\nmulti-round interactions, making it an innovative tool for 3D content creation.\nProject page: https://nju-3dv.github.io/projects/matrix3d.\n","authors":["Yuanxun Lu","Jingyang Zhang","Tian Fang","Jean-Daniel Nahmias","Yanghai Tsin","Long Quan","Xun Cao","Yao Yao","Shiwei Li"],"pdf_url":"https://arxiv.org/pdf/2502.07685v2.pdf","comment":"CVPR 2025 camera ready. Project Page:\n  https://nju-3dv.github.io/projects/matrix3d"},{"id":"http://arxiv.org/abs/2503.09050v2","updated":"2025-03-21T15:07:07Z","published":"2025-03-12T04:27:45Z","title":"Mono2D: A Trainable Monogenic Layer for Robust Knee Cartilage\n  Segmentation on Out-of-Distribution 2D Ultrasound Data","summary":"  Automated knee cartilage segmentation using point-of-care ultrasound devices\nand deep-learning networks has the potential to enhance the management of knee\nosteoarthritis. However, segmentation algorithms often struggle with domain\nshifts caused by variations in ultrasound devices and acquisition parameters,\nlimiting their generalizability. In this paper, we propose Mono2D, a monogenic\nlayer that extracts multi-scale, contrast- and intensity-invariant local phase\nfeatures using trainable bandpass quadrature filters. This layer mitigates\ndomain shifts, improving generalization to out-of-distribution domains. Mono2D\nis integrated before the first layer of a segmentation network, and its\nparameters jointly trained alongside the network's parameters. We evaluated\nMono2D on a multi-domain 2D ultrasound knee cartilage dataset for single-source\ndomain generalization (SSDG). Our results demonstrate that Mono2D outperforms\nother SSDG methods in terms of Dice score and mean average surface distance. To\nfurther assess its generalizability, we evaluate Mono2D on a multi-site\nprostate MRI dataset, where it continues to outperform other SSDG methods,\nhighlighting its potential to improve domain generalization in medical imaging.\nNevertheless, further evaluation on diverse datasets is still necessary to\nassess its clinical utility.\n","authors":["Alvin Kimbowa","Arjun Parmar","Maziar Badii","David Liu","Matthew Harkey","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2503.09050v2.pdf","comment":"11 pages, removed unrelated LaTeX template figure from last page"},{"id":"http://arxiv.org/abs/2406.12082v2","updated":"2025-03-21T15:06:41Z","published":"2024-06-17T20:46:18Z","title":"Uncertainty modeling for fine-tuned implicit functions","summary":"  Implicit functions such as Neural Radiance Fields (NeRFs), occupancy\nnetworks, and signed distance functions (SDFs) have become pivotal in computer\nvision for reconstructing detailed object shapes from sparse views. Achieving\noptimal performance with these models can be challenging due to the extreme\nsparsity of inputs and distribution shifts induced by data corruptions. To this\nend, large, noise-free synthetic datasets can serve as shape priors to help\nmodels fill in gaps, but the resulting reconstructions must be approached with\ncaution. Uncertainty estimation is crucial for assessing the quality of these\nreconstructions, particularly in identifying areas where the model is uncertain\nabout the parts it has inferred from the prior. In this paper, we introduce\nDropsembles, a novel method for uncertainty estimation in tuned implicit\nfunctions. We demonstrate the efficacy of our approach through a series of\nexperiments, starting with toy examples and progressing to a real-world\nscenario. Specifically, we train a Convolutional Occupancy Network on synthetic\nanatomical data and test it on low-resolution MRI segmentations of the lumbar\nspine. Our results show that Dropsembles achieve the accuracy and calibration\nlevels of deep ensembles but with significantly less computational cost.\n","authors":["Anna Susmelj","Mael Macuglia","Nataša Tagasovska","Reto Sutter","Sebastiano Caprara","Jean-Philippe Thiran","Ender Konukoglu"],"pdf_url":"https://arxiv.org/pdf/2406.12082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18249v2","updated":"2025-03-21T14:54:33Z","published":"2024-11-27T11:38:48Z","title":"Deep End-to-end Adaptive k-Space Sampling, Reconstruction, and\n  Registration for Dynamic MRI","summary":"  Dynamic MRI enables a range of clinical applications, including cardiac\nfunction assessment, organ motion tracking, and radiotherapy guidance. However,\nfully sampling the dynamic k-space data is often infeasible due to time\nconstraints and physiological motion such as respiratory and cardiac motion.\nThis necessitates undersampling, which degrades the quality of reconstructed\nimages. Poor image quality not only hinders visualization but also impairs the\nestimation of deformation fields, crucial for registering dynamic (moving)\nimages to a static reference image. This registration enables tasks such as\nmotion correction, treatment planning, and quantitative analysis in\napplications like cardiac imaging and MR-guided radiotherapy. To overcome the\nchallenges posed by undersampling and motion, we introduce an end-to-end deep\nlearning (DL) framework that integrates adaptive dynamic k-space sampling,\nreconstruction, and registration. Our approach begins with a DL-based adaptive\nsampling strategy, optimizing dynamic k-space acquisition to capture the most\nrelevant data for each specific case. This is followed by a DL-based\nreconstruction module that produces images optimized for accurate deformation\nfield estimation from the undersampled moving data. Finally, a registration\nmodule estimates the deformation fields aligning the reconstructed dynamic\nimages with a static reference. The proposed framework is independent of\nspecific reconstruction and registration modules allowing for plug-and-play\nintegration of these components. The entire framework is jointly trained using\na combination of supervised and unsupervised loss functions, enabling\nend-to-end optimization for improved performance across all components. Through\ncontrolled experiments and ablation studies, we validate each component,\ndemonstrating that each choice contributes to robust motion estimation from\nundersampled dynamic data.\n","authors":["George Yiasemis","Jan-Jakob Sonke","Jonas Teuwen"],"pdf_url":"https://arxiv.org/pdf/2411.18249v2.pdf","comment":"48 pages, 23 figures, 8 tables"},{"id":"http://arxiv.org/abs/2412.04455v3","updated":"2025-03-21T14:54:29Z","published":"2024-12-05T18:58:27Z","title":"Code-as-Monitor: Constraint-aware Visual Programming for Reactive and\n  Proactive Robotic Failure Detection","summary":"  Automatic detection and prevention of open-set failures are crucial in\nclosed-loop robotic systems. Recent studies often struggle to simultaneously\nidentify unexpected failures reactively after they occur and prevent\nforeseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a\nnovel paradigm leveraging the vision-language model (VLM) for both open-set\nreactive and proactive failure detection. The core of our method is to\nformulate both tasks as a unified set of spatio-temporal constraint\nsatisfaction problems and use VLM-generated code to evaluate them for real-time\nmonitoring. To enhance the accuracy and efficiency of monitoring, we further\nintroduce constraint elements that abstract constraint-related entities or\ntheir parts into compact geometric elements. This approach offers greater\ngenerality, simplifies tracking, and facilitates constraint-aware visual\nprogramming by leveraging these elements as visual prompts. Experiments show\nthat CaM achieves a 28.7% higher success rate and reduces execution time by\n31.8% under severe disturbances compared to baselines across three simulators\nand a real-world setting. Moreover, CaM can be integrated with open-loop\ncontrol policies to form closed-loop systems, enabling long-horizon tasks in\ncluttered scenes with dynamic environments.\n","authors":["Enshen Zhou","Qi Su","Cheng Chi","Zhizheng Zhang","Zhongyuan Wang","Tiejun Huang","Lu Sheng","He Wang"],"pdf_url":"https://arxiv.org/pdf/2412.04455v3.pdf","comment":"Accepted by CVPR 2025. Project page:\n  https://zhoues.github.io/Code-as-Monitor/"},{"id":"http://arxiv.org/abs/2503.17198v1","updated":"2025-03-21T14:47:33Z","published":"2025-03-21T14:47:33Z","title":"Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising","summary":"  Non-transferable learning (NTL) has been proposed to protect model\nintellectual property (IP) by creating a \"non-transferable barrier\" to restrict\ngeneralization from authorized to unauthorized domains. Recently, well-designed\nattack, which restores the unauthorized-domain performance by fine-tuning NTL\nmodels on few authorized samples, highlights the security risks of NTL-based\napplications. However, such attack requires modifying model weights, thus being\ninvalid in the black-box scenario. This raises a critical question: can we\ntrust the security of NTL models deployed as black-box systems? In this work,\nwe reveal the first loophole of black-box NTL models by proposing a novel\nattack method (dubbed as JailNTL) to jailbreak the non-transferable barrier\nthrough test-time data disguising. The main idea of JailNTL is to disguise\nunauthorized data so it can be identified as authorized by the NTL model,\nthereby bypassing the non-transferable barrier without modifying the NTL model\nweights. Specifically, JailNTL encourages unauthorized-domain disguising in two\nlevels, including: (i) data-intrinsic disguising (DID) for eliminating domain\ndiscrepancy and preserving class-related content at the input-level, and (ii)\nmodel-guided disguising (MGD) for mitigating output-level statistics difference\nof the NTL model. Empirically, when attacking state-of-the-art (SOTA) NTL\nmodels in the black-box scenario, JailNTL achieves an accuracy increase of up\nto 55.7% in the unauthorized domain by using only 1% authorized samples,\nlargely exceeding existing SOTA white-box attacks.\n","authors":["Yongli Xiang","Ziming Hong","Lina Yao","Dadong Wang","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2503.17198v1.pdf","comment":"Code is released at https://github.com/tmllab/2025_CVPR_JailNTL"},{"id":"http://arxiv.org/abs/2503.17197v1","updated":"2025-03-21T14:44:22Z","published":"2025-03-21T14:44:22Z","title":"FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via\n  Cross-Assembly Inference Strategy","summary":"  Recovering high-quality 3D facial textures from single-view 2D images is a\nchallenging task, especially under constraints of limited data and complex\nfacial details such as makeup, wrinkles, and occlusions. In this paper, we\nintroduce FreeUV, a novel ground-truth-free UV texture recovery framework that\neliminates the need for annotated or synthetic UV data. FreeUV leverages\npre-trained stable diffusion model alongside a Cross-Assembly inference\nstrategy to fulfill this objective. In FreeUV, separate networks are trained\nindependently to focus on realistic appearance and structural consistency, and\nthese networks are combined during inference to generate coherent textures. Our\napproach accurately captures intricate facial features and demonstrates robust\nperformance across diverse poses and occlusions. Extensive experiments validate\nFreeUV's effectiveness, with results surpassing state-of-the-art methods in\nboth quantitative and qualitative metrics. Additionally, FreeUV enables new\napplications, including local editing, facial feature interpolation, and\nmulti-view texture recovery. By reducing data requirements, FreeUV offers a\nscalable solution for generating high-fidelity 3D facial textures suitable for\nreal-world scenarios.\n","authors":["Xingchao Yang","Takafumi Taketomi","Yuki Endo","Yoshihiro Kanamori"],"pdf_url":"https://arxiv.org/pdf/2503.17197v1.pdf","comment":"CVPR 2025. Project: https://yangxingchao.github.io/FreeUV-page/"},{"id":"http://arxiv.org/abs/2501.06553v2","updated":"2025-03-21T14:43:37Z","published":"2025-01-11T14:09:34Z","title":"VASparse: Towards Efficient Visual Hallucination Mitigation via\n  Visual-Aware Token Sparsification","summary":"  Large Vision-Language Models (LVLMs) may produce outputs that are unfaithful\nto reality, also known as visual hallucinations (VH), which significantly\nimpedes their real-world usage. To alleviate VH, various decoding strategies\nhave been proposed to enhance visual information. However, many of these\nmethods may require secondary decoding and rollback, which significantly\nreduces inference speed. In this work, we propose an efficient plug-and-play\ndecoding algorithm via Visual-Aware Sparsification (VASparse) from the\nperspective of token sparsity for mitigating VH. VASparse is inspired by\nempirical observations: (1) the sparse activation of attention in LVLMs, and\n(2) visual-agnostic tokens sparsification exacerbates VH. Based on these\ninsights, we propose a novel token sparsification strategy that balances\nefficiency and trustworthiness. Specifically, VASparse implements a\nvisual-aware token selection strategy during decoding to reduce redundant\ntokens while preserving visual context effectively. Additionally, we\ninnovatively introduce a sparse-based visual contrastive decoding method to\nrecalibrate the distribution of hallucinated outputs without the time overhead\nassociated with secondary decoding. Subsequently, VASparse recalibrates\nattention scores to penalize attention sinking of LVLMs towards text tokens.\nExtensive experiments across four popular benchmarks confirm the effectiveness\nof VASparse in mitigating VH across different LVLM families without requiring\nadditional training or post-processing. Impressively, VASparse achieves\nstate-of-the-art performance for mitigating VH while maintaining competitive\ndecoding speed. Code is available at\nhttps://github.com/mengchuang123/VASparse-github.\n","authors":["Xianwei Zhuang","Zhihong Zhu","Yuxin Xie","Liming Liang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2501.06553v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.17193v1","updated":"2025-03-21T14:42:31Z","published":"2025-03-21T14:42:31Z","title":"MSCA-Net:Multi-Scale Context Aggregation Network for Infrared Small\n  Target Detection","summary":"  Detecting infrared small targets in complex backgrounds remains a challenging\ntask because of the low contrast and high noise levels inherent in infrared\nimages. These factors often lead to the loss of crucial details during feature\nextraction. Moreover, existing detection methods have limitations in adequately\nintegrating global and local information, which constrains the efficiency and\naccuracy of infrared small target detection. To address these challenges, this\npaper proposes a novel network architecture named MSCA-Net, which integrates\nthree key components: Multi-Scale Enhanced Detection Attention\nmechanism(MSEDA), Positional Convolutional Block Attention Module (PCBAM), and\nChannel Aggregation Block (CAB). Specifically, MSEDA employs a multi-scale\nfeature fusion attention mechanism to adaptively aggregate information across\ndifferent scales, enriching feature representation. PCBAM captures the\ncorrelation between global and local features through a correlation\nmatrix-based strategy, enabling deep feature interaction. Moreover, CAB\nredistributes input feature channels, facilitating the efficient transmission\nof beneficial features and further enhancing the model detection capability in\ncomplex backgrounds. The experimental results demonstrate that MSCA-Net\nachieves outstanding small target detection performance in complex backgrounds.\nSpecifically, it attains mIoU scores of 78.43\\%, 94.56\\%, and 67.08\\% on the\nNUAA-SIRST, NUDT-SIRST, and IRTSD-1K datasets, respectively, underscoring its\neffectiveness and strong potential for real-world applications.\n","authors":["Xiaojin Lu","Taoran yue","Jiaxi cai","Shibing Chu"],"pdf_url":"https://arxiv.org/pdf/2503.17193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20109v2","updated":"2025-03-21T14:36:09Z","published":"2024-10-26T07:37:43Z","title":"GiVE: Guiding Visual Encoder to Perceive Overlooked Information","summary":"  Multimodal Large Language Models have advanced AI in applications like\ntext-to-video generation and visual question answering. These models rely on\nvisual encoders to convert non-text data into vectors, but current encoders\neither lack semantic alignment or overlook non-salient objects. We propose the\nGuiding Visual Encoder to Perceive Overlooked Information (GiVE) approach. GiVE\nenhances visual representation with an Attention-Guided Adapter (AG-Adapter)\nmodule and an Object-focused Visual Semantic Learning module. These incorporate\nthree novel loss terms: Object-focused Image-Text Contrast (OITC) loss,\nObject-focused Image-Image Contrast (OIIC) loss, and Object-focused Image\nDiscrimination (OID) loss, improving object consideration, retrieval accuracy,\nand comprehensiveness. Our contributions include dynamic visual focus\nadjustment, novel loss functions to enhance object retrieval, and the\nMulti-Object Instruction (MOInst) dataset. Experiments show our approach\nachieves state-of-the-art performance.\n","authors":["Junjie Li","Jianghong Ma","Xiaofeng Zhang","Yuhang Li","Jianyang Shi"],"pdf_url":"https://arxiv.org/pdf/2410.20109v2.pdf","comment":"This paper was accepted by ICME 2025"},{"id":"http://arxiv.org/abs/2503.17184v1","updated":"2025-03-21T14:31:33Z","published":"2025-03-21T14:31:33Z","title":"D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake\n  Detection","summary":"  Deepfake detection is crucial for curbing the harm it causes to society.\nHowever, current Deepfake detection methods fail to thoroughly explore artifact\ninformation across different domains due to insufficient intrinsic\ninteractions. These interactions refer to the fusion and coordination after\nfeature extraction processes across different domains, which are crucial for\nrecognizing complex forgery clues. Focusing on more generalized Deepfake\ndetection, in this work, we introduce a novel bi-directional attention module\nto capture the local positional information of artifact clues from the spatial\ndomain. This enables accurate artifact localization, thus addressing the coarse\nprocessing with artifact features. To further address the limitation that the\nproposed bi-directional attention module may not well capture global subtle\nforgery information in the artifact feature (e.g., textures or edges), we\nemploy a fine-grained frequency attention module in the frequency domain. By\ndoing so, we can obtain high-frequency information in the fine-grained\nfeatures, which contains the global and subtle forgery information. Although\nthese features from the diverse domains can be effectively and independently\nimproved, fusing them directly does not effectively improve the detection\nperformance. Therefore, we propose a feature superposition strategy that\ncomplements information from spatial and frequency domains. This strategy turns\nthe feature components into the form of wave-like tokens, which are updated\nbased on their phase, such that the distinctions between authentic and artifact\nfeatures can be amplified. Our method demonstrates significant improvements\nover state-of-the-art (SOTA) methods on five public Deepfake datasets in\ncapturing abnormalities across different manipulated operations and real-life.\n","authors":["Xueqi Qiu","Xingyu Miao","Fan Wan","Haoran Duan","Tejal Shah","Varun Ojhab","Yang Longa","Rajiv Ranjan"],"pdf_url":"https://arxiv.org/pdf/2503.17184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17182v1","updated":"2025-03-21T14:29:42Z","published":"2025-03-21T14:29:42Z","title":"Radar-Guided Polynomial Fitting for Metric Depth Estimation","summary":"  We propose PolyRad, a novel radar-guided depth estimation method that\nintroduces polynomial fitting to transform scaleless depth predictions from\npretrained monocular depth estimation (MDE) models into metric depth maps.\nUnlike existing approaches that rely on complex architectures or expensive\nsensors, our method is grounded in a simple yet fundamental insight: using\npolynomial coefficients predicted from cheap, ubiquitous radar data to\nadaptively adjust depth predictions non-uniformly across depth ranges. Although\nMDE models often infer reasonably accurate local depth structure within each\nobject or local region, they may misalign these regions relative to one\nanother, making a linear scale-and-shift transformation insufficient given\nthree or more of these regions. In contrast, PolyRad generalizes beyond linear\ntransformations and is able to correct such misalignments by introducing\ninflection points. Importantly, our polynomial fitting framework preserves\nstructural consistency through a novel training objective that enforces\nmonotonicity via first-derivative regularization. PolyRad achieves\nstate-of-the-art performance on the nuScenes, ZJU-4DRadarCam, and View-of-Delft\ndatasets, outperforming existing methods by 30.3% in MAE and 37.2% in RMSE.\n","authors":["Patrick Rim","Hyoungseob Park","Vadim Ezhov","Jeffrey Moon","Alex Wong"],"pdf_url":"https://arxiv.org/pdf/2503.17182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17175v1","updated":"2025-03-21T14:24:07Z","published":"2025-03-21T14:24:07Z","title":"Which2comm: An Efficient Collaborative Perception Framework for 3D\n  Object Detection","summary":"  Collaborative perception allows real-time inter-agent information exchange\nand thus offers invaluable opportunities to enhance the perception capabilities\nof individual agents. However, limited communication bandwidth in practical\nscenarios restricts the inter-agent data transmission volume, consequently\nresulting in performance declines in collaborative perception systems. This\nimplies a trade-off between perception performance and communication cost. To\naddress this issue, we propose Which2comm, a novel multi-agent 3D object\ndetection framework leveraging object-level sparse features. By integrating\nsemantic information of objects into 3D object detection boxes, we introduce\nsemantic detection boxes (SemDBs). Innovatively transmitting these\ninformation-rich object-level sparse features among agents not only\nsignificantly reduces the demanding communication volume, but also improves 3D\nobject detection performance. Specifically, a fully sparse network is\nconstructed to extract SemDBs from individual agents; a temporal fusion\napproach with a relative temporal encoding mechanism is utilized to obtain the\ncomprehensive spatiotemporal features. Extensive experiments on the V2XSet and\nOPV2V datasets demonstrate that Which2comm consistently outperforms other\nstate-of-the-art methods on both perception performance and communication cost,\nexhibiting better robustness to real-world latency. These results present that\nfor multi-agent collaborative 3D object detection, transmitting only\nobject-level sparse features is sufficient to achieve high-precision and robust\nperformance.\n","authors":["Duanrui Yu","Jing You","Xin Pei","Anqi Qu","Dingyu Wang","Shaocheng Jia"],"pdf_url":"https://arxiv.org/pdf/2503.17175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07411v2","updated":"2025-03-21T14:21:30Z","published":"2025-02-11T09:45:06Z","title":"EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering","summary":"  We introduce EgoTextVQA, a novel and rigorously constructed benchmark for\negocentric QA assistance involving scene text. EgoTextVQA contains 1.5K\nego-view videos and 7K scene-text aware questions that reflect real user needs\nin outdoor driving and indoor house-keeping activities. The questions are\ndesigned to elicit identification and reasoning on scene text in an egocentric\nand dynamic environment. With EgoTextVQA, we comprehensively evaluate 10\nprominent multimodal large language models. Currently, all models struggle, and\nthe best results (Gemini 1.5 Pro) are around 33\\% accuracy, highlighting the\nsevere deficiency of these techniques in egocentric QA assistance. Our further\ninvestigations suggest that precise temporal grounding and multi-frame\nreasoning, along with high resolution and auxiliary scene-text inputs, are key\nfor better performance. With thorough analyses and heuristic suggestions, we\nhope EgoTextVQA can serve as a solid testbed for research in egocentric\nscene-text QA assistance. Our dataset is released at:\nhttps://github.com/zhousheng97/EgoTextVQA.\n","authors":["Sheng Zhou","Junbin Xiao","Qingyun Li","Yicong Li","Xun Yang","Dan Guo","Meng Wang","Tat-Seng Chua","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2502.07411v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.17168v1","updated":"2025-03-21T14:17:02Z","published":"2025-03-21T14:17:02Z","title":"Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based\n  Object Detection Systems for Autonomous Driving","summary":"  Light Detection and Ranging (LiDAR) is an essential sensor technology for\nautonomous driving as it can capture high-resolution 3D data. As 3D object\ndetection systems (OD) can interpret such point cloud data, they play a key\nrole in the driving decisions of autonomous vehicles. Consequently, such 3D OD\nmust be robust against all types of perturbations and must therefore be\nextensively tested. One approach is the use of adversarial examples, which are\nsmall, sometimes sophisticated perturbations in the input data that change,\ni.e., falsify, the prediction of the OD. These perturbations are carefully\ndesigned based on the weaknesses of the OD. The robustness of the OD cannot be\nquantified with adversarial examples in general, because if the OD is\nvulnerable to a given attack, it is unclear whether this is due to the\nrobustness of the OD or whether the attack algorithm produces particularly\nstrong adversarial examples. The contribution of this work is Hi-ALPS --\nHierarchical Adversarial-example-based LiDAR Perturbation Level System, where\nhigher robustness of the OD is required to withstand the perturbations as the\nperturbation levels increase. In doing so, the Hi-ALPS levels successively\nimplement a heuristic followed by established adversarial example approaches.\nIn a series of comprehensive experiments using Hi-ALPS, we quantify the\nrobustness of six state-of-the-art 3D OD under different types of\nperturbations. The results of the experiments show that none of the OD is\nrobust against all Hi-ALPS levels; an important factor for the ranking is that\nhuman observers can still correctly recognize the perturbed objects, as the\nrespective perturbations are small. To increase the robustness of the OD, we\ndiscuss the applicability of state-of-the-art countermeasures. In addition, we\nderive further suggestions for countermeasures based on our experimental\nresults.\n","authors":["Alexandra Arzberger","Ramin Tavakoli Kolagari"],"pdf_url":"https://arxiv.org/pdf/2503.17168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14897v2","updated":"2025-03-21T14:15:36Z","published":"2025-03-19T04:48:16Z","title":"When Domain Generalization meets Generalized Category Discovery: An\n  Adaptive Task-Arithmetic Driven Approach","summary":"  Generalized Class Discovery (GCD) clusters base and novel classes in a target\ndomain using supervision from a source domain with only base classes. Current\nmethods often falter with distribution shifts and typically require access to\ntarget data during training, which can sometimes be impractical. To address\nthis issue, we introduce the novel paradigm of Domain Generalization in GCD\n(DG-GCD), where only source data is available for training, while the target\ndomain, with a distinct data distribution, remains unseen until inference. To\nthis end, our solution, DG2CD-Net, aims to construct a domain-independent,\ndiscriminative embedding space for GCD. The core innovation is an episodic\ntraining strategy that enhances cross-domain generalization by adapting a base\nmodel on tasks derived from source and synthetic domains generated by a\nfoundation model. Each episode focuses on a cross-domain GCD task, diversifying\ntask setups over episodes and combining open-set domain adaptation with a novel\nmargin loss and representation learning for optimizing the feature space\nprogressively. To capture the effects of fine-tuning on the base model, we\nextend task arithmetic by adaptively weighting the local task vectors\nconcerning the fine-tuned models based on their GCD performance on a validation\ndistribution. This episodic update mechanism boosts the adaptability of the\nbase model to unseen targets. Experiments across three datasets confirm that\nDG2CD-Net outperforms existing GCD methods customized for DG-GCD.\n","authors":["Vaibhav Rathore","Shubhranil B","Saikat Dutta","Sarthak Mehrotra","Zsolt Kira","Biplab Banerjee"],"pdf_url":"https://arxiv.org/pdf/2503.14897v2.pdf","comment":"Accepted at CVPR 2025 (Main Conference)"},{"id":"http://arxiv.org/abs/2402.10079v2","updated":"2025-03-21T14:13:38Z","published":"2024-01-29T16:56:17Z","title":"Data-driven Camera and Lidar Simulation Models for Autonomous Driving: A\n  Review from Generative Models to Volume Renderers","summary":"  Perception sensors, particularly camera and Lidar, are key elements of\nAutonomous Driving Systems (ADS) that enable them to comprehend their\nsurroundings to informed driving and control decisions. Therefore, developing\nrealistic simulation models for these sensors is essential for conducting\neffective simulation-based testing of ADS. Moreover, the rise of deep\nlearning-based perception models has increased the utility of sensor simulation\nmodels for synthesising diverse training datasets. The traditional sensor\nsimulation models rely on computationally expensive physics-based algorithms,\nspecifically in complex systems such as ADS. Hence, the current potential\nresides in data-driven approaches, fuelled by the exceptional performance of\ndeep generative models in capturing high-dimensional data distribution and\nvolume renderers in accurately representing scenes. This paper reviews the\ncurrent state-of-the-art data-driven camera and Lidar simulation models and\ntheir evaluation methods. It explores a spectrum of models from the novel\nperspective of generative models and volume renderers. Generative models are\ndiscussed in terms of their input-output types, while volume renderers are\ncategorised based on their input encoding. Finally, the paper illustrates\ncommonly used evaluation techniques for assessing sensor simulation models and\nhighlights the existing research gaps in the area.\n","authors":["Hamed Haghighi","Xiaomeng Wang","Hao Jing","Mehrdad Dianati"],"pdf_url":"https://arxiv.org/pdf/2402.10079v2.pdf","comment":"To be published in IEEE Transactions on Intelligent Vehicles"},{"id":"http://arxiv.org/abs/2503.17162v1","updated":"2025-03-21T14:06:23Z","published":"2025-03-21T14:06:23Z","title":"CoRLD: Contrastive Representation Learning Of Deformable Shapes In\n  Images","summary":"  Deformable shape representations, parameterized by deformations relative to a\ngiven template, have proven effective for improved image analysis tasks.\nHowever, their broader applicability is hindered by two major challenges.\nFirst, existing methods mainly rely on a known template during testing, which\nis impractical and limits flexibility. Second, they often struggle to capture\nfine-grained, voxel-level distinctions between similar shapes (e.g., anatomical\nvariations among healthy individuals, those with mild cognitive impairment, and\ndiseased states). To address these limitations, we propose a novel framework -\nContrastive Representation Learning of Deformable shapes (CoRLD) in learned\ndeformation spaces and demonstrate its effectiveness in the context of image\nclassification. Our CoRLD leverages a class-aware contrastive supervised\nlearning objective in latent deformation spaces, promoting proximity among\nrepresentations of similar classes while ensuring separation of dissimilar\ngroups. In contrast to previous deep learning networks that require a reference\nimage as input to predict deformation changes, our approach eliminates this\ndependency. Instead, template images are utilized solely as ground truth in the\nloss function during the training process, making our model more flexible and\ngeneralizable to a wide range of medical applications. We validate CoRLD on\ndiverse datasets, including real brain magnetic resonance imaging (MRIs) and\nadrenal shapes derived from computed tomography (CT) scans. Experimental\nresults show that our model effectively extracts deformable shape features,\nwhich can be easily integrated with existing classifiers to substantially boost\nthe classification accuracy. Our code is available at GitHub.\n","authors":["Tonmoy Hossain ana Miaomiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.17162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17155v1","updated":"2025-03-21T13:58:49Z","published":"2025-03-21T13:58:49Z","title":"D2C: Unlocking the Potential of Continuous Autoregressive Image\n  Generation with Discrete Tokens","summary":"  In the domain of image generation, latent-based generative models occupy a\ndominant status; however, these models rely heavily on image tokenizer. To meet\nmodeling requirements, autoregressive models possessing the characteristics of\nscalability and flexibility embrace a discrete-valued tokenizer, but face the\nchallenge of poor image generation quality. In contrast, diffusion models take\nadvantage of the continuous-valued tokenizer to achieve better generation\nquality but are subject to low efficiency and complexity. The existing hybrid\nmodels are mainly to compensate for information loss and simplify the diffusion\nlearning process. The potential of merging discrete-valued and\ncontinuous-valued tokens in the field of image generation has not yet been\nexplored. In this paper, we propose D2C, a novel two-stage method to enhance\nmodel generation capacity. In the first stage, the discrete-valued tokens\nrepresenting coarse-grained image features are sampled by employing a small\ndiscrete-valued generator. Then in the second stage, the continuous-valued\ntokens representing fine-grained image features are learned conditioned on the\ndiscrete token sequence. In addition, we design two kinds of fusion modules for\nseamless interaction. On the ImageNet-256 benchmark, extensive experiment\nresults validate that our model achieves superior performance compared with\nseveral continuous-valued and discrete-valued generative models on the\nclass-conditional image generation tasks.\n","authors":["Panpan Wang","Liqiang Niu","Fandong Meng","Jinan Xu","Yufeng Chen","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.17155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17064v3","updated":"2025-03-21T13:58:47Z","published":"2024-08-30T07:49:35Z","title":"Instant Adversarial Purification with Adversarial Consistency\n  Distillation","summary":"  Neural networks have revolutionized numerous fields with their exceptional\nperformance, yet they remain susceptible to adversarial attacks through subtle\nperturbations. While diffusion-based purification methods like DiffPure offer\npromising defense mechanisms, their computational overhead presents a\nsignificant practical limitation. In this paper, we introduce One Step Control\nPurification (OSCP), a novel defense framework that achieves robust adversarial\npurification in a single Neural Function Evaluation (NFE) within diffusion\nmodels. We propose Gaussian Adversarial Noise Distillation (GAND) as the\ndistillation objective and Controlled Adversarial Purification (CAP) as the\ninference pipeline, which makes OSCP demonstrate remarkable efficiency while\nmaintaining defense efficacy. Our proposed GAND addresses a fundamental tension\nbetween consistency distillation and adversarial perturbation, bridging the gap\nbetween natural and adversarial manifolds in the latent space, while remaining\ncomputationally efficient through Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA, eliminating the high computational budget request from\nfull parameter fine-tuning. The CAP guides the purification process through the\nunlearnable edge detection operator calculated by the input image as an extra\nprompt, effectively preventing the purified images from deviating from their\noriginal appearance when large purification steps are used. Our experimental\nresults on ImageNet showcase OSCP's superior performance, achieving a 74.19%\ndefense success rate with merely 0.1s per purification -- a 100-fold speedup\ncompared to conventional approaches.\n","authors":["Chun Tong Lei","Hon Ming Yam","Zhongliang Guo","Yifei Qian","Chun Pong Lau"],"pdf_url":"https://arxiv.org/pdf/2408.17064v3.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.15300v2","updated":"2025-03-21T13:58:31Z","published":"2025-03-19T15:22:23Z","title":"SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes","summary":"  Semantic segmentation in urban scene analysis has mainly focused on images or\npoint clouds, while textured meshes - offering richer spatial representation -\nremain underexplored. This paper introduces SUM Parts, the first large-scale\ndataset for urban textured meshes with part-level semantic labels, covering\nabout 2.5 km2 with 21 classes. The dataset was created using our own annotation\ntool, which supports both face- and texture-based annotations with efficient\ninteractive selection. We also provide a comprehensive evaluation of 3D\nsemantic segmentation and interactive annotation methods on this dataset. Our\nproject page is available at https://tudelft3d.github.io/SUMParts/.\n","authors":["Weixiao Gao","Liangliang Nan","Hugo Ledoux"],"pdf_url":"https://arxiv.org/pdf/2503.15300v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.17153v1","updated":"2025-03-21T13:58:08Z","published":"2025-03-21T13:58:08Z","title":"Enhancing Steering Estimation with Semantic-Aware GNNs","summary":"  Steering estimation is a critical task in autonomous driving, traditionally\nrelying on 2D image-based models. In this work, we explore the advantages of\nincorporating 3D spatial information through hybrid architectures that combine\n3D neural network models with recurrent neural networks (RNNs) for temporal\nmodeling, using LiDAR-based point clouds as input. We systematically evaluate\nfour hybrid 3D models, all of which outperform the 2D-only baseline, with the\nGraph Neural Network (GNN) - RNN model yielding the best results.\n  To reduce reliance on LiDAR, we leverage a pretrained unified model to\nestimate depth from monocular images, reconstructing pseudo-3D point clouds. We\nthen adapt the GNN-RNN model, originally designed for LiDAR-based point clouds,\nto work with these pseudo-3D representations, achieving comparable or even\nsuperior performance compared to the LiDAR-based model. Additionally, the\nunified model provides semantic labels for each point, enabling a more\nstructured scene representation. To further optimize graph construction, we\nintroduce an efficient connectivity strategy where connections are\npredominantly formed between points of the same semantic class, with only 20\\%\nof inter-class connections retained. This targeted approach reduces graph\ncomplexity and computational cost while preserving critical spatial\nrelationships.\n  Finally, we validate our approach on the KITTI dataset, achieving a 71%\nimprovement over 2D-only models. Our findings highlight the advantages of 3D\nspatial information and efficient graph construction for steering estimation,\nwhile maintaining the cost-effectiveness of monocular images and avoiding the\nexpense of LiDAR-based systems.\n","authors":["Fouad Makiyeh","Huy-Dung Nguyen","Patrick Chareyre","Ramin Hasani","Marc Blanchon","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2503.17153v1.pdf","comment":"Submitted to ICCV 2025"},{"id":"http://arxiv.org/abs/2503.09949v2","updated":"2025-03-21T13:53:32Z","published":"2025-03-13T01:52:27Z","title":"UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?","summary":"  With the rapid growth of video generative models (VGMs), it is essential to\ndevelop reliable and comprehensive automatic metrics for AI-generated videos\n(AIGVs). Existing methods either use off-the-shelf models optimized for other\ntasks or rely on human assessment data to train specialized evaluators. These\napproaches are constrained to specific evaluation aspects and are difficult to\nscale with the increasing demands for finer-grained and more comprehensive\nevaluations. To address this issue, this work investigates the feasibility of\nusing multimodal large language models (MLLMs) as a unified evaluator for\nAIGVs, leveraging their strong visual perception and language understanding\ncapabilities. To evaluate the performance of automatic metrics in unified AIGV\nevaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects\nvideos generated by state-of-the-art VGMs and provides pairwise human\npreference annotations across 15 evaluation aspects. Using UVE-Bench, we\nextensively evaluate 16 MLLMs. Our empirical results suggest that while\nadvanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human\nevaluators, they demonstrate promising ability in unified AIGV evaluation,\nsignificantly surpassing existing specialized evaluation methods. Additionally,\nwe conduct an in-depth analysis of key design choices that impact the\nperformance of MLLM-driven evaluators, offering valuable insights for future\nresearch on AIGV evaluation. The code is available at\nhttps://github.com/bytedance/UVE.\n","authors":["Yuanxin Liu","Rui Zhu","Shuhuai Ren","Jiacong Wang","Haoyuan Guo","Xu Sun","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.09949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12235v3","updated":"2025-03-21T13:49:32Z","published":"2025-01-21T15:58:16Z","title":"DLEN: Dual Branch of Transformer for Low-Light Image Enhancement in Dual\n  Domains","summary":"  Low-light image enhancement (LLE) aims to improve the visual quality of\nimages captured in poorly lit conditions, which often suffer from low\nbrightness, low contrast, noise, and color distortions. These issues hinder the\nperformance of computer vision tasks such as object detection, facial\nrecognition, and autonomous driving.Traditional enhancement techniques, such as\nmulti-scale fusion and histogram equalization, fail to preserve fine details\nand often struggle with maintaining the natural appearance of enhanced images\nunder complex lighting conditions. Although the Retinex theory provides a\nfoundation for image decomposition, it often amplifies noise, leading to\nsuboptimal image quality. In this paper, we propose the Dual Light Enhance\nNetwork (DLEN), a novel architecture that incorporates two distinct attention\nmechanisms, considering both spatial and frequency domains. Our model\nintroduces a learnable wavelet transform module in the illumination estimation\nphase, preserving high- and low-frequency components to enhance edge and\ntexture details. Additionally, we design a dual-branch structure that leverages\nthe power of the Transformer architecture to enhance both the illumination and\nstructural components of the image.Through extensive experiments, our model\noutperforms state-of-the-art methods on standard benchmarks.Code is available\nhere: https://github.com/LaLaLoXX/DLEN\n","authors":["Junyu Xia","Jiesong Bai","Yihang Dong"],"pdf_url":"https://arxiv.org/pdf/2501.12235v3.pdf","comment":"9 pages and 6 figures"},{"id":"http://arxiv.org/abs/2503.17142v1","updated":"2025-03-21T13:46:53Z","published":"2025-03-21T13:46:53Z","title":"Not Only Text: Exploring Compositionality of Visual Representations in\n  Vision-Language Models","summary":"  Vision-Language Models (VLMs) learn a shared feature space for text and\nimages, enabling the comparison of inputs of different modalities. While prior\nworks demonstrated that VLMs organize natural language representations into\nregular structures encoding composite meanings, it remains unclear if\ncompositional patterns also emerge in the visual embedding space. In this work,\nwe investigate compositionality in the image domain, where the analysis of\ncompositional properties is challenged by noise and sparsity of visual data. We\naddress these problems and propose a framework, called Geodesically\nDecomposable Embeddings (GDE), that approximates image representations with\ngeometry-aware compositional structures in the latent space. We demonstrate\nthat visual embeddings of pre-trained VLMs exhibit a compositional arrangement,\nand evaluate the effectiveness of this property in the tasks of compositional\nclassification and group robustness. GDE achieves stronger performance in\ncompositional classification compared to its counterpart method that assumes\nlinear geometry of the latent space. Notably, it is particularly effective for\ngroup robustness, where we achieve higher results than task-specific solutions.\nOur results indicate that VLMs can automatically develop a human-like form of\ncompositional reasoning in the visual domain, making their underlying processes\nmore interpretable. Code is available at\nhttps://github.com/BerasiDavide/vlm_image_compositionality.\n","authors":["Davide Berasi","Matteo Farina","Massimiliano Mancini","Elisa Ricci","Nicola Strisciuglio"],"pdf_url":"https://arxiv.org/pdf/2503.17142v1.pdf","comment":"Camera-ready version for CVPR 2025 (with Supp.Mat.)"},{"id":"http://arxiv.org/abs/2503.06369v3","updated":"2025-03-21T13:45:43Z","published":"2025-03-09T00:37:43Z","title":"Spectral State Space Model for Rotation-Invariant Visual Representation\n  Learning","summary":"  State Space Models (SSMs) have recently emerged as an alternative to Vision\nTransformers (ViTs) due to their unique ability of modeling global\nrelationships with linear complexity. SSMs are specifically designed to capture\nspatially proximate relationships of image patches. However, they fail to\nidentify relationships between conceptually related yet not adjacent patches.\nThis limitation arises from the non-causal nature of image data, which lacks\ninherent directional relationships. Additionally, current vision-based SSMs are\nhighly sensitive to transformations such as rotation. Their predefined scanning\ndirections depend on the original image orientation, which can cause the model\nto produce inconsistent patch-processing sequences after rotation. To address\nthese limitations, we introduce Spectral VMamba, a novel approach that\neffectively captures the global structure within an image by leveraging\nspectral information derived from the graph Laplacian of image patches. Through\nspectral decomposition, our approach encodes patch relationships independently\nof image orientation, achieving rotation invariance with the aid of our\nRotational Feature Normalizer (RFN) module. Our experiments on classification\ntasks show that Spectral VMamba outperforms the leading SSM models in vision,\nsuch as VMamba, while maintaining invariance to rotations and a providing a\nsimilar runtime efficiency.\n","authors":["Sahar Dastani","Ali Bahri","Moslem Yazdanpanah","Mehrdad Noori","David Osowiechi","Gustavo Adolfo Vargas Hakim","Farzad Beizaee","Milad Cheraghalikhani","Arnab Kumar Mondal","Herve Lombaert","Christian Desrosiers"],"pdf_url":"https://arxiv.org/pdf/2503.06369v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01003v5","updated":"2025-03-21T13:38:56Z","published":"2024-07-01T06:35:53Z","title":"Embedded Visual Prompt Tuning","summary":"  Foundation models pre-trained on large-scale data have been widely witnessed\nto achieve success in various natural imaging downstream tasks.\nParameter-efficient fine-tuning (PEFT) methods aim to adapt foundation models\nto new domains by updating only a small portion of parameters in order to\nreduce computational overhead. However, the effectiveness of these PEFT\nmethods, especially in cross-domain few-shot scenarios, e.g., medical image\nanalysis, has not been fully explored. In this work, we facilitate the study of\nthe performance of PEFT when adapting foundation models to medical image\nclassification tasks. Furthermore, to alleviate the limitations of prompt\nintroducing ways and approximation capabilities on Transformer architectures of\nmainstream prompt tuning methods, we propose the Embedded Prompt Tuning (EPT)\nmethod by embedding prompt tokens into the expanded channels. We also find that\nthere are anomalies in the feature space distribution of foundation models\nduring pre-training process, and prompt tuning can help mitigate this negative\nimpact. To explain this phenomenon, we also introduce a novel perspective to\nunderstand prompt tuning: Prompt tuning is a distribution calibrator. And we\nsupport it by analyzing patch-wise scaling and feature separation operations\ncontained in EPT. Our experiments show that EPT outperforms several\nstate-of-the-art fine-tuning methods by a significant margin on few-shot\nmedical image classification tasks, and completes the fine-tuning process\nwithin highly competitive time, indicating EPT is an effective PEFT method. The\nsource code is available at github.com/zuwenqiang/EPT.\n","authors":["Wenqiang Zu","Shenghao Xie","Qing Zhao","Guoqi Li","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2407.01003v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17132v1","updated":"2025-03-21T13:31:16Z","published":"2025-03-21T13:31:16Z","title":"Temporal-Guided Spiking Neural Networks for Event-Based Human Action\n  Recognition","summary":"  This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.\n","authors":["Siyuan Yang","Shilin Lu","Shizheng Wang","Meng Hwa Er","Zengwei Zheng","Alex C. Kot"],"pdf_url":"https://arxiv.org/pdf/2503.17132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00876v4","updated":"2025-03-21T13:30:33Z","published":"2024-12-01T16:32:31Z","title":"Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification","summary":"  Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .\n","authors":["Wenxuan Huang","Zijie Zhai","Yunhang Shen","Shaosheng Cao","Fei Zhao","Xiangfeng Xu","Zheyu Ye","Yao Hu","Shaohui Lin"],"pdf_url":"https://arxiv.org/pdf/2412.00876v4.pdf","comment":"Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava"},{"id":"http://arxiv.org/abs/2406.09782v2","updated":"2025-03-21T13:23:31Z","published":"2024-06-14T07:31:20Z","title":"Self-supervised Monocular Depth Estimation Based on Hierarchical\n  Feature-Guided Diffusion","summary":"  Self-supervised monocular depth estimation has received widespread attention\nbecause of its capability to train without ground truth. In real-world\nscenarios, the images may be blurry or noisy due to the influence of weather\nconditions and inherent limitations of the camera. Therefore, it is\nparticularly important to develop a robust depth estimation model. Benefiting\nfrom the training strategies of generative networks, generative-based methods\noften exhibit enhanced robustness. In light of this, we employ the\ngenerative-based diffusion model with a unique denoising training process for\nself-supervised monocular depth estimation. Additionally, to further enhance\nthe robustness of the diffusion model, we probe into the influence of\nperturbations on image features and propose a hierarchical feature-guided\ndenoising module. Furthermore, we explore the implicit depth within\nreprojection and design an implicit depth consistency loss. This loss function\nis not interfered by the other subnetwork, which can be targeted to constrain\nthe depth estimation network and ensure the scale consistency of depth within a\nvideo sequence. We conduct experiments on the KITTI and Make3D datasets. The\nresults indicate that our approach stands out among generative-based models,\nwhile also showcasing remarkable robustness.\n","authors":["Runze Liu","Dongchen Zhu","Guanghui Zhang","Lei Wang","Jiamao Li"],"pdf_url":"https://arxiv.org/pdf/2406.09782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17122v1","updated":"2025-03-21T13:17:28Z","published":"2025-03-21T13:17:28Z","title":"R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User\n  Focused Roadside Perception","summary":"  In autonomous driving, the integration of roadside perception systems is\nessential for overcoming occlusion challenges and enhancing the safety of\nVulnerable Road Users (VRUs). While LiDAR and visual (RGB) sensors are commonly\nused, thermal imaging remains underrepresented in datasets, despite its\nacknowledged advantages for VRU detection in extreme lighting conditions. In\nthis paper, we present R-LiViT, the first dataset to combine LiDAR, RGB, and\nthermal imaging from a roadside perspective, with a strong focus on VRUs.\nR-LiViT captures three intersections during both day and night, ensuring a\ndiverse dataset. It includes 10,000 LiDAR frames and 2,400 temporally and\nspatially aligned RGB and thermal images across over 150 traffic scenarios,\nwith 6 and 8 annotated classes respectively, providing a comprehensive resource\nfor tasks such as object detection and tracking. The dataset1 and the code for\nreproducing our evaluation results2 are made publicly available.\n","authors":["Jonas Mirlach","Lei Wan","Andreas Wiedholz","Hannan Ejaz Keen","Andreas Eich"],"pdf_url":"https://arxiv.org/pdf/2503.17122v1.pdf","comment":"10 pages, 7 figures, submitted to ICCV2025"},{"id":"http://arxiv.org/abs/2503.17117v1","updated":"2025-03-21T13:07:55Z","published":"2025-03-21T13:07:55Z","title":"A New Statistical Model of Star Speckles for Learning to Detect and\n  Characterize Exoplanets in Direct Imaging Observations","summary":"  The search for exoplanets is an active field in astronomy, with direct\nimaging as one of the most challenging methods due to faint exoplanet signals\nburied within stronger residual starlight. Successful detection requires\nadvanced image processing to separate the exoplanet signal from this nuisance\ncomponent. This paper presents a novel statistical model that captures nuisance\nfluctuations using a multi-scale approach, leveraging problem symmetries and a\njoint spectral channel representation grounded in physical principles. Our\nmodel integrates into an interpretable, end-to-end learnable framework for\nsimultaneous exoplanet detection and flux estimation. The proposed algorithm is\nevaluated against the state of the art using datasets from the SPHERE\ninstrument operating at the Very Large Telescope (VLT). It significantly\nimproves the precision-recall trade-off, notably on challenging datasets that\nare otherwise unusable by astronomers. The proposed approach is computationally\nefficient, robust to varying data quality, and well suited for large-scale\nobservational surveys.\n","authors":["Théo Bodrito","Olivier Flasseur","Julien Mairal","Jean Ponce","Maud Langlois","Anne-Marie Lagrange"],"pdf_url":"https://arxiv.org/pdf/2503.17117v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2412.04077v2","updated":"2025-03-21T13:03:59Z","published":"2024-12-05T11:17:57Z","title":"SoMA: Singular Value Decomposed Minor Components Adaptation for Domain\n  Generalizable Representation Learning","summary":"  Domain generalization (DG) aims to adapt a model using one or multiple source\ndomains to ensure robust performance in unseen target domains. Recently,\nParameter-Efficient Fine-Tuning (PEFT) of foundation models has shown promising\nresults in the context of DG problem. Nevertheless, existing PEFT methods still\nstruggle to strike a balance between preserving generalizable components of the\npre-trained model and learning task-specific features. To gain insights into\nthe distribution of generalizable components, we begin by analyzing the\npre-trained weights through the lens of singular value decomposition. Building\non these insights, we introduce Singular Value Decomposed Minor Components\nAdaptation (SoMA), an approach that selectively tunes minor singular components\nwhile keeping the residual parts frozen. SoMA effectively retains the\ngeneralization ability of the pre-trained model while efficiently acquiring\ntask-specific skills. Moreover, we freeze domain-generalizable blocks and\nemploy an annealing weight decay strategy, thereby achieving an optimal balance\nin the delicate trade-off between generalizability and discriminability. SoMA\nattains state-of-the-art results on multiple benchmarks that span both domain\ngeneralized semantic segmentation to domain generalized object detection. In\naddition, our methods introduce no additional inference overhead or\nregularization loss, maintain compatibility with any backbone or head, and are\ndesigned to be versatile, allowing easy integration into a wide range of tasks.\n","authors":["Seokju Yun","Seunghye Chae","Dongheon Lee","Youngmin Ro"],"pdf_url":"https://arxiv.org/pdf/2412.04077v2.pdf","comment":"CVPR 2025 Project page: https://ysj9909.github.io/SoRA.github.io/"},{"id":"http://arxiv.org/abs/2412.05276v2","updated":"2025-03-21T13:02:14Z","published":"2024-12-06T18:59:51Z","title":"Sparse autoencoders reveal selective remapping of visual concepts during\n  adaptation","summary":"  Adapting foundation models for specific purposes has become a standard\napproach to build machine learning systems for downstream applications. Yet, it\nis an open question which mechanisms take place during adaptation. Here we\ndevelop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named\nPatchSAE, to extract interpretable concepts at granular levels (e.g., shape,\ncolor, or semantics of an object) and their patch-wise spatial attributions. We\nexplore how these concepts influence the model output in downstream image\nclassification tasks and investigate how recent state-of-the-art prompt-based\nadaptation techniques change the association of model inputs to these concepts.\nWhile activations of concepts slightly change between adapted and non-adapted\nmodels, we find that the majority of gains on common adaptation tasks can be\nexplained with the existing concepts already present in the non-adapted\nfoundation model. This work provides a concrete framework to train and use SAEs\nfor Vision Transformers and provides insights into explaining adaptation\nmechanisms.\n","authors":["Hyesu Lim","Jinho Choi","Jaegul Choo","Steffen Schneider"],"pdf_url":"https://arxiv.org/pdf/2412.05276v2.pdf","comment":"Published as a conference paper at the Thirteenth International\n  Conference on Learning Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2503.17116v1","updated":"2025-03-21T13:01:07Z","published":"2025-03-21T13:01:07Z","title":"The CASTLE 2024 Dataset: Advancing the Art of Multimodal Understanding","summary":"  Egocentric video has seen increased interest in recent years, as it is used\nin a range of areas. However, most existing datasets are limited to a single\nperspective. In this paper, we present the CASTLE 2024 dataset, a multimodal\ncollection containing ego- and exo-centric (i.e., first- and third-person\nperspective) video and audio from 15 time-aligned sources, as well as other\nsensor streams and auxiliary data. The dataset was recorded by volunteer\nparticipants over four days in a fixed location and includes the point of view\nof 10 participants, with an additional 5 fixed cameras providing an exocentric\nperspective. The entire dataset contains over 600 hours of UHD video recorded\nat 50 frames per second. In contrast to other datasets, CASTLE 2024 does not\ncontain any partial censoring, such as blurred faces or distorted audio. The\ndataset is available via https://castle-dataset.github.io/.\n","authors":["Luca Rossetto","Werner Bailer","Duc-Tien Dang-Nguyen","Graham Healy","Björn Þór Jónsson","Onanong Kongmeesub","Hoang-Bao Le","Stevan Rudinac","Klaus Schöffmann","Florian Spiess","Allie Tran","Minh-Triet Tran","Quang-Linh Tran","Cathal Gurrin"],"pdf_url":"https://arxiv.org/pdf/2503.17116v1.pdf","comment":"7 pages, 6 figures, dataset available via\n  https://castle-dataset.github.io/"},{"id":"http://arxiv.org/abs/2502.20625v2","updated":"2025-03-21T12:59:59Z","published":"2025-02-28T01:09:18Z","title":"T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting","summary":"  Zero-shot object counting aims to count instances of arbitrary object\ncategories specified by text descriptions. Existing methods typically rely on\nvision-language models like CLIP, but often exhibit limited sensitivity to text\nprompts. We present T2ICount, a diffusion-based framework that leverages rich\nprior knowledge and fine-grained visual understanding from pretrained diffusion\nmodels. While one-step denoising ensures efficiency, it leads to weakened text\nsensitivity. To address this challenge, we propose a Hierarchical Semantic\nCorrection Module that progressively refines text-image feature alignment, and\na Representational Regional Coherence Loss that provides reliable supervision\nsignals by leveraging the cross-attention maps extracted from the denosing\nU-Net. Furthermore, we observe that current benchmarks mainly focus on majority\nobjects in images, potentially masking models' text sensitivity. To address\nthis, we contribute a challenging re-annotated subset of FSC147 for better\nevaluation of text-guided counting ability. Extensive experiments demonstrate\nthat our method achieves superior performance across different benchmarks. Code\nis available at https://github.com/cha15yq/T2ICount.\n","authors":["Yifei Qian","Zhongliang Guo","Bowen Deng","Chun Tong Lei","Shuai Zhao","Chun Pong Lau","Xiaopeng Hong","Michael P. Pound"],"pdf_url":"https://arxiv.org/pdf/2502.20625v2.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.13214v3","updated":"2025-03-21T12:55:38Z","published":"2025-03-17T14:24:00Z","title":"A General Adaptive Dual-level Weighting Mechanism for Remote Sensing\n  Pansharpening","summary":"  Currently, deep learning-based methods for remote sensing pansharpening have\nadvanced rapidly. However, many existing methods struggle to fully leverage\nfeature heterogeneity and redundancy, thereby limiting their effectiveness. We\nuse the covariance matrix to model the feature heterogeneity and redundancy and\npropose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW\ncaptures these correlations through the covariance matrix, which is then\nprocessed by a nonlinear function to generate weights for adjustment. Building\nupon CACW, we introduce a general adaptive dual-level weighting mechanism\n(ADWM) to address these challenges from two key perspectives, enhancing a wide\nrange of existing deep-learning methods. First, Intra-Feature Weighting (IFW)\nevaluates correlations among channels within each feature to reduce redundancy\nand enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts\ncontributions across layers based on inter-layer correlations, refining the\nfinal output. Extensive experiments demonstrate the superior performance of\nADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we\nvalidate the effectiveness of our approach through generality experiments,\nredundancy visualization, comparison experiments, key variables and complexity\nanalysis, and ablation studies. Our code is available at\nhttps://github.com/Jie-1203/ADWM.\n","authors":["Jie Huang","Haorui Chen","Jiaxuan Ren","Siran Peng","Liangjian Deng"],"pdf_url":"https://arxiv.org/pdf/2503.13214v3.pdf","comment":"This paper is accepted at the CVPR Conference on Computer Vision and\n  Pattern Recognition 2025"},{"id":"http://arxiv.org/abs/2503.17110v1","updated":"2025-03-21T12:54:18Z","published":"2025-03-21T12:54:18Z","title":"Beyond Accuracy: What Matters in Designing Well-Behaved Models?","summary":"  Deep learning has become an essential part of computer vision, with deep\nneural networks (DNNs) excelling in predictive performance. However, they often\nfall short in other critical quality dimensions, such as robustness,\ncalibration, or fairness. While existing studies have focused on a subset of\nthese quality dimensions, none have explored a more general form of\n\"well-behavedness\" of DNNs. With this work, we address this gap by\nsimultaneously studying nine different quality dimensions for image\nclassification. Through a large-scale study, we provide a bird's-eye view by\nanalyzing 326 backbone models and how different training paradigms and model\narchitectures affect the quality dimensions. We reveal various new insights\nsuch that (i) vision-language models exhibit high fairness on ImageNet-1k\nclassification and strong robustness against domain changes; (ii)\nself-supervised learning is an effective training paradigm to improve almost\nall considered quality dimensions; and (iii) the training dataset size is a\nmajor driver for most of the quality dimensions. We conclude our study by\nintroducing the QUBA score (Quality Understanding Beyond Accuracy), a novel\nmetric that ranks models across multiple dimensions of quality, enabling\ntailored recommendations based on specific user needs.\n","authors":["Robin Hesse","Doğukan Bağcı","Bernt Schiele","Simone Schaub-Meyer","Stefan Roth"],"pdf_url":"https://arxiv.org/pdf/2503.17110v1.pdf","comment":"Code: https://github.com/visinf/beyond-accuracy"},{"id":"http://arxiv.org/abs/2503.17109v1","updated":"2025-03-21T12:49:50Z","published":"2025-03-21T12:49:50Z","title":"Missing Target-Relevant Information Prediction with World Model for\n  Accurate Zero-Shot Composed Image Retrieval","summary":"  Zero-Shot Composed Image Retrieval (ZS-CIR) involves diverse tasks with a\nbroad range of visual content manipulation intent across domain, scene, object,\nand attribute. The key challenge for ZS-CIR tasks is to modify a reference\nimage according to manipulation text to accurately retrieve a target image,\nespecially when the reference image is missing essential target content. In\nthis paper, we propose a novel prediction-based mapping network, named\nPrediCIR, to adaptively predict the missing target visual content in reference\nimages in the latent space before mapping for accurate ZS-CIR. Specifically, a\nworld view generation module first constructs a source view by omitting certain\nvisual content of a target view, coupled with an action that includes the\nmanipulation intent derived from existing image-caption pairs. Then, a target\ncontent prediction module trains a world model as a predictor to adaptively\npredict the missing visual information guided by user intention in manipulating\ntext at the latent space. The two modules map an image with the predicted\nrelevant information to a pseudo-word token without extra supervision. Our\nmodel shows strong generalization ability on six ZS-CIR tasks. It obtains\nconsistent and significant performance boosts ranging from 1.73% to 4.45% over\nthe best methods and achieves new state-of-the-art results on ZS-CIR. Our code\nis available at https://github.com/Pter61/predicir.\n","authors":["Yuanmin Tang","Jing Yu","Keke Gai","Jiamin Zhuang","Gang Xiong","Gaopeng Gou","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2503.17109v1.pdf","comment":"This work has been accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.17107v1","updated":"2025-03-21T12:46:49Z","published":"2025-03-21T12:46:49Z","title":"Exploring Few-Shot Object Detection on Blood Smear Images: A Case Study\n  of Leukocytes and Schistocytes","summary":"  The detection of blood disorders often hinges upon the quantification of\nspecific blood cell types. Variations in cell counts may indicate the presence\nof pathological conditions. Thus, the significance of developing precise\nautomatic systems for blood cell enumeration is underscored. The investigation\nfocuses on a novel approach termed DE-ViT. This methodology is employed in a\nFew-Shot paradigm, wherein training relies on a limited number of images. Two\ndistinct datasets are utilised for experimental purposes: the Raabin-WBC\ndataset for Leukocyte detection and a local dataset for Schistocyte\nidentification. In addition to the DE-ViT model, two baseline models, Faster\nR-CNN 50 and Faster R-CNN X 101, are employed, with their outcomes being\ncompared against those of the proposed model. While DE-ViT has demonstrated\nstate-of-the-art performance on the COCO and LVIS datasets, both baseline\nmodels surpassed its performance on the Raabin-WBC dataset. Moreover, only\nFaster R-CNN X 101 yielded satisfactory results on the SC-IDB. The observed\ndisparities in performance may possibly be attributed to domain shift\nphenomena.\n","authors":["Davide Antonio Mura","Michela Pinna","Lorenzo Putzu","Andrea Loddo","Alessandra Perniciano","Olga Mulas","Cecilia Di Ruberto"],"pdf_url":"https://arxiv.org/pdf/2503.17107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17106v1","updated":"2025-03-21T12:46:38Z","published":"2025-03-21T12:46:38Z","title":"GAA-TSO: Geometry-Aware Assisted Depth Completion for Transparent and\n  Specular Objects","summary":"  Transparent and specular objects are frequently encountered in daily life,\nfactories, and laboratories. However, due to the unique optical properties, the\ndepth information on these objects is usually incomplete and inaccurate, which\nposes significant challenges for downstream robotics tasks. Therefore, it is\ncrucial to accurately restore the depth information of transparent and specular\nobjects. Previous depth completion methods for these objects usually use RGB\ninformation as an additional channel of the depth image to perform depth\nprediction. Due to the poor-texture characteristics of transparent and specular\nobjects, these methods that rely heavily on color information tend to generate\nstructure-less depth predictions. Moreover, these 2D methods cannot effectively\nexplore the 3D structure hidden in the depth channel, resulting in depth\nambiguity. To this end, we propose a geometry-aware assisted depth completion\nmethod for transparent and specular objects, which focuses on exploring the 3D\nstructural cues of the scene. Specifically, besides extracting 2D features from\nRGB-D input, we back-project the input depth to a point cloud and build the 3D\nbranch to extract hierarchical scene-level 3D structural features. To exploit\n3D geometric information, we design several gated cross-modal fusion modules to\neffectively propagate multi-level 3D geometric features to the image branch. In\naddition, we propose an adaptive correlation aggregation strategy to\nappropriately assign 3D features to the corresponding 2D features. Extensive\nexperiments on ClearGrasp, OOD, TransCG, and STD datasets show that our method\noutperforms other state-of-the-art methods. We further demonstrate that our\nmethod significantly enhances the performance of downstream robotic grasping\ntasks.\n","authors":["Yizhe Liu","Tong Jia","Da Cai","Hao Wang","Dongyue Chen"],"pdf_url":"https://arxiv.org/pdf/2503.17106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17105v1","updated":"2025-03-21T12:46:22Z","published":"2025-03-21T12:46:22Z","title":"A Comparative Analysis of Image Descriptors for Histopathological\n  Classification of Gastric Cancer","summary":"  Gastric cancer ranks as the fifth most common and fourth most lethal cancer\nglobally, with a dismal 5-year survival rate of approximately 20%. Despite\nextensive research on its pathobiology, the prognostic predictability remains\ninadequate, compounded by pathologists' high workload and potential diagnostic\nerrors. Thus, automated, accurate histopathological diagnosis tools are\ncrucial. This study employs Machine Learning and Deep Learning techniques to\nclassify histopathological images into healthy and cancerous categories. Using\nhandcrafted and deep features with shallow learning classifiers on the\nGasHisSDB dataset, we offer a comparative analysis and insights into the most\nrobust and high-performing combinations of features and classifiers for\ndistinguishing between normal and abnormal histopathological images without\nfine-tuning strategies. With the RF classifier, our approach can reach F1 of\n93.4%, demonstrating its validity.\n","authors":["Marco Usai","Andrea Loddo","Alessandra Perniciano","Maurizio Atzori","Cecilia Di Ruberto"],"pdf_url":"https://arxiv.org/pdf/2503.17105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10332v3","updated":"2025-03-21T12:40:26Z","published":"2024-11-15T16:32:34Z","title":"Number it: Temporal Grounding Videos like Flipping Manga","summary":"  Video Large Language Models (Vid-LLMs) have made remarkable advancements in\ncomprehending video content for QA dialogue. However, they struggle to extend\nthis visual understanding to tasks requiring precise temporal localization,\nknown as Video Temporal Grounding (VTG). To address this gap, we introduce\nNumber-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual\ncomprehension with temporal grounding by adding unique numerical identifiers to\neach video frame. Treating a video as a sequence of numbered frame images,\nNumPro transforms VTG into an intuitive process: flipping through manga panels\nin sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking\nvisual content with corresponding temporal information. Our experiments\ndemonstrate that NumPro significantly boosts VTG performance of top-tier\nVid-LLMs without additional computational cost. Furthermore, fine-tuning on a\nNumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing\nprevious top-performing methods by up to 6.9\\% in mIoU for moment retrieval and\n8.5\\% in mAP for highlight detection. The code will be available at\nhttps://github.com/yongliang-wu/NumPro.\n","authors":["Yongliang Wu","Xinting Hu","Yuyang Sun","Yizhou Zhou","Wenbo Zhu","Fengyun Rao","Bernt Schiele","Xu Yang"],"pdf_url":"https://arxiv.org/pdf/2411.10332v3.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.17097v1","updated":"2025-03-21T12:30:33Z","published":"2025-03-21T12:30:33Z","title":"R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging\n  Diffusion Model","summary":"  We introduce R2LDM, an innovative approach for generating dense and accurate\n4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of\nutilizing range images or bird's eye view (BEV) images, we represent both LiDAR\nand 4D radar point clouds using voxel features, which more effectively capture\n3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model\n(LVDM), which performs the diffusion process in the latent space. Additionally,\na novel Latent Point Cloud Reconstruction (LPCR) module is utilized to\nreconstruct point clouds from high-dimensional latent voxel features. As a\nresult, R2LDM effectively generates LiDAR-like point clouds from paired raw\nradar data. We evaluate our approach on two different datasets, and the\nexperimental results demonstrate that our model achieves 6- to 10-fold\ndensification of radar point clouds, outperforming state-of-the-art baselines\nin 4D radar point cloud super-resolution. Furthermore, the enhanced radar point\nclouds generated by our method significantly improve downstream tasks,\nachieving up to 31.7% improvement in point cloud registration recall rate and\n24.9% improvement in object detection accuracy.\n","authors":["Boyuan Zheng","Shouyi Lu","Renbo Huang","Minqing Huang","Fan Lu","Wei Tian","Guirong Zhuo","Lu Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.17097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17096v1","updated":"2025-03-21T12:27:49Z","published":"2025-03-21T12:27:49Z","title":"Multi-modal Multi-platform Person Re-Identification: Benchmark and\n  Method","summary":"  Conventional person re-identification (ReID) research is often limited to\nsingle-modality sensor data from static cameras, which fails to address the\ncomplexities of real-world scenarios where multi-modal signals are increasingly\nprevalent. For instance, consider an urban ReID system integrating stationary\nRGB cameras, nighttime infrared sensors, and UAVs equipped with dynamic\ntracking capabilities. Such systems face significant challenges due to\nvariations in camera perspectives, lighting conditions, and sensor modalities,\nhindering effective person ReID. To address these challenges, we introduce the\nMP-ReID benchmark, a novel dataset designed specifically for multi-modality and\nmulti-platform ReID. This benchmark uniquely compiles data from 1,930\nidentities across diverse modalities, including RGB, infrared, and thermal\nimaging, captured by both UAVs and ground-based cameras in indoor and outdoor\nenvironments. Building on this benchmark, we introduce Uni-Prompt ReID, a\nframework with specific-designed prompts, tailored for cross-modality and\ncross-platform scenarios. Our method consistently outperforms state-of-the-art\napproaches, establishing a robust foundation for future research in complex and\ndynamic ReID environments. Our dataset are available\nat:https://mp-reid.github.io/.\n","authors":["Ruiyang Ha","Songyi Jiang","Bin Li","Bikang Pan","Yihang Zhu","Junjie Zhang","Xiatian Zhu","Shaogang Gong","Jingya Wang"],"pdf_url":"https://arxiv.org/pdf/2503.17096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17095v1","updated":"2025-03-21T12:24:58Z","published":"2025-03-21T12:24:58Z","title":"FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields","summary":"  Recent 3D face editing methods using masks have produced high-quality edited\nimages by leveraging Neural Radiance Fields (NeRF). Despite their impressive\nperformance, existing methods often provide limited user control due to the use\nof pre-trained segmentation masks. To utilize masks with a desired layout, an\nextensive training dataset is required, which is challenging to gather. We\npresent FFaceNeRF, a NeRF-based face editing technique that can overcome the\nchallenge of limited user control due to the use of fixed mask layouts. Our\nmethod employs a geometry adapter with feature injection, allowing for\neffective manipulation of geometry attributes. Additionally, we adopt latent\nmixing for tri-plane augmentation, which enables training with a few samples.\nThis facilitates rapid model adaptation to desired mask layouts, crucial for\napplications in fields like personalized medical imaging or creative face\nediting. Our comparative evaluations demonstrate that FFaceNeRF surpasses\nexisting mask based face editing methods in terms of flexibility, control, and\ngenerated image quality, paving the way for future advancements in customized\nand high-fidelity 3D face editing. The code is available on the\n{\\href{https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}.\n","authors":["Kwan Yun","Chaelin Kim","Hangyeul Shin","Junyong Noh"],"pdf_url":"https://arxiv.org/pdf/2503.17095v1.pdf","comment":"CVPR2025, 11 pages, 14 figures"},{"id":"http://arxiv.org/abs/2503.17093v1","updated":"2025-03-21T12:21:48Z","published":"2025-03-21T12:21:48Z","title":"ColabSfM: Collaborative Structure-from-Motion by Point Cloud\n  Registration","summary":"  Structure-from-Motion (SfM) is the task of estimating 3D structure and camera\nposes from images. We define Collaborative SfM (ColabSfM) as sharing\ndistributed SfM reconstructions. Sharing maps requires estimating a joint\nreference frame, which is typically referred to as registration. However, there\nis a lack of scalable methods and training datasets for registering SfM\nreconstructions. In this paper, we tackle this challenge by proposing the\nscalable task of point cloud registration for SfM reconstructions. We find that\ncurrent registration methods cannot register SfM point clouds when trained on\nexisting datasets. To this end, we propose a SfM registration dataset\ngeneration pipeline, leveraging partial reconstructions from synthetically\ngenerated camera trajectories for each scene. Finally, we propose a simple but\nimpactful neural refiner on top of the SotA registration method RoITr that\nyields significant improvements, which we call RefineRoITr. Our extensive\nexperimental evaluation shows that our proposed pipeline and model enables\nColabSfM. Code is available at https://github.com/EricssonResearch/ColabSfM\n","authors":["Johan Edstedt","André Mateus","Alberto Jaenal"],"pdf_url":"https://arxiv.org/pdf/2503.17093v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.17089v1","updated":"2025-03-21T12:17:43Z","published":"2025-03-21T12:17:43Z","title":"Does a Rising Tide Lift All Boats? Bias Mitigation for AI-based CMR\n  Segmentation","summary":"  Artificial intelligence (AI) is increasingly being used for medical imaging\ntasks. However, there can be biases in the resulting models, particularly when\nthey were trained using imbalanced training datasets. One such example has been\nthe strong race bias effect in cardiac magnetic resonance (CMR) image\nsegmentation models. Although this phenomenon has been reported in a number of\npublications, little is known about the effectiveness of bias mitigation\nalgorithms in this domain. We aim to investigate the impact of common bias\nmitigation methods to address bias between Black and White subjects in AI-based\nCMR segmentation models. Specifically, we use oversampling, importance\nreweighing and Group DRO as well as combinations of these techniques to\nmitigate the race bias. Furthermore, motivated by recent findings on the root\ncauses of AI-based CMR segmentation bias, we evaluate the same methods using\nmodels trained and evaluated on cropped CMR images. We find that bias can be\nmitigated using oversampling, significantly improving performance for the\nunderrepresented Black subjects whilst not significantly reducing the majority\nWhite subjects' performance. Group DRO also improves performance for Black\nsubjects but not significantly, while reweighing decreases performance for\nBlack subjects. Using a combination of oversampling and Group DRO also improves\nperformance for Black subjects but not significantly. Using cropped images\nincreases performance for both races and reduces the bias, whilst adding\noversampling as a bias mitigation technique with cropped images reduces the\nbias further.\n","authors":["Tiarna Lee","Esther Puyol-Antón","Bram Ruijsink","Miaojing Shi","Andrew P. King"],"pdf_url":"https://arxiv.org/pdf/2503.17089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17080v1","updated":"2025-03-21T12:10:38Z","published":"2025-03-21T12:10:38Z","title":"Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection","summary":"  The CLIP model has demonstrated significant advancements in aligning visual\nand language modalities through large-scale pre-training on image-text pairs,\nenabling strong zero-shot classification and retrieval capabilities on various\ndomains. However, CLIP's training remains computationally intensive, with high\ndemands on both data processing and memory. To address these challenges, recent\nmasking strategies have emerged, focusing on the selective removal of image\npatches to improve training efficiency. Although effective, these methods often\ncompromise key semantic information, resulting in suboptimal alignment between\nvisual features and text descriptions. In this work, we present a concise yet\neffective approach called Patch Generation-to-Selection to enhance CLIP's\ntraining efficiency while preserving critical semantic content. Our method\nintroduces a gradual masking process in which a small set of candidate patches\nis first pre-selected as potential mask regions. Then, we apply Sobel edge\ndetection across the entire image to generate an edge mask that prioritizes the\nretention of the primary object areas. Finally, similarity scores between the\ncandidate mask patches and their neighboring patches are computed, with optimal\ntransport normalization refining the selection process to ensure a balanced\nsimilarity matrix. Our approach, CLIP-PGS, sets new state-of-the-art results in\nzero-shot classification and retrieval tasks, achieving superior performance in\nrobustness evaluation and language compositionality benchmarks.\n","authors":["Gensheng Pei","Tao Chen","Yujia Wang","Xinhao Cai","Xiangbo Shu","Tianfei Zhou","Yazhou Yao"],"pdf_url":"https://arxiv.org/pdf/2503.17080v1.pdf","comment":"accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.17076v1","updated":"2025-03-21T12:00:59Z","published":"2025-03-21T12:00:59Z","title":"Halton Scheduler For Masked Generative Image Transformer","summary":"  Masked Generative Image Transformers (MaskGIT) have emerged as a scalable and\nefficient image generation framework, able to deliver high-quality visuals with\nlow inference costs. However, MaskGIT's token unmasking scheduler, an essential\ncomponent of the framework, has not received the attention it deserves. We\nanalyze the sampling objective in MaskGIT, based on the mutual information\nbetween tokens, and elucidate its shortcomings. We then propose a new sampling\nstrategy based on our Halton scheduler instead of the original Confidence\nscheduler. More precisely, our method selects the token's position according to\na quasi-random, low-discrepancy Halton sequence. Intuitively, that method\nspreads the tokens spatially, progressively covering the image uniformly at\neach step. Our analysis shows that it allows reducing non-recoverable sampling\nerrors, leading to simpler hyper-parameters tuning and better quality images.\nOur scheduler does not require retraining or noise injection and may serve as a\nsimple drop-in replacement for the original sampling strategy. Evaluation of\nboth class-to-image synthesis on ImageNet and text-to-image generation on the\nCOCO dataset demonstrates that the Halton scheduler outperforms the Confidence\nscheduler quantitatively by reducing the FID and qualitatively by generating\nmore diverse and more detailed images. Our code is at\nhttps://github.com/valeoai/Halton-MaskGIT.\n","authors":["Victor Besnier","Mickael Chen","David Hurych","Eduardo Valle","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2503.17076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17074v1","updated":"2025-03-21T11:56:20Z","published":"2025-03-21T11:56:20Z","title":"Zero-Shot Styled Text Image Generation, but Make It Autoregressive","summary":"  Styled Handwritten Text Generation (HTG) has recently received attention from\nthe computer vision and document analysis communities, which have developed\nseveral solutions, either GAN- or diffusion-based, that achieved promising\nresults. Nonetheless, these strategies fail to generalize to novel styles and\nhave technical constraints, particularly in terms of maximum output length and\ntraining efficiency. To overcome these limitations, in this work, we propose a\nnovel framework for text image generation, dubbed Emuru. Our approach leverages\na powerful text image representation model (a variational autoencoder) combined\nwith an autoregressive Transformer. Our approach enables the generation of\nstyled text images conditioned on textual content and style examples, such as\nspecific fonts or handwriting styles. We train our model solely on a diverse,\nsynthetic dataset of English text rendered in over 100,000 typewritten and\ncalligraphy fonts, which gives it the capability to reproduce unseen styles\n(both fonts and users' handwriting) in zero-shot. To the best of our knowledge,\nEmuru is the first autoregressive model for HTG, and the first designed\nspecifically for generalization to novel styles. Moreover, our model generates\nimages without background artifacts, which are easier to use for downstream\napplications. Extensive evaluation on both typewritten and handwritten,\nany-length text image generation scenarios demonstrates the effectiveness of\nour approach.\n","authors":["Vittorio Pippi","Fabio Quattrini","Silvia Cascianelli","Alessio Tonioni","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2503.17074v1.pdf","comment":"Accepted at CVPR2025"},{"id":"http://arxiv.org/abs/2503.17071v1","updated":"2025-03-21T11:54:16Z","published":"2025-03-21T11:54:16Z","title":"Superpowering Open-Vocabulary Object Detectors for X-ray Vision","summary":"  Open-vocabulary object detection (OvOD) is set to revolutionize security\nscreening by enabling systems to recognize any item in X-ray scans. However,\ndeveloping effective OvOD models for X-ray imaging presents unique challenges\ndue to data scarcity and the modality gap that prevents direct adoption of\nRGB-based solutions. To overcome these limitations, we propose RAXO, a\ntraining-free framework that repurposes off-the-shelf RGB OvOD detectors for\nrobust X-ray detection. RAXO builds high-quality X-ray class descriptors using\na dual-source retrieval strategy. It gathers relevant RGB images from the web\nand enriches them via a novel X-ray material transfer mechanism, eliminating\nthe need for labeled databases. These visual descriptors replace text-based\nclassification in OvOD, leveraging intra-modal feature distances for robust\ndetection. Extensive experiments demonstrate that RAXO consistently improves\nOvOD performance, providing an average mAP increase of up to 17.0 points over\nbase detectors. To further support research in this emerging field, we also\nintroduce DET-COMPASS, a new benchmark featuring bounding box annotations for\nover 300 object categories, enabling large-scale evaluation of OvOD in X-ray.\nCode and dataset available at: https://github.com/PAGF188/RAXO.\n","authors":["Pablo Garcia-Fernandez","Lorenzo Vaquero","Mingxuan Liu","Feng Xue","Daniel Cores","Nicu Sebe","Manuel Mucientes","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2503.17071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12231v2","updated":"2025-03-21T11:50:12Z","published":"2025-02-17T18:59:42Z","title":"PUGS: Zero-shot Physical Understanding with Gaussian Splatting","summary":"  Current robotic systems can understand the categories and poses of objects\nwell. But understanding physical properties like mass, friction, and hardness,\nin the wild, remains challenging. We propose a new method that reconstructs 3D\nobjects using the Gaussian splatting representation and predicts various\nphysical properties in a zero-shot manner. We propose two techniques during the\nreconstruction phase: a geometry-aware regularization loss function to improve\nthe shape quality and a region-aware feature contrastive loss function to\npromote region affinity. Two other new techniques are designed during\ninference: a feature-based property propagation module and a volume integration\nmodule tailored for the Gaussian representation. Our framework is named as\nzero-shot physical understanding with Gaussian splatting, or PUGS. PUGS\nachieves new state-of-the-art results on the standard benchmark of ABO-500 mass\nprediction. We provide extensive quantitative ablations and qualitative\nvisualization to demonstrate the mechanism of our designs. We show the proposed\nmethodology can help address challenging real-world grasping tasks. Our codes,\ndata, and models are available at https://github.com/EverNorif/PUGS\n","authors":["Yinghao Shuai","Ran Yu","Yuantao Chen","Zijian Jiang","Xiaowei Song","Nan Wang","Jv Zheng","Jianzhu Ma","Meng Yang","Zhicheng Wang","Wenbo Ding","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.12231v2.pdf","comment":"ICRA 2025, Project page: https://evernorif.github.io/PUGS/"},{"id":"http://arxiv.org/abs/2412.06534v2","updated":"2025-03-21T11:50:06Z","published":"2024-12-09T14:43:06Z","title":"Inverting Transformer-based Vision Models","summary":"  Understanding the mechanisms underlying deep neural networks in computer\nvision remains a fundamental challenge. While many previous approaches have\nfocused on visualizing intermediate representations within deep neural\nnetworks, particularly convolutional neural networks, these techniques have yet\nto be thoroughly explored in transformer-based vision models. In this study, we\napply a modular approach of training inverse models to reconstruct input images\nfrom intermediate layers within a Detection Transformer and a Vision\nTransformer, showing that this approach is efficient and feasible. Through\nqualitative and quantitative evaluations of reconstructed images, we generate\ninsights into the underlying mechanisms of these architectures, highlighting\ntheir similarities and differences in terms of contextual shape and\npreservation of image details, inter-layer correlation, and robustness to color\nperturbations. Our analysis illustrates how these properties emerge within the\nmodels, contributing to a deeper understanding of transformer-based vision\nmodels. The code for reproducing our experiments is available at\ngithub.com/wiskott-lab/inverse-detection-transformer.\n","authors":["Jan Rathjens","Shirin Reyhanian","David Kappel","Laurenz Wiskott"],"pdf_url":"https://arxiv.org/pdf/2412.06534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17069v1","updated":"2025-03-21T11:50:06Z","published":"2025-03-21T11:50:06Z","title":"PVChat: Personalized Video Chat with One-Shot Learning","summary":"  Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.\n","authors":["Yufei Shi","Weilong Yan","Gang Xu","Yumeng Li","Yuchen Li","Zhenxi Li","Fei Richard Yu","Ming Li","Si Yong Yeo"],"pdf_url":"https://arxiv.org/pdf/2503.17069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04901v2","updated":"2025-03-21T11:48:35Z","published":"2023-10-07T19:45:24Z","title":"WAIT: Feature Warping for Animation to Illustration video Translation\n  using GANs","summary":"  In this paper, we explore a new domain for video-to-video translation.\nMotivated by the availability of animation movies that are adopted from\nillustrated books for children, we aim to stylize these videos with the style\nof the original illustrations. Current state-of-the-art video-to-video\ntranslation models rely on having a video sequence or a single style image to\nstylize an input video. We introduce a new problem for video stylizing where an\nunordered set of images are used. This is a challenging task for two reasons:\ni) we do not have the advantage of temporal consistency as in video sequences;\nii) it is more difficult to obtain consistent styles for video frames from a\nset of unordered images compared to using a single image. Most of the\nvideo-to-video translation methods are built on an image-to-image translation\nmodel, and integrate additional networks such as optical flow, or temporal\npredictors to capture temporal relations. These additional networks make the\nmodel training and inference complicated and slow down the process. To ensure\ntemporal coherency in video-to-video style transfer, we propose a new generator\nnetwork with feature warping layers which overcomes the limitations of the\nprevious methods. We show the effectiveness of our method on three datasets\nboth qualitatively and quantitatively. Code and pretrained models are available\nat https://github.com/giddyyupp/wait.\n","authors":["Samet Hicsonmez","Nermin Samet","Fidan Samet","Oguz Bakir","Emre Akbas","Pinar Duygulu"],"pdf_url":"https://arxiv.org/pdf/2310.04901v2.pdf","comment":"Accepted to Neurocomputing"},{"id":"http://arxiv.org/abs/2411.03714v2","updated":"2025-03-21T11:47:18Z","published":"2024-11-06T07:28:57Z","title":"Explaining Human Activity Recognition with SHAP: Validating Insights\n  with Perturbation and Quantitative Measures","summary":"  In Human Activity Recognition (HAR), understanding the intricacy of body\nmovements within high-risk applications is essential. This study uses SHapley\nAdditive exPlanations (SHAP) to explain the decision-making process of Graph\nConvolution Networks (GCNs) when classifying activities with skeleton data. We\nemploy SHAP to explain two real-world datasets: one for cerebral palsy (CP)\nclassification and the widely used NTU RGB+D 60 action recognition dataset. To\ntest the explanation, we introduce a novel perturbation approach that modifies\nthe model's edge importance matrix, allowing us to evaluate the impact of\nspecific body key points on prediction outcomes. To assess the fidelity of our\nexplanations, we employ informed perturbation, targeting body key points\nidentified as important by SHAP and comparing them against random perturbation\nas a control condition. This perturbation enables a judgment on whether the\nbody key points are truly influential or non-influential based on the SHAP\nvalues. Results on both datasets show that body key points identified as\nimportant through SHAP have the largest influence on the accuracy, specificity,\nand sensitivity metrics. Our findings highlight that SHAP can provide granular\ninsights into the input feature contribution to the prediction outcome of GCNs\nin HAR tasks. This demonstrates the potential for more interpretable and\ntrustworthy models in high-stakes applications like healthcare or\nrehabilitation.\n","authors":["Felix Tempel","Espen Alexander F. Ihlen","Lars Adde","Inga Strümke"],"pdf_url":"https://arxiv.org/pdf/2411.03714v2.pdf","comment":"Published in Computers in Biology and Medicine"},{"id":"http://arxiv.org/abs/2503.17059v1","updated":"2025-03-21T11:23:39Z","published":"2025-03-21T11:23:39Z","title":"DIDiffGes: Decoupled Semi-Implicit Diffusion Models for Real-time\n  Gesture Generation from Speech","summary":"  Diffusion models have demonstrated remarkable synthesis quality and diversity\nin generating co-speech gestures. However, the computationally intensive\nsampling steps associated with diffusion models hinder their practicality in\nreal-world applications. Hence, we present DIDiffGes, for a Decoupled\nSemi-Implicit Diffusion model-based framework, that can synthesize\nhigh-quality, expressive gestures from speech using only a few sampling steps.\nOur approach leverages Generative Adversarial Networks (GANs) to enable\nlarge-step sampling for diffusion model. We decouple gesture data into body and\nhands distributions and further decompose them into marginal and conditional\ndistributions. GANs model the marginal distribution implicitly, while L2\nreconstruction loss learns the conditional distributions exciplictly. This\nstrategy enhances GAN training stability and ensures expressiveness of\ngenerated full-body gestures. Our framework also learns to denoise root noise\nconditioned on local body representation, guaranteeing stability and realism.\nDIDiffGes can generate gestures from speech with just 10 sampling steps,\nwithout compromising quality and expressiveness, reducing the number of\nsampling steps by a factor of 100 compared to existing methods. Our user study\nreveals that our method outperforms state-of-the-art approaches in human\nlikeness, appropriateness, and style correctness. Project is\nhttps://cyk990422.github.io/DIDiffGes.\n","authors":["Yongkang Cheng","Shaoli Huang","Xuelin Chen","Jifeng Ning","Mingming Gong"],"pdf_url":"https://arxiv.org/pdf/2503.17059v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2406.17382v3","updated":"2025-03-21T11:23:11Z","published":"2024-06-25T08:58:53Z","title":"Automatic infant 2D pose estimation from videos: comparing seven deep\n  neural network methods","summary":"  Automatic markerless estimation of infant posture and motion from ordinary\nvideos carries great potential for movement studies \"in the wild\", facilitating\nunderstanding of motor development and massively increasing the chances of\nearly diagnosis of disorders. There is rapid development of human pose\nestimation methods in computer vision thanks to advances in deep learning and\nmachine learning. However, these methods are trained on datasets that feature\nadults in different contexts. This work tests and compares seven popular\nmethods (AlphaPose, DeepLabCut/DeeperCut, Detectron2, HRNet,\nMediaPipe/BlazePose, OpenPose, and ViTPose) on videos of infants in supine\nposition and in more complex settings. Surprisingly, all methods except\nDeepLabCut and MediaPipe have competitive performance without additional\nfinetuning, with ViTPose performing best. Next to standard performance metrics\n(average precision and recall), we introduce errors expressed in the\nneck-mid-hip (torso length) ratio and additionally study missed and redundant\ndetections, and the reliability of the internal confidence ratings of the\ndifferent methods, which are relevant for downstream tasks. Among the networks\nwith competitive performance, only AlphaPose could run close to real time (27\nfps) on our machine. We provide documented Docker containers or instructions\nfor all the methods we used, our analysis scripts, and the processed data at\nhttps://hub.docker.com/u/humanoidsctu and https://osf.io/x465b/.\n","authors":["Filipe Gama","Matej Misar","Lukas Navara","Sergiu T. Popescu","Matej Hoffmann"],"pdf_url":"https://arxiv.org/pdf/2406.17382v3.pdf","comment":"34 pages, 7 figures, 20 tables"},{"id":"http://arxiv.org/abs/2503.17057v1","updated":"2025-03-21T11:16:44Z","published":"2025-03-21T11:16:44Z","title":"Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework\n  for Neural Networks","summary":"  Accurate segmentation of ultrasound (US) images of the cervical muscles is\ncrucial for precision healthcare. The demand for automatic computer-assisted\nmethods is high. However, the scarcity of labeled data hinders the development\nof these methods. Advanced semi-supervised learning approaches have displayed\npromise in overcoming this challenge by utilizing labeled and unlabeled data.\nThis study introduces a novel semi-supervised learning (SSL) framework that\nintegrates dual neural networks. This SSL framework utilizes both networks to\ngenerate pseudo-labels and cross-supervise each other at the pixel level.\nAdditionally, a self-supervised contrastive learning strategy is introduced,\nwhich employs a pair of deep representations to enhance feature learning\ncapabilities, particularly on unlabeled data. Our framework demonstrates\ncompetitive performance in cervical segmentation tasks. Our codes are publicly\navailable on https://github.com/13204942/SSL\\_Cervical\\_Segmentation.\n","authors":["Fangyijie Wang","Kathleen M. Curran","Guénolé Silvestre"],"pdf_url":"https://arxiv.org/pdf/2503.17057v1.pdf","comment":"Accepted for an oral presentation at ISBI 2025 Fetal Ultrasound Grand\n  Challenge: Semi-Supervised Cervical Segmentation"},{"id":"http://arxiv.org/abs/2503.17050v1","updated":"2025-03-21T11:08:14Z","published":"2025-03-21T11:08:14Z","title":"Scoring, Remember, and Reference: Catching Camouflaged Objects in Videos","summary":"  Video Camouflaged Object Detection (VCOD) aims to segment objects whose\nappearances closely resemble their surroundings, posing a challenging and\nemerging task. Existing vision models often struggle in such scenarios due to\nthe indistinguishable appearance of camouflaged objects and the insufficient\nexploitation of dynamic information in videos. To address these challenges, we\npropose an end-to-end VCOD framework inspired by human memory-recognition,\nwhich leverages historical video information by integrating memory reference\nframes for camouflaged sequence processing. Specifically, we design a\ndual-purpose decoder that simultaneously generates predicted masks and scores,\nenabling reference frame selection based on scores while introducing auxiliary\nsupervision to enhance feature extraction.Furthermore, this study introduces a\nnovel reference-guided multilevel asymmetric attention mechanism, effectively\nintegrating long-term reference information with short-term motion cues for\ncomprehensive feature extraction. By combining these modules, we develop the\nScoring, Remember, and Reference (SRR) framework, which efficiently extracts\ninformation to locate targets and employs memory guidance to improve subsequent\nprocessing. With its optimized module design and effective utilization of video\ndata, our model achieves significant performance improvements, surpassing\nexisting approaches by 10% on benchmark datasets while requiring fewer\nparameters (54M) and only a single pass through the video. The code will be\nmade publicly available.\n","authors":["Yuang Feng","Shuyong Gao","Fuzhen Yan","Yicheng Song","Lingyi Hong","Junjie Hu","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.17050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17046v1","updated":"2025-03-21T11:04:01Z","published":"2025-03-21T11:04:01Z","title":"HAPI: A Model for Learning Robot Facial Expressions from Human\n  Preferences","summary":"  Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses.\n","authors":["Dongsheng Yang","Qianying Liu","Wataru Sato","Takashi Minato","Chaoran Liu","Shin'ya Nishida"],"pdf_url":"https://arxiv.org/pdf/2503.17046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17044v1","updated":"2025-03-21T11:00:12Z","published":"2025-03-21T11:00:12Z","title":"ExCap3D: Expressive 3D Scene Understanding via Object Captioning with\n  Varying Detail","summary":"  Generating text descriptions of objects in 3D indoor scenes is an important\nbuilding block of embodied understanding. Existing methods do this by\ndescribing objects at a single level of detail, which often does not capture\nfine-grained details such as varying textures, materials, and shapes of the\nparts of objects. We propose the task of expressive 3D captioning: given an\ninput 3D scene, describe objects at multiple levels of detail: a high-level\nobject description, and a low-level description of the properties of its parts.\nTo produce such captions, we present ExCap3D, an expressive 3D captioning model\nwhich takes as input a 3D scan, and for each detected object in the scan,\ngenerates a fine-grained collective description of the parts of the object,\nalong with an object-level description conditioned on the part-level\ndescription. We design ExCap3D to encourage semantic consistency between the\ngenerated text descriptions, as well as textual similarity in the latent space,\nto further increase the quality of the generated captions. To enable this task,\nwe generated the ExCap3D Dataset by leveraging a visual-language model (VLM)\nfor multi-view captioning. The ExCap3D Dataset contains captions on the\nScanNet++ dataset with varying levels of detail, comprising 190k text\ndescriptions of 34k 3D objects in 947 indoor scenes. Our experiments show that\nthe object- and part-level of detail captions generated by ExCap3D are of\nhigher quality than those produced by state-of-the-art methods, with a Cider\nscore improvement of 17% and 124% for object- and part-level details\nrespectively. Our code, dataset and models will be made publicly available.\n","authors":["Chandan Yeshwanth","David Rozenberszki","Angela Dai"],"pdf_url":"https://arxiv.org/pdf/2503.17044v1.pdf","comment":"Project page: https://cy94.github.io/excap3d/, Video:\n  https://www.youtube.com/watch?v=SQRV1l_0oY0"},{"id":"http://arxiv.org/abs/2407.17777v2","updated":"2025-03-21T10:51:22Z","published":"2024-07-25T05:10:48Z","title":"Babel: A Scalable Pre-trained Model for Multi-Modal Sensing via\n  Expandable Modality Alignment","summary":"  This paper presents Babel, the expandable modality alignment model, specially\ndesigned for multi-modal sensing. While there has been considerable work on\nmulti-modality alignment, they all struggle to effectively incorporate multiple\nsensing modalities due to the data scarcity constraints. How to utilize\nmulti-modal data with partial pairings in sensing remains an unresolved\nchallenge. Babel tackles this challenge by introducing the concept of\nexpandable modality alignment. The key idea involves transforming the\nN-modality alignment into a series of binary-modality alignments. Novel\ntechniques are also proposed to further mitigate data scarcity issue and\nbalance the contribution of the newly incorporated modality with the previously\nestablished modality alignment during the expandable alignment process. We\nprovide the comprehensive implementation. In the pre-training phase, Babel\ncurrently aligns 6 sensing modalities, namely Wi-Fi, mmWave, IMU, LiDAR, video,\nand depth. For the deployment phase, as a foundation model, any single or\ncombination of aligned modalities could be selected from Babel and applied to\ndownstream tasks. Evaluation demonstrates Babel's outstanding performance on\neight human activity recognition datasets, compared to a broad range of\nbaselines e.g., the SOTA single-modal sensing networks, multi-modal sensing\nframework, and multi-modal large language models. Babel not only improves the\nperformance of individual modality sensing (12% averaged accuracy improvement),\nbut also effectively fuses multiple available modalities (up to 22% accuracy\nincrease). Case studies also highlight emerging application scenarios empowered\nby Babel, including cross-modality retrieval (i.e., sensing imaging), and\nbridging LLM for sensing comprehension.\n","authors":["Shenghong Dai","Shiqi Jiang","Yifan Yang","Ting Cao","Mo Li","Suman Banerjee","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2407.17777v2.pdf","comment":"Accepted by SenSys'25"},{"id":"http://arxiv.org/abs/2410.22775v2","updated":"2025-03-21T10:45:28Z","published":"2024-10-30T07:43:29Z","title":"Diffusion Beats Autoregressive: An Evaluation of Compositional\n  Generation in Text-to-Image Models","summary":"  Text-to-image (T2I) generative models, such as Stable Diffusion and DALL-E,\nhave shown remarkable proficiency in producing high-quality, realistic, and\nnatural images from textual descriptions. However, these models sometimes fail\nto accurately capture all the details specified in the input prompts,\nparticularly concerning entities, attributes, and spatial relationships. This\nissue becomes more pronounced when the prompt contains novel or complex\ncompositions, leading to what are known as compositional generation failure\nmodes. Recently, a new open-source diffusion-based T2I model, FLUX, has been\nintroduced, demonstrating strong performance in high-quality image generation.\nAdditionally, autoregressive T2I models like LlamaGen have claimed competitive\nvisual quality performance compared to diffusion-based models. In this study,\nwe evaluate the compositional generation capabilities of these newly introduced\nmodels against established models using the T2I-CompBench benchmark. Our\nfindings reveal that LlamaGen, as a vanilla autoregressive model, is not yet on\npar with state-of-the-art diffusion models for compositional generation tasks\nunder the same criteria, such as model size and inference time. On the other\nhand, the open-source diffusion-based model FLUX exhibits compositional\ngeneration capabilities comparable to the state-of-the-art closed-source model\nDALL-E3.\n","authors":["Arash Marioriyad","Parham Rezaei","Mahdieh Soleymani Baghshah","Mohammad Hossein Rohban"],"pdf_url":"https://arxiv.org/pdf/2410.22775v2.pdf","comment":"NeurIPS 2024 Workshop on Compositional Learning: Perspectives,\n  Methods, and Paths Forward"},{"id":"http://arxiv.org/abs/2411.16173v2","updated":"2025-03-21T10:44:15Z","published":"2024-11-25T08:04:47Z","title":"SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval\n  and Routing in Long-Form Video Analysis","summary":"  Despite advances in Large Multi-modal Models, applying them to long and\nuntrimmed video content remains challenging due to limitations in context\nlength and substantial memory overhead. These constraints often lead to\nsignificant information loss and reduced relevance in the model responses. With\nthe exponential growth of video data across web platforms, understanding\nlong-form video is crucial for advancing generalized intelligence. In this\npaper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel\nvideo-LLM framework designed to enhance the comprehension of lengthy video\ncontent through targeted retrieval process. We address two main challenges to\nachieve it: (i) We present the SceneWalk dataset, a high-quality collection of\n87.8K long videos, each densely captioned at the segment level to enable models\nto capture scene continuity and maintain rich descriptive context. (ii) We\ndevelop robust architectural designs integrating dynamic routing mechanism and\nspatio-temporal projector to efficiently retrieve and process relevant video\nsegments based on user queries. Our framework mitigates the limitations of\ncurrent video-LMMs by allowing for precise identification and retrieval of\nrelevant video segments in response to queries, thereby improving the\ncontextual relevance of the generated responses. Through extensive experiments,\nSALOVA demonstrates enhanced capability in processing complex long-form videos,\nshowing significant capability to maintain contextual integrity across extended\nsequences.\n","authors":["Junho Kim","Hyunjun Kim","Hosu Lee","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2411.16173v2.pdf","comment":"Project page: https://ivy-lvlm.github.io/SALOVA/"},{"id":"http://arxiv.org/abs/2308.02738v2","updated":"2025-03-21T10:42:26Z","published":"2023-08-04T23:13:49Z","title":"Exploring Part-Informed Visual-Language Learning for Person\n  Re-Identification","summary":"  Recently, visual-language learning (VLL) has shown great potential in\nenhancing visual-based person re-identification (ReID). Existing VLL-based ReID\nmethods typically focus on image-text feature alignment at the whole-body\nlevel, while neglecting supervision on fine-grained part features, thus lacking\nconstraints for local feature semantic consistency. To this end, we propose\nPart-Informed Visual-language Learning ($\\pi$-VL) to enhance fine-grained\nvisual features with part-informed language supervisions for ReID tasks.\nSpecifically, $\\pi$-VL introduces a human parsing-guided prompt tuning strategy\nand a hierarchical visual-language alignment paradigm to ensure within-part\nfeature semantic consistency. The former combines both identity labels and\nhuman parsing maps to constitute pixel-level text prompts, and the latter fuses\nmulti-scale visual features with a light-weight auxiliary head to perform\nfine-grained image-text alignment. As a plug-and-play and inference-free\nsolution, our $\\pi$-VL achieves performance comparable to or better than\nstate-of-the-art methods on four commonly used ReID benchmarks. Notably, it\nreports 91.0% Rank-1 and 76.9% mAP on the challenging MSMT17 database, without\nbells and whistles.\n","authors":["Yin Lin","Yehansen Chen","Baocai Yin","Jinshui Hu","Bing Yin","Cong Liu","Zengfu Wang"],"pdf_url":"https://arxiv.org/pdf/2308.02738v2.pdf","comment":"6 pages, 4 figures, ICME 2025"},{"id":"http://arxiv.org/abs/2503.17034v1","updated":"2025-03-21T10:42:22Z","published":"2025-03-21T10:42:22Z","title":"An Attentive Representative Sample Selection Strategy Combined with\n  Balanced Batch Training for Skin Lesion Segmentation","summary":"  An often overlooked problem in medical image segmentation research is the\neffective selection of training subsets to annotate from a complete set of\nunlabelled data. Many studies select their training sets at random, which may\nlead to suboptimal model performance, especially in the minimal supervision\nsetting where each training image has a profound effect on performance\noutcomes. This work aims to address this issue. We use prototypical contrasting\nlearning and clustering to extract representative and diverse samples for\nannotation. We improve upon prior works with a bespoke cluster-based image\nselection process. Additionally, we introduce the concept of unsupervised\nbalanced batch dataloading to medical image segmentation, which aims to improve\nmodel learning with minimally annotated data. We evaluated our method on a\npublic skin lesion dataset (ISIC 2018) and compared it to another\nstate-of-the-art data sampling method. Our method achieved superior performance\nin a low annotation budget scenario.\n","authors":["Stephen Lloyd-Brown","Susan Francis","Caroline Hoad","Penny Gowland","Karen Mullinger","Andrew French","Xin Chen"],"pdf_url":"https://arxiv.org/pdf/2503.17034v1.pdf","comment":"Accepted to ISBI 2025"},{"id":"http://arxiv.org/abs/2503.17032v1","updated":"2025-03-21T10:40:37Z","published":"2025-03-21T10:40:37Z","title":"TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented\n  Reality via 3D Gaussian Splatting","summary":"  Realistic 3D full-body talking avatars hold great potential in AR, with\napplications ranging from e-commerce live streaming to holographic\ncommunication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike\navatar creation, existing methods struggle with fine-grained control of facial\nexpressions and body movements in full-body talking tasks. Additionally, they\noften lack sufficient details and cannot run in real-time on mobile devices. We\npresent TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking\navatar driven by various signals. Our approach starts by creating a\npersonalized clothed human parametric template that binds Gaussians to\nrepresent appearances. We then pre-train a StyleUnet-based network to handle\ncomplex pose-dependent non-rigid deformation, which can capture high-frequency\nappearance details but is too resource-intensive for mobile devices. To\novercome this, we \"bake\" the non-rigid deformations into a lightweight\nMLP-based network using a distillation technique and develop blend shapes to\ncompensate for details. Extensive experiments show that TaoAvatar achieves\nstate-of-the-art rendering quality while running in real-time across various\ndevices, maintaining 90 FPS on high-definition stereo devices such as the Apple\nVision Pro.\n","authors":["Jianchuan Chen","Jingchuan Hu","Gaige Wang","Zhonghua Jiang","Tiansong Zhou","Zhiwen Chen","Chengfei Lv"],"pdf_url":"https://arxiv.org/pdf/2503.17032v1.pdf","comment":"Accepted by CVPR 2025, project page:\n  https://PixelAI-Team.github.io/TaoAvatar"},{"id":"http://arxiv.org/abs/2503.17030v1","updated":"2025-03-21T10:39:21Z","published":"2025-03-21T10:39:21Z","title":"Exploring the Efficacy of Partial Denoising Using Bit Plane Slicing for\n  Enhanced Fracture Identification: A Comparative Study of Deep Learning-Based\n  Approaches and Handcrafted Feature Extraction Techniques","summary":"  Computer vision has transformed medical diagnosis, treatment, and research\nthrough advanced image processing and machine learning techniques. Fracture\nclassification, a critical area in healthcare, has greatly benefited from these\nadvancements, yet accurate detection is challenged by complex patterns and\nimage noise. Bit plane slicing enhances medical images by reducing noise\ninterference and extracting informative features. This research explores\npartial denoising techniques to provide practical solutions for improved\nfracture analysis, ultimately enhancing patient care. The study explores deep\nlearning model DenseNet and handcrafted feature extraction. Decision Tree and\nRandom Forest, were employed to train and evaluate distinct image\nrepresentations. These include the original image, the concatenation of the\nfour bit planes from the LSB as well as MSB, the fully denoised image, and an\nimage consisting of 6 bit planes from MSB and 2 denoised bit planes from LSB.\nThe purpose of forming these diverse image representations is to analyze SNR as\nwell as classification accuracy and identify the bit planes that contain the\nmost informative features. Moreover, the study delves into the significance of\npartial denoising techniques in preserving crucial features, leading to\nimprovements in classification results. Notably, this study shows that\nemploying the Random Forest classifier, the partially denoised image\nrepresentation exhibited a testing accuracy of 95.61% surpassing the\nperformance of other image representations. The outcomes of this research\nprovide valuable insights into the development of efficient preprocessing,\nfeature extraction and classification approaches for fracture identification.\nBy enhancing diagnostic accuracy, these advancements hold the potential to\npositively impact patient care and overall medical outcomes.\n","authors":["Snigdha Paul","Sambit Mallick","Anindya Sen"],"pdf_url":"https://arxiv.org/pdf/2503.17030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17029v1","updated":"2025-03-21T10:39:04Z","published":"2025-03-21T10:39:04Z","title":"AnimatePainter: A Self-Supervised Rendering Framework for Reconstructing\n  Painting Process","summary":"  Humans can intuitively decompose an image into a sequence of strokes to\ncreate a painting, yet existing methods for generating drawing processes are\nlimited to specific data types and often rely on expensive human-annotated\ndatasets. We propose a novel self-supervised framework for generating drawing\nprocesses from any type of image, treating the task as a video generation\nproblem. Our approach reverses the drawing process by progressively removing\nstrokes from a reference image, simulating a human-like creation sequence.\nCrucially, our method does not require costly datasets of real human drawing\nprocesses; instead, we leverage depth estimation and stroke rendering to\nconstruct a self-supervised dataset. We model human drawings as \"refinement\"\nand \"layering\" processes and introduce depth fusion layers to enable video\ngeneration models to learn and replicate human drawing behavior. Extensive\nexperiments validate the effectiveness of our approach, demonstrating its\nability to generate realistic drawings without the need for real drawing\nprocess data.\n","authors":["Junjie Hu","Shuyong Gao","Qianyu Guo","Yan Wang","Qishan Wang","Yuang Feng","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.17029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17027v1","updated":"2025-03-21T10:37:42Z","published":"2025-03-21T10:37:42Z","title":"RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images and\n  A Benchmark","summary":"  In the computer vision community, the preference for pre-training visual\nmodels has largely shifted toward sRGB images due to their ease of acquisition\nand compact storage. However, camera RAW images preserve abundant physical\ndetails across diverse real-world scenarios. Despite this, most existing visual\nperception methods that utilize RAW data directly integrate image signal\nprocessing (ISP) stages with subsequent network modules, often overlooking\npotential synergies at the model level. Building on recent advances in\nadapter-based methodologies in both NLP and computer vision, we propose\nRAW-Adapter, a novel framework that incorporates learnable ISP modules as\ninput-level adapters to adjust RAW inputs. At the same time, it employs\nmodel-level adapters to seamlessly bridge ISP processing with high-level\ndownstream architectures. Moreover, RAW-Adapter serves as a general framework\napplicable to various computer vision frameworks.\n  Furthermore, we introduce RAW-Bench, which incorporates 17 types of RAW-based\ncommon corruptions, including lightness degradations, weather effects,\nblurriness, camera imaging degradations, and variations in camera color\nresponse. Using this benchmark, we systematically compare the performance of\nRAW-Adapter with state-of-the-art (SOTA) ISP methods and other RAW-based\nhigh-level vision algorithms. Additionally, we propose a RAW-based data\naugmentation strategy to further enhance RAW-Adapter's performance and improve\nits out-of-domain (OOD) generalization ability. Extensive experiments\nsubstantiate the effectiveness and efficiency of RAW-Adapter, highlighting its\nrobust performance across diverse scenarios.\n","authors":["Ziteng Cui","Jianfei Yang","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2503.17027v1.pdf","comment":"23 pages, 17 figures, extension of ECCV 2024 work: arXiv:2408.14802"},{"id":"http://arxiv.org/abs/2503.17024v1","updated":"2025-03-21T10:34:51Z","published":"2025-03-21T10:34:51Z","title":"A Tale of Two Classes: Adapting Supervised Contrastive Learning to\n  Binary Imbalanced Datasets","summary":"  Supervised contrastive learning (SupCon) has proven to be a powerful\nalternative to the standard cross-entropy loss for classification of\nmulti-class balanced datasets. However, it struggles to learn well-conditioned\nrepresentations of datasets with long-tailed class distributions. This problem\nis potentially exacerbated for binary imbalanced distributions, which are\ncommonly encountered during many real-world problems such as medical diagnosis.\nIn experiments on seven binary datasets of natural and medical images, we show\nthat the performance of SupCon decreases with increasing class imbalance. To\nsubstantiate these findings, we introduce two novel metrics that evaluate the\nquality of the learned representation space. By measuring the class\ndistribution in local neighborhoods, we are able to uncover structural\ndeficiencies of the representation space that classical metrics cannot detect.\nInformed by these insights, we propose two new supervised contrastive learning\nstrategies tailored to binary imbalanced datasets that improve the structure of\nthe representation space and increase downstream classification accuracy over\nstandard SupCon by up to 35%. We make our code available.\n","authors":["David Mildenberger","Paul Hager","Daniel Rueckert","Martin J Menten"],"pdf_url":"https://arxiv.org/pdf/2503.17024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03473v2","updated":"2025-03-21T10:30:57Z","published":"2024-12-04T16:59:49Z","title":"UrbanGS: Semantic-Guided Gaussian Splatting for Urban Scene\n  Reconstruction","summary":"  Reconstructing urban scenes is challenging due to their complex geometries\nand the presence of potentially dynamic objects. 3D Gaussian Splatting\n(3DGS)-based methods have shown strong performance, but existing approaches\noften incorporate manual 3D annotations to improve dynamic object modeling,\nwhich is impractical due to high labeling costs. Some methods leverage 4D\nGaussian Splatting (4DGS) to represent the entire scene, but they treat static\nand dynamic objects uniformly, leading to unnecessary updates for static\nelements and ultimately degrading reconstruction quality. To address these\nissues, we propose UrbanGS, which leverages 2D semantic maps and an existing\ndynamic Gaussian approach to distinguish static objects from the scene,\nenabling separate processing of definite static and potentially dynamic\nelements. Specifically, for definite static regions, we enforce global\nconsistency to prevent unintended changes in dynamic Gaussian and introduce a\nK-nearest neighbor (KNN)-based regularization to improve local coherence on\nlow-textured ground surfaces. Notably, for potentially dynamic objects, we\naggregate temporal information using learnable time embeddings, allowing each\nGaussian to model deformations over time. Extensive experiments on real-world\ndatasets demonstrate that our approach outperforms state-of-the-art methods in\nreconstruction quality and efficiency, accurately preserving static content\nwhile capturing dynamic elements.\n","authors":["Ziwen Li","Jiaxin Huang","Runnan Chen","Yunlong Che","Yandong Guo","Tongliang Liu","Fakhri Karray","Mingming Gong"],"pdf_url":"https://arxiv.org/pdf/2412.03473v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17017v1","updated":"2025-03-21T10:26:32Z","published":"2025-03-21T10:26:32Z","title":"Specifying What You Know or Not for Multi-Label Class-Incremental\n  Learning","summary":"  Existing class incremental learning is mainly designed for single-label\nclassification task, which is ill-equipped for multi-label scenarios due to the\ninherent contradiction of learning objectives for samples with incomplete\nlabels. We argue that the main challenge to overcome this contradiction in\nmulti-label class-incremental learning (MLCIL) lies in the model's inability to\nclearly distinguish between known and unknown knowledge. This ambiguity hinders\nthe model's ability to retain historical knowledge, master current classes, and\nprepare for future learning simultaneously. In this paper, we target at\nspecifying what is known or not to accommodate Historical, Current, and\nProspective knowledge for MLCIL and propose a novel framework termed as HCP.\nSpecifically, (i) we clarify the known classes by dynamic feature purification\nand recall enhancement with distribution prior, enhancing the precision and\nretention of known information. (ii) We design prospective knowledge mining to\nprobe the unknown, preparing the model for future learning. Extensive\nexperiments validate that our method effectively alleviates catastrophic\nforgetting in MLCIL, surpassing the previous state-of-the-art by 3.3% on\naverage accuracy for MS-COCO B0-C10 setting without replay buffers.\n","authors":["Aoting Zhang","Dongbao Yang","Chang Liu","Xiaopeng Hong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.17017v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2312.03640v2","updated":"2025-03-21T10:23:16Z","published":"2023-12-06T17:47:16Z","title":"Training Neural Networks on RAW and HDR Images for Restoration Tasks","summary":"  The vast majority of standard image and video content available online is\nrepresented in display-encoded color spaces, in which pixel values are\nconveniently scaled to a limited range (0-1) and the color distribution is\napproximately perceptually uniform. In contrast, both camera RAW and high\ndynamic range (HDR) images are often represented in linear color spaces, in\nwhich color values are linearly related to colorimetric quantities of light.\nWhile training on commonly available display-encoded images is a\nwell-established practice, there is no consensus on how neural networks should\nbe trained for tasks on RAW and HDR images in linear color spaces. In this\nwork, we test several approaches on three popular image restoration\napplications: denoising, deblurring, and single-image super-resolution. We\nexamine whether HDR/RAW images need to be display-encoded using popular\ntransfer functions (PQ, PU21, and mu-law), or whether it is better to train in\nlinear color spaces, but use loss functions that correct for perceptual\nnon-uniformity. Our results indicate that neural networks train significantly\nbetter on HDR and RAW images represented in display-encoded color spaces, which\noffer better perceptual uniformity than linear spaces. This small change to the\ntraining strategy can bring a very substantial gain in performance, between 2\nand 9 dB.\n","authors":["Andrew Yanzhe Ke","Lei Luo","Alexandre Chapiro","Xiaoyu Xiang","Yuchen Fan","Rakesh Ranjan","Rafal Mantiuk"],"pdf_url":"https://arxiv.org/pdf/2312.03640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05935v3","updated":"2025-03-21T10:19:01Z","published":"2024-02-08T18:59:48Z","title":"SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large\n  Language Models","summary":"  We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory\n","authors":["Dongyang Liu","Renrui Zhang","Longtian Qiu","Siyuan Huang","Weifeng Lin","Shitian Zhao","Shijie Geng","Ziyi Lin","Peng Jin","Kaipeng Zhang","Wenqi Shao","Chao Xu","Conghui He","Junjun He","Hao Shao","Pan Lu","Hongsheng Li","Yu Qiao","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2402.05935v3.pdf","comment":"Accepted by ICML 2024. Code and models are released at\n  https://github.com/Alpha-VLLM/LLaMA2-Accessory"},{"id":"http://arxiv.org/abs/2412.08376v2","updated":"2025-03-21T10:18:18Z","published":"2024-12-11T13:36:18Z","title":"Reloc3r: Large-Scale Training of Relative Camera Pose Regression for\n  Generalizable, Fast, and Accurate Visual Localization","summary":"  Visual localization aims to determine the camera pose of a query image\nrelative to a database of posed images. In recent years, deep neural networks\nthat directly regress camera poses have gained popularity due to their fast\ninference capabilities. However, existing methods struggle to either generalize\nwell to new scenes or provide accurate camera pose estimates. To address these\nissues, we present Reloc3r, a simple yet effective visual localization\nframework. It consists of an elegantly designed relative pose regression\nnetwork, and a minimalist motion averaging module for absolute pose estimation.\nTrained on approximately eight million posed image pairs, Reloc3r achieves\nsurprisingly good performance and generalization ability. We conduct extensive\nexperiments on six public datasets, consistently demonstrating the\neffectiveness and efficiency of the proposed method. It provides high-quality\ncamera pose estimates in real time and generalizes to novel scenes. Code:\nhttps://github.com/ffrivera0/reloc3r.\n","authors":["Siyan Dong","Shuzhe Wang","Shaohui Liu","Lulu Cai","Qingnan Fan","Juho Kannala","Yanchao Yang"],"pdf_url":"https://arxiv.org/pdf/2412.08376v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2311.17978v3","updated":"2025-03-21T10:15:21Z","published":"2023-11-29T17:24:04Z","title":"AutArch: An AI-assisted workflow for object detection and automated\n  recording in archaeological catalogues","summary":"  The context of this paper is the creation of large uniform archaeological\ndatasets from heterogeneous published resources, such as find catalogues - with\nthe help of AI and Big Data. The paper is concerned with the challenge of\nconsistent assemblages of archaeological data. We cannot simply combine\nexisting records, as they differ in terms of quality and recording standards.\nThus, records have to be recreated from published archaeological illustrations.\nThis is only a viable path with the help of automation. The contribution of\nthis paper is a new workflow for collecting data from archaeological find\ncatalogues available as legacy resources, such as archaeological drawings and\nphotographs in large unsorted PDF files; the workflow relies on custom software\n(AutArch) supporting image processing, object detection, and interactive means\nof validating and adjusting automatically retrieved data. We integrate\nartificial intelligence (AI) in terms of neural networks for object detection\nand classification into the workflow, thereby speeding up, automating, and\nstandardising data collection. Objects commonly found in archaeological\ncatalogues - such as graves, skeletons, ceramics, ornaments, stone tools and\nmaps - are detected. Those objects are spatially related and analysed to\nextract real-life attributes, such as the size and orientation of graves based\non the north arrow and the scale. We also automate recording of geometric\nwhole-outlines through contour detection, as an alternative to landmark-based\ngeometric morphometrics. Detected objects, contours, and other automatically\nretrieved data can be manually validated and adjusted. We use third millennium\nBC Europe (encompassing cultures such as 'Corded Ware' and 'Bell Beaker', and\ntheir burial practices) as a 'testing ground' and for evaluation purposes; this\nincludes a user study for the workflow and the AutArch software.\n","authors":["Kevin Klein","Antoine Muller","Alyssa Wohde","Alexander V. Gorelik","Volker Heyd","Ralf Lämmel","Yoan Diekmann","Maxime Brami"],"pdf_url":"https://arxiv.org/pdf/2311.17978v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16997v1","updated":"2025-03-21T10:03:32Z","published":"2025-03-21T10:03:32Z","title":"Steady Progress Beats Stagnation: Mutual Aid of Foundation and\n  Conventional Models in Mixed Domain Semi-Supervised Medical Image\n  Segmentation","summary":"  Large pretrained visual foundation models exhibit impressive general\ncapabilities. However, the extensive prior knowledge inherent in these models\ncan sometimes be a double-edged sword when adapting them to downstream tasks in\nspecific domains. In the context of semi-supervised medical image segmentation\nwith domain shift, foundation models like MedSAM tend to make overconfident\npredictions, some of which are incorrect. The error accumulation hinders the\neffective utilization of unlabeled data and limits further improvements. In\nthis paper, we introduce a Synergistic training framework for Foundation and\nConventional models (SynFoC) to address the issue. We observe that a\nconventional model trained from scratch has the ability to correct the\nhigh-confidence mispredictions of the foundation model, while the foundation\nmodel can supervise it with high-quality pseudo-labels in the early training\nstages. Furthermore, to enhance the collaborative training effectiveness of\nboth models and promote reliable convergence towards optimization, the\nconsensus-divergence consistency regularization is proposed. We demonstrate the\nsuperiority of our method across four public multi-domain datasets. In\nparticular, our method improves the Dice score by 10.31\\% on the Prostate\ndataset. Our code is available at https://github.com/MQinghe/SynFoC .\n","authors":["Qinghe Ma","Jian Zhang","Zekun Li","Lei Qi","Qian Yu","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2503.16997v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2407.09230v3","updated":"2025-03-21T09:57:02Z","published":"2024-07-12T12:49:11Z","title":"Surgical Text-to-Image Generation","summary":"  Acquiring surgical data for research and development is significantly\nhindered by high annotation costs and practical and ethical constraints.\nUtilizing synthetically generated images could offer a valuable alternative. In\nthis work, we explore adapting text-to-image generative models for the surgical\ndomain using the CholecT50 dataset, which provides surgical images annotated\nwith action triplets (instrument, verb, target). We investigate several\nlanguage models and find T5 to offer more distinct features for differentiating\nsurgical actions on triplet-based textual inputs, and showcasing stronger\nalignment between long and triplet-based captions. To address challenges in\ntraining text-to-image models solely on triplet-based captions without\nadditional inputs and supervisory signals, we discover that triplet text\nembeddings are instrument-centric in the latent space. Leveraging this insight,\nwe design an instrument-based class balancing technique to counteract data\nimbalance and skewness, improving training convergence. Extending Imagen, a\ndiffusion-based generative model, we develop Surgical Imagen to generate\nphotorealistic and activity-aligned surgical images from triplet-based textual\nprompts. We assess the model on quality, alignment, reasoning, and knowledge,\nachieving FID and CLIP scores of 3.7 and 26.8% respectively. Human expert\nsurvey shows that participants were highly challenged by the realistic\ncharacteristics of the generated samples, demonstrating Surgical Imagen's\neffectiveness as a practical alternative to real data collection.\n","authors":["Chinedu Innocent Nwoye","Rupak Bose","Kareem Elgohary","Lorenzo Arboit","Giorgio Carlino","Joël L. Lavanchy","Pietro Mascagni","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2407.09230v3.pdf","comment":"13 pages, 13 figures, 3 tables, published in Pattern Recognition\n  Letters 2025, project page at https://camma-public.github.io/endogen/"},{"id":"http://arxiv.org/abs/2503.16988v1","updated":"2025-03-21T09:54:42Z","published":"2025-03-21T09:54:42Z","title":"High Accuracy Pulmonary Vessel Segmentation for Contrast and\n  Non-contrast CT Images and Its Clinical Evaluation","summary":"  Accurate segmentation of pulmonary vessels plays a very critical role in\ndiagnosing and assessing various lung diseases. In clinical practice, diagnosis\nis typically carried out using CTPA images. However, there is a lack of\nhigh-precision pulmonary vessel segmentation algorithms for CTPA, and pulmonary\nvessel segmentation for NCCT poses an even greater challenge. In this study, we\npropose a 3D image segmentation algorithm for automated pulmonary vessel\nsegmentation from both contrast and non-contrast CT images. In the network, we\ndesigned a Vessel Lumen Structure Optimization Module (VLSOM), which extracts\nthe centerline of vessels and adjusts the weights based on the positional\ninformation and adds a Cl-Dice-Loss to supervise the stability of the vessels\nstructure. In addition, we designed a method for generating vessel GT from CTPA\nto NCCT for training models that support both CTPA and NCCT. In this work, we\nused 427 sets of high-precision annotated CT data from multiple vendors and\ncountries. Finally, our experimental model achieved Cl-Recall, Cl-DICE and\nRecall values of 0.879, 0.909, 0.934 (CTPA) and 0.928, 0.936, 0.955 (NCCT)\nrespectively. This shows that our model has achieved good performance in both\naccuracy and completeness of pulmonary vessel segmentation. In clinical visual\nevaluation, our model also had good segmentation performance on various disease\ntypes and can assist doctors in medical diagnosis, verifying the great\npotential of this method in clinical application.\n","authors":["Ying Ming","Shaoze Luo","Longfei Zhao","Qiqi Xu","Wei Song"],"pdf_url":"https://arxiv.org/pdf/2503.16988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16983v1","updated":"2025-03-21T09:48:00Z","published":"2025-03-21T09:48:00Z","title":"Enabling Versatile Controls for Video Diffusion Models","summary":"  Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.\n","authors":["Xu Zhang","Hao Zhou","Haoming Qin","Xiaobin Lu","Jiaxing Yan","Guanzhong Wang","Zeyu Chen","Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2503.16983v1.pdf","comment":"Codes and Supplementary Material:\n  http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl"},{"id":"http://arxiv.org/abs/2503.16980v1","updated":"2025-03-21T09:46:31Z","published":"2025-03-21T09:46:31Z","title":"Token Dynamics: Towards Efficient and Dynamic Video Token Representation\n  for Video Large Language Models","summary":"  Token-based video representation has emerged as a promising approach for\nenabling large language models to interpret video content. However, existing\ntoken reduction techniques, such as token pruning and token merging, often\ndisrupt essential spatial-temporal positional embeddings, failing to adequately\nbalance computational efficiency with fewer tokens. Consequently, these methods\nresult in relatively lengthy token sequences, limiting their applicability in\nscenarios requiring extreme token compression, such as video large language\nmodels. In this paper, we introduce the novel task of extreme short token\nreduction, aiming to represent extensive video sequences with a minimal number\nof tokens. To address this challenge, we propose Token Dynamics, a new video\nrepresentation framework that dynamically reduces token count while preserving\nspatial-temporal coherence. Specifically, we disentangle video representations\nby separating visual embeddings from grid-level motion information, structuring\nthem into: 1. a concise token base, created by clustering tokens that describe\nobject-level content; 2. a token dynamics map, capturing detailed\nspatial-temporal motion patterns across grids. Furthermore, we introduce a\ncross-dynamics attention mechanism that integrates motion features into the\ntoken base without increasing token length, thereby maintaining compactness and\nspatial-temporal integrity. The experiments demonstrate a reduction of token\ncount to merely 0.07% of the original tokens, with only a minor performance\ndrop of 1.13%. Additionally, we propose two novel subtasks within extreme token\nreduction (fixed-length and adaptive-length compression), both effectively\nrepresenting long token sequences for video-language tasks. Our method offers\nsignificantly lower theoretical complexity, fewer tokens, and enhanced\nthroughput, thus providing an efficient solution for video LLMs.\n","authors":["Haichao Zhang","Zhuowei Li","Dimitris Metaxas","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2503.16980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16979v1","updated":"2025-03-21T09:46:22Z","published":"2025-03-21T09:46:22Z","title":"Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic\n  Scene Reconstruction via Gaussian Splatting","summary":"  Building Free-Viewpoint Videos in a streaming manner offers the advantage of\nrapid responsiveness compared to offline training methods, greatly enhancing\nuser experience. However, current streaming approaches face challenges of high\nper-frame reconstruction time (10s+) and error accumulation, limiting their\nbroader application. In this paper, we propose Instant Gaussian Stream (IGS), a\nfast and generalizable streaming framework, to address these issues. First, we\nintroduce a generalized Anchor-driven Gaussian Motion Network, which projects\nmulti-view 2D motion features into 3D space, using anchor points to drive the\nmotion of all Gaussians. This generalized Network generates the motion of\nGaussians for each target frame in the time required for a single inference.\nSecond, we propose a Key-frame-guided Streaming Strategy that refines each key\nframe, enabling accurate reconstruction of temporally complex scenes while\nmitigating error accumulation. We conducted extensive in-domain and\ncross-domain evaluations, demonstrating that our approach can achieve streaming\nwith a average per-frame reconstruction time of 2s+, alongside a enhancement in\nview synthesis quality.\n","authors":["Jinbo Yan","Rui Peng","Zhiyan Wang","Luyang Tang","Jiayu Yang","Jie Liang","Jiahao Wu","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16976v1","updated":"2025-03-21T09:43:57Z","published":"2025-03-21T09:43:57Z","title":"GeoT: Geometry-guided Instance-dependent Transition Matrix for\n  Semi-supervised Tooth Point Cloud Segmentation","summary":"  Achieving meticulous segmentation of tooth point clouds from intra-oral scans\nstands as an indispensable prerequisite for various orthodontic applications.\nGiven the labor-intensive nature of dental annotation, a significant amount of\ndata remains unlabeled, driving increasing interest in semi-supervised\napproaches. One primary challenge of existing semi-supervised medical\nsegmentation methods lies in noisy pseudo labels generated for unlabeled data.\nTo address this challenge, we propose GeoT, the first framework that employs\ninstance-dependent transition matrix (IDTM) to explicitly model noise in pseudo\nlabels for semi-supervised dental segmentation. Specifically, to handle the\nextensive solution space of IDTM arising from tens of thousands of dental\npoints, we introduce tooth geometric priors through two key components:\npoint-level geometric regularization (PLGR) to enhance consistency between\npoint adjacency relationships in 3D and IDTM spaces, and class-level geometric\nsmoothing (CLGS) to leverage the fixed spatial distribution of tooth categories\nfor optimal IDTM estimation. Extensive experiments performed on the public\nTeeth3DS dataset and private dataset demonstrate that our method can make full\nutilization of unlabeled data to facilitate segmentation, achieving performance\ncomparable to fully supervised methods with only $20\\%$ of the labeled data.\n","authors":["Weihao Yu","Xiaoqing Guo","Chenxin Li","Yifan Liu","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.16976v1.pdf","comment":"IPMI2025"},{"id":"http://arxiv.org/abs/2503.16975v1","updated":"2025-03-21T09:43:42Z","published":"2025-03-21T09:43:42Z","title":"EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and\n  Generalized Vision","summary":"  Deep neural networks (DNNs) has shown great promise in computer vision tasks.\nHowever, machine vision achieved by DNNs cannot be as robust as human\nperception. Adversarial attacks and data distribution shifts have been known as\ntwo major scenarios which degrade machine performance and obstacle the wide\ndeployment of machines \"in the wild\". In order to break these obstructions and\nfacilitate the research of model robustness, we develop EasyRobust, a\ncomprehensive and easy-to-use toolkit for training, evaluation and analysis of\nrobust vision models. EasyRobust targets at two types of robustness: 1)\nAdversarial robustness enables the model to defense against malicious inputs\ncrafted by worst-case perturbations, also known as adversarial examples; 2)\nNon-adversarial robustness enhances the model performance on natural test\nimages with corruptions or distribution shifts. Thorough benchmarks on image\nclassification enable EasyRobust to provide an accurate robustness evaluation\non vision models. We wish our EasyRobust can help for training\npractically-robust models and promote academic and industrial progress in\nclosing the gap between human and machine vision. Codes and models of\nEasyRobust have been open-sourced in https://github.com/alibaba/easyrobust.\n","authors":["Xiaofeng Mao","Yuefeng Chen","Rong Zhang","Hui Xue","Zhao Li","Hang Su"],"pdf_url":"https://arxiv.org/pdf/2503.16975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16973v1","updated":"2025-03-21T09:41:24Z","published":"2025-03-21T09:41:24Z","title":"ARFlow: Human Action-Reaction Flow Matching with Physical Guidance","summary":"  Human action-reaction synthesis, a fundamental challenge in modeling causal\nhuman interactions, plays a critical role in applications ranging from virtual\nreality to social robotics. While diffusion-based models have demonstrated\npromising performance, they exhibit two key limitations for interaction\nsynthesis: reliance on complex noise-to-reaction generators with intricate\nconditional mechanisms, and frequent physical violations in generated motions.\nTo address these issues, we propose Action-Reaction Flow Matching (ARFlow), a\nnovel framework that establishes direct action-to-reaction mappings,\neliminating the need for complex conditional mechanisms. Our approach\nintroduces two key innovations: an x1-prediction method that directly outputs\nhuman motions instead of velocity fields, enabling explicit constraint\nenforcement; and a training-free, gradient-based physical guidance mechanism\nthat effectively prevents body penetration artifacts during sampling. Extensive\nexperiments on NTU120 and Chi3D datasets demonstrate that ARFlow not only\noutperforms existing methods in terms of Fr\\'echet Inception Distance and\nmotion diversity but also significantly reduces body collisions, as measured by\nour new Intersection Volume and Intersection Frequency metrics.\n","authors":["Wentao Jiang","Jingya Wang","Haotao Lu","Kaiyang Ji","Baoxiong Jia","Siyuan Huang","Ye Shi"],"pdf_url":"https://arxiv.org/pdf/2503.16973v1.pdf","comment":"Computer Vision and Pattern Recognition (cs.CV); Artificial\n  Intelligence (cs.AI)"},{"id":"http://arxiv.org/abs/2503.12485v2","updated":"2025-03-21T09:36:55Z","published":"2025-03-16T12:34:07Z","title":"Cross-Modal Consistency Learning for Sign Language Recognition","summary":"  Pre-training has been proven to be effective in boosting the performance of\nIsolated Sign Language Recognition (ISLR). Existing pre-training methods solely\nfocus on the compact pose data, which eliminates background perturbation but\ninevitably suffers from insufficient semantic cues compared to raw RGB videos.\nNevertheless, learning representation directly from RGB videos remains\nchallenging due to the presence of sign-independent visual features. To address\nthis dilemma, we propose a Cross-modal Consistency Learning framework\n(CCL-SLR), which leverages the cross-modal consistency from both RGB and pose\nmodalities based on self-supervised pre-training. First, CCL-SLR employs\ncontrastive learning for instance discrimination within and across modalities.\nThrough the single-modal and cross-modal contrastive learning, CCL-SLR\ngradually aligns the feature spaces of RGB and pose modalities, thereby\nextracting consistent sign representations. Second, we further introduce\nMotion-Preserving Masking (MPM) and Semantic Positive Mining (SPM) techniques\nto improve cross-modal consistency from the perspective of data augmentation\nand sample similarity, respectively. Extensive experiments on four ISLR\nbenchmarks show that CCL-SLR achieves impressive performance, demonstrating its\neffectiveness. The code will be released to the public.\n","authors":["Kepeng Wu","Zecheng Li","Hezhen Hu","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2503.12485v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16970v1","updated":"2025-03-21T09:34:01Z","published":"2025-03-21T09:34:01Z","title":"Distilling Monocular Foundation Model for Fine-grained Depth Completion","summary":"  Depth completion involves predicting dense depth maps from sparse LiDAR\ninputs. However, sparse depth annotations from sensors limit the availability\nof dense supervision, which is necessary for learning detailed geometric\nfeatures. In this paper, we propose a two-stage knowledge distillation\nframework that leverages powerful monocular foundation models to provide dense\nsupervision for depth completion. In the first stage, we introduce a\npre-training strategy that generates diverse training data from natural images,\nwhich distills geometric knowledge to depth completion. Specifically, we\nsimulate LiDAR scans by utilizing monocular depth and mesh reconstruction,\nthereby creating training data without requiring ground-truth depth. Besides,\nmonocular depth estimation suffers from inherent scale ambiguity in real-world\nsettings. To address this, in the second stage, we employ a scale- and\nshift-invariant loss (SSI Loss) to learn real-world scales when fine-tuning on\nreal-world datasets. Our two-stage distillation framework enables depth\ncompletion models to harness the strengths of monocular foundation models.\nExperimental results demonstrate that models trained with our two-stage\ndistillation framework achieve state-of-the-art performance, ranking\n\\textbf{first place} on the KITTI benchmark. Code is available at\nhttps://github.com/Sharpiless/DMD3C\n","authors":["Yingping Liang","Yutao Hu","Wenqi Shao","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2503.16970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11935v3","updated":"2025-03-21T09:31:13Z","published":"2025-03-15T00:59:34Z","title":"Design of an Expression Recognition Solution Based on the Global\n  Channel-Spatial Attention Mechanism and Proportional Criterion Fusion","summary":"  Facial expression recognition is a challenging classification task that holds\nbroad application prospects in the field of human-computer interaction. This\npaper aims to introduce the method we will adopt in the 8th Affective and\nBehavioral Analysis in the Wild (ABAW) Competition, which will be held during\nthe Conference on Computer Vision and Pattern Recognition (CVPR) in 2025.First\nof all, we apply the frequency masking technique and the method of extracting\ndata at equal time intervals to conduct targeted processing on the original\nvideos. Then, based on the residual hybrid convolutional neural network and the\nmulti-branch convolutional neural network respectively, we design feature\nextraction models for image and audio sequences. In particular, we propose a\nglobal channel-spatial attention mechanism to enhance the features initially\nextracted from both the audio and image modalities respectively.Finally, we\nadopt a decision fusion strategy based on the proportional criterion to fuse\nthe classification results of the two single modalities, obtain an emotion\nprobability vector, and output the final emotional classification. We also\ndesign a coarse - fine granularity loss function to optimize the performance of\nthe entire network, which effectively improves the accuracy of facial\nexpression recognition.In the facial expression recognition task of the 8th\nABAW Competition, our method ranked third on the official validation set. This\nresult fully confirms the effectiveness and competitiveness of the method we\nhave proposed.\n","authors":["Jun Yu","Yang Zheng","Lei Wang","Yongqi Wang","Shengfan Xu"],"pdf_url":"https://arxiv.org/pdf/2503.11935v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16965v1","updated":"2025-03-21T09:25:23Z","published":"2025-03-21T09:25:23Z","title":"When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making","summary":"  Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.\n","authors":["Zhe Hu","Jing Li","Yu Yin"],"pdf_url":"https://arxiv.org/pdf/2503.16965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00254v4","updated":"2025-03-21T09:24:14Z","published":"2023-12-30T14:53:09Z","title":"Morphing Tokens Draw Strong Masked Image Models","summary":"  Masked image modeling (MIM) has emerged as a promising approach for\npre-training Vision Transformers (ViTs). MIMs predict masked tokens token-wise\nto recover target signals that are tokenized from images or generated by\npre-trained models like vision-language models. While using tokenizers or\npre-trained models is viable, they often offer spatially inconsistent\nsupervision even for neighboring tokens, hindering models from learning\ndiscriminative representations. Our pilot study identifies spatial\ninconsistency in supervisory signals and suggests that addressing it can\nimprove representation learning. Building upon this insight, we introduce\nDynamic Token Morphing (DTM), a novel method that dynamically aggregates tokens\nwhile preserving context to generate contextualized targets, thereby likely\nreducing spatial inconsistency. DTM is compatible with various SSL frameworks;\nwe showcase significantly improved MIM results, barely introducing extra\ntraining costs. Our method facilitates MIM training by using more spatially\nconsistent targets, resulting in improved training trends as evidenced by lower\nlosses. Experiments on ImageNet-1K and ADE20K demonstrate DTM's superiority,\nwhich surpasses complex state-of-the-art MIM methods. Furthermore, the\nevaluation of transfer learning on downstream tasks like iNaturalist, along\nwith extensive empirical studies, supports DTM's effectiveness.\n","authors":["Taekyung Kim","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2401.00254v4.pdf","comment":"24 pages, 16 tables, 8 figures. To be presented at ICLR'25"},{"id":"http://arxiv.org/abs/2503.16964v1","updated":"2025-03-21T09:21:43Z","published":"2025-03-21T09:21:43Z","title":"DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from\n  In-the-Wild Drone Imagery","summary":"  Drones have become essential tools for reconstructing wild scenes due to\ntheir outstanding maneuverability. Recent advances in radiance field methods\nhave achieved remarkable rendering quality, providing a new avenue for 3D\nreconstruction from drone imagery. However, dynamic distractors in wild\nenvironments challenge the static scene assumption in radiance fields, while\nlimited view constraints hinder the accurate capture of underlying scene\ngeometry. To address these challenges, we introduce DroneSplat, a novel\nframework designed for robust 3D reconstruction from in-the-wild drone imagery.\nOur method adaptively adjusts masking thresholds by integrating local-global\nsegmentation heuristics with statistical approaches, enabling precise\nidentification and elimination of dynamic distractors in static scenes. We\nenhance 3D Gaussian Splatting with multi-view stereo predictions and a\nvoxel-guided optimization strategy, supporting high-quality rendering under\nlimited view constraints. For comprehensive evaluation, we provide a\ndrone-captured 3D reconstruction dataset encompassing both dynamic and static\nscenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS\nand NeRF baselines in handling in-the-wild drone imagery.\n","authors":["Jiadong Tang","Yu Gao","Dianyi Yang","Liqi Yan","Yufeng Yue","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2503.16964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16963v1","updated":"2025-03-21T09:21:37Z","published":"2025-03-21T09:21:37Z","title":"Center-guided Classifier for Semantic Segmentation of Remote Sensing\n  Images","summary":"  Compared with natural images, remote sensing images (RSIs) have the unique\ncharacteristic. i.e., larger intraclass variance, which makes semantic\nsegmentation for remote sensing images more challenging. Moreover, existing\nsemantic segmentation models for remote sensing images usually employ a vanilla\nsoftmax classifier, which has three drawbacks: (1) non-direct supervision for\nthe pixel representations during training; (2) inadequate modeling ability of\nparametric softmax classifiers under large intraclass variance; and (3) opaque\nprocess of classification decision. In this paper, we propose a novel\nclassifier (called CenterSeg) customized for RSI semantic segmentation, which\nsolves the abovementioned problems with multiple prototypes, direct supervision\nunder Grassmann manifold, and interpretability strategy. Specifically, for each\nclass, our CenterSeg obtains local class centers by aggregating corresponding\npixel features based on ground-truth masks, and generates multiple prototypes\nthrough hard attention assignment and momentum updating. In addition, we\nintroduce the Grassmann manifold and constrain the joint embedding space of\npixel features and prototypes based on two additional regularization terms.\nEspecially, during the inference, CenterSeg can further provide\ninterpretability to the model by restricting the prototype as a sample of the\ntraining set. Experimental results on three remote sensing segmentation\ndatasets validate the effectiveness of the model. Besides the superior\nperformance, CenterSeg has the advantages of simplicity, lightweight,\ncompatibility, and interpretability. Code is available at\nhttps://github.com/xwmaxwma/rssegmentation.\n","authors":["Wei Zhang","Mengting Ma","Yizhen Jiang","Rongrong Lian","Zhenkai Wu","Kangning Cui","Xiaowen Ma"],"pdf_url":"https://arxiv.org/pdf/2503.16963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10705v2","updated":"2025-03-21T09:15:37Z","published":"2025-03-12T15:48:13Z","title":"Enhanced Continual Learning of Vision-Language Models with Model Fusion","summary":"  Vision-Language Models (VLMs) represent a breakthrough in artificial\nintelligence by integrating visual and textual modalities to achieve impressive\nzero-shot capabilities. However, VLMs are susceptible to catastrophic\nforgetting when sequentially fine-tuned on multiple downstream tasks. Existing\ncontinual learning methods for VLMs often rely heavily on additional reference\ndatasets, compromise zero-shot performance, or are limited to\nparameter-efficient fine-tuning scenarios. In this paper, we propose Continual\nDecoupling-Unifying (ConDU), a novel approach, by introducing model fusion into\ncontinual learning for VLMs. ConDU maintains a unified model along with task\ntriggers and prototype sets, employing an iterative process of decoupling\ntask-specific models for previous tasks and unifying them with the model for\nthe newly learned task. Additionally, we introduce an inference strategy for\nzero-shot scenarios by aggregating predictions from multiple decoupled\ntask-specific models. Extensive experiments across various settings show that\nConDU achieves up to a 2\\% improvement in average performance across all seen\ntasks compared to state-of-the-art baselines, while also enhancing zero-shot\ncapabilities relative to the original VLM.\n","authors":["Haoyuan Gao","Zicong Zhang","Yuqi Wei","Linglan Zhao","Guilin Li","Yexin Li","Linghe Kong","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2503.10705v2.pdf","comment":"Accepted by ICLR 2025 workshop"},{"id":"http://arxiv.org/abs/2411.15604v2","updated":"2025-03-21T09:08:03Z","published":"2024-11-23T16:47:48Z","title":"FATE: Full-head Gaussian Avatar with Textural Editing from Monocular\n  Video","summary":"  Reconstructing high-fidelity, animatable 3D head avatars from effortlessly\ncaptured monocular videos is a pivotal yet formidable challenge. Although\nsignificant progress has been made in rendering performance and manipulation\ncapabilities, notable challenges remain, including incomplete reconstruction\nand inefficient Gaussian representation. To address these challenges, we\nintroduce FATE, a novel method for reconstructing an editable full-head avatar\nfrom a single monocular video. FATE integrates a sampling-based densification\nstrategy to ensure optimal positional distribution of points, improving\nrendering efficiency. A neural baking technique is introduced to convert\ndiscrete Gaussian representations into continuous attribute maps, facilitating\nintuitive appearance editing. Furthermore, we propose a universal completion\nframework to recover non-frontal appearance, culminating in a\n360$^\\circ$-renderable 3D head avatar. FATE outperforms previous approaches in\nboth qualitative and quantitative evaluations, achieving state-of-the-art\nperformance. To the best of our knowledge, FATE is the first animatable and\n360$^\\circ$ full-head monocular reconstruction method for a 3D head avatar.\n","authors":["Jiawei Zhang","Zijian Wu","Zhiyang Liang","Yicheng Gong","Dongfang Hu","Yao Yao","Xun Cao","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.15604v2.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.16956v1","updated":"2025-03-21T09:02:38Z","published":"2025-03-21T09:02:38Z","title":"From Faces to Voices: Learning Hierarchical Representations for\n  High-quality Video-to-Speech","summary":"  The objective of this study is to generate high-quality speech from silent\ntalking face videos, a task also known as video-to-speech synthesis. A\nsignificant challenge in video-to-speech synthesis lies in the substantial\nmodality gap between silent video and multi-faceted speech. In this paper, we\npropose a novel video-to-speech system that effectively bridges this modality\ngap, significantly enhancing the quality of synthesized speech. This is\nachieved by learning of hierarchical representations from video to speech.\nSpecifically, we gradually transform silent video into acoustic feature spaces\nthrough three sequential stages -- content, timbre, and prosody modeling. In\neach stage, we align visual factors -- lip movements, face identity, and facial\nexpressions -- with corresponding acoustic counterparts to ensure the seamless\ntransformation. Additionally, to generate realistic and coherent speech from\nthe visual representations, we employ a flow matching model that estimates\ndirect trajectories from a simple prior distribution to the target speech\ndistribution. Extensive experiments demonstrate that our method achieves\nexceptional generation quality comparable to real utterances, outperforming\nexisting methods by a significant margin.\n","authors":["Ji-Hoon Kim","Jeongsoo Choi","Jaehun Kim","Chaeyoung Jung","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2503.16956v1.pdf","comment":"CVPR 2025, demo page: https://mm.kaist.ac.kr/projects/faces2voices/"},{"id":"http://arxiv.org/abs/2412.06699v3","updated":"2025-03-21T08:55:03Z","published":"2024-12-09T17:44:56Z","title":"You See it, You Got it: Learning 3D Creation on Pose-Free Videos at\n  Scale","summary":"  Recent 3D generation models typically rely on limited-scale 3D `gold-labels'\nor 2D diffusion priors for 3D content creation. However, their performance is\nupper-bounded by constrained 3D priors due to the lack of scalable learning\nparadigms. In this work, we present See3D, a visual-conditional multi-view\ndiffusion model trained on large-scale Internet videos for open-world 3D\ncreation. The model aims to Get 3D knowledge by solely Seeing the visual\ncontents from the vast and rapidly growing video data -- You See it, You Got\nit. To achieve this, we first scale up the training data using a proposed data\ncuration pipeline that automatically filters out multi-view inconsistencies and\ninsufficient observations from source videos. This results in a high-quality,\nrichly diverse, large-scale dataset of multi-view images, termed WebVi3D,\ncontaining 320M frames from 16M video clips. Nevertheless, learning generic 3D\npriors from videos without explicit 3D geometry or camera pose annotations is\nnontrivial, and annotating poses for web-scale videos is prohibitively\nexpensive. To eliminate the need for pose conditions, we introduce an\ninnovative visual-condition - a purely 2D-inductive visual signal generated by\nadding time-dependent noise to the masked video data. Finally, we introduce a\nnovel visual-conditional 3D generation framework by integrating See3D into a\nwarping-based pipeline for high-fidelity 3D generation. Our numerical and\nvisual comparisons on single and sparse reconstruction benchmarks show that\nSee3D, trained on cost-effective and scalable video data, achieves notable\nzero-shot and open-world generation capabilities, markedly outperforming models\ntrained on costly and constrained 3D datasets. Please refer to our project page\nat: https://vision.baai.ac.cn/see3d\n","authors":["Baorui Ma","Huachen Gao","Haoge Deng","Zhengxiong Luo","Tiejun Huang","Lulu Tang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.06699v3.pdf","comment":"Accepted by CVPR 2025, Project Page: https://vision.baai.ac.cn/see3d"},{"id":"http://arxiv.org/abs/2503.16948v1","updated":"2025-03-21T08:53:14Z","published":"2025-03-21T08:53:14Z","title":"MagicColor: Multi-Instance Sketch Colorization","summary":"  We present \\textit{MagicColor}, a diffusion-based framework for\nmulti-instance sketch colorization. The production of multi-instance 2D line\nart colorization adheres to an industry-standard workflow, which consists of\nthree crucial stages: the design of line art characters, the coloring of\nindividual objects, and the refinement process. The artists are required to\nrepeat the process of coloring each instance one by one, which is inaccurate\nand inefficient. Meanwhile, current generative methods fail to solve this task\ndue to the challenge of multi-instance pair data collection. To tackle these\nchallenges, we incorporate three technical designs to ensure precise character\ndetail transcription and achieve multi-instance sketch colorization in a single\nforward. Specifically, we first propose the self-play training strategy to\nsolve the lack of training data. Then we introduce an instance guider to feed\nthe color of the instance. To achieve accurate color matching, we present\nfine-grained color matching with edge loss to enhance visual quality. Equipped\nwith the proposed modules, MagicColor enables automatically transforming\nsketches into vividly-colored images with accurate consistency and\nmulti-instance control. Experiments on our collected datasets show that our\nmodel outperforms existing methods regarding chromatic precision. Specifically,\nour model critically automates the colorization process with zero manual\nadjustments, so novice users can produce stylistically consistent artwork by\nproviding reference instances and the original line art. Our code and\nadditional details are available at https://yinhan-zhang.github.io/color\n","authors":["Yinhan Zhang","Yue Ma","Bingyuan Wang","Qifeng Chen","Zeyu Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16945v1","updated":"2025-03-21T08:45:50Z","published":"2025-03-21T08:45:50Z","title":"PE-CLIP: A Parameter-Efficient Fine-Tuning of Vision Language Models for\n  Dynamic Facial Expression Recognition","summary":"  Vision-Language Models (VLMs) like CLIP offer promising solutions for Dynamic\nFacial Expression Recognition (DFER) but face challenges such as inefficient\nfull fine-tuning, high complexity, and poor alignment between textual and\nvisual representations. Additionally, existing methods struggle with\nineffective temporal modeling. To address these issues, we propose PE-CLIP, a\nparameter-efficient fine-tuning (PEFT) framework that adapts CLIP for DFER\nwhile significantly reducing trainable parameters while maintaining high\naccuracy. PE-CLIP introduces two specialized adapters: a Temporal Dynamic\nAdapter (TDA) and a Shared Adapter (ShA). The TDA is a GRU-based module with\ndynamic scaling that captures sequential dependencies while emphasizing\ninformative temporal features and suppressing irrelevant variations. The ShA is\na lightweight adapter that refines representations within both textual and\nvisual encoders, ensuring consistency and efficiency. Additionally, we\nintegrate Multi-modal Prompt Learning (MaPLe), introducing learnable prompts\nfor visual and action unit-based textual inputs, enhancing semantic alignment\nbetween modalities and enabling efficient CLIP adaptation for dynamic tasks. We\nevaluate PE-CLIP on two benchmark datasets, DFEW and FERV39K, achieving\ncompetitive performance compared to state-of-the-art methods while requiring\nfewer trainable parameters. By balancing efficiency and accuracy, PE-CLIP sets\na new benchmark in resource-efficient DFER. The source code of the proposed\nPE-CLIP will be publicly available at https://github.com/Ibtissam-SAADI/PE-CLIP .\n","authors":["Ibtissam Saadi","Abdenour Hadid","Douglas W. Cunningham","Abdelmalik Taleb-Ahmed","Yassin El Hillali"],"pdf_url":"https://arxiv.org/pdf/2503.16945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16944v1","updated":"2025-03-21T08:44:27Z","published":"2025-03-21T08:44:27Z","title":"HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait\n  Synthesis","summary":"  Personalized portrait synthesis, essential in domains like social\nentertainment, has recently made significant progress. Person-wise fine-tuning\nbased methods, such as LoRA and DreamBooth, can produce photorealistic outputs\nbut need training on individual samples, consuming time and resources and\nposing an unstable risk. Adapter based techniques such as IP-Adapter freeze the\nfoundational model parameters and employ a plug-in architecture to enable\nzero-shot inference, but they often exhibit a lack of naturalness and\nauthenticity, which are not to be overlooked in portrait synthesis tasks. In\nthis paper, we introduce a parameter-efficient adaptive generation method,\nnamely HyperLoRA, that uses an adaptive plug-in network to generate LoRA\nweights, merging the superior performance of LoRA with the zero-shot capability\nof adapter scheme. Through our carefully designed network structure and\ntraining strategy, we achieve zero-shot personalized portrait generation\n(supporting both single and multiple image inputs) with high photorealism,\nfidelity, and editability.\n","authors":["Mengtian Li","Jinshu Chen","Wanquan Feng","Bingchuan Li","Fei Dai","Songtao Zhao","Qian He"],"pdf_url":"https://arxiv.org/pdf/2503.16944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14275v2","updated":"2025-03-21T08:42:51Z","published":"2025-03-18T14:10:43Z","title":"Free-Lunch Color-Texture Disentanglement for Stylized Image Generation","summary":"  Recent advances in Text-to-Image (T2I) diffusion models have transformed\nimage generation, enabling significant progress in stylized generation using\nonly a few style reference images. However, current diffusion-based methods\nstruggle with fine-grained style customization due to challenges in controlling\nmultiple style attributes, such as color and texture. This paper introduces the\nfirst tuning-free approach to achieve free-lunch color-texture disentanglement\nin stylized T2I generation, addressing the need for independently controlled\nstyle elements for the Disentangled Stylized Image Generation (DisIG) problem.\nOur approach leverages the Image-Prompt Additivity property in the CLIP image\nembedding space to develop techniques for separating and extracting\nColor-Texture Embeddings (CTE) from individual color and texture reference\nimages. To ensure that the color palette of the generated image aligns closely\nwith the color reference, we apply a whitening and coloring transformation to\nenhance color consistency. Additionally, to prevent texture loss due to the\nsignal-leak bias inherent in diffusion training, we introduce a noise term that\npreserves textural fidelity during the Regularized Whitening and Coloring\nTransformation (RegWCT). Through these methods, our Style Attributes\nDisentanglement approach (SADis) delivers a more precise and customizable\nsolution for stylized image generation. Experiments on images from the WikiArt\nand StyleDrop datasets demonstrate that, both qualitatively and quantitatively,\nSADis surpasses state-of-the-art stylization methods in the DisIG task.Code\nwill be released at https://deepffff.github.io/sadis.github.io/.\n","authors":["Jiang Qin","Senmao Li","Alexandra Gomez-Villa","Shiqi Yang","Yaxing Wang","Kai Wang","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2503.14275v2.pdf","comment":"Code will be available at https://deepffff.github.io/sadis.github.io/"},{"id":"http://arxiv.org/abs/2501.05933v2","updated":"2025-03-21T08:41:23Z","published":"2025-01-10T12:56:18Z","title":"Weakly Supervised Segmentation of Hyper-Reflective Foci with Compact\n  Convolutional Transformers and SAM2","summary":"  Weakly supervised segmentation has the potential to greatly reduce the\nannotation effort for training segmentation models for small structures such as\nhyper-reflective foci (HRF) in optical coherence tomography (OCT). However,\nmost weakly supervised methods either involve a strong downsampling of input\nimages, or only achieve localization at a coarse resolution, both of which are\nunsatisfactory for small structures. We propose a novel framework that\nincreases the spatial resolution of a traditional attention-based Multiple\nInstance Learning (MIL) approach by using Layer-wise Relevance Propagation\n(LRP) to prompt the Segment Anything Model (SAM~2), and increases recall with\niterative inference. Moreover, we demonstrate that replacing MIL with a Compact\nConvolutional Transformer (CCT), which adds a positional encoding, and permits\nan exchange of information between different regions of the OCT image, leads to\na further and substantial increase in segmentation accuracy.\n","authors":["Olivier Morelle","Justus Bisten","Maximilian W. M. Wintergerst","Robert P. Finger","Thomas Schultz"],"pdf_url":"https://arxiv.org/pdf/2501.05933v2.pdf","comment":"7 pages, 1 figure, accepted at German Conference on Medical Image\n  Computing 2025"},{"id":"http://arxiv.org/abs/2503.16942v1","updated":"2025-03-21T08:40:35Z","published":"2025-03-21T08:40:35Z","title":"Re-HOLD: Video Hand Object Interaction Reenactment via adaptive\n  Layout-instructed Diffusion Model","summary":"  Current digital human studies focusing on lip-syncing and body movement are\nno longer sufficient to meet the growing industrial demand, while human video\ngeneration techniques that support interacting with real-world environments\n(e.g., objects) have not been well investigated. Despite human hand synthesis\nalready being an intricate problem, generating objects in contact with hands\nand their interactions presents an even more challenging task, especially when\nthe objects exhibit obvious variations in size and shape. To cope with these\nissues, we present a novel video Reenactment framework focusing on Human-Object\nInteraction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD).\nOur key insight is to employ specialized layout representation for hands and\nobjects, respectively. Such representations enable effective disentanglement of\nhand modeling and object adaptation to diverse motion sequences. To further\nimprove the generation quality of HOI, we have designed an interactive textural\nenhancement module for both hands and objects by introducing two independent\nmemory banks. We also propose a layout-adjusting strategy for the cross-object\nreenactment scenario to adaptively adjust unreasonable layouts caused by\ndiverse object sizes during inference. Comprehensive qualitative and\nquantitative evaluations demonstrate that our proposed framework significantly\noutperforms existing methods. Project page: https://fyycs.github.io/Re-HOLD.\n","authors":["Yingying Fan","Quanwei Yang","Kaisiyuan Wang","Hang Zhou","Yingying Li","Haocheng Feng","Yu Wu","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06903v2","updated":"2025-03-21T08:37:44Z","published":"2025-03-10T04:12:56Z","title":"When Lighting Deceives: Exposing Vision-Language Models' Illumination\n  Vulnerability Through Illumination Transformation Attack","summary":"  Vision-Language Models (VLMs) have achieved remarkable success in various\ntasks, yet their robustness to real-world illumination variations remains\nlargely unexplored. To bridge this gap, we propose \\textbf{I}llumination\n\\textbf{T}ransformation \\textbf{A}ttack (\\textbf{ITA}), the first framework to\nsystematically assess VLMs' robustness against illumination changes. However,\nthere still exist two key challenges: (1) how to model global illumination with\nfine-grained control to achieve diverse lighting conditions and (2) how to\nensure adversarial effectiveness while maintaining naturalness. To address the\nfirst challenge, we innovatively decompose global illumination into multiple\nparameterized point light sources based on the illumination rendering equation.\nThis design enables us to model more diverse lighting variations that previous\nmethods could not capture. Then, by integrating these parameterized lighting\nvariations with physics-based lighting reconstruction techniques, we could\nprecisely render such light interactions in the original scenes, finally\nmeeting the goal of fine-grained lighting control. For the second challenge, by\ncontrolling illumination through the lighting reconstrution model's latent\nspace rather than direct pixel manipulation, we inherently preserve physical\nlighting priors. Furthermore, to prevent potential reconstruction artifacts, we\ndesign additional perceptual constraints for maintaining visual consistency\nwith original images and diversity constraints for avoiding light source\nconvergence.\n  Extensive experiments demonstrate that our ITA could significantly reduce the\nperformance of advanced VLMs, e.g., LLaVA-1.6, while possessing competitive\nnaturalness, exposing VLMS' critical illuminiation vulnerabilities.\n","authors":["Hanqing Liu","Shouwei Ruan","Yao Huang","Shiji Zhao","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2503.06903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09258v2","updated":"2025-03-21T08:19:55Z","published":"2024-01-17T15:05:00Z","title":"Efficient Training of Generalizable Visuomotor Policies via\n  Control-Aware Augmentation","summary":"  Improving generalization is one key challenge in embodied AI, where obtaining\nlarge-scale datasets across diverse scenarios is costly. Traditional weak\naugmentations, such as cropping and flipping, are insufficient for improving a\nmodel's performance in new environments. Existing data augmentation methods\noften disrupt task-relevant information in images, potentially degrading\nperformance. To overcome these challenges, we introduce EAGLE, an efficient\ntraining framework for generalizable visuomotor policies that improves upon\nexisting methods by (1) enhancing generalization by applying augmentation only\nto control-related regions identified through a self-supervised control-aware\nmask and (2) improving training stability and efficiency by distilling\nknowledge from an expert to a visuomotor student policy, which is then deployed\nto unseen environments without further fine-tuning. Comprehensive experiments\non three domains, including the DMControl Generalization Benchmark, the\nenhanced Robot Manipulation Distraction Benchmark, and a long-sequential\ndrawer-opening task, validate the effectiveness of our method.\n","authors":["Yinuo Zhao","Kun Wu","Tianjiao Yi","Zhiyuan Xu","Xiaozhu Ju","Zhengping Che","Chi Harold Liu","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2401.09258v2.pdf","comment":null}]},"2025-03-24T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2503.18945v1","updated":"2025-03-24T17:59:51Z","published":"2025-03-24T17:59:51Z","title":"Aether: Geometric-Aware Unified World Modeling","summary":"  The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance far\nexceeds that of domain-specific models. Additionally, Aether leverages a\ngeometry-informed action space to seamlessly translate predictions into\nactions, enabling effective autonomous trajectory planning. We hope our work\ninspires the community to explore new frontiers in physically-reasonable world\nmodeling and its applications.\n","authors":[" Aether Team","Haoyi Zhu","Yifan Wang","Jianjun Zhou","Wenzheng Chang","Yang Zhou","Zizun Li","Junyi Chen","Chunhua Shen","Jiangmiao Pang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2503.18945v1.pdf","comment":"Project Page: https://aether-world.github.io/"},{"id":"http://arxiv.org/abs/2503.18938v1","updated":"2025-03-24T17:58:15Z","published":"2025-03-24T17:58:15Z","title":"AdaWorld: Learning Adaptable World Models with Latent Actions","summary":"  World models aim to learn action-controlled prediction models and have proven\nessential for the development of intelligent agents. However, most existing\nworld models rely heavily on substantial action-labeled data and costly\ntraining, making it challenging to adapt to novel environments with\nheterogeneous actions through limited interactions. This limitation can hinder\ntheir applicability across broader domains. To overcome this challenge, we\npropose AdaWorld, an innovative world model learning approach that enables\nefficient adaptation. The key idea is to incorporate action information during\nthe pretraining of world models. This is achieved by extracting latent actions\nfrom videos in a self-supervised manner, capturing the most critical\ntransitions between frames. We then develop an autoregressive world model that\nconditions on these latent actions. This learning paradigm enables highly\nadaptable world models, facilitating efficient transfer and learning of new\nactions even with limited interactions and finetuning. Our comprehensive\nexperiments across multiple environments demonstrate that AdaWorld achieves\nsuperior performance in both simulation quality and visual planning.\n","authors":["Shenyuan Gao","Siyuan Zhou","Yilun Du","Jun Zhang","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2503.18938v1.pdf","comment":"Project page: https://adaptable-world-model.github.io/"},{"id":"http://arxiv.org/abs/2503.18914v1","updated":"2025-03-24T17:26:20Z","published":"2025-03-24T17:26:20Z","title":"Autonomous Generation of Sub-goals for Lifelong Learning in Robots","summary":"  One of the challenges of open-ended learning in robots is the need to\nautonomously discover goals and learn skills to achieve them. However, when in\nlifelong learning settings, it is always desirable to generate sub-goals with\ntheir associated skills, without relying on explicit reward, as steppingstones\nto a goal. This allows sub-goals and skills to be reused to facilitate\nachieving other goals. This work proposes a two-pronged approach for sub-goal\ngeneration to address this challenge: a top-down approach, where sub-goals are\nhierarchically derived from general goals using intrinsic motivations to\ndiscover them, and a bottom-up approach, where sub-goal chains emerge from\nmaking latent relationships between goals and perceptual classes that were\npreviously learned in different domains explicit. These methods help the robot\nto autonomously generate and chain sub-goals as a way to achieve more general\ngoals. Additionally, they create more abstract representations of goals,\nhelping to reduce sub-goal duplication and make the learning of skills more\nefficient. Implemented within an existing cognitive architecture for lifelong\nopen-ended learning and tested with a real robot, our approach enhances the\nrobot's ability to discover and achieve goals, generate sub-goals in an\nefficient manner, generalize learned skills, and operate in dynamic and unknown\nenvironments without explicit intermediate rewards.\n","authors":["Emanuel Fallas Hernández","Sergio Martínez Alonso","Alejandro Romero","Jose A. Becerra Permuy","Richard J. Duro"],"pdf_url":"https://arxiv.org/pdf/2503.18914v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2503.18897v1","updated":"2025-03-24T17:09:36Z","published":"2025-03-24T17:09:36Z","title":"Online 3D Scene Reconstruction Using Neural Object Priors","summary":"  This paper addresses the problem of reconstructing a scene online at the\nlevel of objects given an RGB-D video sequence. While current object-aware\nneural implicit representations hold promise, they are limited in online\nreconstruction efficiency and shape completion. Our main contributions to\nalleviate the above limitations are twofold. First, we propose a feature grid\ninterpolation mechanism to continuously update grid-based object-centric neural\nimplicit representations as new object parts are revealed. Second, we construct\nan object library with previously mapped objects in advance and leverage the\ncorresponding shape priors to initialize geometric object models in new videos,\nsubsequently completing them with novel views as well as synthesized past views\nto avoid losing original object details. Extensive experiments on synthetic\nenvironments from the Replica dataset, real-world ScanNet sequences and videos\ncaptured in our laboratory demonstrate that our approach outperforms\nstate-of-the-art neural implicit models for this task in terms of\nreconstruction accuracy and completeness.\n","authors":["Thomas Chabal","Shizhe Chen","Jean Ponce","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2503.18897v1.pdf","comment":"3DV 2025. Project page:\n  https://www.di.ens.fr/willow/research/online-scene-reconstruction/"},{"id":"http://arxiv.org/abs/2503.18871v1","updated":"2025-03-24T16:46:36Z","published":"2025-03-24T16:46:36Z","title":"Bootstrapped Model Predictive Control","summary":"  Model Predictive Control (MPC) has been demonstrated to be effective in\ncontinuous control tasks. When a world model and a value function are\navailable, planning a sequence of actions ahead of time leads to a better\npolicy. Existing methods typically obtain the value function and the\ncorresponding policy in a model-free manner. However, we find that such an\napproach struggles with complex tasks, resulting in poor policy learning and\ninaccurate value estimation. To address this problem, we leverage the strengths\nof MPC itself. In this work, we introduce Bootstrapped Model Predictive Control\n(BMPC), a novel algorithm that performs policy learning in a bootstrapped\nmanner. BMPC learns a network policy by imitating an MPC expert, and in turn,\nuses this policy to guide the MPC process. Combined with model-based\nTD-learning, our policy learning yields better value estimation and further\nboosts the efficiency of MPC. We also introduce a lazy reanalyze mechanism,\nwhich enables computationally efficient imitation learning. Our method achieves\nsuperior performance over prior works on diverse continuous control tasks. In\nparticular, on challenging high-dimensional locomotion tasks, BMPC\nsignificantly improves data efficiency while also enhancing asymptotic\nperformance and training stability, with comparable training time and smaller\nnetwork sizes. Code is available at https://github.com/wertyuilife2/bmpc.\n","authors":["Yuhang Wang","Hanwei Guo","Sizhe Wang","Long Qian","Xuguang Lan"],"pdf_url":"https://arxiv.org/pdf/2503.18871v1.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.18816v1","updated":"2025-03-24T16:00:16Z","published":"2025-03-24T16:00:16Z","title":"Learning Multi-Robot Coordination through Locality-Based Factorized\n  Multi-Agent Actor-Critic Algorithm","summary":"  In this work, we present a novel cooperative multi-agent reinforcement\nlearning method called \\textbf{Loc}ality based \\textbf{Fac}torized\n\\textbf{M}ulti-Agent \\textbf{A}ctor-\\textbf{C}ritic (Loc-FACMAC). Existing\nstate-of-the-art algorithms, such as FACMAC, rely on global reward information,\nwhich may not accurately reflect the quality of individual robots' actions in\ndecentralized systems. We integrate the concept of locality into critic\nlearning, where strongly related robots form partitions during training. Robots\nwithin the same partition have a greater impact on each other, leading to more\nprecise policy evaluation. Additionally, we construct a dependency graph to\ncapture the relationships between robots, facilitating the partitioning\nprocess. This approach mitigates the curse of dimensionality and prevents\nrobots from using irrelevant information. Our method improves existing\nalgorithms by focusing on local rewards and leveraging partition-based learning\nto enhance training efficiency and performance. We evaluate the performance of\nLoc-FACMAC in three environments: Hallway, Multi-cartpole, and\nBounded-Cooperative-Navigation. We explore the impact of partition sizes on the\nperformance and compare the result with baseline MARL algorithms such as LOMAQ,\nFACMAC, and QMIX. The experiments reveal that, if the locality structure is\ndefined properly, Loc-FACMAC outperforms these baseline algorithms up to 108\\%,\nindicating that exploiting the locality structure in the actor-critic framework\nimproves the MARL performance.\n","authors":["Chak Lam Shek","Amrit Singh Bedi","Anjon Basak","Ellen Novoseller","Nick Waytowich","Priya Narayanan","Dinesh Manocha","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2503.18816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11791v2","updated":"2025-03-24T15:57:28Z","published":"2023-12-19T02:03:28Z","title":"Double Oracle Algorithm for Game-Theoretic Robot Allocation on Graphs","summary":"  We study the problem of game-theoretic robot allocation where two players\nstrategically allocate robots to compete for multiple sites of interest. Robots\npossess offensive or defensive capabilities to interfere and weaken their\nopponents to take over a competing site. This problem belongs to the\nconventional Colonel Blotto Game. Considering the robots' heterogeneous\ncapabilities and environmental factors, we generalize the conventional Blotto\ngame by incorporating heterogeneous robot types and graph constraints that\ncapture the robot transitions between sites. Then we employ the Double Oracle\nAlgorithm (DOA) to solve for the Nash equilibrium of the generalized Blotto\ngame. Particularly, for cyclic-dominance-heterogeneous (CDH) robots that\ninhibit each other, we define a new transformation rule between any two robot\ntypes. Building on the transformation, we design a novel utility function to\nmeasure the game's outcome quantitatively. Moreover, we rigorously prove the\ncorrectness of the designed utility function. Finally, we conduct extensive\nsimulations to demonstrate the effectiveness of DOA on computing Nash\nequilibrium for homogeneous, linear heterogeneous, and CDH robot allocation on\ngraphs.\n","authors":["Zijian An","Lifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.11791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18769v1","updated":"2025-03-24T15:16:51Z","published":"2025-03-24T15:16:51Z","title":"AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning","summary":"  This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet.\n","authors":["Alan Dao","Dinh Bach Vu","Bui Quang Huy"],"pdf_url":"https://arxiv.org/pdf/2503.18769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18752v1","updated":"2025-03-24T15:01:00Z","published":"2025-03-24T15:01:00Z","title":"Robust Tube-based Control Strategy for Vision-guided Autonomous Vehicles","summary":"  A robust control strategy for autonomous vehicles can improve system\nstability, enhance riding comfort, and prevent driving accidents. This paper\npresents a novel interpolation tube-based constrained iterative linear\nquadratic regulator (itube-CILQR) algorithm for autonomous\ncomputer-vision-based vehicle lane-keeping. The goal of the algorithm is to\nenhance robustness during high-speed cornering on tight turns. The advantages\nof itube-CILQR over the standard tube-approach include reduced system\nconservatism and increased computational speed. Numerical and vision-based\nexperiments were conducted to examine the feasibility of the proposed\nalgorithm. The proposed itube-CILQR algorithm is better suited to vehicle\nlane-keeping than variational CILQR-based methods and model predictive control\n(MPC) approaches using a classical interior-point solver. Specifically, in\nevaluation experiments, itube-CILQR achieved an average runtime of 3.16 ms to\ngenerate a control signal to guide a self-driving vehicle; itube-MPC typically\nrequired a 4.67-times longer computation time to complete the same task.\nMoreover, the influence of conservatism on system behavior was investigated by\nexploring the interpolation variable trajectories derived from the proposed\nitube-CILQR algorithm during lane-keeping maneuvers.\n","authors":["Der-Hau Lee"],"pdf_url":"https://arxiv.org/pdf/2503.18752v1.pdf","comment":"13 pages, 14 figures"},{"id":"http://arxiv.org/abs/2503.18738v1","updated":"2025-03-24T14:46:14Z","published":"2025-03-24T14:46:14Z","title":"RoboEngine: Plug-and-Play Robot Data Augmentation with Semantic Robot\n  Segmentation and Background Generation","summary":"  Visual augmentation has become a crucial technique for enhancing the visual\nrobustness of imitation learning. However, existing methods are often limited\nby prerequisites such as camera calibration or the need for controlled\nenvironments (e.g., green screen setups). In this work, we introduce\nRoboEngine, the first plug-and-play visual robot data augmentation toolkit. For\nthe first time, users can effortlessly generate physics- and task-aware robot\nscenes with just a few lines of code. To achieve this, we present a novel robot\nscene segmentation dataset, a generalizable high-quality robot segmentation\nmodel, and a fine-tuned background generation model, which together form the\ncore components of the out-of-the-box toolkit. Using RoboEngine, we demonstrate\nthe ability to generalize robot manipulation tasks across six entirely new\nscenes, based solely on demonstrations collected from a single scene, achieving\na more than 200% performance improvement compared to the no-augmentation\nbaseline. All datasets, model weights, and the toolkit will be publicly\nreleased.\n","authors":["Chengbo Yuan","Suraj Joshi","Shaoting Zhu","Hang Su","Hang Zhao","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2503.18738v1.pdf","comment":"Project Page: https://roboengine.github.io/"},{"id":"http://arxiv.org/abs/2401.00212v3","updated":"2025-03-24T14:36:24Z","published":"2023-12-30T12:12:35Z","title":"Physics-Informed Multi-Agent Reinforcement Learning for Distributed\n  Multi-Robot Problems","summary":"  The networked nature of multi-robot systems presents challenges in the\ncontext of multi-agent reinforcement learning. Centralized control policies do\nnot scale with increasing numbers of robots, whereas independent control\npolicies do not exploit the information provided by other robots, exhibiting\npoor performance in cooperative-competitive tasks. In this work we propose a\nphysics-informed reinforcement learning approach able to learn distributed\nmulti-robot control policies that are both scalable and make use of all the\navailable information to each robot. Our approach has three key\ncharacteristics. First, it imposes a port-Hamiltonian structure on the policy\nrepresentation, respecting energy conservation properties of physical robot\nsystems and the networked nature of robot team interactions. Second, it uses\nself-attention to ensure a sparse policy representation able to handle\ntime-varying information at each robot from the interaction graph. Third, we\npresent a soft actor-critic reinforcement learning algorithm parameterized by\nour self-attention port-Hamiltonian control policy, which accounts for the\ncorrelation among robots during training while overcoming the need of value\nfunction factorization. Extensive simulations in different multi-robot\nscenarios demonstrate the success of the proposed approach, surpassing previous\nmulti-robot reinforcement learning solutions in scalability, while achieving\nsimilar or superior performance (with averaged cumulative reward up to x2\ngreater than the state-of-the-art with robot teams x6 larger than the number of\nrobots at training time). We also validate our approach on multiple real robots\nin the Georgia Tech Robotarium under imperfect communication, demonstrating\nzero-shot sim-to-real transfer and scalability across number of robots.\n","authors":["Eduardo Sebastian","Thai Duong","Nikolay Atanasov","Eduardo Montijano","Carlos Sagues"],"pdf_url":"https://arxiv.org/pdf/2401.00212v3.pdf","comment":"This paper is under review at IEEE T-RO"},{"id":"http://arxiv.org/abs/2408.10562v2","updated":"2025-03-24T14:22:53Z","published":"2024-08-20T06:03:40Z","title":"Kalib: Easy Hand-Eye Calibration with Reference Point Tracking","summary":"  Hand-eye calibration aims to estimate the transformation between a camera and\na robot. Traditional methods rely on fiducial markers, which require\nconsiderable manual effort and precise setup. Recent advances in deep learning\nhave introduced markerless techniques but come with more prerequisites, such as\nretraining networks for each robot, and accessing accurate mesh models for data\ngeneration. In this paper, we propose Kalib, an automatic and easy-to-setup\nhand-eye calibration method that leverages the generalizability of visual\nfoundation models to overcome these challenges. It features only two basic\nprerequisites, the robot's kinematic chain and a predefined reference point on\nthe robot. During calibration, the reference point is tracked in the camera\nspace. Its corresponding 3D coordinates in the robot coordinate can be inferred\nby forward kinematics. Then, a PnP solver directly estimates the transformation\nbetween the camera and the robot without training new networks or accessing\nmesh models. Evaluations in simulated and real-world benchmarks show that Kalib\nachieves good accuracy with a lower manual workload compared with recent\nbaseline methods. We also demonstrate its application in multiple real-world\nsettings with various robot arms and grippers. Kalib's user-friendly design and\nminimal setup requirements make it a possible solution for continuous operation\nin unstructured environments.\n","authors":["Tutian Tang","Minghao Liu","Wenqiang Xu","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2408.10562v2.pdf","comment":"The code, data, and supplementary materials are available at\n  https://sites.google.com/view/hand-eye-kalib"},{"id":"http://arxiv.org/abs/2412.00133v2","updated":"2025-03-24T14:08:39Z","published":"2024-11-28T15:13:24Z","title":"ETAP: Event-based Tracking of Any Point","summary":"  Tracking any point (TAP) recently shifted the motion estimation paradigm from\nfocusing on individual salient points with local templates to tracking\narbitrary points with global image contexts. However, while research has mostly\nfocused on driving the accuracy of models in nominal settings, addressing\nscenarios with difficult lighting conditions and high-speed motions remains out\nof reach due to the limitations of the sensor. This work addresses this\nchallenge with the first event camera-based TAP method. It leverages the high\ntemporal resolution and high dynamic range of event cameras for robust\nhigh-speed tracking, and the global contexts in TAP methods to handle\nasynchronous and sparse event measurements. We further extend the TAP framework\nto handle event feature variations induced by motion -- thereby addressing an\nopen challenge in purely event-based tracking -- with a novel feature-alignment\nloss which ensures the learning of motion-robust features. Our method is\ntrained with data from a new data generation pipeline and systematically\nablated across all design decisions. Our method shows strong cross-dataset\ngeneralization and performs 136% better on the average Jaccard metric than the\nbaselines. Moreover, on an established feature tracking benchmark, it achieves\na 20% improvement over the previous best event-only method and even surpasses\nthe previous best events-and-frames method by 4.1%. Our code is available at\nhttps://github.com/tub-rip/ETAP\n","authors":["Friedhelm Hamann","Daniel Gehrig","Filbert Febryanto","Kostas Daniilidis","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2412.00133v2.pdf","comment":"17 pages, 15 figures, 8 tables. Project page:\n  https://github.com/tub-rip/ETAP"},{"id":"http://arxiv.org/abs/2502.19908v3","updated":"2025-03-24T14:03:59Z","published":"2025-02-27T09:26:22Z","title":"CarPlanner: Consistent Auto-regressive Trajectory Planning for\n  Large-scale Reinforcement Learning in Autonomous Driving","summary":"  Trajectory planning is vital for autonomous driving, ensuring safe and\nefficient navigation in complex environments. While recent learning-based\nmethods, particularly reinforcement learning (RL), have shown promise in\nspecific scenarios, RL planners struggle with training inefficiencies and\nmanaging large-scale, real-world driving scenarios. In this paper, we introduce\n\\textbf{CarPlanner}, a \\textbf{C}onsistent \\textbf{a}uto-\\textbf{r}egressive\n\\textbf{Planner} that uses RL to generate multi-modal trajectories. The\nauto-regressive structure enables efficient large-scale RL training, while the\nincorporation of consistency ensures stable policy learning by maintaining\ncoherent temporal consistency across time steps. Moreover, CarPlanner employs a\ngeneration-selection framework with an expert-guided reward function and an\ninvariant-view module, simplifying RL training and enhancing policy\nperformance. Extensive analysis demonstrates that our proposed RL framework\neffectively addresses the challenges of training efficiency and performance\nenhancement, positioning CarPlanner as a promising solution for trajectory\nplanning in autonomous driving. To the best of our knowledge, we are the first\nto demonstrate that the RL-based planner can surpass both IL- and rule-based\nstate-of-the-arts (SOTAs) on the challenging large-scale real-world dataset\nnuPlan. Our proposed CarPlanner surpasses RL-, IL-, and rule-based SOTA\napproaches within this demanding dataset.\n","authors":["Dongkun Zhang","Jiaming Liang","Ke Guo","Sha Lu","Qi Wang","Rong Xiong","Zhenwei Miao","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2502.19908v3.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18684v1","updated":"2025-03-24T13:55:47Z","published":"2025-03-24T13:55:47Z","title":"Efficient Continual Adaptation of Pretrained Robotic Policy with Online\n  Meta-Learned Adapters","summary":"  Continual adaptation is essential for general autonomous agents. For example,\na household robot pretrained with a repertoire of skills must still adapt to\nunseen tasks specific to each household. Motivated by this, building upon\nparameter-efficient fine-tuning in language models, prior works have explored\nlightweight adapters to adapt pretrained policies, which can preserve learned\nfeatures from the pretraining phase and demonstrate good adaptation\nperformances. However, these approaches treat task learning separately,\nlimiting knowledge transfer between tasks. In this paper, we propose Online\nMeta-Learned adapters (OMLA). Instead of applying adapters directly, OMLA can\nfacilitate knowledge transfer from previously learned tasks to current learning\ntasks through a novel meta-learning objective. Extensive experiments in both\nsimulated and real-world environments demonstrate that OMLA can lead to better\nadaptation performances compared to the baseline methods. The project link:\nhttps://ricky-zhu.github.io/OMLA/.\n","authors":["Ruiqi Zhu","Endong Sun","Guanhe Huang","Oya Celiktutan"],"pdf_url":"https://arxiv.org/pdf/2503.18684v1.pdf","comment":"Project link: https://ricky-zhu.github.io/OMLA/"},{"id":"http://arxiv.org/abs/2503.18673v1","updated":"2025-03-24T13:46:21Z","published":"2025-03-24T13:46:21Z","title":"Any6D: Model-free 6D Pose Estimation of Novel Objects","summary":"  We introduce Any6D, a model-free framework for 6D object pose estimation that\nrequires only a single RGB-D anchor image to estimate both the 6D pose and size\nof unknown objects in novel scenes. Unlike existing methods that rely on\ntextured 3D models or multiple viewpoints, Any6D leverages a joint object\nalignment process to enhance 2D-3D alignment and metric scale estimation for\nimproved pose accuracy. Our approach integrates a render-and-compare strategy\nto generate and refine pose hypotheses, enabling robust performance in\nscenarios with occlusions, non-overlapping views, diverse lighting conditions,\nand large cross-environment variations. We evaluate our method on five\nchallenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,\ndemonstrating its effectiveness in significantly outperforming state-of-the-art\nmethods for novel object pose estimation. Project page:\nhttps://taeyeop.com/any6d\n","authors":["Taeyeop Lee","Bowen Wen","Minjun Kang","Gyuree Kang","In So Kweon","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2503.18673v1.pdf","comment":"CVPR 2025, Project Page: https://taeyeop.com/any6d"},{"id":"http://arxiv.org/abs/2410.05869v4","updated":"2025-03-24T13:41:43Z","published":"2024-10-08T09:57:14Z","title":"Believing is Seeing: Unobserved Object Detection using Generative Models","summary":"  Can objects that are not visible in an image -- but are in the vicinity of\nthe camera -- be detected? This study introduces the novel tasks of 2D, 2.5D\nand 3D unobserved object detection for predicting the location of nearby\nobjects that are occluded or lie outside the image frame. We adapt several\nstate-of-the-art pre-trained generative models to address this task, including\n2D and 3D diffusion models and vision-language models, and show that they can\nbe used to infer the presence of objects that are not directly observed. To\nbenchmark this task, we propose a suite of metrics that capture different\naspects of performance. Our empirical evaluation on indoor scenes from the\nRealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the\nuse of generative models for the unobserved object detection task.\n","authors":["Subhransu S. Bhattacharjee","Dylan Campbell","Rahul Shome"],"pdf_url":"https://arxiv.org/pdf/2410.05869v4.pdf","comment":"IEEE/CVF Computer Vision and Pattern Recognition 2025; 22 pages"},{"id":"http://arxiv.org/abs/2503.18616v1","updated":"2025-03-24T12:19:50Z","published":"2025-03-24T12:19:50Z","title":"FF-SRL: High Performance GPU-Based Surgical Simulation For Robot\n  Learning","summary":"  Robotic surgery is a rapidly developing field that can greatly benefit from\nthe automation of surgical tasks. However, training techniques such as\nReinforcement Learning (RL) require a high number of task repetitions, which\nare generally unsafe and impractical to perform on real surgical systems. This\nstresses the need for simulated surgical environments, which are not only\nrealistic, but also computationally efficient and scalable. We introduce FF-SRL\n(Fast and Flexible Surgical Reinforcement Learning), a high-performance\nlearning environment for robotic surgery. In FF-SRL both physics simulation and\nRL policy training reside entirely on a single GPU. This avoids typical\nbottlenecks associated with data transfer between the CPU and GPU, leading to\naccelerated learning rates. Our results show that FF-SRL reduces the training\ntime of a complex tissue manipulation task by an order of magnitude, down to a\ncouple of minutes, compared to a common CPU/GPU simulator. Such speed-up may\nfacilitate the experimentation with RL techniques and contribute to the\ndevelopment of new generation of surgical systems. To this end, we make our\ncode publicly available to the community.\n","authors":["Diego Dall'Alba","Michał Nasket","Sabina Kaminska","Przemysław Korzeniowski"],"pdf_url":"https://arxiv.org/pdf/2503.18616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18546v1","updated":"2025-03-24T10:59:31Z","published":"2025-03-24T10:59:31Z","title":"Multi-agent coordination for data gathering with periodic requests and\n  deliveries","summary":"  In this demo work we develop a method to plan and coordinate a multi-agent\nteam to gather information on demand. The data is periodically requested by a\nstatic Operation Center (OC) from changeable goals locations. The mission of\nthe team is to reach these locations, taking measurements and delivering the\ndata to the OC. Due to the limited communication range as well as signal\nattenuation because of the obstacles, the agents must travel to the OC, to\nupload the data. The agents can play two roles: ones as workers gathering data,\nthe others as collectors traveling invariant paths for collecting the data of\nthe workers to re-transmit it to the OC. The refreshing time of the delivered\ninformation depends on the number of available agents as well as of the\nscenario. The proposed algorithm finds out the best balance between the number\nof collectors-workers and the partition of the scenario into working areas in\nthe planning phase, which provides the minimum refreshing time and will be the\none executed by the agents.\n","authors":["Yaroslav Marchukov","Luis Montano"],"pdf_url":"https://arxiv.org/pdf/2503.18546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18545v1","updated":"2025-03-24T10:59:23Z","published":"2025-03-24T10:59:23Z","title":"Communication-aware planning for robot teams deployment","summary":"  In the present work we address the problem of deploying a team of robots in a\nscenario where some locations of interest must be reached. Thus, a planning for\na deployment is required, before sending the robots. The obstacles, the limited\ncommunication range, and the need of communicating to a base station, constrain\nthe connectivity of the team and the deployment planning. We propose a method\nconsisting of three algorithms: a distributed path planner to obtain\ncommunication-aware trajectories; a deployment planner providing dual-use of\nthe robots, visiting primary goals and performing connectivity tasks; and a\nclustering algorithm to allocate the tasks to robots, and obtain the best goal\nvisit order for the mission.\n","authors":["Yaroslav Marchukov","Luis Montano"],"pdf_url":"https://arxiv.org/pdf/2503.18545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18531v1","updated":"2025-03-24T10:40:03Z","published":"2025-03-24T10:40:03Z","title":"Parental Guidance: Efficient Lifelong Learning through Evolutionary\n  Distillation","summary":"  Developing robotic agents that can perform well in diverse environments while\nshowing a variety of behaviors is a key challenge in AI and robotics.\nTraditional reinforcement learning (RL) methods often create agents that\nspecialize in narrow tasks, limiting their adaptability and diversity. To\novercome this, we propose a preliminary, evolution-inspired framework that\nincludes a reproduction module, similar to natural species reproduction,\nbalancing diversity and specialization. By integrating RL, imitation learning\n(IL), and a coevolutionary agent-terrain curriculum, our system evolves agents\ncontinuously through complex tasks. This approach promotes adaptability,\ninheritance of useful traits, and continual learning. Agents not only refine\ninherited skills but also surpass their predecessors. Our initial experiments\nshow that this method improves exploration efficiency and supports open-ended\nlearning, offering a scalable solution where sparse reward coupled with diverse\nterrain environments induces a multi-task setting.\n","authors":["Octi Zhang","Quanquan Peng","Rosario Scalise","Bryon Boots"],"pdf_url":"https://arxiv.org/pdf/2503.18531v1.pdf","comment":"4 pages, 3 figures, CoRL 2024 Workshop MAPoDeL"},{"id":"http://arxiv.org/abs/2502.17034v3","updated":"2025-03-24T10:31:53Z","published":"2025-02-24T10:34:35Z","title":"Evolution 6.0: Evolving Robotic Capabilities Through Generative Design","summary":"  We propose a new concept, Evolution 6.0, which represents the evolution of\nrobotics driven by Generative AI. When a robot lacks the necessary tools to\naccomplish a task requested by a human, it autonomously designs the required\ninstruments and learns how to use them to achieve the goal. Evolution 6.0 is an\nautonomous robotic system powered by Vision-Language Models (VLMs),\nVision-Language Action (VLA) models, and Text-to-3D generative models for tool\ndesign and task execution. The system comprises two key modules: the Tool\nGeneration Module, which fabricates task-specific tools from visual and textual\ndata, and the Action Generation Module, which converts natural language\ninstructions into robotic actions. It integrates QwenVLM for environmental\nunderstanding, OpenVLA for task execution, and Llama-Mesh for 3D tool\ngeneration. Evaluation results demonstrate a 90% success rate for tool\ngeneration with a 10-second inference time, and action generation achieving\n83.5% in physical and visual generalization, 70% in motion generalization, and\n37% in semantic generalization. Future improvements will focus on bimanual\nmanipulation, expanded task capabilities, and enhanced environmental\ninterpretation to improve real-world adaptability.\n","authors":["Muhammad Haris Khan","Artyom Myshlyaev","Artem Lykov","Miguel Altamirano Cabrera","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2502.17034v3.pdf","comment":"Submitted to IROS"},{"id":"http://arxiv.org/abs/2503.18525v1","updated":"2025-03-24T10:29:47Z","published":"2025-03-24T10:29:47Z","title":"P3Nav: A Unified Framework for Embodied Navigation Integrating\n  Perception, Planning, and Prediction","summary":"  In language-guided visual navigation, agents locate target objects in unseen\nenvironments using natural language instructions. For reliable navigation in\nunfamiliar scenes, agents must possess strong perception, planning, and\nprediction capabilities. Additionally, when agents revisit previously explored\nareas during long-term navigation, they may retain irrelevant and redundant\nhistorical perceptions, leading to suboptimal results. In this work, we\nintroduce \\textbf{P3Nav}, a unified framework that integrates\n\\textbf{P}erception, \\textbf{P}lanning, and \\textbf{P}rediction capabilities\nthrough \\textbf{Multitask Collaboration} on navigation and embodied question\nanswering (EQA) tasks, thereby enhancing navigation performance. Furthermore,\nP3Nav employs an \\textbf{Adaptive 3D-aware History Sampling} strategy to\neffectively and efficiently utilize historical observations. By leveraging the\nlarge language models (LLM), P3Nav comprehends diverse commands and complex\nvisual scenes, resulting in appropriate navigation actions. P3Nav achieves a\n75\\% success rate in object goal navigation on the\n$\\mathrm{CHORES}$-$\\mathbb{S}$ benchmark, setting a new state-of-the-art\nperformance.\n","authors":["Yufeng Zhong","Chengjian Feng","Feng Yan","Fanfan Liu","Liming Zheng","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2503.18525v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2302.06554v2","updated":"2025-03-24T09:47:43Z","published":"2023-02-13T17:54:50Z","title":"Improving robot navigation in crowded environments using intrinsic\n  rewards","summary":"  Autonomous navigation in crowded environments is an open problem with many\napplications, essential for the coexistence of robots and humans in the smart\ncities of the future. In recent years, deep reinforcement learning approaches\nhave proven to outperform model-based algorithms. Nevertheless, even though the\nresults provided are promising, the works are not able to take advantage of the\ncapabilities that their models offer. They usually get trapped in local optima\nin the training process, that prevent them from learning the optimal policy.\nThey are not able to visit and interact with every possible state\nappropriately, such as with the states near the goal or near the dynamic\nobstacles. In this work, we propose using intrinsic rewards to balance between\nexploration and exploitation and explore depending on the uncertainty of the\nstates instead of on the time the agent has been trained, encouraging the agent\nto get more curious about unknown states. We explain the benefits of the\napproach and compare it with other exploration algorithms that may be used for\ncrowd navigation. Many simulation experiments are performed modifying several\nalgorithms of the state-of-the-art, showing that the use of intrinsic rewards\nmakes the robot learn faster and reach higher rewards and success rates (fewer\ncollisions) in shorter navigation times, outperforming the state-of-the-art.\n","authors":["Diego Martinez-Baselga","Luis Riazuelo","Luis Montano"],"pdf_url":"https://arxiv.org/pdf/2302.06554v2.pdf","comment":"Paper accepted in 2023 IEEE International Conference on Robotics and\n  Automation (ICRA)"},{"id":"http://arxiv.org/abs/2407.00507v3","updated":"2025-03-24T09:34:11Z","published":"2024-06-29T18:22:43Z","title":"AVOCADO: Adaptive Optimal Collision Avoidance driven by Opinion","summary":"  We present AVOCADO (AdaptiVe Optimal Collision Avoidance Driven by Opinion),\na novel navigation approach to address holonomic robot collision avoidance when\nthe robot does not know how cooperative the other agents in the environment\nare. AVOCADO departs from a Velocity Obstacle's (VO) formulation akin to the\nOptimal Reciprocal Collision Avoidance method. However, instead of assuming\nreciprocity, it poses an adaptive control problem to adapt to the cooperation\nlevel of other robots and agents in real time. This is achieved through a novel\nnonlinear opinion dynamics design that relies solely on sensor observations. As\na by-product, we leverage tools from the opinion dynamics formulation to\nnaturally avoid the deadlocks in geometrically symmetric scenarios that\ntypically suffer VO-based planners. Extensive numerical simulations show that\nAVOCADO surpasses existing motion planners in mixed cooperative/non-cooperative\nnavigation environments in terms of success rate, time to goal and\ncomputational time. In addition, we conduct multiple real experiments that\nverify that AVOCADO is able to avoid collisions in environments crowded with\nother robots and humans.\n","authors":["Diego Martinez-Baselga","Eduardo Sebastián","Eduardo Montijano","Luis Riazuelo","Carlos Sagüés","Luis Montano"],"pdf_url":"https://arxiv.org/pdf/2407.00507v3.pdf","comment":"This paper is published at IEEE Transactions on Robotics under DOI\n  10.1109/TRO.2025.3552350"},{"id":"http://arxiv.org/abs/2503.13441v2","updated":"2025-03-24T08:31:56Z","published":"2025-03-17T17:59:09Z","title":"Humanoid Policy ~ Human Policy","summary":"  Training manipulation policies for humanoid robots with diverse data enhances\ntheir robustness and generalization across tasks and platforms. However,\nlearning solely from robot demonstrations is labor-intensive, requiring\nexpensive tele-operated data collection which is difficult to scale. This paper\ninvestigates a more scalable data source, egocentric human demonstrations, to\nserve as cross-embodiment training data for robot learning. We mitigate the\nembodiment gap between humanoids and humans from both the data and modeling\nperspectives. We collect an egocentric task-oriented dataset (PH2D) that is\ndirectly aligned with humanoid manipulation demonstrations. We then train a\nhuman-humanoid behavior policy, which we term Human Action Transformer (HAT).\nThe state-action space of HAT is unified for both humans and humanoid robots\nand can be differentiably retargeted to robot actions. Co-trained with\nsmaller-scale robot data, HAT directly models humanoid robots and humans as\ndifferent embodiments without additional supervision. We show that human data\nimproves both generalization and robustness of HAT with significantly better\ndata collection efficiency. Code and data: https://human-as-robot.github.io/\n","authors":["Ri-Zhao Qiu","Shiqi Yang","Xuxin Cheng","Chaitanya Chawla","Jialong Li","Tairan He","Ge Yan","David J. Yoon","Ryan Hoque","Lars Paulsen","Ge Yang","Jian Zhang","Sha Yi","Guanya Shi","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13441v2.pdf","comment":"Code and data: https://human-as-robot.github.io/"},{"id":"http://arxiv.org/abs/2501.01831v3","updated":"2025-03-24T07:44:43Z","published":"2025-01-03T14:32:17Z","title":"Emergency-Brake Simplex: Toward A Verifiably Safe Control-CPS\n  Architecture for Abrupt Runtime Reachability Constraint Changes","summary":"  When a system's constraints change abruptly, the system's reachability safety\ndoes no longer sustain. Thus, the system can reach a forbidden/dangerous value.\nConventional remedy practically involves online controller redesign (OCR) to\nre-establish the reachability's compliance with the new constraints, which,\nhowever, is usually too slow. There is a need for an online strategy capable of\nmanaging runtime changes in reachability constraints. However, to the best of\nthe authors' knowledge, this topic has not been addressed in the existing\nliterature. In this paper, we propose a fast fault tolerance strategy to\nrecover the system's reachability safety in runtime. Instead of redesigning the\nsystem's controller, we propose to change the system's reference state to\nmodify the system's reachability to comply with the new constraints. We frame\nthe reference state search as an optimization problem and employ the\nKarush-Kuhn-Tucker (KKT) method as well as the Interior Point Method (IPM)\nbased Newton's method (as a fallback for the KKT method) for fast solution\nderivation. The optimization also allows more future fault tolerance. Numerical\nsimulations demonstrate that our method outperforms the conventional OCR method\nin terms of computational efficiency and success rate. Specifically, the\nresults show that the proposed method finds a solution $10^{2}$ (with the IPM\nbased Newton's method) $\\sim 10^{4}$ (with the KKT method) times faster than\nthe OCR method. Additionally, the improvement rate of the success rate of our\nmethod over the OCR method is $40.81\\%$ without considering the deadline of run\ntime. The success rate remains at $49.44\\%$ for the proposed method, while it\nbecomes $0\\%$ for the OCR method when a deadline of $1.5 \\; seconds$ is\nimposed.\n","authors":["Henghua Shen","Qixin Wang"],"pdf_url":"https://arxiv.org/pdf/2501.01831v3.pdf","comment":"12 pages, 2 figures,"},{"id":"http://arxiv.org/abs/2503.08317v3","updated":"2025-03-24T07:18:42Z","published":"2025-03-11T11:25:57Z","title":"Uni-Gaussians: Unifying Camera and Lidar Simulation with Gaussians for\n  Dynamic Driving Scenarios","summary":"  Ensuring the safety of autonomous vehicles necessitates comprehensive\nsimulation of multi-sensor data, encompassing inputs from both cameras and\nLiDAR sensors, across various dynamic driving scenarios. Neural rendering\ntechniques, which utilize collected raw sensor data to simulate these dynamic\nenvironments, have emerged as a leading methodology. While NeRF-based\napproaches can uniformly represent scenes for rendering data from both camera\nand LiDAR, they are hindered by slow rendering speeds due to dense sampling.\nConversely, Gaussian Splatting-based methods employ Gaussian primitives for\nscene representation and achieve rapid rendering through rasterization.\nHowever, these rasterization-based techniques struggle to accurately model\nnon-linear optical sensors. This limitation restricts their applicability to\nsensors beyond pinhole cameras. To address these challenges and enable unified\nrepresentation of dynamic driving scenarios using Gaussian primitives, this\nstudy proposes a novel hybrid approach. Our method utilizes rasterization for\nrendering image data while employing Gaussian ray-tracing for LiDAR data\nrendering. Experimental results on public datasets demonstrate that our\napproach outperforms current state-of-the-art methods. This work presents a\nunified and efficient solution for realistic simulation of camera and LiDAR\ndata in autonomous driving scenarios using Gaussian primitives, offering\nsignificant advancements in both rendering quality and computational\nefficiency.\n","authors":["Zikang Yuan","Yuechuan Pu","Hongcheng Luo","Fengtian Lang","Cheng Chi","Teng Li","Yingying Shen","Haiyang Sun","Bing Wang","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.08317v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2503.17125v2","updated":"2025-03-24T06:16:20Z","published":"2025-03-21T13:20:39Z","title":"LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in\n  Reinforcement Learning","summary":"  Deep Reinforcement Learning (DRL) has demonstrated strong performance in\nrobotic control but remains susceptible to out-of-distribution (OOD) states,\noften resulting in unreliable actions and task failure. While previous methods\nhave focused on minimizing or preventing OOD occurrences, they largely neglect\nrecovery once an agent encounters such states. Although the latest research has\nattempted to address this by guiding agents back to in-distribution states,\ntheir reliance on uncertainty estimation hinders scalability in complex\nenvironments. To overcome this limitation, we introduce Language Models for\nOut-of-Distribution Recovery (LaMOuR), which enables recovery learning without\nrelying on uncertainty estimation. LaMOuR generates dense reward codes that\nguide the agent back to a state where it can successfully perform its original\ntask, leveraging the capabilities of LVLMs in image description, logical\nreasoning, and code generation. Experimental results show that LaMOuR\nsubstantially enhances recovery efficiency across diverse locomotion tasks and\neven generalizes effectively to complex environments, including humanoid\nlocomotion and mobile manipulation, where existing methods struggle. The code\nand supplementary materials are available at https://lamour-rl.github.io/.\n","authors":["Chan Kim","Seung-Woo Seo","Seong-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2503.17125v2.pdf","comment":"14 pages, 17 figures"},{"id":"http://arxiv.org/abs/2503.18376v1","updated":"2025-03-24T06:15:08Z","published":"2025-03-24T06:15:08Z","title":"Analysis of Forces Exerted by Shoulder and Elbow Fabric-based Pneumatic\n  Actuators for Pediatric Exosuits","summary":"  To enhance pediatric exosuit design, it is crucial to assess the\nactuator-generated forces. This work evaluates the contact forces exerted by\nsoft fabric-based pneumatic actuators in an upper extremity pediatric exosuit.\nTwo actuators were examined: a single-cell bidirectional actuator for shoulder\nabduction/adduction and a bellow-type actuator for elbow extension/flexion.\nExperiments assessed the impact of actuator anchoring points and the adjacent\njoint's angle on exerted forces and actuated joint range of motion (ROM). These\nwere measured via load cells and encoders integrated into a custom infant-scale\nengineered apparatus with two degrees of freedom (two revolute joints). For the\nshoulder actuator, results show that anchoring it further from the shoulder\njoint center while the elbow is flexed at $90^\\circ$ yields the highest ROM\nwhile minimizing the peak force exerted on the body. For the elbow actuator,\nanchoring it symmetrically while the shoulder joint is at $0^\\circ$ optimizes\nactuator performance. These findings contribute a key step toward co-optimizing\nthe considered exosuit design for functionality and wearability.\n","authors":["Mehrnoosh Ayazi","Ipsita Sahin","Caio Mucchiani","Elena Kokkoni","Konstantinos Karydis"],"pdf_url":"https://arxiv.org/pdf/2503.18376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18373v1","updated":"2025-03-24T06:10:43Z","published":"2025-03-24T06:10:43Z","title":"Innovative Automated Stretch Elastic Waistband Sewing Machine for\n  Garment Manufacturing","summary":"  There is applied research for the development of the Automated Stretch\nElastic Waistband Sewing Machine represents a significant advancement in\ngarment manufacturing, addressing the industry's need for increased efficiency,\nprecision, and adaptability. This machine integrates innovative features such\nas a sensor-based automatic waistband expansion system, synchronized sewing\nspeed and rolling wheel speed, and a differential feed top-loading mechanism.\nThese enhancements streamline the sewing process, reduce manual intervention,\nand ensure consistent product quality. The machine's design incorporates both\n3-wheel and 2-wheel rolling systems, each optimized for different elastic band\ndimensions and elongation factors. The 3-wheel rolling system accommodates a\nlarger maximum boundary, while the 2-wheel rolling system offers a tighter\noperational range, providing flexibility to meet diverse manufacturing\nrequirements. The Automated Stretch Elastic Waistband Sewing Machine has a\ndesign that controls the pulling apart force so as not to break the elastic\nwaistband. It sets a new standard for quality and innovation, empowering\nmanufacturers to meet the demands of a competitive market with precision and\nease.\n","authors":["Prof Dr Ray Wai Man Kong"],"pdf_url":"https://arxiv.org/pdf/2503.18373v1.pdf","comment":"13 pages, 10 Figures"},{"id":"http://arxiv.org/abs/2503.18366v1","updated":"2025-03-24T06:02:41Z","published":"2025-03-24T06:02:41Z","title":"Reinforcement Learning for Adaptive Planner Parameter Tuning: A\n  Perspective on Hierarchical Architecture","summary":"  Automatic parameter tuning methods for planning algorithms, which integrate\npipeline approaches with learning-based techniques, are regarded as promising\ndue to their stability and capability to handle highly constrained\nenvironments. While existing parameter tuning methods have demonstrated\nconsiderable success, further performance improvements require a more\nstructured approach. In this paper, we propose a hierarchical architecture for\nreinforcement learning-based parameter tuning. The architecture introduces a\nhierarchical structure with low-frequency parameter tuning, mid-frequency\nplanning, and high-frequency control, enabling concurrent enhancement of both\nupper-layer parameter tuning and lower-layer control through iterative\ntraining. Experimental evaluations in both simulated and real-world\nenvironments show that our method surpasses existing parameter tuning\napproaches. Furthermore, our approach achieves first place in the Benchmark for\nAutonomous Robot Navigation (BARN) Challenge.\n","authors":["Lu Wangtao","Wei Yufei","Xu Jiadong","Jia Wenhao","Li Liang","Xiong Rong","Wang Yue"],"pdf_url":"https://arxiv.org/pdf/2503.18366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18347v1","updated":"2025-03-24T05:11:58Z","published":"2025-03-24T05:11:58Z","title":"Latent Embedding Adaptation for Human Preference Alignment in Diffusion\n  Planners","summary":"  This work addresses the challenge of personalizing trajectories generated in\nautomated decision-making systems by introducing a resource-efficient approach\nthat enables rapid adaptation to individual users' preferences. Our method\nleverages a pretrained conditional diffusion model with Preference Latent\nEmbeddings (PLE), trained on a large, reward-free offline dataset. The PLE\nserves as a compact representation for capturing specific user preferences. By\nadapting the pretrained model using our proposed preference inversion method,\nwhich directly optimizes the learnable PLE, we achieve superior alignment with\nhuman preferences compared to existing solutions like Reinforcement Learning\nfrom Human Feedback (RLHF) and Low-Rank Adaptation (LoRA). To better reflect\npractical applications, we create a benchmark experiment using real human\npreferences on diverse, high-reward trajectories.\n","authors":["Wen Zheng Terence Ng","Jianda Chen","Yuan Xu","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.18347v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2503.18308v1","updated":"2025-03-24T03:19:41Z","published":"2025-03-24T03:19:41Z","title":"Vision-Guided Loco-Manipulation with a Snake Robot","summary":"  This paper presents the development and integration of a vision-guided\nloco-manipulation pipeline for Northeastern University's snake robot, COBRA.\nThe system leverages a YOLOv8-based object detection model and depth data from\nan onboard stereo camera to estimate the 6-DOF pose of target objects in real\ntime. We introduce a framework for autonomous detection and control, enabling\nclosed-loop loco-manipulation for transporting objects to specified goal\nlocations. Additionally, we demonstrate open-loop experiments in which COBRA\nsuccessfully performs real-time object detection and loco-manipulation tasks.\n","authors":["Adarsh Salagame","Sasank Potluri","Keshav Bharadwaj Vaidyanathan","Kruthika Gangaraju","Eric Sihite","Milad Ramezani","Alireza Ramezani"],"pdf_url":"https://arxiv.org/pdf/2503.18308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18307v1","updated":"2025-03-24T03:12:02Z","published":"2025-03-24T03:12:02Z","title":"NMPC-based Unified Posture Manipulation and Thrust Vectoring for Fault\n  Recovery","summary":"  Multi-rotors face significant risks, as actuator failures at high altitudes\ncan easily result in a crash and the robot's destruction. Therefore, rapid\nfault recovery in the event of an actuator failure is necessary for the\nfault-tolerant and safe operation of unmanned aerial robots. In this work, we\npresent a fault recovery approach based on the unification of posture\nmanipulation and thrust vectoring. The key contributions of this work are: 1)\nDerivation of two flight dynamics models (high-fidelity and reduced-order) that\ncapture posture control and thrust vectoring. 2) Design of a controller based\non Nonlinear Model Predictive Control (NMPC) and demonstration of fault\nrecovery in simulation using a high-fidelity model of the Multi-Modal Mobility\nMorphobot (M4) in Simscape.\n","authors":["Adarsh Salagame","Shashwat Pandya","Ioannis Mandralis","Eric Sihite","Alireza Ramezani","Morteza Gharib"],"pdf_url":"https://arxiv.org/pdf/2503.18307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14868v4","updated":"2025-03-24T03:10:14Z","published":"2024-10-18T21:28:50Z","title":"Diff-DAgger: Uncertainty Estimation with Diffusion Policy for Robotic\n  Manipulation","summary":"  Recently, diffusion policy has shown impressive results in handling\nmulti-modal tasks in robotic manipulation. However, it has fundamental\nlimitations in out-of-distribution failures that persist due to compounding\nerrors and its limited capability to extrapolate. One way to address these\nlimitations is robot-gated DAgger, an interactive imitation learning with a\nrobot query system to actively seek expert help during policy rollout. While\nrobot-gated DAgger has high potential for learning at scale, existing methods\nlike Ensemble-DAgger struggle with highly expressive policies: They often\nmisinterpret policy disagreements as uncertainty at multi-modal decision\npoints. To address this problem, we introduce Diff-DAgger, an efficient\nrobot-gated DAgger algorithm that leverages the training objective of diffusion\npolicy. We evaluate Diff-DAgger across different robot tasks including\nstacking, pushing, and plugging, and show that Diff-DAgger improves the task\nfailure prediction by 39.0%, the task completion rate by 20.6%, and reduces the\nwall-clock time by a factor of 7.8. We hope that this work opens up a path for\nefficiently incorporating expressive yet data-hungry policies into interactive\nrobot learning settings. The project website is available at:\nhttps://diffdagger.github.io.\n","authors":["Sung-Wook Lee","Xuhui Kang","Yen-Ling Kuo"],"pdf_url":"https://arxiv.org/pdf/2410.14868v4.pdf","comment":"Project website: diffdagger.github.io 8 pages, 6 figures, accepted by\n  International Conference on Robotics and Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2411.01321v2","updated":"2025-03-24T03:10:05Z","published":"2024-11-02T17:53:45Z","title":"Control Strategies for Pursuit-Evasion Under Occlusion Using Visibility\n  and Safety Barrier Functions","summary":"  This paper develops a control strategy for pursuit-evasion problems in\nenvironments with occlusions. We address the challenge of a mobile pursuer\nkeeping a mobile evader within its field of view (FoV) despite line-of-sight\nobstructions. The signed distance function (SDF) of the FoV is used to\nformulate visibility as a control barrier function (CBF) constraint on the\npursuer's control inputs. Similarly, obstacle avoidance is formulated as a CBF\nconstraint based on the SDF of the obstacle set. While the visibility and\nsafety CBFs are Lipschitz continuous, they are not differentiable everywhere,\nnecessitating the use of generalized gradients. To achieve non-myopic pursuit,\nwe generate reference control trajectories leading to evader visibility using a\nsampling-based kinodynamic planner. The pursuer then tracks this reference via\nconvex optimization under the CBF constraints. We validate our approach in\nCARLA simulations and real-world robot experiments, demonstrating successful\nvisibility maintenance using only onboard sensing, even under severe occlusions\nand dynamic evader movements.\n","authors":["Minnan Zhou","Mustafa Shaikh","Vatsalya Chaubey","Patrick Haggerty","Shumon Koga","Dimitra Panagou","Nikolay Atanasov"],"pdf_url":"https://arxiv.org/pdf/2411.01321v2.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2503.18301v1","updated":"2025-03-24T03:07:28Z","published":"2025-03-24T03:07:28Z","title":"Ground Penetrating Radar-Assisted Multimodal Robot Odometry Using\n  Subsurface Feature Matrix","summary":"  Localization of robots using subsurface features observed by\nground-penetrating radar (GPR) enhances and adds robustness to common sensor\nmodalities, as subsurface features are less affected by weather, seasons, and\nsurface changes. We introduce an innovative multimodal odometry approach using\ninputs from GPR, an inertial measurement unit (IMU), and a wheel encoder. To\nefficiently address GPR signal noise, we introduce an advanced feature\nrepresentation called the subsurface feature matrix (SFM). The SFM leverages\nfrequency domain data and identifies peaks within radar scans. Additionally, we\npropose a novel feature matching method that estimates GPR displacement by\naligning SFMs. The integrations from these three input sources are consolidated\nusing a factor graph approach to achieve multimodal robot odometry. Our method\nhas been developed and evaluated with the CMU-GPR public dataset, demonstrating\nimprovements in accuracy and robustness with real-time performance in robotic\nodometry tasks.\n","authors":["Haifeng Li","Jiajun Guo","Xuanxin Fan","Dezhen Song"],"pdf_url":"https://arxiv.org/pdf/2503.18301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15139v2","updated":"2025-03-24T03:02:15Z","published":"2024-11-22T18:59:47Z","title":"DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous\n  Driving","summary":"  Recently, the diffusion model has emerged as a powerful generative technique\nfor robotic policy learning, capable of modeling multi-mode action\ndistributions. Leveraging its capability for end-to-end autonomous driving is a\npromising direction. However, the numerous denoising steps in the robotic\ndiffusion policy and the more dynamic, open-world nature of traffic scenes pose\nsubstantial challenges for generating diverse driving actions at a real-time\nspeed. To address these challenges, we propose a novel truncated diffusion\npolicy that incorporates prior multi-mode anchors and truncates the diffusion\nschedule, enabling the model to learn denoising from anchored Gaussian\ndistribution to the multi-mode driving action distribution. Additionally, we\ndesign an efficient cascade diffusion decoder for enhanced interaction with\nconditional scene context. The proposed model, DiffusionDrive, demonstrates\n10$\\times$ reduction in denoising steps compared to vanilla diffusion policy,\ndelivering superior diversity and quality in just 2 steps. On the\nplanning-oriented NAVSIM dataset, with the aligned ResNet-34 backbone,\nDiffusionDrive achieves 88.1 PDMS without bells and whistles, setting a new\nrecord, while running at a real-time speed of 45 FPS on an NVIDIA 4090.\nQualitative results on challenging scenarios further confirm that\nDiffusionDrive can robustly generate diverse plausible driving actions. Code\nand model will be available at https://github.com/hustvl/DiffusionDrive.\n","authors":["Bencheng Liao","Shaoyu Chen","Haoran Yin","Bo Jiang","Cheng Wang","Sixu Yan","Xinbang Zhang","Xiangyu Li","Ying Zhang","Qian Zhang","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15139v2.pdf","comment":"Accepted to CVPR 2025. Code & demo & model are available at\n  https://github.com/hustvl/DiffusionDrive"},{"id":"http://arxiv.org/abs/2311.13186v4","updated":"2025-03-24T02:51:37Z","published":"2023-11-22T06:26:24Z","title":"Applications of Spiking Neural Networks in Visual Place Recognition","summary":"  In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for\ntheir largely-unrealized potential energy efficiency and low latency\nparticularly when implemented on neuromorphic hardware. Our paper highlights\nthree advancements for SNNs in Visual Place Recognition (VPR). Firstly, we\npropose Modular SNNs, where each SNN represents a set of non-overlapping\ngeographically distinct places, enabling scalable networks for large\nenvironments. Secondly, we present Ensembles of Modular SNNs, where multiple\nnetworks represent the same place, significantly enhancing accuracy compared to\nsingle-network models. Each of our Modular SNN modules is compact, comprising\nonly 1500 neurons and 474k synapses, making them ideally suited for ensembling\ndue to their small size. Lastly, we investigate the role of sequence matching\nin SNN-based VPR, a technique where consecutive images are used to refine place\nrecognition. We demonstrate competitive performance of our method on a range of\ndatasets, including higher responsiveness to ensembling compared to\nconventional VPR techniques and higher R@1 improvements with sequence matching\nthan VPR techniques with comparable baseline performance. Our contributions\nhighlight the viability of SNNs for VPR, offering scalable and robust\nsolutions, and paving the way for their application in various energy-sensitive\nrobotic tasks.\n","authors":["Somayeh Hussaini","Michael Milford","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2311.13186v4.pdf","comment":"20 pages, 10 figures, IEEE Transactions on Robotics (TRO)"},{"id":"http://arxiv.org/abs/2503.01439v3","updated":"2025-03-24T02:28:32Z","published":"2025-03-03T11:44:55Z","title":"AVR: Active Vision-Driven Robotic Precision Manipulation with Viewpoint\n  and Focal Length Optimization","summary":"  Robotic manipulation within dynamic environments presents challenges to\nprecise control and adaptability. Traditional fixed-view camera systems face\nchallenges adapting to change viewpoints and scale variations, limiting\nperception and manipulation precision. To tackle these issues, we propose the\nActive Vision-driven Robotic (AVR) framework, a teleoperation hardware solution\nthat supports dynamic viewpoint and dynamic focal length adjustments to\ncontinuously center targets and maintain optimal scale, accompanied by a\ncorresponding algorithm that effectively enhances the success rates of various\noperational tasks. Using the RoboTwin platform with a real-time image\nprocessing plugin, AVR framework improves task success rates by 5%-16% on five\nmanipulation tasks. Physical deployment on a dual-arm system demonstrates in\ncollaborative tasks and 36% precision in screwdriver insertion, outperforming\nbaselines by over 25%. Experimental results confirm that AVR framework enhances\nenvironmental perception, manipulation repeatability (40% $\\le $1 cm error),\nand robustness in complex scenarios, paving the way for future robotic\nprecision manipulation methods in the pursuit of human-level robot dexterity\nand precision.\n","authors":["Yushan Liu","Shilong Mu","Xintao Chao","Zizhen Li","Yao Mu","Tianxing Chen","Shoujie Li","Chuqiao Lyu","Xiao-ping Zhang","Wenbo Ding"],"pdf_url":"https://arxiv.org/pdf/2503.01439v3.pdf","comment":"Previously, there were some problems with our experimental data, and\n  the conclusions need to be further verified. Now that we have completed a\n  full-scale experiment and analysis, and added supporting materials to our\n  website, we hope to be able to resubmit it"},{"id":"http://arxiv.org/abs/2503.18276v1","updated":"2025-03-24T01:46:17Z","published":"2025-03-24T01:46:17Z","title":"Learning Orientation Field for OSM-Guided Autonomous Navigation","summary":"  OpenStreetMap (OSM) has gained popularity recently in autonomous navigation\ndue to its public accessibility, lower maintenance costs, and broader\ngeographical coverage. However, existing methods often struggle with noisy OSM\ndata and incomplete sensor observations, leading to inaccuracies in trajectory\nplanning. These challenges are particularly evident in complex driving\nscenarios, such as at intersections or facing occlusions. To address these\nchallenges, we propose a robust and explainable two-stage framework to learn an\nOrientation Field (OrField) for robot navigation by integrating LiDAR scans and\nOSM routes. In the first stage, we introduce the novel representation, OrField,\nwhich can provide orientations for each grid on the map, reasoning jointly from\nnoisy LiDAR scans and OSM routes. To generate a robust OrField, we train a deep\nneural network by encoding a versatile initial OrField and output an optimized\nOrField. Based on OrField, we propose two trajectory planners for OSM-guided\nrobot navigation, called Field-RRT* and Field-Bezier, respectively, in the\nsecond stage by improving the Rapidly Exploring Random Tree (RRT) algorithm and\nBezier curve to estimate the trajectories. Thanks to the robustness of OrField\nwhich captures both global and local information, Field-RRT* and Field-Bezier\ncan generate accurate and reliable trajectories even in challenging conditions.\nWe validate our approach through experiments on the SemanticKITTI dataset and\nour own campus dataset. The results demonstrate the effectiveness of our\nmethod, achieving superior performance in complex and noisy conditions. Our\ncode for network training and real-world deployment is available at\nhttps://github.com/IMRL/OriField.\n","authors":["Yuming Huang","Wei Gao","Zhiyuan Zhang","Maani Ghaffari","Dezhen Song","Cheng-Zhong Xu","Hui Kong"],"pdf_url":"https://arxiv.org/pdf/2503.18276v1.pdf","comment":"14 pages, 12 figures, and 5 tables"},{"id":"http://arxiv.org/abs/2503.18275v1","updated":"2025-03-24T01:45:40Z","published":"2025-03-24T01:45:40Z","title":"GI-SLAM: Gaussian-Inertial SLAM","summary":"  3D Gaussian Splatting (3DGS) has recently emerged as a powerful\nrepresentation of geometry and appearance for dense Simultaneous Localization\nand Mapping (SLAM). Through rapid, differentiable rasterization of 3D\nGaussians, many 3DGS SLAM methods achieve near real-time rendering and\naccelerated training. However, these methods largely overlook inertial data,\nwitch is a critical piece of information collected from the inertial\nmeasurement unit (IMU). In this paper, we present GI-SLAM, a novel\ngaussian-inertial SLAM system which consists of an IMU-enhanced camera tracking\nmodule and a realistic 3D Gaussian-based scene representation for mapping. Our\nmethod introduces an IMU loss that seamlessly integrates into the deep learning\nframework underpinning 3D Gaussian Splatting SLAM, effectively enhancing the\naccuracy, robustness and efficiency of camera tracking. Moreover, our SLAM\nsystem supports a wide range of sensor configurations, including monocular,\nstereo, and RGBD cameras, both with and without IMU integration. Our method\nachieves competitive performance compared with existing state-of-the-art\nreal-time methods on the EuRoC and TUM-RGBD datasets.\n","authors":["Xulang Liu","Ning Tan"],"pdf_url":"https://arxiv.org/pdf/2503.18275v1.pdf","comment":"10 pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.18158v2","updated":"2025-03-24T00:39:57Z","published":"2024-06-26T08:17:59Z","title":"3D-MVP: 3D Multiview Pretraining for Robotic Manipulation","summary":"  Recent works have shown that visual pretraining on egocentric datasets using\nmasked autoencoders (MAE) can improve generalization for downstream robotics\ntasks. However, these approaches pretrain only on 2D images, while many\nrobotics applications require 3D scene understanding. In this work, we propose\n3D-MVP, a novel approach for 3D Multi-View Pretraining using masked\nautoencoders. We leverage Robotic View Transformer (RVT), which uses a\nmulti-view transformer to understand the 3D scene and predict gripper pose\nactions. We split RVT's multi-view transformer into visual encoder and action\ndecoder, and pretrain its visual encoder using masked autoencoding on\nlarge-scale 3D datasets such as Objaverse. We evaluate 3D-MVP on a suite of\nvirtual robot manipulation tasks and demonstrate improved performance over\nbaselines. Our results suggest that 3D-aware pretraining is a promising\napproach to improve generalization of vision-based robotic manipulation\npolicies. Project site: https://jasonqsy.github.io/3DMVP\n","authors":["Shengyi Qian","Kaichun Mo","Valts Blukis","David F. Fouhey","Dieter Fox","Ankit Goyal"],"pdf_url":"https://arxiv.org/pdf/2406.18158v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2412.02734v4","updated":"2025-03-24T23:48:06Z","published":"2024-12-03T18:18:33Z","title":"MVCTrack: Boosting 3D Point Cloud Tracking via Multimodal-Guided Virtual\n  Cues","summary":"  3D single object tracking is essential in autonomous driving and robotics.\nExisting methods often struggle with sparse and incomplete point cloud\nscenarios. To address these limitations, we propose a Multimodal-guided Virtual\nCues Projection (MVCP) scheme that generates virtual cues to enrich sparse\npoint clouds. Additionally, we introduce an enhanced tracker MVCTrack based on\nthe generated virtual cues. Specifically, the MVCP scheme seamlessly integrates\nRGB sensors into LiDAR-based systems, leveraging a set of 2D detections to\ncreate dense 3D virtual cues that significantly improve the sparsity of point\nclouds. These virtual cues can naturally integrate with existing LiDAR-based 3D\ntrackers, yielding substantial performance gains. Extensive experiments\ndemonstrate that our method achieves competitive performance on the NuScenes\ndataset.\n","authors":["Zhaofeng Hu","Sifan Zhou","Shibo Zhao","Zhihang Yuan","Ci-Jyun Liang"],"pdf_url":"https://arxiv.org/pdf/2412.02734v4.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2503.19200v1","updated":"2025-03-24T22:56:59Z","published":"2025-03-24T22:56:59Z","title":"Optimal Modified Feedback Strategies in LQ Games under Control\n  Imperfections","summary":"  Game-theoretic approaches and Nash equilibrium have been widely applied\nacross various engineering domains. However, practical challenges such as\ndisturbances, delays, and actuator limitations can hinder the precise execution\nof Nash equilibrium strategies. This work explores the impact of such\nimplementation imperfections on game trajectories and players' costs within the\ncontext of a two-player linear quadratic (LQ) nonzero-sum game. Specifically,\nwe analyze how small deviations by one player affect the state and cost\nfunction of the other player. To address these deviations, we propose an\nadjusted control policy that not only mitigates adverse effects optimally but\ncan also exploit the deviations to enhance performance. Rigorous mathematical\nanalysis and proofs are presented, demonstrating through a representative\nexample that the proposed policy modification achieves up to $61\\%$ improvement\ncompared to the unadjusted feedback policy and up to $0.59\\%$ compared to the\nfeedback Nash strategy.\n","authors":["Mahdis Rabbani","Navid Mojahed","Shima Nazari"],"pdf_url":"https://arxiv.org/pdf/2503.19200v1.pdf","comment":"6 pages, 2 figures, Preprint version of a paper submitted to L-CSS\n  and CDC"},{"id":"http://arxiv.org/abs/2503.19171v1","updated":"2025-03-24T21:50:31Z","published":"2025-03-24T21:50:31Z","title":"Contact-based Grasp Control and Inverse Kinematics for a Five-fingered\n  Robotic Hand","summary":"  This paper presents an implementation and analysis of a five-fingered robotic\ngrasping system that combines contact-based control with inverse kinematics\nsolutions. Using the PyBullet simulation environment and the DexHand v2 model,\nwe demonstrate a comprehensive approach to achieving stable grasps through\ncontact point optimization with force closure validation. Our method achieves\nmovement efficiency ratings between 0.966-0.996 for non-thumb fingers and 0.879\nfor the thumb, while maintaining positional accuracy within 0.0267-0.0283m for\nnon-thumb digits and 0.0519m for the thumb. The system demonstrates rapid\nposition stabilization at 240Hz simulation frequency and maintains stable\ncontact configurations throughout the grasp execution. Experimental results\nvalidate the effectiveness of our approach, while also identifying areas for\nfuture enhancement in thumb opposition movements and horizontal plane control.\n","authors":["Robinson Umeike"],"pdf_url":"https://arxiv.org/pdf/2503.19171v1.pdf","comment":"10 Pages, 5 Figures, 1 Table"},{"id":"http://arxiv.org/abs/2501.06235v2","updated":"2025-03-24T21:19:49Z","published":"2025-01-08T09:08:06Z","title":"NextStop: An Improved Tracker For Panoptic LIDAR Segmentation Data","summary":"  4D panoptic LiDAR segmentation is essential for scene understanding in\nautonomous driving and robotics, combining semantic and instance segmentation\nwith temporal consistency. Current methods, like 4D-PLS and 4D-STOP, use a\ntracking-by-detection methodology, employing deep learning networks to perform\nsemantic and instance segmentation on each frame. To maintain temporal\nconsistency, large-size instances detected in the current frame are compared\nand associated with instances within a temporal window that includes the\ncurrent and preceding frames. However, their reliance on short-term instance\ndetection, lack of motion estimation, and exclusion of small-sized instances\nlead to frequent identity switches and reduced tracking performance. We address\nthese issues with the NextStop1 tracker, which integrates Kalman filter-based\nmotion estimation, data association, and lifespan management, along with a\ntracklet state concept to improve prioritization. Evaluated using the LiDAR\nSegmentation and Tracking Quality (LSTQ) metric on the SemanticKITTI validation\nset, NextStop demonstrated enhanced tracking performance, particularly for\nsmall-sized objects like people and bicyclists, with fewer ID switches, earlier\ntracking initiation, and improved reliability in complex environments. The\nsource code is available at https://github.com/AIROTAU/NextStop\n","authors":["Nirit Alkalay","Roy Orfaig","Ben-Zion Bobrovsky"],"pdf_url":"https://arxiv.org/pdf/2501.06235v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13055v2","updated":"2025-03-24T21:17:35Z","published":"2024-09-19T19:07:05Z","title":"MGSO: Monocular Real-time Photometric SLAM with Efficient 3D Gaussian\n  Splatting","summary":"  Real-time SLAM with dense 3D mapping is computationally challenging,\nespecially on resource-limited devices. The recent development of 3D Gaussian\nSplatting (3DGS) offers a promising approach for real-time dense 3D\nreconstruction. However, existing 3DGS-based SLAM systems struggle to balance\nhardware simplicity, speed, and map quality. Most systems excel in one or two\nof the aforementioned aspects but rarely achieve all. A key issue is the\ndifficulty of initializing 3D Gaussians while concurrently conducting SLAM. To\naddress these challenges, we present Monocular GSO (MGSO), a novel real-time\nSLAM system that integrates photometric SLAM with 3DGS. Photometric SLAM\nprovides dense structured point clouds for 3DGS initialization, accelerating\noptimization and producing more efficient maps with fewer Gaussians. As a\nresult, experiments show that our system generates reconstructions with a\nbalance of quality, memory efficiency, and speed that outperforms the\nstate-of-the-art. Furthermore, our system achieves all results using RGB\ninputs. We evaluate the Replica, TUM-RGBD, and EuRoC datasets against current\nlive dense reconstruction systems. Not only do we surpass contemporary systems,\nbut experiments also show that we maintain our performance on laptop hardware,\nmaking it a practical solution for robotics, A/R, and other real-time\napplications.\n","authors":["Yan Song Hu","Nicolas Abboud","Muhammad Qasim Ali","Adam Srebrnjak Yang","Imad Elhajj","Daniel Asmar","Yuhao Chen","John S. Zelek"],"pdf_url":"https://arxiv.org/pdf/2409.13055v2.pdf","comment":"The final version of this work has been approved by the IEEE for\n  publication. This version may no longer be accessible without notice.\n  Copyright 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses"},{"id":"http://arxiv.org/abs/2503.19140v1","updated":"2025-03-24T20:51:22Z","published":"2025-03-24T20:51:22Z","title":"Dom, cars don't fly! -- Or do they? In-Air Vehicle Maneuver for\n  High-Speed Off-Road Navigation","summary":"  When pushing the speed limit for aggressive off-road navigation on uneven\nterrain, it is inevitable that vehicles may become airborne from time to time.\nDuring time-sensitive tasks, being able to fly over challenging terrain can\nalso save time, instead of cautiously circumventing or slowly negotiating\nthrough. However, most off-road autonomy systems operate under the assumption\nthat the vehicles are always on the ground and therefore limit operational\nspeed. In this paper, we present a novel approach for in-air vehicle maneuver\nduring high-speed off-road navigation. Based on a hybrid forward kinodynamic\nmodel using both physics principles and machine learning, our fixed-horizon,\nsampling-based motion planner ensures accurate vehicle landing poses and their\nderivatives within a short airborne time window using vehicle throttle and\nsteering commands. We test our approach in extensive in-air experiments both\nindoors and outdoors, compare it against an error-driven control method, and\ndemonstrate that precise and timely in-air vehicle maneuver is possible through\nexisting ground vehicle controls.\n","authors":["Anuj Pokhrel","Aniket Datar","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.19140v1.pdf","comment":"8 Pages, 4 Figures"},{"id":"http://arxiv.org/abs/2503.19135v1","updated":"2025-03-24T20:45:24Z","published":"2025-03-24T20:45:24Z","title":"Cooperative Control of Multi-Quadrotors for Transporting Cable-Suspended\n  Payloads: Obstacle-Aware Planning and Event-Based Nonlinear Model Predictive\n  Control","summary":"  This paper introduces a novel methodology for the cooperative control of\nmultiple quadrotors transporting cablesuspended payloads, emphasizing\nobstacle-aware planning and event-based Nonlinear Model Predictive Control\n(NMPC). Our approach integrates trajectory planning with real-time control\nthrough a combination of the A* algorithm for global path planning and NMPC for\nlocal control, enhancing trajectory adaptability and obstacle avoidance. We\npropose an advanced event-triggered control system that updates based on events\nidentified through dynamically generated environmental maps. These maps are\nconstructed using a dual-camera setup, which includes multi-camera systems for\nstatic obstacle detection and event cameras for high-resolution, low-latency\ndetection of dynamic obstacles. This design is crucial for addressing\nfast-moving and transient obstacles that conventional cameras may overlook,\nparticularly in environments with rapid motion and variable lighting\nconditions. When new obstacles are detected, the A* algorithm recalculates\nwaypoints based on the updated map, ensuring safe and efficient navigation.\nThis real-time obstacle detection and map updating integration allows the\nsystem to adaptively respond to environmental changes, markedly improving\nsafety and navigation efficiency. The system employs SLAM and object detection\ntechniques utilizing data from multi-cameras, event cameras, and IMUs for\naccurate localization and comprehensive environmental mapping. The NMPC\nframework adeptly manages the complex dynamics of multiple quadrotors and\nsuspended payloads, incorporating safety constraints to maintain dynamic\nfeasibility and stability. Extensive simulations validate the proposed\napproach, demonstrating significant enhancements in energy efficiency,\ncomputational resource management, and responsiveness.\n","authors":["Tohid Kargar Tasooji","Sakineh Khodadadi","Guangjun Liu","Richard Wang"],"pdf_url":"https://arxiv.org/pdf/2503.19135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07065v2","updated":"2025-03-24T20:38:45Z","published":"2024-02-10T23:35:33Z","title":"CAHSOR: Competence-Aware High-Speed Off-Road Ground Navigation in SE(3)","summary":"  While the workspace of traditional ground vehicles is usually assumed to be\nin a 2D plane, i.e., SE(2), such an assumption may not hold when they drive at\nhigh speeds on unstructured off-road terrain: High-speed sharp turns on\nhigh-friction surfaces may lead to vehicle rollover; Turning aggressively on\nloose gravel or grass may violate the non-holonomic constraint and cause\nsignificant lateral sliding; Driving quickly on rugged terrain will produce\nextensive vibration along the vertical axis. Therefore, most offroad vehicles\nare currently limited to drive only at low speeds to assure vehicle stability\nand safety. In this work, we aim at empowering high-speed off-road vehicles\nwith competence awareness in SE(3) so that they can reason about the\nconsequences of taking aggressive maneuvers on different terrain with a 6-DoF\nforward kinodynamic model. The model is learned from visual and inertial\nTerrain Representation for Off-road Navigation (TRON) using multimodal,\nself-supervised vehicle-terrain interactions. We demonstrate the efficacy of\nour Competence-Aware High-Speed Off-Road (CAHSOR) navigation approach on a\nphysical ground robot in both an autonomous navigation and a human\nshared-control setup and show that CAHSOR can efficiently reduce vehicle\ninstability by 62% while only compromising 8.6% average speed with the help of\nTRON.\n","authors":["Anuj Pokhrel","Aniket Datar","Mohammad Nazeri","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2402.07065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07413v2","updated":"2025-03-24T20:30:44Z","published":"2024-10-09T20:28:42Z","title":"A Rapid Trajectory Optimization and Control Framework for\n  Resource-Constrained Applications","summary":"  This paper presents a computationally efficient model predictive control\nformulation that uses an integral Chebyshev collocation method to enable rapid\noperations of autonomous agents. By posing the finite-horizon optimal control\nproblem and recursive re-evaluation of the optimal trajectories, minimization\nof the L2 norms of the state and control errors are transcribed into a\nquadratic program. Control and state variable constraints are parameterized\nusing Chebyshev polynomials and are accommodated in the optimal trajectory\ngeneration programs to incorporate the actuator limits and keep-out\nconstraints. Differentiable collision detection of polytopes is leveraged for\noptimal collision avoidance. Results obtained from the collocation methods are\nbenchmarked against the existing approaches on an edge computer to outline the\nperformance improvements. Finally, collaborative control scenarios involving\nmulti-agent space systems are considered to demonstrate the technical merits of\nthe proposed work.\n","authors":["Deep Parikh","Thomas L. Ahrens","Manoranjan Majji"],"pdf_url":"https://arxiv.org/pdf/2410.07413v2.pdf","comment":"This work has been accepted for publication at the IEEE ACC 2025"},{"id":"http://arxiv.org/abs/2402.15552v4","updated":"2025-03-24T18:59:34Z","published":"2024-02-23T17:21:21Z","title":"Morphological Symmetries in Robotics","summary":"  We present a comprehensive framework for studying and leveraging\nmorphological symmetries in robotic systems. These are intrinsic properties of\nthe robot's morphology, frequently observed in animal biology and robotics,\nwhich stem from the replication of kinematic structures and the symmetrical\ndistribution of mass. We illustrate how these symmetries extend to the robot's\nstate space and both proprioceptive and exteroceptive sensor measurements,\nresulting in the equivariance of the robot's equations of motion and optimal\ncontrol policies. Thus, we recognize morphological symmetries as a relevant and\npreviously unexplored physics-informed geometric prior, with significant\nimplications for both data-driven and analytical methods used in modeling,\ncontrol, estimation and design in robotics. For data-driven methods, we\ndemonstrate that morphological symmetries can enhance the sample efficiency and\ngeneralization of machine learning models through data augmentation, or by\napplying equivariant/invariant constraints on the model's architecture. In the\ncontext of analytical methods, we employ abstract harmonic analysis to\ndecompose the robot's dynamics into a superposition of lower-dimensional,\nindependent dynamics. We substantiate our claims with both synthetic and\nreal-world experiments conducted on bipedal and quadrupedal robots. Lastly, we\nintroduce the repository MorphoSymm to facilitate the practical use of the\ntheory and applications outlined in this work.\n","authors":["Daniel Ordoñez-Apraez","Giulio Turrisi","Vladimir Kostic","Mario Martin","Antonio Agudo","Francesc Moreno-Noguer","Massimiliano Pontil","Claudio Semini","Carlos Mastalli"],"pdf_url":"https://arxiv.org/pdf/2402.15552v4.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2503.19037v1","updated":"2025-03-24T18:08:54Z","published":"2025-03-24T18:08:54Z","title":"Evolutionary Policy Optimization","summary":"  Despite its extreme sample inefficiency, on-policy reinforcement learning has\nbecome a fundamental tool in real-world applications. With recent advances in\nGPU-driven simulation, the ability to collect vast amounts of data for RL\ntraining has scaled exponentially. However, studies show that current on-policy\nmethods, such as PPO, fail to fully leverage the benefits of parallelized\nenvironments, leading to performance saturation beyond a certain scale. In\ncontrast, Evolutionary Algorithms (EAs) excel at increasing diversity through\nrandomization, making them a natural complement to RL. However, existing EvoRL\nmethods have struggled to gain widespread adoption due to their extreme sample\ninefficiency. To address these challenges, we introduce Evolutionary Policy\nOptimization (EPO), a novel policy gradient algorithm that combines the\nstrengths of EA and policy gradients. We show that EPO significantly improves\nperformance across diverse and challenging environments, demonstrating superior\nscalability with parallelized simulations.\n","authors":["Jianren Wang","Yifan Su","Abhinav Gupta","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2503.19037v1.pdf","comment":"Website at https://sites.google.com/view/epo-rl"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2503.18950v1","updated":"2025-03-24T17:59:59Z","published":"2025-03-24T17:59:59Z","title":"Target-Aware Video Diffusion Models","summary":"  We present a target-aware video diffusion model that generates videos from an\ninput image in which an actor interacts with a specified target while\nperforming a desired action. The target is defined by a segmentation mask and\nthe desired action is described via a text prompt. Unlike existing controllable\nimage-to-video diffusion models that often rely on dense structural or motion\ncues to guide the actor's movements toward the target, our target-aware model\nrequires only a simple mask to indicate the target, leveraging the\ngeneralization capabilities of pretrained models to produce plausible actions.\nThis makes our method particularly effective for human-object interaction (HOI)\nscenarios, where providing precise action guidance is challenging, and further\nenables the use of video diffusion models for high-level action planning in\napplications such as robotics. We build our target-aware model by extending a\nbaseline model to incorporate the target mask as an additional input. To\nenforce target awareness, we introduce a special token that encodes the\ntarget's spatial information within the text prompt. We then fine-tune the\nmodel with our curated dataset using a novel cross-attention loss that aligns\nthe cross-attention maps associated with this token with the input target mask.\nTo further improve performance, we selectively apply this loss to the most\nsemantically relevant transformer blocks and attention regions. Experimental\nresults show that our target-aware model outperforms existing solutions in\ngenerating videos where actors interact accurately with the specified targets.\nWe further demonstrate its efficacy in two downstream applications: video\ncontent creation and zero-shot 3D HOI motion synthesis.\n","authors":["Taeksoo Kim","Hanbyul Joo"],"pdf_url":"https://arxiv.org/pdf/2503.18950v1.pdf","comment":"The project page is available at https://taeksuu.github.io/tavid/"},{"id":"http://arxiv.org/abs/2503.18948v1","updated":"2025-03-24T17:59:57Z","published":"2025-03-24T17:59:57Z","title":"Equivariant Image Modeling","summary":"  Current generative models, such as autoregressive and diffusion approaches,\ndecompose high-dimensional data distribution learning into a series of simpler\nsubtasks. However, inherent conflicts arise during the joint optimization of\nthese subtasks, and existing solutions fail to resolve such conflicts without\nsacrificing efficiency or scalability. We propose a novel equivariant image\nmodeling framework that inherently aligns optimization targets across subtasks\nby leveraging the translation invariance of natural visual signals. Our method\nintroduces (1) column-wise tokenization which enhances translational symmetry\nalong the horizontal axis, and (2) windowed causal attention which enforces\nconsistent contextual relationships across positions. Evaluated on\nclass-conditioned ImageNet generation at 256x256 resolution, our approach\nachieves performance comparable to state-of-the-art AR models while using fewer\ncomputational resources. Systematic analysis demonstrates that enhanced\nequivariance reduces inter-task conflicts, significantly improving zero-shot\ngeneralization and enabling ultra-long image synthesis. This work establishes\nthe first framework for task-aligned decomposition in generative modeling,\noffering insights into efficient parameter sharing and conflict-free\noptimization. The code and models are publicly available at\nhttps://github.com/drx-code/EquivariantModeling.\n","authors":["Ruixiao Dong","Mengde Xu","Zigang Geng","Li Li","Han Hu","Shuyang Gu"],"pdf_url":"https://arxiv.org/pdf/2503.18948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18947v1","updated":"2025-03-24T17:59:56Z","published":"2025-03-24T17:59:56Z","title":"Tuning-Free Amodal Segmentation via the Occlusion-Free Bias of\n  Inpainting Models","summary":"  Amodal segmentation aims to predict segmentation masks for both the visible\nand occluded regions of an object. Most existing works formulate this as a\nsupervised learning problem, requiring manually annotated amodal masks or\nsynthetic training data. Consequently, their performance depends on the quality\nof the datasets, which often lack diversity and scale. This work introduces a\ntuning-free approach that repurposes pretrained diffusion-based inpainting\nmodels for amodal segmentation. Our approach is motivated by the\n\"occlusion-free bias\" of inpainting models, i.e., the inpainted objects tend to\nbe complete objects without occlusions. Specifically, we reconstruct the\noccluded regions of an object via inpainting and then apply segmentation, all\nwithout additional training or fine-tuning. Experiments on five datasets\ndemonstrate the generalizability and robustness of our approach. On average,\nour approach achieves 5.3% more accurate masks over the state-of-the-art.\n","authors":["Jae Joong Lee","Bedrich Benes","Raymond A. Yeh"],"pdf_url":"https://arxiv.org/pdf/2503.18947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18945v1","updated":"2025-03-24T17:59:51Z","published":"2025-03-24T17:59:51Z","title":"Aether: Geometric-Aware Unified World Modeling","summary":"  The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance far\nexceeds that of domain-specific models. Additionally, Aether leverages a\ngeometry-informed action space to seamlessly translate predictions into\nactions, enabling effective autonomous trajectory planning. We hope our work\ninspires the community to explore new frontiers in physically-reasonable world\nmodeling and its applications.\n","authors":[" Aether Team","Haoyi Zhu","Yifan Wang","Jianjun Zhou","Wenzheng Chang","Yang Zhou","Zizun Li","Junyi Chen","Chunhua Shen","Jiangmiao Pang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2503.18945v1.pdf","comment":"Project Page: https://aether-world.github.io/"},{"id":"http://arxiv.org/abs/2503.18944v1","updated":"2025-03-24T17:59:11Z","published":"2025-03-24T17:59:11Z","title":"DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation","summary":"  Vision foundation models (VFMs) trained on large-scale image datasets provide\nhigh-quality features that have significantly advanced 2D visual recognition.\nHowever, their potential in 3D vision remains largely untapped, despite the\ncommon availability of 2D images alongside 3D point cloud datasets. While\nsignificant research has been dedicated to 2D-3D fusion, recent\nstate-of-the-art 3D methods predominantly focus on 3D data, leaving the\nintegration of VFMs into 3D models underexplored. In this work, we challenge\nthis trend by introducing DITR, a simple yet effective approach that extracts\n2D foundation model features, projects them to 3D, and finally injects them\ninto a 3D point cloud segmentation model. DITR achieves state-of-the-art\nresults on both indoor and outdoor 3D semantic segmentation benchmarks. To\nenable the use of VFMs even when images are unavailable during inference, we\nfurther propose to distill 2D foundation models into a 3D backbone as a\npretraining task. By initializing the 3D backbone with knowledge distilled from\n2D VFMs, we create a strong basis for downstream 3D segmentation tasks,\nultimately boosting performance across various datasets.\n","authors":["Karim Abou Zeid","Kadir Yilmaz","Daan de Geus","Alexander Hermans","David Adrian","Timm Linder","Bastian Leibe"],"pdf_url":"https://arxiv.org/pdf/2503.18944v1.pdf","comment":"Project page at https://vision.rwth-aachen.de/DITR"},{"id":"http://arxiv.org/abs/2503.18943v1","updated":"2025-03-24T17:59:07Z","published":"2025-03-24T17:59:07Z","title":"SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding","summary":"  We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. This model family employs the two-stream\nSlowFast mechanism, enabling efficient modeling of long-range temporal context\nto meet the demand for lightweight, mobile-friendly Video LLMs. We provide\nmodels ranging from 1B to 7B parameters, optimized through a streamlined\ntraining pipeline and a high-quality data mixture composed of publicly\navailable datasets. Experimental results demonstrate that SF-LLaVA-1.5 achieves\ncompetitive performance on a wide range of video and image benchmarks, with\nrobust results across all model sizes. Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales (1B and 3B) across various video\nbenchmarks.\n","authors":["Mingze Xu","Mingfei Gao","Shiyu Li","Jiasen Lu","Zhe Gan","Zhengfeng Lai","Meng Cao","Kai Kang","Yinfei Yang","Afshin Dehghan"],"pdf_url":"https://arxiv.org/pdf/2503.18943v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2503.18942v1","updated":"2025-03-24T17:59:04Z","published":"2025-03-24T17:59:04Z","title":"Video-T1: Test-Time Scaling for Video Generation","summary":"  With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1\n","authors":["Fangfu Liu","Hanyang Wang","Yimo Cai","Kaiyan Zhang","Xiaohang Zhan","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2503.18942v1.pdf","comment":"Project page: https://liuff19.github.io/Video-T1"},{"id":"http://arxiv.org/abs/2503.18940v1","updated":"2025-03-24T17:59:02Z","published":"2025-03-24T17:59:02Z","title":"Training-free Diffusion Acceleration with Bottleneck Sampling","summary":"  Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3$\\times$ for image generation and 2.5$\\times$ for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics. Code is available at:\nhttps://github.com/tyfeld/Bottleneck-Sampling\n","authors":["Ye Tian","Xin Xia","Yuxi Ren","Shanchuan Lin","Xing Wang","Xuefeng Xiao","Yunhai Tong","Ling Yang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2503.18940v1.pdf","comment":"Code Repo: https://github.com/tyfeld/Bottleneck-Sampling ,Project\n  Page: https://tyfeld.github.io/BottleneckSampling.github.io/"},{"id":"http://arxiv.org/abs/2503.18938v1","updated":"2025-03-24T17:58:15Z","published":"2025-03-24T17:58:15Z","title":"AdaWorld: Learning Adaptable World Models with Latent Actions","summary":"  World models aim to learn action-controlled prediction models and have proven\nessential for the development of intelligent agents. However, most existing\nworld models rely heavily on substantial action-labeled data and costly\ntraining, making it challenging to adapt to novel environments with\nheterogeneous actions through limited interactions. This limitation can hinder\ntheir applicability across broader domains. To overcome this challenge, we\npropose AdaWorld, an innovative world model learning approach that enables\nefficient adaptation. The key idea is to incorporate action information during\nthe pretraining of world models. This is achieved by extracting latent actions\nfrom videos in a self-supervised manner, capturing the most critical\ntransitions between frames. We then develop an autoregressive world model that\nconditions on these latent actions. This learning paradigm enables highly\nadaptable world models, facilitating efficient transfer and learning of new\nactions even with limited interactions and finetuning. Our comprehensive\nexperiments across multiple environments demonstrate that AdaWorld achieves\nsuperior performance in both simulation quality and visual planning.\n","authors":["Shenyuan Gao","Siyuan Zhou","Yilun Du","Jun Zhang","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2503.18938v1.pdf","comment":"Project page: https://adaptable-world-model.github.io/"},{"id":"http://arxiv.org/abs/2503.18933v1","updated":"2025-03-24T17:53:44Z","published":"2025-03-24T17:53:44Z","title":"SyncVP: Joint Diffusion for Synchronous Multi-Modal Video Prediction","summary":"  Predicting future video frames is essential for decision-making systems, yet\nRGB frames alone often lack the information needed to fully capture the\nunderlying complexities of the real world. To address this limitation, we\npropose a multi-modal framework for Synchronous Video Prediction (SyncVP) that\nincorporates complementary data modalities, enhancing the richness and accuracy\nof future predictions. SyncVP builds on pre-trained modality-specific diffusion\nmodels and introduces an efficient spatio-temporal cross-attention module to\nenable effective information sharing across modalities. We evaluate SyncVP on\nstandard benchmark datasets, such as Cityscapes and BAIR, using depth as an\nadditional modality. We furthermore demonstrate its generalization to other\nmodalities on SYNTHIA with semantic information and ERA5-Land with climate\ndata. Notably, SyncVP achieves state-of-the-art performance, even in scenarios\nwhere only one modality is present, demonstrating its robustness and potential\nfor a wide range of applications.\n","authors":["Enrico Pallotta","Sina Mokhtarzadeh Azar","Shuai Li","Olga Zatsarynna","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2503.18933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18931v1","updated":"2025-03-24T17:52:47Z","published":"2025-03-24T17:52:47Z","title":"CoMP: Continual Multimodal Pre-training for Vision Foundation Models","summary":"  Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.\n","authors":["Yitong Chen","Lingchen Meng","Wujian Peng","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.18931v1.pdf","comment":"Code is available in https://github.com/SliMM-X/CoMP-MM"},{"id":"http://arxiv.org/abs/2502.13898v2","updated":"2025-03-24T17:51:52Z","published":"2025-02-19T17:31:59Z","title":"GroundCap: A Visually Grounded Image Captioning Dataset","summary":"  Current image captioning systems lack the ability to link descriptive text to\nspecific visual elements, making their outputs difficult to verify. While\nrecent approaches offer some grounding capabilities, they cannot track object\nidentities across multiple references or ground both actions and objects\nsimultaneously. We propose a novel ID-based grounding system that enables\nconsistent object reference tracking and action-object linking, and present\nGroundCap, a dataset containing 52,016 images from 77 movies, with 344\nhuman-annotated and 52,016 automatically generated captions. Each caption is\ngrounded on detected objects (132 classes) and actions (51 classes) using a tag\nsystem that maintains object identity while linking actions to the\ncorresponding objects. Our approach features persistent object IDs for\nreference tracking, explicit action-object linking, and segmentation of\nbackground elements through K-means clustering. We propose gMETEOR, a metric\ncombining caption quality with grounding accuracy, and establish baseline\nperformance by fine-tuning Pixtral-12B. Human evaluation demonstrates our\napproach's effectiveness in producing verifiable descriptions with coherent\nobject references.\n","authors":["Daniel A. P. Oliveira","Lourenço Teodoro","David Martins de Matos"],"pdf_url":"https://arxiv.org/pdf/2502.13898v2.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2503.18923v1","updated":"2025-03-24T17:46:09Z","published":"2025-03-24T17:46:09Z","title":"Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models","summary":"  Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.\n","authors":["Meng Cao","Pengfei Hu","Yingyao Wang","Jihao Gu","Haoran Tang","Haoze Zhao","Jiahua Dong","Wangbo Yu","Ge Zhang","Ian Reid","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2503.18923v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2503.17074v2","updated":"2025-03-24T17:23:51Z","published":"2025-03-21T11:56:20Z","title":"Zero-Shot Styled Text Image Generation, but Make It Autoregressive","summary":"  Styled Handwritten Text Generation (HTG) has recently received attention from\nthe computer vision and document analysis communities, which have developed\nseveral solutions, either GAN- or diffusion-based, that achieved promising\nresults. Nonetheless, these strategies fail to generalize to novel styles and\nhave technical constraints, particularly in terms of maximum output length and\ntraining efficiency. To overcome these limitations, in this work, we propose a\nnovel framework for text image generation, dubbed Emuru. Our approach leverages\na powerful text image representation model (a variational autoencoder) combined\nwith an autoregressive Transformer. Our approach enables the generation of\nstyled text images conditioned on textual content and style examples, such as\nspecific fonts or handwriting styles. We train our model solely on a diverse,\nsynthetic dataset of English text rendered in over 100,000 typewritten and\ncalligraphy fonts, which gives it the capability to reproduce unseen styles\n(both fonts and users' handwriting) in zero-shot. To the best of our knowledge,\nEmuru is the first autoregressive model for HTG, and the first designed\nspecifically for generalization to novel styles. Moreover, our model generates\nimages without background artifacts, which are easier to use for downstream\napplications. Extensive evaluation on both typewritten and handwritten,\nany-length text image generation scenarios demonstrates the effectiveness of\nour approach.\n","authors":["Vittorio Pippi","Fabio Quattrini","Silvia Cascianelli","Alessio Tonioni","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2503.17074v2.pdf","comment":"Accepted at CVPR2025"},{"id":"http://arxiv.org/abs/2503.18903v1","updated":"2025-03-24T17:15:24Z","published":"2025-03-24T17:15:24Z","title":"Building Blocks for Robust and Effective Semi-Supervised Real-World\n  Object Detection","summary":"  Semi-supervised object detection (SSOD) based on pseudo-labeling\nsignificantly reduces dependence on large labeled datasets by effectively\nleveraging both labeled and unlabeled data. However, real-world applications of\nSSOD often face critical challenges, including class imbalance, label noise,\nand labeling errors. We present an in-depth analysis of SSOD under real-world\nconditions, uncovering causes of suboptimal pseudo-labeling and key trade-offs\nbetween label quality and quantity. Based on our findings, we propose four\nbuilding blocks that can be seamlessly integrated into an SSOD framework. Rare\nClass Collage (RCC): a data augmentation method that enhances the\nrepresentation of rare classes by creating collages of rare objects. Rare Class\nFocus (RCF): a stratified batch sampling strategy that ensures a more balanced\nrepresentation of all classes during training. Ground Truth Label Correction\n(GLC): a label refinement method that identifies and corrects false, missing,\nand noisy ground truth labels by leveraging the consistency of teacher model\npredictions. Pseudo-Label Selection (PLS): a selection method for removing\nlow-quality pseudo-labeled images, guided by a novel metric estimating the\nmissing detection rate while accounting for class rarity. We validate our\nmethods through comprehensive experiments on autonomous driving datasets,\nresulting in up to 6% increase in SSOD performance. Overall, our investigation\nand novel, data-centric, and broadly applicable building blocks enable robust\nand effective SSOD in complex, real-world scenarios. Code is available at\nhttps://mos-ks.github.io/publications.\n","authors":["Moussa Kassem Sbeyti","Nadja Klein","Azarm Nowzad","Fikret Sivrikaya","Sahin Albayrak"],"pdf_url":"https://arxiv.org/pdf/2503.18903v1.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR).\n  OpenReview: https://openreview.net/forum?id=vRYt8QLKqK"},{"id":"http://arxiv.org/abs/2501.05446v2","updated":"2025-03-24T17:14:43Z","published":"2025-01-09T18:58:30Z","title":"Relative Pose Estimation through Affine Corrections of Monocular Depth\n  Priors","summary":"  Monocular depth estimation (MDE) models have undergone significant\nadvancements over recent years. Many MDE models aim to predict affine-invariant\nrelative depth from monocular images, while recent developments in large-scale\ntraining and vision foundation models enable reasonable estimation of metric\n(absolute) depth. However, effectively leveraging these predictions for\ngeometric vision tasks, in particular relative pose estimation, remains\nrelatively under explored. While depths provide rich constraints for cross-view\nimage alignment, the intrinsic noise and ambiguity from the monocular depth\npriors present practical challenges to improving upon classic keypoint-based\nsolutions. In this paper, we develop three solvers for relative pose estimation\nthat explicitly account for independent affine (scale and shift) ambiguities,\ncovering both calibrated and uncalibrated conditions. We further propose a\nhybrid estimation pipeline that combines our proposed solvers with classic\npoint-based solvers and epipolar constraints. We find that the affine\ncorrection modeling is beneficial to not only the relative depth priors but\nalso, surprisingly, the \"metric\" ones. Results across multiple datasets\ndemonstrate large improvements of our approach over classic keypoint-based\nbaselines and PnP-based solutions, under both calibrated and uncalibrated\nsetups. We also show that our method improves consistently with different\nfeature matchers and MDE models, and can further benefit from very recent\nadvances on both modules. Code is available at\nhttps://github.com/MarkYu98/madpose.\n","authors":["Yifan Yu","Shaohui Liu","Rémi Pautrat","Marc Pollefeys","Viktor Larsson"],"pdf_url":"https://arxiv.org/pdf/2501.05446v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18897v1","updated":"2025-03-24T17:09:36Z","published":"2025-03-24T17:09:36Z","title":"Online 3D Scene Reconstruction Using Neural Object Priors","summary":"  This paper addresses the problem of reconstructing a scene online at the\nlevel of objects given an RGB-D video sequence. While current object-aware\nneural implicit representations hold promise, they are limited in online\nreconstruction efficiency and shape completion. Our main contributions to\nalleviate the above limitations are twofold. First, we propose a feature grid\ninterpolation mechanism to continuously update grid-based object-centric neural\nimplicit representations as new object parts are revealed. Second, we construct\nan object library with previously mapped objects in advance and leverage the\ncorresponding shape priors to initialize geometric object models in new videos,\nsubsequently completing them with novel views as well as synthesized past views\nto avoid losing original object details. Extensive experiments on synthetic\nenvironments from the Replica dataset, real-world ScanNet sequences and videos\ncaptured in our laboratory demonstrate that our approach outperforms\nstate-of-the-art neural implicit models for this task in terms of\nreconstruction accuracy and completeness.\n","authors":["Thomas Chabal","Shizhe Chen","Jean Ponce","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2503.18897v1.pdf","comment":"3DV 2025. Project page:\n  https://www.di.ens.fr/willow/research/online-scene-reconstruction/"},{"id":"http://arxiv.org/abs/2503.18886v1","updated":"2025-03-24T16:59:57Z","published":"2025-03-24T16:59:57Z","title":"CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models","summary":"  Classifier-Free Guidance (CFG) is a widely adopted technique in\ndiffusion/flow models to improve image fidelity and controllability. In this\nwork, we first analytically study the effect of CFG on flow matching models\ntrained on Gaussian mixtures where the ground-truth flow can be derived. We\nobserve that in the early stages of training, when the flow estimation is\ninaccurate, CFG directs samples toward incorrect trajectories. Building on this\nobservation, we propose CFG-Zero*, an improved CFG with two contributions: (a)\noptimized scale, where a scalar is optimized to correct for the inaccuracies in\nthe estimated velocity, hence the * in the name; and (b) zero-init, which\ninvolves zeroing out the first few steps of the ODE solver. Experiments on both\ntext-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video\n(Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG,\nhighlighting its effectiveness in guiding Flow Matching models. (Code is\navailable at github.com/WeichenFan/CFG-Zero-star)\n","authors":["Weichen Fan","Amber Yijia Zheng","Raymond A. Yeh","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2503.18886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18883v1","updated":"2025-03-24T16:58:37Z","published":"2025-03-24T16:58:37Z","title":"Efficient and Accurate Scene Text Recognition with Cascaded-Transformers","summary":"  In recent years, vision transformers with text decoder have demonstrated\nremarkable performance on Scene Text Recognition (STR) due to their ability to\ncapture long-range dependencies and contextual relationships with high learning\ncapacity. However, the computational and memory demands of these models are\nsignificant, limiting their deployment in resource-constrained applications. To\naddress this challenge, we propose an efficient and accurate STR system.\nSpecifically, we focus on improving the efficiency of encoder models by\nintroducing a cascaded-transformers structure. This structure progressively\nreduces the vision token size during the encoding step, effectively eliminating\nredundant tokens and reducing computational cost. Our experimental results\nconfirm that our STR system achieves comparable performance to state-of-the-art\nbaselines while substantially decreasing computational requirements. In\nparticular, for large-models, the accuracy remains same, 92.77 to 92.68, while\ncomputational complexity is almost halved with our structure.\n","authors":["Savas Ozkan","Andrea Maracani","Hyowon Kim","Sijun Cho","Eunchung Noh","Jeongwon Min","Jung Min Cho","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2503.18883v1.pdf","comment":"Accepted to ACM-MMSys2025"},{"id":"http://arxiv.org/abs/2412.16645v2","updated":"2025-03-24T16:58:13Z","published":"2024-12-21T14:31:36Z","title":"Complementary Advantages: Exploiting Cross-Field Frequency Correlation\n  for NIR-Assisted Image Denoising","summary":"  Existing single-image denoising algorithms often struggle to restore details\nwhen dealing with complex noisy images. The introduction of near-infrared (NIR)\nimages offers new possibilities for RGB image denoising. However, due to the\ninconsistency between NIR and RGB images, the existing works still struggle to\nbalance the contributions of two fields in the process of image fusion. In\nresponse to this, in this paper, we develop a cross-field Frequency Correlation\nExploiting Network (FCENet) for NIR-assisted image denoising. We first propose\nthe frequency correlation prior based on an in-depth statistical frequency\nanalysis of NIR-RGB image pairs. The prior reveals the complementary\ncorrelation of NIR and RGB images in the frequency domain. Leveraging frequency\ncorrelation prior, we then establish a frequency learning framework composed of\nFrequency Dynamic Selection Mechanism (FDSM) and Frequency Exhaustive Fusion\nMechanism (FEFM). FDSM dynamically selects complementary information from NIR\nand RGB images in the frequency domain, and FEFM strengthens the control of\ncommon and differential features during the fusion process of NIR and RGB\nfeatures. Extensive experiments on simulated and real data validate that the\nproposed method outperforms other state-of-the-art methods. The code will be\nreleased at https://github.com/yuchenwang815/FCENet.\n","authors":["Yuchen Wang","Hongyuan Wang","Lizhi Wang","Xin Wang","Lin Zhu","Wanxuan Lu","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2412.16645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01255v2","updated":"2025-03-24T16:57:58Z","published":"2024-12-02T08:24:49Z","title":"Merging synthetic and real embryo data for advanced AI predictions","summary":"  Accurate embryo morphology assessment is essential in assisted reproductive\ntechnology for selecting the most viable embryo. Artificial intelligence has\nthe potential to enhance this process. However, the limited availability of\nembryo data presents challenges for training deep learning models. To address\nthis, we trained two generative models using two datasets-one we created and\nmade publicly available, and one existing public dataset-to generate synthetic\nembryo images at various cell stages, including 2-cell, 4-cell, 8-cell, morula,\nand blastocyst. These were combined with real images to train classification\nmodels for embryo cell stage prediction. Our results demonstrate that\nincorporating synthetic images alongside real data improved classification\nperformance, with the model achieving 97% accuracy compared to 94.5% when\ntrained solely on real data. This trend remained consistent when tested on an\nexternal Blastocyst dataset from a different clinic. Notably, even when trained\nexclusively on synthetic data and tested on real data, the model achieved a\nhigh accuracy of 92%. Furthermore, combining synthetic data from both\ngenerative models yielded better classification results than using data from a\nsingle generative model. Four embryologists evaluated the fidelity of the\nsynthetic images through a Turing test, during which they annotated\ninaccuracies and offered feedback. The analysis showed the diffusion model\noutperformed the generative adversarial network, deceiving embryologists 66.6%\nversus 25.3% and achieving lower Frechet inception distance scores.\n","authors":["Oriana Presacan","Alexandru Dorobantiu","Vajira Thambawita","Michael A. Riegler","Mette H. Stensen","Mario Iliceto","Alexandru C. Aldea","Akriti Sharma"],"pdf_url":"https://arxiv.org/pdf/2412.01255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18880v1","updated":"2025-03-24T16:56:04Z","published":"2025-03-24T16:56:04Z","title":"Seeing Speech and Sound: Distinguishing and Locating Audios in Visual\n  Scenes","summary":"  We present a unified model capable of simultaneously grounding both spoken\nlanguage and non-speech sounds within a visual scene, addressing key\nlimitations in current audio-visual grounding models. Existing approaches are\ntypically limited to handling either speech or non-speech sounds independently,\nor at best, together but sequentially without mixing. This limitation prevents\nthem from capturing the complexity of real-world audio sources that are often\nmixed. Our approach introduces a 'mix-and-separate' framework with audio-visual\nalignment objectives that jointly learn correspondence and disentanglement\nusing mixed audio. Through these objectives, our model learns to produce\ndistinct embeddings for each audio type, enabling effective disentanglement and\ngrounding across mixed audio sources. Additionally, we created a new dataset to\nevaluate simultaneous grounding of mixed audio sources, demonstrating that our\nmodel outperforms prior methods. Our approach also achieves comparable or\nbetter performance in standard segmentation and cross-modal retrieval tasks,\nhighlighting the benefits of our mix-and-separate approach.\n","authors":["Hyeonggon Ryu","Seongyu Kim","Joon Son Chung","Arda Senocak"],"pdf_url":"https://arxiv.org/pdf/2503.18880v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18874v1","updated":"2025-03-24T16:49:06Z","published":"2025-03-24T16:49:06Z","title":"A semantic communication-based workload-adjustable transceiver for\n  wireless AI-generated content (AIGC) delivery","summary":"  With the significant advances in generative AI (GAI) and the proliferation of\nmobile devices, providing high-quality AI-generated content (AIGC) services via\nwireless networks is becoming the future direction. However, the primary\nchallenges of AIGC service delivery in wireless networks lie in unstable\nchannels, limited bandwidth resources, and unevenly distributed computational\nresources. In this paper, we employ semantic communication (SemCom) in\ndiffusion-based GAI models to propose a Resource-aware wOrkload-adjUstable\nTransceivEr (ROUTE) for AIGC delivery in dynamic wireless networks.\nSpecifically, to relieve the communication resource bottleneck, SemCom is\nutilized to prioritize semantic information of the generated content. Then, to\nimprove computational resource utilization in both edge and local and reduce\nAIGC semantic distortion in transmission, modified diffusion-based models are\napplied to adjust the computing workload and semantic density in cooperative\ncontent generation. Simulations verify the superiority of our proposed ROUTE in\nterms of latency and content quality compared to conventional AIGC approaches.\n","authors":["Runze Cheng","Yao Sun","Lan Zhang","Lei Feng","Lei Zhang","Muhammad Ali Imran"],"pdf_url":"https://arxiv.org/pdf/2503.18874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18873v1","updated":"2025-03-24T16:48:42Z","published":"2025-03-24T16:48:42Z","title":"Efficient Self-Supervised Adaptation for Medical Image Analysis","summary":"  Self-supervised adaptation (SSA) improves foundation model transfer to\nmedical domains but is computationally prohibitive. Although parameter\nefficient fine-tuning methods such as LoRA have been explored for supervised\nadaptation, their effectiveness for SSA remains unknown. In this work, we\nintroduce efficient self-supervised adaptation (ESSA), a framework that applies\nparameter-efficient fine-tuning techniques to SSA with the aim of reducing\ncomputational cost and improving adaptation performance. Among the methods\ntested, Attention Projection Layer Adaptation (APLA) sets a new\nstate-of-the-art, consistently surpassing full-parameter SSA and supervised\nfine-tuning across diverse medical tasks, while reducing GPU memory by up to\n40.1% and increasing training throughput by 25.2%, all while maintaining\ninference efficiency.\n","authors":["Moein Sorkhei","Emir Konuk","Jingyu Guo","Christos Matsoukas","Kevin Smith"],"pdf_url":"https://arxiv.org/pdf/2503.18873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12725v2","updated":"2025-03-24T16:47:54Z","published":"2024-12-17T09:47:48Z","title":"RaCFormer: Towards High-Quality 3D Object Detection via Query-based\n  Radar-Camera Fusion","summary":"  We propose Radar-Camera fusion transformer (RaCFormer) to boost the accuracy\nof 3D object detection by the following insight. The Radar-Camera fusion in\noutdoor 3D scene perception is capped by the image-to-BEV transformation--if\nthe depth of pixels is not accurately estimated, the naive combination of BEV\nfeatures actually integrates unaligned visual content. To avoid this problem,\nwe propose a query-based framework that enables adaptive sampling of\ninstance-relevant features from both the bird's-eye view (BEV) and the original\nimage view. Furthermore, we enhance system performance by two key designs:\noptimizing query initialization and strengthening the representational capacity\nof BEV. For the former, we introduce an adaptive circular distribution in polar\ncoordinates to refine the initialization of object queries, allowing for a\ndistance-based adjustment of query density. For the latter, we initially\nincorporate a radar-guided depth head to refine the transformation from image\nview to BEV. Subsequently, we focus on leveraging the Doppler effect of radar\nand introduce an implicit dynamic catcher to capture the temporal elements\nwithin the BEV. Extensive experiments on nuScenes and View-of-Delft (VoD)\ndatasets validate the merits of our design. Remarkably, our method achieves\nsuperior results of 64.9% mAP and 70.2% NDS on nuScenes. RaCFormer also secures\nthe state-of-the-art performance on the VoD dataset. Code is available at\nhttps://github.com/cxmomo/RaCFormer.\n","authors":["Xiaomeng Chu","Jiajun Deng","Guoliang You","Yifan Duan","Houqiang Li","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12725v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18872v1","updated":"2025-03-24T16:47:40Z","published":"2025-03-24T16:47:40Z","title":"Curriculum Coarse-to-Fine Selection for High-IPC Dataset Distillation","summary":"  Dataset distillation (DD) excels in synthesizing a small number of images per\nclass (IPC) but struggles to maintain its effectiveness in high-IPC settings.\nRecent works on dataset distillation demonstrate that combining distilled and\nreal data can mitigate the effectiveness decay. However, our analysis of the\ncombination paradigm reveals that the current one-shot and independent\nselection mechanism induces an incompatibility issue between distilled and real\nimages. To address this issue, we introduce a novel curriculum coarse-to-fine\nselection (CCFS) method for efficient high-IPC dataset distillation. CCFS\nemploys a curriculum selection framework for real data selection, where we\nleverage a coarse-to-fine strategy to select appropriate real data based on the\ncurrent synthetic dataset in each curriculum. Extensive experiments validate\nCCFS, surpassing the state-of-the-art by +6.6\\% on CIFAR-10, +5.8\\% on\nCIFAR-100, and +3.4\\% on Tiny-ImageNet under high-IPC settings. Notably, CCFS\nachieves 60.2\\% test accuracy on ResNet-18 with a 20\\% compression ratio of\nTiny-ImageNet, closely matching full-dataset training with only 0.3\\%\ndegradation. Code: https://github.com/CYDaaa30/CCFS.\n","authors":["Yanda Chen","Gongwei Chen","Miao Zhang","Weili Guan","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2503.18872v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2412.18883v2","updated":"2025-03-24T16:42:33Z","published":"2024-12-25T11:47:26Z","title":"MotionMap: Representing Multimodality in Human Pose Forecasting","summary":"  Human pose forecasting is inherently multimodal since multiple futures exist\nfor an observed pose sequence. However, evaluating multimodality is challenging\nsince the task is ill-posed. Therefore, we first propose an alternative\nparadigm to make the task well-posed. Next, while state-of-the-art methods\npredict multimodality, this requires oversampling a large volume of\npredictions. This raises key questions: (1) Can we capture multimodality by\nefficiently sampling a smaller number of predictions? (2) Subsequently, which\nof the predicted futures is more likely for an observed pose sequence? We\naddress these questions with MotionMap, a simple yet effective heatmap based\nrepresentation for multimodality. We extend heatmaps to represent a spatial\ndistribution over the space of all possible motions, where different local\nmaxima correspond to different forecasts for a given observation. MotionMap can\ncapture a variable number of modes per observation and provide confidence\nmeasures for different modes. Further, MotionMap allows us to introduce the\nnotion of uncertainty and controllability over the forecasted pose sequence.\nFinally, MotionMap captures rare modes that are non-trivial to evaluate yet\ncritical for safety. We support our claims through multiple qualitative and\nquantitative experiments using popular 3D human pose datasets: Human3.6M and\nAMASS, highlighting the strengths and limitations of our proposed method.\nProject Page: https://vita-epfl.github.io/MotionMap\n","authors":["Reyhaneh Hosseininejad","Megh Shukla","Saeed Saadatnejad","Mathieu Salzmann","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2412.18883v2.pdf","comment":"CVPR 2025. We propose a new representation for learning multimodality\n  in human pose forecasting which does not depend on generative models"},{"id":"http://arxiv.org/abs/2412.17808v3","updated":"2025-03-24T16:41:50Z","published":"2024-12-23T18:59:06Z","title":"Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders","summary":"  Recent 3D content generation pipelines commonly employ Variational\nAutoencoders (VAEs) to encode shapes into compact latent representations for\ndiffusion-based generation. However, the widely adopted uniform point sampling\nstrategy in Shape VAE training often leads to a significant loss of geometric\ndetails, limiting the quality of shape reconstruction and downstream generation\ntasks. We present Dora-VAE, a novel approach that enhances VAE reconstruction\nthrough our proposed sharp edge sampling strategy and a dual cross-attention\nmechanism. By identifying and prioritizing regions with high geometric\ncomplexity during training, our method significantly improves the preservation\nof fine-grained shape features. Such sampling strategy and the dual attention\nmechanism enable the VAE to focus on crucial geometric details that are\ntypically missed by uniform sampling approaches. To systematically evaluate VAE\nreconstruction quality, we additionally propose Dora-bench, a benchmark that\nquantifies shape complexity through the density of sharp edges, introducing a\nnew metric focused on reconstruction accuracy at these salient geometric\nfeatures. Extensive experiments on the Dora-bench demonstrate that Dora-VAE\nachieves comparable reconstruction quality to the state-of-the-art dense\nXCube-VAE while requiring a latent space at least 8$\\times$ smaller (1,280 vs.\n> 10,000 codes).\n","authors":["Rui Chen","Jianfeng Zhang","Yixun Liang","Guan Luo","Weiyu Li","Jiarui Liu","Xiu Li","Xiaoxiao Long","Jiashi Feng","Ping Tan"],"pdf_url":"https://arxiv.org/pdf/2412.17808v3.pdf","comment":"Accepted by CVPR 2025. Project page: https://aruichen.github.io/Dora/"},{"id":"http://arxiv.org/abs/2503.18862v1","updated":"2025-03-24T16:38:31Z","published":"2025-03-24T16:38:31Z","title":"Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation","summary":"  While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.\n","authors":["DeShin Hwa","Tobias Holmes","Klaus Drechsler"],"pdf_url":"https://arxiv.org/pdf/2503.18862v1.pdf","comment":"6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71"},{"id":"http://arxiv.org/abs/2503.18860v1","updated":"2025-03-24T16:35:41Z","published":"2025-03-24T16:35:41Z","title":"HunyuanPortrait: Implicit Condition Control for Enhanced Portrait\n  Animation","summary":"  We introduce HunyuanPortrait, a diffusion-based condition control method that\nemploys implicit representations for highly controllable and lifelike portrait\nanimation. Given a single portrait image as an appearance reference and video\nclips as driving templates, HunyuanPortrait can animate the character in the\nreference image by the facial expression and head pose of the driving videos.\nIn our framework, we utilize pre-trained encoders to achieve the decoupling of\nportrait motion information and identity in videos. To do so, implicit\nrepresentation is adopted to encode motion information and is employed as\ncontrol signals in the animation phase. By leveraging the power of stable video\ndiffusion as the main building block, we carefully design adapter layers to\ninject control signals into the denoising unet through attention mechanisms.\nThese bring spatial richness of details and temporal consistency.\nHunyuanPortrait also exhibits strong generalization performance, which can\neffectively disentangle appearance and motion under different image styles. Our\nframework outperforms existing methods, demonstrating superior temporal\nconsistency and controllability. Our project is available at\nhttps://kkakkkka.github.io/HunyuanPortrait.\n","authors":["Zunnan Xu","Zhentao Yu","Zixiang Zhou","Jun Zhou","Xiaoyu Jin","Fa-Ting Hong","Xiaozhong Ji","Junwei Zhu","Chengfei Cai","Shiyu Tang","Qin Lin","Xiu Li","Qinglin Lu"],"pdf_url":"https://arxiv.org/pdf/2503.18860v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.15426v2","updated":"2025-03-24T16:34:55Z","published":"2025-03-19T17:08:13Z","title":"Visual Position Prompt for MLLM based Visual Grounding","summary":"  Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address this\nissue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms. The global VPP overlays learnable, axis-like\nembeddings onto the input image to provide structured spatial cues. The local\nVPP focuses on fine-grained localization by incorporating position-aware\nqueries, which suggests probable object locations. We also introduce a VPP-SFT\ndataset with 0.6M samples, consolidating high-quality visual grounding data\ninto a compact format for efficient model training. Training on this dataset\nwith VPP enhances the model's performance, achieving state-of-the-art results\non standard grounding benchmarks despite using fewer training samples compared\nto other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\\sim$21M\nsamples). The code and VPP-SFT dataset will be available at\nhttps://github.com/WayneTomas/VPP-LLaVA upon acceptance.\n","authors":["Wei Tang","Yanpeng Sun","Qinying Gu","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2503.15426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08085v3","updated":"2025-03-24T16:34:35Z","published":"2025-03-11T06:37:54Z","title":"PRISM: Privacy-Preserving Improved Stochastic Masking for Federated\n  Generative Models","summary":"  Despite recent advancements in federated learning (FL), the integration of\ngenerative models into FL has been limited due to challenges such as high\ncommunication costs and unstable training in heterogeneous data environments.\nTo address these issues, we propose PRISM, a FL framework tailored for\ngenerative models that ensures (i) stable performance in heterogeneous data\ndistributions and (ii) resource efficiency in terms of communication cost and\nfinal model size. The key of our method is to search for an optimal stochastic\nbinary mask for a random network rather than updating the model weights,\nidentifying a sparse subnetwork with high generative performance; i.e., a\n``strong lottery ticket''. By communicating binary masks in a stochastic\nmanner, PRISM minimizes communication overhead. This approach, combined with\nthe utilization of maximum mean discrepancy (MMD) loss and a mask-aware dynamic\nmoving average aggregation method (MADA) on the server side, facilitates stable\nand strong generative capabilities by mitigating local divergence in FL\nscenarios. Moreover, thanks to its sparsifying characteristic, PRISM yields a\nlightweight model without extra pruning or quantization, making it ideal for\nenvironments such as edge devices. Experiments on MNIST, FMNIST, CelebA, and\nCIFAR10 demonstrate that PRISM outperforms existing methods, while maintaining\nprivacy with minimal communication costs. PRISM is the first to successfully\ngenerate images under challenging non-IID and privacy-preserving FL\nenvironments on complex datasets, where previous methods have struggled.\n","authors":["Kyeongkook Seo","Dong-Jun Han","Jaejun Yoo"],"pdf_url":"https://arxiv.org/pdf/2503.08085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12532v2","updated":"2025-03-24T16:33:28Z","published":"2025-03-16T14:53:43Z","title":"STEVE: A Step Verification Pipeline for Computer-use Agent Training","summary":"  Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.\n","authors":["Fanbin Lu","Zhisheng Zhong","Ziqin Wei","Shu Liu","Chi-Wing Fu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2503.12532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18854v1","updated":"2025-03-24T16:32:17Z","published":"2025-03-24T16:32:17Z","title":"MC-LLaVA: Multi-Concept Personalized Vision-Language Model","summary":"  Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at\n$\\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}$.\n","authors":["Ruichuan An","Sihan Yang","Ming Lu","Renrui Zhang","Kai Zeng","Yulin Luo","Jiajun Cao","Hao Liang","Ying Chen","Qi She","Shanghang Zhang","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.18854v1.pdf","comment":"The code and dataset will be publicly available at\n  $\\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}$"},{"id":"http://arxiv.org/abs/2503.18853v1","updated":"2025-03-24T16:31:52Z","published":"2025-03-24T16:31:52Z","title":"3DSwapping: Texture Swapping For 3D Object From Single Reference Image","summary":"  3D texture swapping allows for the customization of 3D object textures,\nenabling efficient and versatile visual transformations in 3D editing. While no\ndedicated method exists, adapted 2D editing and text-driven 3D editing\napproaches can serve this purpose. However, 2D editing requires frame-by-frame\nmanipulation, causing inconsistencies across views, while text-driven 3D\nediting struggles to preserve texture characteristics from reference images. To\ntackle these challenges, we introduce 3DSwapping, a 3D texture swapping method\nthat integrates: 1) progressive generation, 2) view-consistency gradient\nguidance, and 3) prompt-tuned gradient guidance. To ensure view consistency,\nour progressive generation process starts by editing a single reference image\nand gradually propagates the edits to adjacent views. Our view-consistency\ngradient guidance further reinforces consistency by conditioning the generation\nmodel on feature differences between consistent and inconsistent outputs. To\npreserve texture characteristics, we introduce prompt-tuning-based gradient\nguidance, which learns a token that precisely captures the difference between\nthe reference image and the 3D object. This token then guides the editing\nprocess, ensuring more consistent texture preservation across views. Overall,\n3DSwapping integrates these novel strategies to achieve higher-fidelity texture\ntransfer while preserving structural coherence across multiple viewpoints.\nExtensive qualitative and quantitative evaluations confirm that our three novel\ncomponents enable convincing and effective 2D texture swapping for 3D objects.\nCode will be available upon acceptance.\n","authors":["Xiao Cao","Beibei Lin","Bo Wang","Zhiyong Huang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2503.18853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16460v2","updated":"2025-03-24T16:27:13Z","published":"2024-12-21T03:25:01Z","title":"Positive2Negative: Breaking the Information-Lossy Barrier in\n  Self-Supervised Single Image Denoising","summary":"  Image denoising enhances image quality, serving as a foundational technique\nacross various computational photography applications. The obstacle to clean\nimage acquisition in real scenarios necessitates the development of\nself-supervised image denoising methods only depending on noisy images,\nespecially a single noisy image. Existing self-supervised image denoising\nparadigms (Noise2Noise and Noise2Void) rely heavily on information-lossy\noperations, such as downsampling and masking, culminating in low quality\ndenoising performance. In this paper, we propose a novel self-supervised single\nimage denoising paradigm, Positive2Negative, to break the information-lossy\nbarrier. Our paradigm involves two key steps: Renoised Data Construction (RDC)\nand Denoised Consistency Supervision (DCS). RDC renoises the predicted denoised\nimage by the predicted noise to construct multiple noisy images, preserving all\nthe information of the original image. DCS ensures consistency across the\nmultiple denoised images, supervising the network to learn robust denoising.\nOur Positive2Negative paradigm achieves state-of-the-art performance in\nself-supervised single image denoising with significant speed improvements. The\ncode is released to the public at https://github.com/Li-Tong-621/P2N.\n","authors":["Tong Li","Lizhi Wang","Zhiyuan Xu","Lin Zhu","Wanxuan Lu","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2412.16460v2.pdf","comment":"8 figures, 5 tables, 11 pages"},{"id":"http://arxiv.org/abs/2411.17188v2","updated":"2025-03-24T16:16:20Z","published":"2024-11-26T07:55:57Z","title":"Interleaved Scene Graphs for Interleaved Text-and-Image Generation\n  Assessment","summary":"  Many real-world user queries (e.g. \"How do to make egg fried rice?\") could\nbenefit from systems capable of generating responses with both textual steps\nwith accompanying images, similar to a cookbook. Models designed to generate\ninterleaved text and images face challenges in ensuring consistency within and\nacross these modalities. To address these challenges, we present ISG, a\ncomprehensive evaluation framework for interleaved text-and-image generation.\nISG leverages a scene graph structure to capture relationships between text and\nimage blocks, evaluating responses on four levels of granularity: holistic,\nstructural, block-level, and image-specific. This multi-tiered evaluation\nallows for a nuanced assessment of consistency, coherence, and accuracy, and\nprovides interpretable question-answer feedback. In conjunction with ISG, we\nintroduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8\ncategories and 21 subcategories. This benchmark dataset includes complex\nlanguage-vision dependencies and golden answers to evaluate models effectively\non vision-centric tasks such as style transfer, a challenging area for current\nmodels. Using ISG-Bench, we demonstrate that recent unified vision-language\nmodels perform poorly on generating interleaved content. While compositional\napproaches that combine separate language and image models show a 111%\nimprovement over unified models at the holistic level, their performance\nremains suboptimal at both block and image levels. To facilitate future work,\nwe develop ISG-Agent, a baseline agent employing a \"plan-execute-refine\"\npipeline to invoke tools, achieving a 122% performance improvement.\n","authors":["Dongping Chen","Ruoxi Chen","Shu Pu","Zhaoyi Liu","Yanru Wu","Caixi Chen","Benlin Liu","Yue Huang","Yao Wan","Pan Zhou","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2411.17188v2.pdf","comment":"Accepted by ICLR 2025 as Spotlight. Project homepage:\n  https://interleave-eval.github.io/"},{"id":"http://arxiv.org/abs/2503.18840v1","updated":"2025-03-24T16:13:04Z","published":"2025-03-24T16:13:04Z","title":"Learning to segment anatomy and lesions from disparately labeled sources\n  in brain MRI","summary":"  Segmenting healthy tissue structures alongside lesions in brain Magnetic\nResonance Images (MRI) remains a challenge for today's algorithms due to\nlesion-caused disruption of the anatomy and lack of jointly labeled training\ndatasets, where both healthy tissues and lesions are labeled on the same\nimages. In this paper, we propose a method that is robust to lesion-caused\ndisruptions and can be trained from disparately labeled training sets, i.e.,\nwithout requiring jointly labeled samples, to automatically segment both. In\ncontrast to prior work, we decouple healthy tissue and lesion segmentation in\ntwo paths to leverage multi-sequence acquisitions and merge information with an\nattention mechanism. During inference, an image-specific adaptation reduces\nadverse influences of lesion regions on healthy tissue predictions. During\ntraining, the adaptation is taken into account through meta-learning and\nco-training is used to learn from disparately labeled training images. Our\nmodel shows an improved performance on several anatomical structures and\nlesions on a publicly available brain glioblastoma dataset compared to the\nstate-of-the-art segmentation methods.\n","authors":["Meva Himmetoglu","Ilja Ciernik","Ender Konukoglu"],"pdf_url":"https://arxiv.org/pdf/2503.18840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18836v1","updated":"2025-03-24T16:10:51Z","published":"2025-03-24T16:10:51Z","title":"Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated\n  MRI Reconstruction","summary":"  Magnetic resonance imaging (MRI) is a vital diagnostic tool, but its\ninherently long acquisition times reduce clinical efficiency and patient\ncomfort. Recent advancements in deep learning, particularly diffusion models,\nhave improved accelerated MRI reconstruction. However, existing diffusion\nmodels' training often relies on fully sampled data, models incur high\ncomputational costs, and often lack uncertainty estimation, limiting their\nclinical applicability. To overcome these challenges, we propose a novel\nframework, called Dual-domain Multi-path Self-supervised Diffusion Model\n(DMSM), that integrates a self-supervised dual-domain diffusion model training\nscheme, a lightweight hybrid attention network for the reconstruction diffusion\nmodel, and a multi-path inference strategy, to enhance reconstruction accuracy,\nefficiency, and explainability. Unlike traditional diffusion-based models, DMSM\neliminates the dependency on training from fully sampled data, making it more\npractical for real-world clinical settings. We evaluated DMSM on two human MRI\ndatasets, demonstrating that it achieves favorable performance over several\nsupervised and self-supervised baselines, particularly in preserving fine\nanatomical structures and suppressing artifacts under high acceleration\nfactors. Additionally, our model generates uncertainty maps that correlate\nreasonably well with reconstruction errors, offering valuable clinically\ninterpretable guidance and potentially enhancing diagnostic confidence.\n","authors":["Yuxuan Zhang","Jinkui Hao","Bo Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.18836v1.pdf","comment":"10 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.18830v1","updated":"2025-03-24T16:08:21Z","published":"2025-03-24T16:08:21Z","title":"DAGait: Generalized Skeleton-Guided Data Alignment for Gait Recognition","summary":"  Gait recognition is emerging as a promising and innovative area within the\nfield of computer vision, widely applied to remote person identification.\nAlthough existing gait recognition methods have achieved substantial success in\ncontrolled laboratory datasets, their performance often declines significantly\nwhen transitioning to wild datasets.We argue that the performance gap can be\nprimarily attributed to the spatio-temporal distribution inconsistencies\npresent in wild datasets, where subjects appear at varying angles, positions,\nand distances across the frames. To achieve accurate gait recognition in the\nwild, we propose a skeleton-guided silhouette alignment strategy, which uses\nprior knowledge of the skeletons to perform affine transformations on the\ncorresponding silhouettes.To the best of our knowledge, this is the first study\nto explore the impact of data alignment on gait recognition. We conducted\nextensive experiments across multiple datasets and network architectures, and\nthe results demonstrate the significant advantages of our proposed alignment\nstrategy.Specifically, on the challenging Gait3D dataset, our method achieved\nan average performance improvement of 7.9% across all evaluated networks.\nFurthermore, our method achieves substantial improvements on cross-domain\ndatasets, with accuracy improvements of up to 24.0%.\n","authors":["Zhengxian Wu","Chuanrui Zhang","Hangrui Xu","Peng Jiao","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2503.18830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16683v2","updated":"2025-03-24T16:08:09Z","published":"2024-11-25T18:59:57Z","title":"Generative Omnimatte: Learning to Decompose Video into Layers","summary":"  Given a video and a set of input object masks, an omnimatte method aims to\ndecompose the video into semantically meaningful layers containing individual\nobjects along with their associated effects, such as shadows and reflections.\nExisting omnimatte methods assume a static background or accurate pose and\ndepth estimation and produce poor decompositions when these assumptions are\nviolated. Furthermore, due to the lack of generative prior on natural videos,\nexisting methods cannot complete dynamic occluded regions. We present a novel\ngenerative layered video decomposition framework to address the omnimatte\nproblem. Our method does not assume a stationary scene or require camera pose\nor depth information and produces clean, complete layers, including convincing\ncompletions of occluded dynamic regions. Our core idea is to train a video\ndiffusion model to identify and remove scene effects caused by a specific\nobject. We show that this model can be finetuned from an existing video\ninpainting model with a small, carefully curated dataset, and demonstrate\nhigh-quality decompositions and editing results for a wide range of casually\ncaptured videos containing soft shadows, glossy reflections, splashing water,\nand more.\n","authors":["Yao-Chih Lee","Erika Lu","Sarah Rumbley","Michal Geyer","Jia-Bin Huang","Tali Dekel","Forrester Cole"],"pdf_url":"https://arxiv.org/pdf/2411.16683v2.pdf","comment":"CVPR 2025. Project page: https://gen-omnimatte.github.io/"},{"id":"http://arxiv.org/abs/2503.18817v1","updated":"2025-03-24T16:00:21Z","published":"2025-03-24T16:00:21Z","title":"Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal\n  Representations","summary":"  Prior research on out-of-distribution detection (OoDD) has primarily focused\non single-modality models. Recently, with the advent of large-scale pretrained\nvision-language models such as CLIP, OoDD methods utilizing such multi-modal\nrepresentations through zero-shot and prompt learning strategies have emerged.\nHowever, these methods typically involve either freezing the pretrained weights\nor only partially tuning them, which can be suboptimal for downstream datasets.\nIn this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve\nnotable OoDD performance. Despite some recent works demonstrating the impact of\nfine-tuning methods for OoDD, there remains significant potential for\nperformance improvement. We investigate the limitation of na\\\"ive fine-tuning\nmethods, examining why they fail to fully leverage the pretrained knowledge.\nOur empirical analysis suggests that this issue could stem from the modality\ngap within in-distribution (ID) embeddings. To address this, we propose a\ntraining objective that enhances cross-modal alignment by regularizing the\ndistances between image and text embeddings of ID data. This adjustment helps\nin better utilizing pretrained textual information by aligning similar\nsemantics from different modalities (i.e., text and image) more closely in the\nhyperspherical representation space. We theoretically demonstrate that the\nproposed regularization corresponds to the maximum likelihood estimation of an\nenergy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark\ndatasets, we show that our method, combined with post-hoc OoDD approaches\nleveraging pretrained knowledge (e.g., NegLabel), significantly outperforms\nexisting methods, achieving state-of-the-art OoDD performance and leading ID\naccuracy.\n","authors":["Jeonghyeon Kim","Sangheum Hwang"],"pdf_url":"https://arxiv.org/pdf/2503.18817v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18812v1","updated":"2025-03-24T15:53:54Z","published":"2025-03-24T15:53:54Z","title":"SKDU at De-Factify 4.0: Vision Transformer with Data Augmentation for\n  AI-Generated Image Detection","summary":"  The aim of this work is to explore the potential of pre-trained\nvision-language models, e.g. Vision Transformers (ViT), enhanced with advanced\ndata augmentation strategies for the detection of AI-generated images. Our\napproach leverages a fine-tuned ViT model trained on the Defactify-4.0 dataset,\nwhich includes images generated by state-of-the-art models such as Stable\nDiffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and\nMidJourney. We employ perturbation techniques like flipping, rotation, Gaussian\nnoise injection, and JPEG compression during training to improve model\nrobustness and generalisation. The experimental results demonstrate that our\nViT-based pipeline achieves state-of-the-art performance, significantly\noutperforming competing methods on both validation and test datasets.\n","authors":["Shrikant Malviya","Neelanjan Bhowmik","Stamos Katsigiannis"],"pdf_url":"https://arxiv.org/pdf/2503.18812v1.pdf","comment":"De-Factify 4.0 workshop at the 39th Annual AAAI Conference on\n  Artificial Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2503.18808v1","updated":"2025-03-24T15:50:19Z","published":"2025-03-24T15:50:19Z","title":"CRCL: Causal Representation Consistency Learning for Anomaly Detection\n  in Surveillance Videos","summary":"  Video Anomaly Detection (VAD) remains a fundamental yet formidable task in\nthe video understanding community, with promising applications in areas such as\ninformation forensics and public safety protection. Due to the rarity and\ndiversity of anomalies, existing methods only use easily collected regular\nevents to model the inherent normality of normal spatial-temporal patterns in\nan unsupervised manner. Previous studies have shown that existing unsupervised\nVAD models are incapable of label-independent data offsets (e.g., scene\nchanges) in real-world scenarios and may fail to respond to light anomalies due\nto the overgeneralization of deep neural networks. Inspired by causality\nlearning, we argue that there exist causal factors that can adequately\ngeneralize the prototypical patterns of regular events and present significant\ndeviations when anomalous instances occur. In this regard, we propose Causal\nRepresentation Consistency Learning (CRCL) to implicitly mine potential\nscene-robust causal variable in unsupervised video normality learning.\nSpecifically, building on the structural causal models, we propose\nscene-debiasing learning and causality-inspired normality learning to strip\naway entangled scene bias in deep representations and learn causal video\nnormality, respectively. Extensive experiments on benchmarks validate the\nsuperiority of our method over conventional deep representation learning.\nMoreover, ablation studies and extension validation show that the CRCL can cope\nwith label-independent biases in multi-scene settings and maintain stable\nperformance with only limited training data available.\n","authors":["Yang Liu","Hongjin Wang","Zepu Wang","Xiaoguang Zhu","Jing Liu","Peng Sun","Rui Tang","Jianwei Du","Victor C. M. Leung","Liang Song"],"pdf_url":"https://arxiv.org/pdf/2503.18808v1.pdf","comment":"Accepted for publication by IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2310.16102v2","updated":"2025-03-24T15:48:32Z","published":"2023-10-24T18:06:03Z","title":"Learned, uncertainty-driven adaptive acquisition for photon-efficient\n  scanning microscopy","summary":"  Scanning microscopy systems, such as confocal and multiphoton microscopy, are\npowerful imaging tools for probing deep into biological tissue. However,\nscanning systems have an inherent trade-off between acquisition time, field of\nview, phototoxicity, and image quality, often resulting in noisy measurements\nwhen fast, large field of view, and/or gentle imaging is needed. Deep learning\ncould be used to denoise noisy microscopy measurements, but these algorithms\ncan be prone to hallucination, which can be disastrous for medical and\nscientific applications. We propose a method to simultaneously denoise and\npredict pixel-wise uncertainty for scanning microscopy systems, improving\nalgorithm trustworthiness and providing statistical guarantees for deep\nlearning predictions. Furthermore, we propose to leverage this learned,\npixel-wise uncertainty to drive an adaptive acquisition technique that rescans\nonly the most uncertain regions of a sample, saving time and reducing the total\nlight dose to the sample. We demonstrate our method on experimental confocal\nand multiphoton microscopy systems, showing that our uncertainty maps can\npinpoint hallucinations in the deep learned predictions. Finally, with our\nadaptive acquisition technique, we demonstrate up to 16X reduction in\nacquisition time and total light dose while successfully recovering fine\nfeatures in the sample and reducing hallucinations. We are the first to\ndemonstrate distribution-free uncertainty quantification for a denoising task\nwith real experimental data and the first to propose adaptive acquisition based\non reconstruction uncertainty.\n","authors":["Cassandra Tong Ye","Jiashu Han","Kunzan Liu","Anastasios Angelopoulos","Linda Griffith","Kristina Monakhova","Sixian You"],"pdf_url":"https://arxiv.org/pdf/2310.16102v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18803v1","updated":"2025-03-24T15:48:07Z","published":"2025-03-24T15:48:07Z","title":"Change3D: Revisiting Change Detection and Captioning from A Video\n  Modeling Perspective","summary":"  In this paper, we present Change3D, a framework that reconceptualizes the\nchange detection and captioning tasks through video modeling. Recent methods\nhave achieved remarkable success by regarding each pair of bi-temporal images\nas separate frames. They employ a shared-weight image encoder to extract\nspatial features and then use a change extractor to capture differences between\nthe two images. However, image feature encoding, being a task-agnostic process,\ncannot attend to changed regions effectively. Furthermore, different change\nextractors designed for various change detection and captioning tasks make it\ndifficult to have a unified framework. To tackle these challenges, Change3D\nregards the bi-temporal images as comprising two frames akin to a tiny video.\nBy integrating learnable perception frames between the bi-temporal images, a\nvideo encoder enables the perception frames to interact with the images\ndirectly and perceive their differences. Therefore, we can get rid of the\nintricate change extractors, providing a unified framework for different change\ndetection and captioning tasks. We verify Change3D on multiple tasks,\nencompassing change detection (including binary change detection, semantic\nchange detection, and building damage assessment) and change captioning, across\neight standard benchmarks. Without bells and whistles, this simple yet\neffective framework can achieve superior performance with an ultra-light video\nmodel comprising only ~6%-13% of the parameters and ~8%-34% of the FLOPs\ncompared to state-of-the-art methods. We hope that Change3D could be an\nalternative to 2D-based models and facilitate future research.\n","authors":["Duowang Zhu","Xiaohu Huang","Haiyan Huang","Hao Zhou","Zhenfeng Shao"],"pdf_url":"https://arxiv.org/pdf/2503.18803v1.pdf","comment":"conference paper, accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.04832v4","updated":"2025-03-24T15:42:11Z","published":"2025-03-05T10:59:32Z","title":"Lightweight Embedded FPGA Deployment of Learned Image Compression with\n  Knowledge Distillation and Hybrid Quantization","summary":"  Learnable Image Compression (LIC) has shown the potential to outperform\nstandardized video codecs in RD efficiency, prompting the research for\nhardware-friendly implementations. Most existing LIC hardware implementations\nprioritize latency to RD-efficiency and through an extensive exploration of the\nhardware design space. We present a novel design paradigm where the burden of\ntuning the design for a specific hardware platform is shifted towards model\ndimensioning and without compromising on RD-efficiency. First, we design a\nframework for distilling a leaner student LIC model from a reference teacher:\nby tuning a single model hyperparameters, we can meet the constraints of\ndifferent hardware platforms without a complex hardware design exploration.\nSecond, we propose a hardware-friendly implementation of the Generalized\nDivisive Normalization - GDN activation that preserves RD efficiency even post\nparameter quantization. Third, we design a pipelined FPGA configuration which\ntakes full advantage of available FPGA resources by leveraging parallel\nprocessing and optimizing resource allocation. Our experiments with a state of\nthe art LIC model show that we outperform all existing FPGA implementations\nwhile performing very close to the original model.\n","authors":["Alaa Mazouz","Sumanta Chaudhuri","Marco Cagnanzzo","Mihai Mitrea","Enzo Tartaglione","Attilio Fiandrotti"],"pdf_url":"https://arxiv.org/pdf/2503.04832v4.pdf","comment":"Submitted to IEEE Transactions on Circuits and Systems for Video\n  Technology in March 2025"},{"id":"http://arxiv.org/abs/2503.18794v1","updated":"2025-03-24T15:40:17Z","published":"2025-03-24T15:40:17Z","title":"NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian\n  Splatting","summary":"  Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have noticeably\nadvanced photo-realistic novel view synthesis using images from densely spaced\ncamera viewpoints. However, these methods struggle in few-shot scenarios due to\nlimited supervision. In this paper, we present NexusGS, a 3DGS-based approach\nthat enhances novel view synthesis from sparse-view images by directly\nembedding depth information into point clouds, without relying on complex\nmanual regularizations. Exploiting the inherent epipolar geometry of 3DGS, our\nmethod introduces a novel point cloud densification strategy that initializes\n3DGS with a dense point cloud, reducing randomness in point placement while\npreventing over-smoothing and overfitting. Specifically, NexusGS comprises\nthree key steps: Epipolar Depth Nexus, Flow-Resilient Depth Blending, and\nFlow-Filtered Depth Pruning. These steps leverage optical flow and camera poses\nto compute accurate depth maps, while mitigating the inaccuracies often\nassociated with optical flow. By incorporating epipolar depth priors, NexusGS\nensures reliable dense point cloud coverage and supports stable 3DGS training\nunder sparse-view conditions. Experiments demonstrate that NexusGS\nsignificantly enhances depth accuracy and rendering quality, surpassing\nstate-of-the-art methods by a considerable margin. Furthermore, we validate the\nsuperiority of our generated point clouds by substantially boosting the\nperformance of competing methods. Project page:\nhttps://usmizuki.github.io/NexusGS/.\n","authors":["Yulong Zheng","Zicheng Jiang","Shengfeng He","Yandu Sun","Junyu Dong","Huaidong Zhang","Yong Du"],"pdf_url":"https://arxiv.org/pdf/2503.18794v1.pdf","comment":"This paper is accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18785v1","updated":"2025-03-24T15:32:49Z","published":"2025-03-24T15:32:49Z","title":"LGI-DETR: Local-Global Interaction for UAV Object Detection","summary":"  UAV has been widely used in various fields. However, most of the existing\nobject detectors used in drones are not end-to-end and require the design of\nvarious complex components and careful fine-tuning. Most of the existing\nend-to-end object detectors are designed for natural scenes. It is not ideal to\napply them directly to UAV images. In order to solve the above challenges, we\ndesign an local-global information interaction DETR for UAVs, namely LGI-DETR.\nCross-layer bidirectional low-level and high-level feature information\nenhancement, this fusion method is effective especially in the field of small\nobjection detection. At the initial stage of encoder, we propose a local\nspatial enhancement module (LSE), which enhances the low-level rich local\nspatial information into the high-level feature, and reduces the loss of local\ninformation in the transmission process of high-level information. At the final\nstage of the encoder, we propose a novel global information injection module\n(GII) designed to integrate rich high-level global semantic representations\nwith low-level feature maps. This hierarchical fusion mechanism effectively\naddresses the inherent limitations of local receptive fields by propagating\ncontextual information across the feature hierarchy. Experimental results on\ntwo challenging UAV image object detection benchmarks, VisDrone2019 and UAVDT,\nshow that our proposed model outperforms the SOTA model. Compared to the\nbaseline model, AP and AP50 improved by 1.9% and 2.4%, respectively.\n","authors":["Zifa Chen"],"pdf_url":"https://arxiv.org/pdf/2503.18785v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2405.17811v2","updated":"2025-03-24T15:32:40Z","published":"2024-05-28T04:13:21Z","title":"Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh","summary":"  Neural 3D representations such as Neural Radiance Fields (NeRF), excel at\nproducing photo-realistic rendering results but lack the flexibility for\nmanipulation and editing which is crucial for content creation. Previous works\nhave attempted to address this issue by deforming a NeRF in canonical space or\nmanipulating the radiance field based on an explicit mesh. However,\nmanipulating NeRF is not highly controllable and requires a long training and\ninference time. With the emergence of 3D Gaussian Splatting (3DGS), extremely\nhigh-fidelity novel view synthesis can be achieved using an explicit\npoint-based 3D representation with much faster training and rendering speed.\nHowever, there is still a lack of effective means to manipulate 3DGS freely\nwhile maintaining rendering quality. In this work, we aim to tackle the\nchallenge of achieving manipulable photo-realistic rendering. We propose to\nutilize a triangular mesh to manipulate 3DGS directly with self-adaptation.\nThis approach reduces the need to design various algorithms for different types\nof Gaussian manipulation. By utilizing a triangle shape-aware Gaussian binding\nand adapting method, we can achieve 3DGS manipulation and preserve\nhigh-fidelity rendering after manipulation. Our approach is capable of handling\nlarge deformations, local manipulations, and soft body simulations while\nkeeping high-quality rendering. Furthermore, we demonstrate that our method is\nalso effective with inaccurate meshes extracted from 3DGS. Experiments\nconducted demonstrate the effectiveness of our method and its superiority over\nbaseline approaches.\n","authors":["Xiangjun Gao","Xiaoyu Li","Yiyu Zhuang","Qi Zhang","Wenbo Hu","Chaopeng Zhang","Yao Yao","Ying Shan","Long Quan"],"pdf_url":"https://arxiv.org/pdf/2405.17811v2.pdf","comment":"CVPR 2025. Project page here: https://gaoxiangjun.github.io/mani_gs/"},{"id":"http://arxiv.org/abs/2503.18784v1","updated":"2025-03-24T15:32:33Z","published":"2025-03-24T15:32:33Z","title":"Leveraging Perturbation Robustness to Enhance Out-of-Distribution\n  Detection","summary":"  Out-of-distribution (OOD) detection is the task of identifying inputs that\ndeviate from the training data distribution. This capability is essential for\nsafely deploying deep computer vision models in open-world environments. In\nthis work, we propose a post-hoc method, Perturbation-Rectified OOD detection\n(PRO), based on the insight that prediction confidence for OOD inputs is more\nsusceptible to reduction under perturbation than in-distribution (IND) inputs.\nBased on the observation, we propose an adversarial score function that\nsearches for the local minimum scores near the original inputs by applying\ngradient descent. This procedure enhances the separability between IND and OOD\nsamples. Importantly, the approach improves OOD detection performance without\ncomplex modifications to the underlying model architectures. We conduct\nextensive experiments using the OpenOOD benchmark~\\cite{yang2022openood}. Our\napproach further pushes the limit of softmax-based OOD detection and is the\nleading post-hoc method for small-scale models. On a CIFAR-10 model with\nadversarial training, PRO effectively detects near-OOD inputs, achieving a\nreduction of more than 10\\% on FPR@95 compared to state-of-the-art methods.\n","authors":["Wenxi Chen","Raymond A. Yeh","Shaoshuai Mou","Yan Gu"],"pdf_url":"https://arxiv.org/pdf/2503.18784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18783v1","updated":"2025-03-24T15:32:06Z","published":"2025-03-24T15:32:06Z","title":"Frequency Dynamic Convolution for Dense Image Prediction","summary":"  While Dynamic Convolution (DY-Conv) has shown promising performance by\nenabling adaptive weight selection through multiple parallel weights combined\nwith an attention mechanism, the frequency response of these weights tends to\nexhibit high similarity, resulting in high parameter costs but limited\nadaptability. In this work, we introduce Frequency Dynamic Convolution\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\nparameter budget in the Fourier domain. FDConv divides this budget into\nfrequency-based groups with disjoint Fourier indices, enabling the construction\nof frequency-diverse weights without increasing the parameter cost. To further\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\nfilter at the spatial level, while FBM decomposes weights into distinct\nfrequency bands in the frequency domain and modulates them dynamically based on\nlocal content. Extensive experiments on object detection, segmentation, and\nclassification validate the effectiveness of FDConv. We demonstrate that when\napplied to ResNet-50, FDConv achieves superior performance with a modest\nincrease of +3.6M parameters, outperforming previous methods that require\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\nMoreover, FDConv seamlessly integrates into a variety of architectures,\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\nsolution for modern vision tasks. The code is made publicly available at\nhttps://github.com/Linwei-Chen/FDConv.\n","authors":["Linwei Chen","Lin Gu","Liang Li","Chenggang Yan","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2503.18783v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2502.07856v4","updated":"2025-03-24T15:18:25Z","published":"2025-02-11T14:57:33Z","title":"MaRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE\n  Solvers","summary":"  In applications of diffusion models, controllable generation is of practical\nsignificance, but is also challenging. Current methods for controllable\ngeneration primarily focus on modifying the score function of diffusion models,\nwhile Mean Reverting (MR) Diffusion directly modifies the structure of the\nstochastic differential equation (SDE), making the incorporation of image\nconditions simpler and more natural. However, current training-free fast\nsamplers are not directly applicable to MR Diffusion. And thus MR Diffusion\nrequires hundreds of NFEs (number of function evaluations) to obtain\nhigh-quality samples. In this paper, we propose a new algorithm named MaRS (MR\nSampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time\nSDE and the probability flow ordinary differential equation (PF-ODE) associated\nwith MR Diffusion, and derive semi-analytical solutions. The solutions consist\nof an analytical function and an integral parameterized by a neural network.\nBased on this solution, we can generate high-quality samples in fewer steps.\nOur approach does not require training and supports all mainstream\nparameterizations, including noise prediction, data prediction and velocity\nprediction. Extensive experiments demonstrate that MR Sampler maintains high\nsampling quality with a speedup of 10 to 20 times across ten different image\nrestoration tasks. Our algorithm accelerates the sampling procedure of MR\nDiffusion, making it more practical in controllable generation.\n","authors":["Ao Li","Wei Fang","Hongbo Zhao","Le Lu","Ge Yang","Minfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2502.07856v4.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2503.18767v1","updated":"2025-03-24T15:15:36Z","published":"2025-03-24T15:15:36Z","title":"Good Keypoints for the Two-View Geometry Estimation Problem","summary":"  Local features are essential to many modern downstream applications.\nTherefore, it is of interest to determine the properties of local features that\ncontribute to the downstream performance for a better design of feature\ndetectors and descriptors. In our work, we propose a new theoretical model for\nscoring feature points (keypoints) in the context of the two-view geometry\nestimation problem. The model determines two properties that a good keypoint\nfor solving the homography estimation problem should have: be repeatable and\nhave a small expected measurement error. This result provides key insights into\nwhy maximizing the number of correspondences doesn't always lead to better\nhomography estimation accuracy. We use the developed model to design a method\nthat detects keypoints that benefit the homography estimation introducing the\nBounded NeSS-ST (BoNeSS-ST) keypoint detector. The novelty of BoNeSS-ST comes\nfrom strong theoretical foundations, a more accurate keypoint scoring due to\nsubpixel refinement and a cost designed for superior robustness to low saliency\nkeypoints. As a result, BoNeSS-ST outperforms prior self-supervised local\nfeature detectors in both planar homography and epipolar geometry estimation\nproblems.\n","authors":["Konstantin Pakulev","Alexander Vakhitov","Gonzalo Ferrer"],"pdf_url":"https://arxiv.org/pdf/2503.18767v1.pdf","comment":"Camera-ready version of the CVPR 2025 paper"},{"id":"http://arxiv.org/abs/2411.14401v2","updated":"2025-03-24T15:08:16Z","published":"2024-11-21T18:30:11Z","title":"Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding","summary":"  Recent advancements in multimodal large language models (MLLMs) have opened\nnew avenues for video understanding. However, achieving high fidelity in\nzero-shot video tasks remains challenging. Traditional video processing methods\nrely heavily on fine-tuning to capture nuanced spatial-temporal details, which\nincurs significant data and computation costs. In contrast, training-free\napproaches, though efficient, often lack robustness in preserving context-rich\nfeatures across complex video content. To this end, we propose DYTO, a novel\ndynamic token merging framework for zero-shot video understanding that\nadaptively optimizes token efficiency while preserving crucial scene details.\nDYTO integrates a hierarchical frame selection and a bipartite token merging\nstrategy to dynamically cluster key frames and selectively compress token\nsequences, striking a balance between computational efficiency with semantic\nrichness. Extensive experiments across multiple benchmarks demonstrate the\neffectiveness of DYTO, achieving superior performance compared to both\nfine-tuned and training-free methods and setting a new state-of-the-art for\nzero-shot video understanding.\n","authors":["Yiming Zhang","Zhuokai Zhao","Zhaorun Chen","Zenghui Ding","Xianjun Yang","Yining Sun"],"pdf_url":"https://arxiv.org/pdf/2411.14401v2.pdf","comment":"Code is available at https://github.com/Jam1ezhang/DYTO"},{"id":"http://arxiv.org/abs/2411.19824v3","updated":"2025-03-24T15:07:22Z","published":"2024-11-29T16:34:46Z","title":"SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive\n  Tokens","summary":"  We propose a one-stage framework for real-time multi-person 3D human mesh\nestimation from a single RGB image. While current one-stage methods, which\nfollow a DETR-style pipeline, achieve state-of-the-art (SOTA) performance with\nhigh-resolution inputs, we observe that this particularly benefits the\nestimation of individuals in smaller scales of the image (e.g., those far from\nthe camera), but at the cost of significantly increased computation overhead.\nTo address this, we introduce scale-adaptive tokens that are dynamically\nadjusted based on the relative scale of each individual in the image within the\nDETR framework. Specifically, individuals in smaller scales are processed at\nhigher resolutions, larger ones at lower resolutions, and background regions\nare further distilled. These scale-adaptive tokens more efficiently encode the\nimage features, facilitating subsequent decoding to regress the human mesh,\nwhile allowing the model to allocate computational resources more effectively\nand focus on more challenging cases. Experiments show that our method preserves\nthe accuracy benefits of high-resolution processing while substantially\nreducing computational cost, achieving real-time inference with performance\ncomparable to SOTA methods.\n","authors":["Chi Su","Xiaoxuan Ma","Jiajun Su","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.19824v3.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2503.18755v1","updated":"2025-03-24T15:04:32Z","published":"2025-03-24T15:04:32Z","title":"EgoSurgery-HTS: A Dataset for Egocentric Hand-Tool Segmentation in Open\n  Surgery Videos","summary":"  Egocentric open-surgery videos capture rich, fine-grained details essential\nfor accurately modeling surgical procedures and human behavior in the operating\nroom. A detailed, pixel-level understanding of hands and surgical tools is\ncrucial for interpreting a surgeon's actions and intentions. We introduce\nEgoSurgery-HTS, a new dataset with pixel-wise annotations and a benchmark suite\nfor segmenting surgical tools, hands, and interacting tools in egocentric\nopen-surgery videos. Specifically, we provide a labeled dataset for (1) tool\ninstance segmentation of 14 distinct surgical tools, (2) hand instance\nsegmentation, and (3) hand-tool segmentation to label hands and the tools they\nmanipulate. Using EgoSurgery-HTS, we conduct extensive evaluations of\nstate-of-the-art segmentation methods and demonstrate significant improvements\nin the accuracy of hand and hand-tool segmentation in egocentric open-surgery\nvideos compared to existing datasets. The dataset will be released at\nhttps://github.com/Fujiry0/EgoSurgery.\n","authors":["Nathan Darjana","Ryo Fujii","Hideo Saito","Hiroki Kajita"],"pdf_url":"https://arxiv.org/pdf/2503.18755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18753v1","updated":"2025-03-24T15:01:50Z","published":"2025-03-24T15:01:50Z","title":"Self-Supervised Learning based on Transformed Image Reconstruction for\n  Equivariance-Coherent Feature Representation","summary":"  The equivariant behaviour of features is essential in many computer vision\ntasks, yet popular self-supervised learning (SSL) methods tend to constrain\nequivariance by design. We propose a self-supervised learning approach where\nthe system learns transformations independently by reconstructing images that\nhave undergone previously unseen transformations. Specifically, the model is\ntasked to reconstruct intermediate transformed images, e.g. translated or\nrotated images, without prior knowledge of these transformations. This\nauxiliary task encourages the model to develop equivariance-coherent features\nwithout relying on predefined transformation rules. To this end, we apply\ntransformations to the input image, generating an image pair, and then split\nthe extracted features into two sets per image. One set is used with a usual\nSSL loss encouraging invariance, the other with our loss based on the auxiliary\ntask to reconstruct the intermediate transformed images. Our loss and the SSL\nloss are linearly combined with weighted terms. Evaluating on synthetic tasks\nwith natural images, our proposed method strongly outperforms all competitors,\nregardless of whether they are designed to learn equivariance. Furthermore,\nwhen trained alongside augmentation-based methods as the invariance tasks, such\nas iBOT or DINOv2, we successfully learn a balanced combination of invariant\nand equivariant features. Our approach performs strong on a rich set of\nrealistic computer vision downstream tasks, almost always improving over all\nbaselines.\n","authors":["Qin Wang","Benjamin Bruns","Hanno Scharr","Kai Krajsek"],"pdf_url":"https://arxiv.org/pdf/2503.18753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18752v1","updated":"2025-03-24T15:01:00Z","published":"2025-03-24T15:01:00Z","title":"Robust Tube-based Control Strategy for Vision-guided Autonomous Vehicles","summary":"  A robust control strategy for autonomous vehicles can improve system\nstability, enhance riding comfort, and prevent driving accidents. This paper\npresents a novel interpolation tube-based constrained iterative linear\nquadratic regulator (itube-CILQR) algorithm for autonomous\ncomputer-vision-based vehicle lane-keeping. The goal of the algorithm is to\nenhance robustness during high-speed cornering on tight turns. The advantages\nof itube-CILQR over the standard tube-approach include reduced system\nconservatism and increased computational speed. Numerical and vision-based\nexperiments were conducted to examine the feasibility of the proposed\nalgorithm. The proposed itube-CILQR algorithm is better suited to vehicle\nlane-keeping than variational CILQR-based methods and model predictive control\n(MPC) approaches using a classical interior-point solver. Specifically, in\nevaluation experiments, itube-CILQR achieved an average runtime of 3.16 ms to\ngenerate a control signal to guide a self-driving vehicle; itube-MPC typically\nrequired a 4.67-times longer computation time to complete the same task.\nMoreover, the influence of conservatism on system behavior was investigated by\nexploring the interpolation variable trajectories derived from the proposed\nitube-CILQR algorithm during lane-keeping maneuvers.\n","authors":["Der-Hau Lee"],"pdf_url":"https://arxiv.org/pdf/2503.18752v1.pdf","comment":"13 pages, 14 figures"},{"id":"http://arxiv.org/abs/2503.18746v1","updated":"2025-03-24T14:53:35Z","published":"2025-03-24T14:53:35Z","title":"Linguistics-aware Masked Image Modeling for Self-supervised Scene Text\n  Recognition","summary":"  Text images are unique in their dual nature, encompassing both visual and\nlinguistic information. The visual component encompasses structural and\nappearance-based features, while the linguistic dimension incorporates\ncontextual and semantic elements. In scenarios with degraded visual quality,\nlinguistic patterns serve as crucial supplements for comprehension,\nhighlighting the necessity of integrating both aspects for robust scene text\nrecognition (STR). Contemporary STR approaches often use language models or\nsemantic reasoning modules to capture linguistic features, typically requiring\nlarge-scale annotated datasets. Self-supervised learning, which lacks\nannotations, presents challenges in disentangling linguistic features related\nto the global context. Typically, sequence contrastive learning emphasizes the\nalignment of local features, while masked image modeling (MIM) tends to exploit\nlocal structures to reconstruct visual patterns, resulting in limited\nlinguistic knowledge. In this paper, we propose a Linguistics-aware Masked\nImage Modeling (LMIM) approach, which channels the linguistic information into\nthe decoding process of MIM through a separate branch. Specifically, we design\na linguistics alignment module to extract vision-independent features as\nlinguistic guidance using inputs with different visual appearances. As features\nextend beyond mere visual structures, LMIM must consider the global context to\nachieve reconstruction. Extensive experiments on various benchmarks\nquantitatively demonstrate our state-of-the-art performance, and attention\nvisualizations qualitatively show the simultaneous capture of both visual and\nlinguistic information.\n","authors":["Yifei Zhang","Chang Liu","Jin Wei","Xiaomeng Yang","Yu Zhou","Can Ma","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2503.18746v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18742v1","updated":"2025-03-24T14:50:28Z","published":"2025-03-24T14:50:28Z","title":"SFDLA: Source-Free Document Layout Analysis","summary":"  Document Layout Analysis (DLA) is a fundamental task in document\nunderstanding. However, existing DLA and adaptation methods often require\naccess to large-scale source data and target labels. This requirements severely\nlimiting their real-world applicability, particularly in privacy-sensitive and\nresource-constrained domains, such as financial statements, medical records,\nand proprietary business documents. According to our observation, directly\ntransferring source-domain fine-tuned models on target domains often results in\na significant performance drop (Avg. -32.64%). In this work, we introduce\nSource-Free Document Layout Analysis (SFDLA), aiming for adapting a pre-trained\nsource DLA models to an unlabeled target domain, without access to any source\ndata. To address this challenge, we establish the first SFDLA benchmark,\ncovering three major DLA datasets for geometric- and content-aware adaptation.\nFurthermore, we propose Document Layout Analysis Adapter (DLAdapter), a novel\nframework that is designed to improve source-free adaptation across document\ndomains. Our method achieves a +4.21% improvement over the source-only baseline\nand a +2.26% gain over existing source-free methods from PubLayNet to\nDocLayNet. We believe this work will inspire the DLA community to further\ninvestigate source-free document understanding. To support future research of\nthe community, the benchmark, models, and code will be publicly available at\nhttps://github.com/s3setewe/sfdla-DLAdapter.\n","authors":["Sebastian Tewes","Yufan Chen","Omar Moured","Jiaming Zhang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2503.18742v1.pdf","comment":"The benchmark, models, and code will be publicly available at\n  https://github.com/s3setewe/sfdla-DLAdapter"},{"id":"http://arxiv.org/abs/2404.09387v3","updated":"2025-03-24T14:48:12Z","published":"2024-04-15T00:12:27Z","title":"RankCLIP: Ranking-Consistent Language-Image Pretraining","summary":"  Self-supervised contrastive learning models, such as CLIP, have set new\nbenchmarks for vision-language models in many downstream tasks. However, their\ndependency on rigid one-to-one mappings overlooks the complex and often\nmultifaceted relationships between and within texts and images. To this end, we\nintroduce RankCLIP, a novel pre-training method that extends beyond the rigid\none-to-one matching framework of CLIP and its variants. By extending the\ntraditional pair-wise loss to list-wise, and leveraging both in-modal and\ncross-modal ranking consistency, RankCLIP improves the alignment process,\nenabling it to capture the nuanced many-to-many relationships between and\nwithin each modality. Through comprehensive experiments, we demonstrate the\neffectiveness of RankCLIP in various downstream tasks, notably achieving\nsignificant gains in zero-shot classifications over state-of-the-art methods,\nunderscoring the importance of this enhanced learning process.\n","authors":["Yiming Zhang","Zhuokai Zhao","Zhaorun Chen","Zhili Feng","Zenghui Ding","Yining Sun"],"pdf_url":"https://arxiv.org/pdf/2404.09387v3.pdf","comment":"Code and model checkpoints are available at\n  https://github.com/Jam1ezhang/RankCLIP"},{"id":"http://arxiv.org/abs/2503.14405v2","updated":"2025-03-24T14:41:25Z","published":"2025-03-18T16:47:27Z","title":"DUNE: Distilling a Universal Encoder from Heterogeneous 2D and 3D\n  Teachers","summary":"  Recent multi-teacher distillation methods have unified the encoders of\nmultiple foundation models into a single encoder, achieving competitive\nperformance on core vision tasks like classification, segmentation, and depth\nestimation. This led us to ask: Could similar success be achieved when the pool\nof teachers also includes vision models specialized in diverse tasks across\nboth 2D and 3D perception? In this paper, we define and investigate the problem\nof heterogeneous teacher distillation, or co-distillation, a challenging\nmulti-teacher distillation scenario where teacher models vary significantly in\nboth (a) their design objectives and (b) the data they were trained on. We\nexplore data-sharing strategies and teacher-specific encoding, and introduce\nDUNE, a single encoder excelling in 2D vision, 3D understanding, and 3D human\nperception. Our model achieves performance comparable to that of its larger\nteachers, sometimes even outperforming them, on their respective tasks.\nNotably, DUNE surpasses MASt3R in Map-free Visual Relocalization with a much\nsmaller encoder.\n","authors":["Mert Bulent Sariyildiz","Philippe Weinzaepfel","Thomas Lucas","Pau de Jorge","Diane Larlus","Yannis Kalantidis"],"pdf_url":"https://arxiv.org/pdf/2503.14405v2.pdf","comment":"Accepted to CVPR-2025. Project page:\n  https://europe.naverlabs.com/dune"},{"id":"http://arxiv.org/abs/2501.19047v3","updated":"2025-03-24T14:38:27Z","published":"2025-01-31T11:18:45Z","title":"Understanding Model Calibration -- A gentle introduction and visual\n  exploration of calibration and the expected calibration error (ECE)","summary":"  To be considered reliable, a model must be calibrated so that its confidence\nin each decision closely reflects its true outcome. In this blogpost we'll take\na look at the most commonly used definition for calibration and then dive into\na frequently used evaluation measure for model calibration. We'll then cover\nsome of the drawbacks of this measure and how these surfaced the need for\nadditional notions of calibration, which require their own new evaluation\nmeasures. This post is not intended to be an in-depth dissection of all works\non calibration, nor does it focus on how to calibrate models. Instead, it is\nmeant to provide a gentle introduction to the different notions and their\nevaluation measures as well as to re-highlight some issues with a measure that\nis still widely used to evaluate calibration.\n","authors":["Maja Pavlovic"],"pdf_url":"https://arxiv.org/pdf/2501.19047v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18725v1","updated":"2025-03-24T14:34:20Z","published":"2025-03-24T14:34:20Z","title":"FG$^2$: Fine-Grained Cross-View Localization by Fine-Grained Feature\n  Matching","summary":"  We propose a novel fine-grained cross-view localization method that estimates\nthe 3 Degrees of Freedom pose of a ground-level image in an aerial image of the\nsurroundings by matching fine-grained features between the two images. The pose\nis estimated by aligning a point plane generated from the ground image with a\npoint plane sampled from the aerial image. To generate the ground points, we\nfirst map ground image features to a 3D point cloud. Our method then learns to\nselect features along the height dimension to pool the 3D points to a\nBird's-Eye-View (BEV) plane. This selection enables us to trace which feature\nin the ground image contributes to the BEV representation. Next, we sample a\nset of sparse matches from computed point correspondences between the two point\nplanes and compute their relative pose using Procrustes alignment. Compared to\nthe previous state-of-the-art, our method reduces the mean localization error\nby 28% on the VIGOR cross-area test set. Qualitative results show that our\nmethod learns semantically consistent matches across ground and aerial views\nthrough weakly supervised learning from the camera pose.\n","authors":["Zimin Xia","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2503.18725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18719v1","updated":"2025-03-24T14:30:38Z","published":"2025-03-24T14:30:38Z","title":"Boosting Resolution Generalization of Diffusion Transformers with\n  Randomized Positional Encodings","summary":"  Resolution generalization in image generation tasks enables the production of\nhigher-resolution images with lower training resolution overhead. However, a\nsignificant challenge in resolution generalization, particularly in the widely\nused Diffusion Transformers, lies in the mismatch between the positional\nencodings encountered during testing and those used during training. While\nexisting methods have employed techniques such as interpolation, extrapolation,\nor their combinations, none have fully resolved this issue. In this paper, we\npropose a novel two-dimensional randomized positional encodings (RPE-2D)\nframework that focuses on learning positional order of image patches instead of\nthe specific distances between them, enabling seamless high- and low-resolution\nimage generation without requiring high- and low-resolution image training.\nSpecifically, RPE-2D independently selects positions over a broader range along\nboth the horizontal and vertical axes, ensuring that all position encodings are\ntrained during the inference phase, thus improving resolution generalization.\nAdditionally, we propose a random data augmentation technique to enhance the\nmodeling of position order. To address the issue of image cropping caused by\nthe augmentation, we introduce corresponding micro-conditioning to enable the\nmodel to perceive the specific cropping patterns. On the ImageNet dataset, our\nproposed RPE-2D achieves state-of-the-art resolution generalization\nperformance, outperforming existing competitive methods when trained at a\nresolution of $256 \\times 256$ and inferred at $384 \\times 384$ and $512 \\times\n512$, as well as when scaling from $512 \\times 512$ to $768 \\times 768$ and\n$1024 \\times 1024$. And it also exhibits outstanding capabilities in\nlow-resolution image generation, multi-stage training acceleration and\nmulti-resolution inheritance.\n","authors":["Cong Liu","Liang Hou","Mingwu Zheng","Xin Tao","Pengfei Wan","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2503.18719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16423v2","updated":"2025-03-24T14:29:42Z","published":"2025-03-20T17:59:47Z","title":"GAEA: A Geolocation Aware Conversational Model","summary":"  Image geolocalization, in which, traditionally, an AI model predicts the\nprecise GPS coordinates of an image is a challenging task with many downstream\napplications. However, the user cannot utilize the model to further their\nknowledge other than the GPS coordinate; the model lacks an understanding of\nthe location and the conversational ability to communicate with the user. In\nrecent days, with tremendous progress of large multimodal models (LMMs) --\nproprietary and open-source -- researchers have attempted to geolocalize images\nvia LMMs. However, the issues remain unaddressed; beyond general tasks, for\nmore specialized downstream tasks, one of which is geolocalization, LMMs\nstruggle. In this work, we propose to solve this problem by introducing a\nconversational model GAEA that can provide information regarding the location\nof an image, as required by a user. No large-scale dataset enabling the\ntraining of such a model exists. Thus we propose GAEA-1.6M, a comprehensive\ndataset with 800K images and around 1.6M question-answer pairs constructed by\nleveraging OpenStreetMap (OSM) attributes and geographical context clues. For\nquantitative evaluation, we propose a diverse benchmark, GAEA-Bench, comprising\n4K image-text pairs to evaluate conversational capabilities equipped with\ndiverse question types. We consider 11 state-of-the-art open-source and\nproprietary LMMs and demonstrate that GAEA significantly outperforms the best\nopen-source model, LLaVA-OneVision by 25.69% and the best proprietary model,\nGPT-4o by 8.28%. Our dataset, model and codes are available.\n","authors":["Ron Campos","Ashmal Vayani","Parth Parag Kulkarni","Rohit Gupta","Aritra Dutta","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2503.16423v2.pdf","comment":"The dataset and code used in this submission is available at:\n  https://ucf-crcv.github.io/GAEA/"},{"id":"http://arxiv.org/abs/2503.18718v1","updated":"2025-03-24T14:29:14Z","published":"2025-03-24T14:29:14Z","title":"GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian\n  Splatting","summary":"  In the Generative AI era, safeguarding 3D models has become increasingly\nurgent. While invisible watermarking is well-established for 2D images with\nencoder-decoder frameworks, generalizable and robust solutions for 3D remain\nelusive. The main difficulty arises from the renderer between the 3D encoder\nand 2D decoder, which disrupts direct gradient flow and complicates training.\nExisting 3D methods typically rely on per-scene iterative optimization,\nresulting in time inefficiency and limited generalization. In this work, we\npropose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a\nwell-known yet underexplored representation for watermarking. We identify two\nmajor challenges: (1) ensuring effective training generalized across diverse 3D\nmodels, and (2) reliably extracting watermarks from free-view renderings, even\nunder distortions. Our framework, named GS-Marker, incorporates a 3D encoder to\nembed messages, distortion layers to enhance resilience against various\ndistortions, and a 2D decoder to extract watermarks from renderings. A key\ninnovation is the Adaptive Marker Control mechanism that adaptively perturbs\nthe initially optimized 3DGS, escaping local minima and improving both training\nstability and convergence. Extensive experiments show that GS-Marker\noutperforms per-scene training approaches in terms of decoding accuracy and\nmodel fidelity, while also significantly reducing computation time.\n","authors":["Lijiang Li","Jinglu Wang","Xiang Ming","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2503.18718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18712v1","updated":"2025-03-24T14:24:17Z","published":"2025-03-24T14:24:17Z","title":"LLaVAction: evaluating and training multi-modal large language models\n  for action recognition","summary":"  Understanding human behavior requires measuring behavioral actions. Due to\nits complexity, behavior is best mapped onto a rich, semantic structure such as\nlanguage. The recent development of multi-modal large language models (MLLMs)\nis a promising candidate for a wide range of action understanding tasks. In\nthis work, we focus on evaluating and then improving MLLMs to perform action\nrecognition. We reformulate EPIC-KITCHENS-100, one of the largest and most\nchallenging egocentric action datasets, to the form of video multiple question\nanswering (EPIC-KITCHENS-100-MQA). We show that when we sample difficult\nincorrect answers as distractors, leading MLLMs struggle to recognize the\ncorrect actions. We propose a series of methods that greatly improve the MLLMs'\nability to perform action recognition, achieving state-of-the-art on both the\nEPIC-KITCHENS-100 validation set, as well as outperforming GPT-4o by 21 points\nin accuracy on EPIC-KITCHENS-100-MQA. Lastly, we show improvements on other\naction-related video benchmarks such as EgoSchema, PerceptionTest,\nLongVideoBench, VideoMME and MVBench, suggesting that MLLMs are a promising\npath forward for complex action tasks. Code and models are available at:\nhttps://github.com/AdaptiveMotorControlLab/LLaVAction.\n","authors":["Shaokai Ye","Haozhe Qi","Alexander Mathis","Mackenzie W. Mathis"],"pdf_url":"https://arxiv.org/pdf/2503.18712v1.pdf","comment":"https://github.com/AdaptiveMotorControlLab/LLaVAction"},{"id":"http://arxiv.org/abs/2503.18711v1","updated":"2025-03-24T14:24:08Z","published":"2025-03-24T14:24:08Z","title":"Accenture-NVS1: A Novel View Synthesis Dataset","summary":"  This paper introduces ACC-NVS1, a specialized dataset designed for research\non Novel View Synthesis specifically for airborne and ground imagery. Data for\nACC-NVS1 was collected in Austin, TX and Pittsburgh, PA in 2023 and 2024. The\ncollection encompasses six diverse real-world scenes captured from both\nairborne and ground cameras, resulting in a total of 148,000 images. ACC-NVS1\naddresses challenges such as varying altitudes and transient objects. This\ndataset is intended to supplement existing datasets, providing additional\nresources for comprehensive research, rather than serving as a benchmark.\n","authors":["Thomas Sugg","Kyle O'Brien","Lekh Poudel","Alex Dumouchelle","Michelle Jou","Marc Bosch","Deva Ramanan","Srinivasa Narasimhan","Shubham Tulsiani"],"pdf_url":"https://arxiv.org/pdf/2503.18711v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2503.18709v1","updated":"2025-03-24T14:23:48Z","published":"2025-03-24T14:23:48Z","title":"Revisiting Automatic Data Curation for Vision Foundation Models in\n  Digital Pathology","summary":"  Vision foundation models (FMs) are accelerating the development of digital\npathology algorithms and transforming biomedical research. These models learn,\nin a self-supervised manner, to represent histological features in highly\nheterogeneous tiles extracted from whole-slide images (WSIs) of real-world\npatient samples. The performance of these FMs is significantly influenced by\nthe size, diversity, and balance of the pre-training data. However, data\nselection has been primarily guided by expert knowledge at the WSI level,\nfocusing on factors such as disease classification and tissue types, while\nlargely overlooking the granular details available at the tile level. In this\npaper, we investigate the potential of unsupervised automatic data curation at\nthe tile-level, taking into account 350 million tiles. Specifically, we apply\nhierarchical clustering trees to pre-extracted tile embeddings, allowing us to\nsample balanced datasets uniformly across the embedding space of the pretrained\nFM. We further identify these datasets are subject to a trade-off between size\nand balance, potentially compromising the quality of representations learned by\nFMs, and propose tailored batch sampling strategies to mitigate this effect. We\ndemonstrate the effectiveness of our method through improved performance on a\ndiverse range of clinically relevant downstream tasks.\n","authors":["Boqi Chen","Cédric Vincent-Cuaz","Lydia A. Schoenpflug","Manuel Madeira","Lisa Fournier","Vaishnavi Subramanian","Sonali Andani","Samuel Ruiperez-Campillo","Julia E. Vogt","Raphaëlle Luisier","Dorina Thanou","Viktor H. Koelzer","Pascal Frossard","Gabriele Campanella","Gunnar Rätsch"],"pdf_url":"https://arxiv.org/pdf/2503.18709v1.pdf","comment":"MICCAI 2025"},{"id":"http://arxiv.org/abs/2403.16848v2","updated":"2025-03-24T14:23:00Z","published":"2024-03-25T15:09:54Z","title":"Multiple Object Tracking as ID Prediction","summary":"  Multi-Object Tracking (MOT) has been a long-standing challenge in video\nunderstanding. A natural and intuitive approach is to split this task into two\nparts: object detection and association. Most mainstream methods employ\nmeticulously crafted heuristic techniques to maintain trajectory information\nand compute cost matrices for object matching. Although these methods can\nachieve notable tracking performance, they often require a series of elaborate\nhandcrafted modifications while facing complicated scenarios. We believe that\nmanually assumed priors limit the method's adaptability and flexibility in\nlearning optimal tracking capabilities from domain-specific data. Therefore, we\nintroduce a new perspective that treats Multiple Object Tracking as an\nin-context ID Prediction task, transforming the aforementioned object\nassociation into an end-to-end trainable task. Based on this, we propose a\nsimple yet effective method termed MOTIP. Given a set of trajectories carried\nwith ID information, MOTIP directly decodes the ID labels for current\ndetections to accomplish the association process. Without using tailored or\nsophisticated architectures, our method achieves state-of-the-art results\nacross multiple benchmarks by solely leveraging object-level features as\ntracking cues. The simplicity and impressive results of MOTIP leave substantial\nroom for future advancements, thereby making it a promising baseline for\nsubsequent research. Our code and checkpoints are released at\nhttps://github.com/MCG-NJU/MOTIP.\n","authors":["Ruopeng Gao","Ji Qi","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16848v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2408.10562v2","updated":"2025-03-24T14:22:53Z","published":"2024-08-20T06:03:40Z","title":"Kalib: Easy Hand-Eye Calibration with Reference Point Tracking","summary":"  Hand-eye calibration aims to estimate the transformation between a camera and\na robot. Traditional methods rely on fiducial markers, which require\nconsiderable manual effort and precise setup. Recent advances in deep learning\nhave introduced markerless techniques but come with more prerequisites, such as\nretraining networks for each robot, and accessing accurate mesh models for data\ngeneration. In this paper, we propose Kalib, an automatic and easy-to-setup\nhand-eye calibration method that leverages the generalizability of visual\nfoundation models to overcome these challenges. It features only two basic\nprerequisites, the robot's kinematic chain and a predefined reference point on\nthe robot. During calibration, the reference point is tracked in the camera\nspace. Its corresponding 3D coordinates in the robot coordinate can be inferred\nby forward kinematics. Then, a PnP solver directly estimates the transformation\nbetween the camera and the robot without training new networks or accessing\nmesh models. Evaluations in simulated and real-world benchmarks show that Kalib\nachieves good accuracy with a lower manual workload compared with recent\nbaseline methods. We also demonstrate its application in multiple real-world\nsettings with various robot arms and grippers. Kalib's user-friendly design and\nminimal setup requirements make it a possible solution for continuous operation\nin unstructured environments.\n","authors":["Tutian Tang","Minghao Liu","Wenqiang Xu","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2408.10562v2.pdf","comment":"The code, data, and supplementary materials are available at\n  https://sites.google.com/view/hand-eye-kalib"},{"id":"http://arxiv.org/abs/2412.01820v3","updated":"2025-03-24T14:22:47Z","published":"2024-12-02T18:58:04Z","title":"Towards Universal Soccer Video Understanding","summary":"  As a globally celebrated sport, soccer has attracted widespread interest from\nfans all over the world. This paper aims to develop a comprehensive multi-modal\nframework for soccer video understanding. Specifically, we make the following\ncontributions in this paper: (i) we introduce SoccerReplay-1988, the largest\nmulti-modal soccer dataset to date, featuring videos and detailed annotations\nfrom 1,988 complete matches, with an automated annotation pipeline; (ii) we\npresent an advanced soccer-specific visual encoder, MatchVision, which\nleverages spatiotemporal information across soccer videos and excels in various\ndownstream tasks; (iii) we conduct extensive experiments and ablation studies\non event classification, commentary generation, and multi-view foul\nrecognition. MatchVision demonstrates state-of-the-art performance on all of\nthem, substantially outperforming existing models, which highlights the\nsuperiority of our proposed data and model. We believe that this work will\noffer a standard paradigm for sports understanding research.\n","authors":["Jiayuan Rao","Haoning Wu","Hao Jiang","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2412.01820v3.pdf","comment":"CVPR 2025; Project Page: https://jyrao.github.io/UniSoccer/"},{"id":"http://arxiv.org/abs/2403.14368v2","updated":"2025-03-24T14:21:55Z","published":"2024-03-21T12:50:15Z","title":"CAGE: Unsupervised Visual Composition and Animation for Controllable\n  Video Generation","summary":"  The field of video generation has expanded significantly in recent years,\nwith controllable and compositional video generation garnering considerable\ninterest. Most methods rely on leveraging annotations such as text, objects'\nbounding boxes, and motion cues, which require substantial human effort and\nthus limit their scalability. In contrast, we address the challenge of\ncontrollable and compositional video generation without any annotations by\nintroducing a novel unsupervised approach. Our model is trained from scratch on\na dataset of unannotated videos. At inference time, it can compose plausible\nnovel scenes and animate objects by placing object parts at the desired\nlocations in space and time. The core innovation of our method lies in the\nunified control format and the training process, where video generation is\nconditioned on a randomly selected subset of pre-trained self-supervised local\nfeatures. This conditioning compels the model to learn how to inpaint the\nmissing information in the video both spatially and temporally, thereby\nlearning the inherent compositionality of a scene and the dynamics of moving\nobjects. The abstraction level and the imposed invariance of the conditioning\ninput to minor visual perturbations enable control over object motion by simply\nusing the same features at all the desired future locations. We call our model\nCAGE, which stands for visual Composition and Animation for video GEneration.\nWe conduct extensive experiments to validate the effectiveness of CAGE across\nvarious scenarios, demonstrating its capability to accurately follow the\ncontrol and to generate high-quality videos that exhibit coherent scene\ncomposition and realistic animation.\n","authors":["Aram Davtyan","Sepehr Sameni","Björn Ommer","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2403.14368v2.pdf","comment":"Published at AAAI2025; Project website:\n  https://araachie.github.io/cage"},{"id":"http://arxiv.org/abs/2503.16942v2","updated":"2025-03-24T14:18:59Z","published":"2025-03-21T08:40:35Z","title":"Re-HOLD: Video Hand Object Interaction Reenactment via adaptive\n  Layout-instructed Diffusion Model","summary":"  Current digital human studies focusing on lip-syncing and body movement are\nno longer sufficient to meet the growing industrial demand, while human video\ngeneration techniques that support interacting with real-world environments\n(e.g., objects) have not been well investigated. Despite human hand synthesis\nalready being an intricate problem, generating objects in contact with hands\nand their interactions presents an even more challenging task, especially when\nthe objects exhibit obvious variations in size and shape. To cope with these\nissues, we present a novel video Reenactment framework focusing on Human-Object\nInteraction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD).\nOur key insight is to employ specialized layout representation for hands and\nobjects, respectively. Such representations enable effective disentanglement of\nhand modeling and object adaptation to diverse motion sequences. To further\nimprove the generation quality of HOI, we have designed an interactive textural\nenhancement module for both hands and objects by introducing two independent\nmemory banks. We also propose a layout-adjusting strategy for the cross-object\nreenactment scenario to adaptively adjust unreasonable layouts caused by\ndiverse object sizes during inference. Comprehensive qualitative and\nquantitative evaluations demonstrate that our proposed framework significantly\noutperforms existing methods. Project page: https://fyycs.github.io/Re-HOLD.\n","authors":["Yingying Fan","Quanwei Yang","Kaisiyuan Wang","Hang Zhou","Yingying Li","Haocheng Feng","Errui Ding","Yu Wu","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16942v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18705v1","updated":"2025-03-24T14:17:18Z","published":"2025-03-24T14:17:18Z","title":"Benchmarking Burst Super-Resolution for Polarization Images: Noise\n  Dataset and Analysis","summary":"  Snapshot polarization imaging calculates polarization states from linearly\npolarized subimages. To achieve this, a polarization camera employs a double\nBayer-patterned sensor to capture both color and polarization. It demonstrates\nlow light efficiency and low spatial resolution, resulting in increased noise\nand compromised polarization measurements. Although burst super-resolution\neffectively reduces noise and enhances spatial resolution, applying it to\npolarization imaging poses challenges due to the lack of tailored datasets and\nreliable ground truth noise statistics. To address these issues, we introduce\nPolarNS and PolarBurstSR, two innovative datasets developed specifically for\npolarization imaging. PolarNS provides characterization of polarization noise\nstatistics, facilitating thorough analysis, while PolarBurstSR functions as a\nbenchmark for burst super-resolution in polarization images. These datasets,\ncollected under various real-world conditions, enable comprehensive evaluation.\nAdditionally, we present a model for analyzing polarization noise to quantify\nnoise propagation, tested on a large dataset captured in a darkroom\nenvironment. As part of our application, we compare the latest burst\nsuper-resolution models, highlighting the advantages of training tailored to\npolarization compared to RGB-based methods. This work establishes a benchmark\nfor polarization burst super-resolution and offers critical insights into noise\npropagation, thereby enhancing polarization image reconstruction.\n","authors":["Inseung Hwang","Kiseok Choi","Hyunho Ha","Min H. Kim"],"pdf_url":"https://arxiv.org/pdf/2503.18705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08107v3","updated":"2025-03-24T14:16:42Z","published":"2024-10-10T16:54:23Z","title":"IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera","summary":"  Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for\nnovel view synthesis have achieved remarkable progress with frame-based camera\n(e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel\ntype of bio-inspired visual sensor, i.e. event camera, has demonstrated\nadvantages in high temporal resolution, high dynamic range, low power\nconsumption and low latency. Due to its unique asynchronous and irregular data\ncapturing process, limited work has been proposed to apply neural\nrepresentation or 3D Gaussian splatting for an event camera. In this work, we\npresent IncEventGS, an incremental 3D Gaussian Splatting reconstruction\nalgorithm with a single event camera. To recover the 3D scene representation\nincrementally, we exploit the tracking and mapping paradigm of conventional\nSLAM pipelines for IncEventGS. Given the incoming event stream, the tracker\nfirstly estimates an initial camera motion based on prior reconstructed 3D-GS\nscene representation. The mapper then jointly refines both the 3D scene\nrepresentation and camera motion based on the previously estimated motion\ntrajectory from the tracker. The experimental results demonstrate that\nIncEventGS delivers superior performance compared to prior NeRF-based methods\nand other related baselines, even we do not have the ground-truth camera poses.\nFurthermore, our method can also deliver better performance compared to\nstate-of-the-art event visual odometry methods in terms of camera motion\nestimation. Code is publicly available at:\nhttps://github.com/wu-cvgl/IncEventGS.\n","authors":["Jian Huang","Chengrui Dong","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.08107v3.pdf","comment":"Code Page: https://github.com/wu-cvgl/IncEventGS"},{"id":"http://arxiv.org/abs/2503.18703v1","updated":"2025-03-24T14:15:48Z","published":"2025-03-24T14:15:48Z","title":"Channel Consistency Prior and Self-Reconstruction Strategy Based\n  Unsupervised Image Deraining","summary":"  Recently, deep image deraining models based on paired datasets have made a\nseries of remarkable progress. However, they cannot be well applied in\nreal-world applications due to the difficulty of obtaining real paired datasets\nand the poor generalization performance. In this paper, we propose a novel\nChannel Consistency Prior and Self-Reconstruction Strategy Based Unsupervised\nImage Deraining framework, CSUD, to tackle the aforementioned challenges.\nDuring training with unpaired data, CSUD is capable of generating high-quality\npseudo clean and rainy image pairs which are used to enhance the performance of\nderaining network. Specifically, to preserve more image background details\nwhile transferring rain streaks from rainy images to the unpaired clean images,\nwe propose a novel Channel Consistency Loss (CCLoss) by introducing the Channel\nConsistency Prior (CCP) of rain streaks into training process, thereby ensuring\nthat the generated pseudo rainy images closely resemble the real ones.\nFurthermore, we propose a novel Self-Reconstruction (SR) strategy to alleviate\nthe redundant information transfer problem of the generator, further improving\nthe deraining performance and the generalization capability of our method.\nExtensive experiments on multiple synthetic and real-world datasets demonstrate\nthat the deraining performance of CSUD surpasses other state-of-the-art\nunsupervised methods and CSUD exhibits superior generalization capability.\n","authors":["Guanglu Dong","Tianheng Zheng","Yuanzhouhan Cao","Linbo Qing","Chao Ren"],"pdf_url":"https://arxiv.org/pdf/2503.18703v1.pdf","comment":"Accepted to CVPR2025"},{"id":"http://arxiv.org/abs/2312.07352v2","updated":"2025-03-24T14:12:43Z","published":"2023-12-12T15:18:15Z","title":"CholecTrack20: A Multi-Perspective Tracking Dataset for Surgical Tools","summary":"  Tool tracking in surgical videos is essential for advancing computer-assisted\ninterventions, such as skill assessment, safety zone estimation, and\nhuman-machine collaboration. However, the lack of context-rich datasets limits\nAI applications in this field. Existing datasets rely on overly generic\ntracking formalizations that fail to capture surgical-specific dynamics, such\nas tools moving out of the camera's view or exiting the body. This results in\nless clinically relevant trajectories and a lack of flexibility for real-world\nsurgical applications. Methods trained on these datasets often struggle with\nvisual challenges such as smoke, reflection, and bleeding, further exposing the\nlimitations of current approaches. We introduce CholecTrack20, a specialized\ndataset for multi-class, multi-tool tracking in surgical procedures. It\nredefines tracking formalization with three perspectives: (i) intraoperative,\n(ii) intracorporeal, and (iii) visibility, enabling adaptable and clinically\nmeaningful tool trajectories. The dataset comprises 20 full-length surgical\nvideos, annotated at 1 fps, yielding over 35K frames and 65K labeled tool\ninstances. Annotations include spatial location, category, identity, operator,\nphase, and scene visual challenge. Benchmarking state-of-the-art methods on\nCholecTrack20 reveals significant performance gaps, with current approaches (<\n45\\% HOTA) failing to meet the accuracy required for clinical translation.\nThese findings motivate the need for advanced and intuitive tracking algorithms\nand establish CholecTrack20 as a foundation for developing robust AI-driven\nsurgical assistance systems.\n","authors":["Chinedu Innocent Nwoye","Kareem Elgohary","Anvita Srinivas","Fauzan Zaid","Joël L. Lavanchy","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2312.07352v2.pdf","comment":"Surgical tool tracking dataset paper, 11 pages, 10 figures, 3 tables,\n  CVPR 2025"},{"id":"http://arxiv.org/abs/2412.00133v2","updated":"2025-03-24T14:08:39Z","published":"2024-11-28T15:13:24Z","title":"ETAP: Event-based Tracking of Any Point","summary":"  Tracking any point (TAP) recently shifted the motion estimation paradigm from\nfocusing on individual salient points with local templates to tracking\narbitrary points with global image contexts. However, while research has mostly\nfocused on driving the accuracy of models in nominal settings, addressing\nscenarios with difficult lighting conditions and high-speed motions remains out\nof reach due to the limitations of the sensor. This work addresses this\nchallenge with the first event camera-based TAP method. It leverages the high\ntemporal resolution and high dynamic range of event cameras for robust\nhigh-speed tracking, and the global contexts in TAP methods to handle\nasynchronous and sparse event measurements. We further extend the TAP framework\nto handle event feature variations induced by motion -- thereby addressing an\nopen challenge in purely event-based tracking -- with a novel feature-alignment\nloss which ensures the learning of motion-robust features. Our method is\ntrained with data from a new data generation pipeline and systematically\nablated across all design decisions. Our method shows strong cross-dataset\ngeneralization and performs 136% better on the average Jaccard metric than the\nbaselines. Moreover, on an established feature tracking benchmark, it achieves\na 20% improvement over the previous best event-only method and even surpasses\nthe previous best events-and-frames method by 4.1%. Our code is available at\nhttps://github.com/tub-rip/ETAP\n","authors":["Friedhelm Hamann","Daniel Gehrig","Filbert Febryanto","Kostas Daniilidis","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2412.00133v2.pdf","comment":"17 pages, 15 figures, 8 tables. Project page:\n  https://github.com/tub-rip/ETAP"},{"id":"http://arxiv.org/abs/2307.01069v2","updated":"2025-03-24T14:04:21Z","published":"2023-07-03T14:50:14Z","title":"NeSS-ST: Detecting Good and Stable Keypoints with a Neural Stability\n  Score and the Shi-Tomasi Detector","summary":"  Learning a feature point detector presents a challenge both due to the\nambiguity of the definition of a keypoint and, correspondingly, the need for\nspecially prepared ground truth labels for such points. In our work, we address\nboth of these issues by utilizing a combination of a hand-crafted Shi-Tomasi\ndetector, a specially designed metric that assesses the quality of keypoints,\nthe stability score (SS), and a neural network. We build on the principled and\nlocalized keypoints provided by the Shi-Tomasi detector and learn the neural\nnetwork to select good feature points via the stability score. The neural\nnetwork incorporates the knowledge from the training targets in the form of the\nneural stability score (NeSS). Therefore, our method is named NeSS-ST since it\ncombines the Shi-Tomasi detector and the properties of the neural stability\nscore. It only requires sets of images for training without dataset\npre-labeling or the need for reconstructed correspondence labels. We evaluate\nNeSS-ST on HPatches, ScanNet, MegaDepth and IMC-PT demonstrating\nstate-of-the-art performance and good generalization on downstream tasks.\n","authors":["Konstantin Pakulev","Alexander Vakhitov","Gonzalo Ferrer"],"pdf_url":"https://arxiv.org/pdf/2307.01069v2.pdf","comment":"Camera-ready version of ICCV 2023 paper"},{"id":"http://arxiv.org/abs/2503.18695v1","updated":"2025-03-24T14:04:17Z","published":"2025-03-24T14:04:17Z","title":"OCRT: Boosting Foundation Models in the Open World with\n  Object-Concept-Relation Triad","summary":"  Although foundation models (FMs) claim to be powerful, their generalization\nability significantly decreases when faced with distribution shifts, weak\nsupervision, or malicious attacks in the open world. On the other hand, most\ndomain generalization or adversarial fine-tuning methods are task-related or\nmodel-specific, ignoring the universality in practical applications and the\ntransferability between FMs. This paper delves into the problem of generalizing\nFMs to the out-of-domain data. We propose a novel framework, the\nObject-Concept-Relation Triad (OCRT), that enables FMs to extract sparse,\nhigh-level concepts and intricate relational structures from raw visual inputs.\nThe key idea is to bind objects in visual scenes and a set of object-centric\nrepresentations through unsupervised decoupling and iterative refinement. To be\nspecific, we project the object-centric representations onto a semantic concept\nspace that the model can readily interpret and estimate their importance to\nfilter out irrelevant elements. Then, a concept-based graph, which has a\nflexible degree, is constructed to incorporate the set of concepts and their\ncorresponding importance, enabling the extraction of high-order factors from\ninformative concepts and facilitating relational reasoning among these\nconcepts. Extensive experiments demonstrate that OCRT can substantially boost\nthe generalizability and robustness of SAM and CLIP across multiple downstream\ntasks.\n","authors":["Luyao Tang","Yuxuan Yuan","Chaoqi Chen","Zeyu Zhang","Yue Huang","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.18695v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2502.19908v3","updated":"2025-03-24T14:03:59Z","published":"2025-02-27T09:26:22Z","title":"CarPlanner: Consistent Auto-regressive Trajectory Planning for\n  Large-scale Reinforcement Learning in Autonomous Driving","summary":"  Trajectory planning is vital for autonomous driving, ensuring safe and\nefficient navigation in complex environments. While recent learning-based\nmethods, particularly reinforcement learning (RL), have shown promise in\nspecific scenarios, RL planners struggle with training inefficiencies and\nmanaging large-scale, real-world driving scenarios. In this paper, we introduce\n\\textbf{CarPlanner}, a \\textbf{C}onsistent \\textbf{a}uto-\\textbf{r}egressive\n\\textbf{Planner} that uses RL to generate multi-modal trajectories. The\nauto-regressive structure enables efficient large-scale RL training, while the\nincorporation of consistency ensures stable policy learning by maintaining\ncoherent temporal consistency across time steps. Moreover, CarPlanner employs a\ngeneration-selection framework with an expert-guided reward function and an\ninvariant-view module, simplifying RL training and enhancing policy\nperformance. Extensive analysis demonstrates that our proposed RL framework\neffectively addresses the challenges of training efficiency and performance\nenhancement, positioning CarPlanner as a promising solution for trajectory\nplanning in autonomous driving. To the best of our knowledge, we are the first\nto demonstrate that the RL-based planner can surpass both IL- and rule-based\nstate-of-the-arts (SOTAs) on the challenging large-scale real-world dataset\nnuPlan. Our proposed CarPlanner surpasses RL-, IL-, and rule-based SOTA\napproaches within this demanding dataset.\n","authors":["Dongkun Zhang","Jiaming Liang","Ke Guo","Sha Lu","Qi Wang","Rong Xiong","Zhenwei Miao","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2502.19908v3.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2412.19165v2","updated":"2025-03-24T14:01:28Z","published":"2024-12-26T10:51:50Z","title":"Revisiting Monocular 3D Object Detection with Depth Thickness Field","summary":"  Monocular 3D object detection is challenging due to the lack of accurate\ndepth. However, existing depth-assisted solutions still exhibit inferior\nperformance, whose reason is universally acknowledged as the unsatisfactory\naccuracy of monocular depth estimation models. In this paper, we revisit\nmonocular 3D object detection from the depth perspective and formulate an\nadditional issue as the limited 3D structure-aware capability of existing depth\nrepresentations (e.g., depth one-hot encoding or depth distribution). To\naddress this issue, we introduce a novel Depth Thickness Field approach to\nembed clear 3D structures of the scenes. Specifically, we present MonoDTF, a\nscene-to-instance depth-adapted network for monocular 3D object detection. The\nframework mainly comprises a Scene-Level Depth Retargeting (SDR) module and an\nInstance-Level Spatial Refinement (ISR) module. The former retargets\ntraditional depth representations to the proposed depth thickness field,\nincorporating the scene-level perception of 3D structures. The latter refines\nthe voxel space with the guidance of instances, enhancing the 3D instance-aware\ncapability of the depth thickness field and thus improving detection accuracy.\nExtensive experiments on the KITTI and Waymo datasets demonstrate our\nsuperiority to existing state-of-the-art (SoTA) methods and the universality\nwhen equipped with different depth estimation models. The code will be\navailable.\n","authors":["Qiude Zhang","Chunyu Lin","Zhijie Shen","Nie Lang","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.19165v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18682v1","updated":"2025-03-24T13:53:30Z","published":"2025-03-24T13:53:30Z","title":"Hardware-Rasterized Ray-Based Gaussian Splatting","summary":"  We present a novel, hardware rasterized rendering approach for ray-based 3D\nGaussian Splatting (RayGS), obtaining both fast and high-quality results for\nnovel view synthesis. Our work contains a mathematically rigorous and\ngeometrically intuitive derivation about how to efficiently estimate all\nrelevant quantities for rendering RayGS models, structured with respect to\nstandard hardware rasterization shaders. Our solution is the first enabling\nrendering RayGS models at sufficiently high frame rates to support\nquality-sensitive applications like Virtual and Mixed Reality. Our second\ncontribution enables alias-free rendering for RayGS, by addressing MIP-related\nissues arising when rendering diverging scales during training and testing. We\ndemonstrate significant performance gains, across different benchmark scenes,\nwhile retaining state-of-the-art appearance quality of RayGS.\n","authors":["Samuel Rota Bulò","Nemanja Bartolovic","Lorenzo Porzi","Peter Kontschieder"],"pdf_url":"https://arxiv.org/pdf/2503.18682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01309v2","updated":"2025-03-24T13:50:32Z","published":"2025-03-03T08:48:06Z","title":"OnlineAnySeg: Online Zero-Shot 3D Segmentation by Visual Foundation\n  Model Guided 2D Mask Merging","summary":"  Online 3D open-vocabulary segmentation of a progressively reconstructed scene\nis both a critical and challenging task for embodied applications. With the\nsuccess of visual foundation models (VFMs) in the image domain, leveraging 2D\npriors to address 3D online segmentation has become a prominent research focus.\nSince segmentation results provided by 2D priors often require spatial\nconsistency to be lifted into final 3D segmentation, an efficient method for\nidentifying spatial overlap among 2D masks is essential - yet existing methods\nrarely achieve this in real time, mainly limiting its use to offline\napproaches. To address this, we propose an efficient method that lifts 2D masks\ngenerated by VFMs into a unified 3D instance using a hashing technique. By\nemploying voxel hashing for efficient 3D scene querying, our approach reduces\nthe time complexity of costly spatial overlap queries from $O(n^2)$ to $O(n)$.\nAccurate spatial associations further enable 3D merging of 2D masks through\nsimple similarity-based filtering in a zero-shot manner, making our approach\nmore robust to incomplete and noisy data. Evaluated on the ScanNet and SceneNN\nbenchmarks, our approach achieves state-of-the-art performance in online,\nopen-vocabulary 3D instance segmentation with leading efficiency.\n","authors":["Yijie Tang","Jiazhao Zhang","Yuqing Lan","Yulan Guo","Dezun Dong","Chenyang Zhu","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2503.01309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18678v1","updated":"2025-03-24T13:49:39Z","published":"2025-03-24T13:49:39Z","title":"NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping","summary":"  Suffering from performance bottlenecks in passively detecting high-quality\nDeepfake images due to the advancement of generative models, proactive\nperturbations offer a promising approach to disabling Deepfake manipulations by\ninserting signals into benign images. However, existing proactive perturbation\napproaches remain unsatisfactory in several aspects: 1) visual degradation due\nto direct element-wise addition; 2) limited effectiveness against face swapping\nmanipulation; 3) unavoidable reliance on white- and grey-box settings to\ninvolve generative models during training. In this study, we analyze the\nessence of Deepfake face swapping and argue the necessity of protecting source\nidentities rather than target images, and we propose NullSwap, a novel\nproactive defense approach that cloaks source image identities and nullifies\nface swapping under a pure black-box scenario. We design an Identity Extraction\nmodule to obtain facial identity features from the source image, while a\nPerturbation Block is then devised to generate identity-guided perturbations\naccordingly. Meanwhile, a Feature Block extracts shallow-level image features,\nwhich are then fused with the perturbation in the Cloaking Block for image\nreconstruction. Furthermore, to ensure adaptability across different identity\nextractors in face swapping algorithms, we propose Dynamic Loss Weighting to\nadaptively balance identity losses. Experiments demonstrate the outstanding\nability of our approach to fool various identity recognition models,\noutperforming state-of-the-art proactive perturbations in preventing face\nswapping models from generating images with correct source identities.\n","authors":["Tianyi Wang","Harry Cheng","Xiao Zhang","Yinglong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.18678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06842v5","updated":"2025-03-24T13:49:19Z","published":"2024-04-10T09:14:28Z","title":"MoCha-Stereo: Motif Channel Attention Network for Stereo Matching","summary":"  Learning-based stereo matching techniques have made significant progress.\nHowever, existing methods inevitably lose geometrical structure information\nduring the feature channel generation process, resulting in edge detail\nmismatches. In this paper, the Motif Cha}nnel Attention Stereo Matching Network\n(MoCha-Stereo) is designed to address this problem. We provide the Motif\nChannel Correlation Volume (MCCV) to determine more accurate edge matching\ncosts. MCCV is achieved by projecting motif channels, which capture common\ngeometric structures in feature channels, onto feature maps and cost volumes.\nIn addition, edge variations in %potential feature channels of the\nreconstruction error map also affect details matching, we propose the\nReconstruction Error Motif Penalty (REMP) module to further refine the\nfull-resolution disparity estimation. REMP integrates the frequency information\nof typical channel features from the reconstruction error. MoCha-Stereo ranks\n1st on the KITTI-2015 and KITTI-2012 Reflective leaderboards. Our structure\nalso shows excellent performance in Multi-View Stereo. Code is avaliable at\nhttps://github.com/ZYangChen/MoCha-Stereo.\n","authors":["Ziyang Chen","Wei Long","He Yao","Yongjun Zhang","Bingshu Wang","Yongbin Qin","Jia Wu"],"pdf_url":"https://arxiv.org/pdf/2404.06842v5.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2503.18674v1","updated":"2025-03-24T13:46:27Z","published":"2025-03-24T13:46:27Z","title":"Human Motion Unlearning","summary":"  We introduce the task of human motion unlearning to prevent the synthesis of\ntoxic animations while preserving the general text-to-motion generative\nperformance. Unlearning toxic motions is challenging as those can be generated\nfrom explicit text prompts and from implicit toxic combinations of safe motions\n(e.g., ``kicking\" is ``loading and swinging a leg\"). We propose the first\nmotion unlearning benchmark by filtering toxic motions from the large and\nrecent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines,\nby adapting state-of-the-art image unlearning techniques to process\nspatio-temporal signals. Finally, we propose a novel motion unlearning model\nbased on Latent Code Replacement, which we dub LCR. LCR is training-free and\nsuitable to the discrete latent spaces of state-of-the-art text-to-motion\ndiffusion models. LCR is simple and consistently outperforms baselines\nqualitatively and quantitatively. Project page:\n\\href{https://www.pinlab.org/hmu}{https://www.pinlab.org/hmu}.\n","authors":["Edoardo De Matteis","Matteo Migliarini","Alessio Sampieri","Indro Spinelli","Fabio Galasso"],"pdf_url":"https://arxiv.org/pdf/2503.18674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18673v1","updated":"2025-03-24T13:46:21Z","published":"2025-03-24T13:46:21Z","title":"Any6D: Model-free 6D Pose Estimation of Novel Objects","summary":"  We introduce Any6D, a model-free framework for 6D object pose estimation that\nrequires only a single RGB-D anchor image to estimate both the 6D pose and size\nof unknown objects in novel scenes. Unlike existing methods that rely on\ntextured 3D models or multiple viewpoints, Any6D leverages a joint object\nalignment process to enhance 2D-3D alignment and metric scale estimation for\nimproved pose accuracy. Our approach integrates a render-and-compare strategy\nto generate and refine pose hypotheses, enabling robust performance in\nscenarios with occlusions, non-overlapping views, diverse lighting conditions,\nand large cross-environment variations. We evaluate our method on five\nchallenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,\ndemonstrating its effectiveness in significantly outperforming state-of-the-art\nmethods for novel object pose estimation. Project page:\nhttps://taeyeop.com/any6d\n","authors":["Taeyeop Lee","Bowen Wen","Minjun Kang","Gyuree Kang","In So Kweon","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2503.18673v1.pdf","comment":"CVPR 2025, Project Page: https://taeyeop.com/any6d"},{"id":"http://arxiv.org/abs/2503.18672v1","updated":"2025-03-24T13:44:12Z","published":"2025-03-24T13:44:12Z","title":"Feature Calibration enhanced Parameter Synthesis for CLIP-based\n  Class-incremental Learning","summary":"  Class-incremental Learning (CIL) enables models to continuously learn new\nclass knowledge while memorizing previous classes, facilitating their\nadaptation and evolution in dynamic environments. Traditional CIL methods are\nmainly based on visual features, which limits their ability to handle complex\nscenarios. In contrast, Vision-Language Models (VLMs) show promising potential\nto promote CIL by integrating pretrained knowledge with textual features.\nHowever, previous methods make it difficult to overcome catastrophic forgetting\nwhile preserving the generalization capabilities of VLMs. To tackle these\nchallenges, we propose Feature Calibration enhanced Parameter Synthesis (FCPS)\nin this paper. Specifically, our FCPS employs a specific parameter adjustment\nmechanism to iteratively refine the proportion of original visual features\nparticipating in the final class determination, ensuring the model's\nfoundational generalization capabilities. Meanwhile, parameter integration\nacross different tasks achieves a balance between learning new class knowledge\nand retaining old knowledge. Experimental results on popular benchmarks (e.g.,\nCIFAR100 and ImageNet100) validate the superiority of the proposed method.\n","authors":["Juncen Guo","Xiaoguang Zhu","Lianlong Sun","Liangyu Teng","Di Li","Yang Liu","Liang Song"],"pdf_url":"https://arxiv.org/pdf/2503.18672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18671v1","updated":"2025-03-24T13:43:44Z","published":"2025-03-24T13:43:44Z","title":"Structure-Aware Correspondence Learning for Relative Pose Estimation","summary":"  Relative pose estimation provides a promising way for achieving\nobject-agnostic pose estimation. Despite the success of existing 3D\ncorrespondence-based methods, the reliance on explicit feature matching suffers\nfrom small overlaps in visible regions and unreliable feature estimation for\ninvisible regions. Inspired by humans' ability to assemble two object parts\nthat have small or no overlapping regions by considering object structure, we\npropose a novel Structure-Aware Correspondence Learning method for Relative\nPose Estimation, which consists of two key modules. First, a structure-aware\nkeypoint extraction module is designed to locate a set of kepoints that can\nrepresent the structure of objects with different shapes and appearance, under\nthe guidance of a keypoint based image reconstruction loss. Second, a\nstructure-aware correspondence estimation module is designed to model the\nintra-image and inter-image relationships between keypoints to extract\nstructure-aware features for correspondence estimation. By jointly leveraging\nthese two modules, the proposed method can naturally estimate 3D-3D\ncorrespondences for unseen objects without explicit feature matching for\nprecise relative pose estimation. Experimental results on the CO3D, Objaverse\nand LineMOD datasets demonstrate that the proposed method significantly\noutperforms prior methods, i.e., with 5.7{\\deg}reduction in mean angular error\non the CO3D dataset.\n","authors":["Yihan Chen","Wenfei Yang","Huan Ren","Shifeng Zhang","Tianzhu Zhang","Feng Wu"],"pdf_url":"https://arxiv.org/pdf/2503.18671v1.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2410.05869v4","updated":"2025-03-24T13:41:43Z","published":"2024-10-08T09:57:14Z","title":"Believing is Seeing: Unobserved Object Detection using Generative Models","summary":"  Can objects that are not visible in an image -- but are in the vicinity of\nthe camera -- be detected? This study introduces the novel tasks of 2D, 2.5D\nand 3D unobserved object detection for predicting the location of nearby\nobjects that are occluded or lie outside the image frame. We adapt several\nstate-of-the-art pre-trained generative models to address this task, including\n2D and 3D diffusion models and vision-language models, and show that they can\nbe used to infer the presence of objects that are not directly observed. To\nbenchmark this task, we propose a suite of metrics that capture different\naspects of performance. Our empirical evaluation on indoor scenes from the\nRealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the\nuse of generative models for the unobserved object detection task.\n","authors":["Subhransu S. Bhattacharjee","Dylan Campbell","Rahul Shome"],"pdf_url":"https://arxiv.org/pdf/2410.05869v4.pdf","comment":"IEEE/CVF Computer Vision and Pattern Recognition 2025; 22 pages"},{"id":"http://arxiv.org/abs/2409.03643v2","updated":"2025-03-24T13:31:05Z","published":"2024-09-05T16:01:21Z","title":"Image Over Text: Transforming Formula Recognition Evaluation with\n  Character Detection Matching","summary":"  Formula recognition presents significant challenges due to the complicated\nstructure and varied notation of mathematical expressions. Despite continuous\nadvancements in formula recognition models, the evaluation metrics employed by\nthese models, such as BLEU and Edit Distance, still exhibit notable\nlimitations. They overlook the fact that the same formula has diverse\nrepresentations and is highly sensitive to the distribution of training data,\nthereby causing unfairness in formula recognition evaluation. To this end, we\npropose a Character Detection Matching (CDM) metric, ensuring the evaluation\nobjectivity by designing an image-level rather than a LaTeX-level metric score.\nSpecifically, CDM renders both the model-predicted LaTeX and the ground-truth\nLaTeX formulas into image-formatted formulas, then employs visual feature\nextraction and localization techniques for precise character-level matching,\nincorporating spatial position information. Such a spatially-aware and\ncharacter-matching method offers a more accurate and equitable evaluation\ncompared with previous BLEU and Edit Distance metrics that rely solely on\ntext-based character matching. Experimentally, we evaluated various formula\nrecognition models using CDM, BLEU, and ExpRate metrics. Their results\ndemonstrate that the CDM aligns more closely with human evaluation standards\nand provides a fairer comparison across different models by eliminating\ndiscrepancies caused by diverse formula representations. Code is available at\nhttps://github.com/opendatalab/UniMERNet/tree/main/cdm.\n","authors":["Bin Wang","Fan Wu","Linke Ouyang","Zhuangcheng Gu","Rui Zhang","Renqiu Xia","Bo Zhang","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2409.03643v2.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.18665v1","updated":"2025-03-24T13:30:47Z","published":"2025-03-24T13:30:47Z","title":"Boosting Virtual Agent Learning and Reasoning: A Step-wise,\n  Multi-dimensional, and Generalist Reward Model with Benchmark","summary":"  The development of Generalist Virtual Agents (GVAs) powered by Multimodal\nLarge Language Models (MLLMs) has shown significant promise in autonomous task\nexecution. However, current training paradigms face critical limitations,\nincluding reliance on outcome supervision and labor-intensive human\nannotations. To address these challenges, we propose Similar, a Step-wise\nMulti-dimensional Generalist Reward Model, which offers fine-grained signals\nfor agent training and can choose better action for inference-time scaling.\nSpecifically, we begin by systematically defining five dimensions for\nevaluating agent actions. Building on this framework, we design an MCTS-P\nalgorithm to automatically collect and annotate step-wise, five-dimensional\nagent execution data. Using this data, we train Similar with the Triple-M\nstrategy. Furthermore, we introduce the first benchmark in the virtual agent\ndomain for step-wise, multi-dimensional reward model training and evaluation,\nnamed SRM. This benchmark consists of two components: SRMTrain, which serves as\nthe training set for Similar, and SRMEval, a manually selected test set for\nevaluating the reward model. Experimental results demonstrate that Similar,\nthrough its step-wise, multi-dimensional assessment and synergistic gain,\nprovides GVAs with effective intermediate signals during both training and\ninference-time scaling. The code is available at\nhttps://github.com/Galery23/Similar-v1.\n","authors":["Bingchen Miao","Yang Wu","Minghe Gao","Qifan Yu","Wendong Bu","Wenqiao Zhang","Yunfei Li","Siliang Tang","Tat-Seng Chua","Juncheng Li"],"pdf_url":"https://arxiv.org/pdf/2503.18665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18658v1","updated":"2025-03-24T13:23:46Z","published":"2025-03-24T13:23:46Z","title":"Leveraging Land Cover Priors for Isoprene Emission Super-Resolution","summary":"  Remote sensing plays a crucial role in monitoring Earth's ecosystems, yet\nsatellite-derived data often suffer from limited spatial resolution,\nrestricting their applicability in atmospheric modeling and climate research.\nIn this work, we propose a deep learning-based Super-Resolution (SR) framework\nthat leverages land cover information to enhance the spatial accuracy of\nBiogenic Volatile Organic Compounds (BVOCs) emissions, with a particular focus\non isoprene. Our approach integrates land cover priors as emission drivers,\ncapturing spatial patterns more effectively than traditional methods. We\nevaluate the model's performance across various climate conditions and analyze\nstatistical correlations between isoprene emissions and key environmental\ninformation such as cropland and tree cover data. Additionally, we assess the\ngeneralization capabilities of our SR model by applying it to unseen climate\nzones and geographical regions. Experimental results demonstrate that\nincorporating land cover data significantly improves emission SR accuracy,\nparticularly in heterogeneous landscapes. This study contributes to atmospheric\nchemistry and climate modeling by providing a cost-effective, data-driven\napproach to refining BVOC emission maps. The proposed method enhances the\nusability of satellite-based emissions data, supporting applications in air\nquality forecasting, climate impact assessments, and environmental studies.\n","authors":["Christopher Ummerle","Antonio Giganti","Sara Mandelli","Paolo Bestagini","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2503.18658v1.pdf","comment":"17 pages, 16 figures, 4 tables"},{"id":"http://arxiv.org/abs/2503.14912v2","updated":"2025-03-24T13:18:16Z","published":"2025-03-19T05:33:28Z","title":"Deep Polycuboid Fitting for Compact 3D Representation of Indoor Scenes","summary":"  This paper presents a novel framework for compactly representing a 3D indoor\nscene using a set of polycuboids through a deep learning-based fitting method.\nIndoor scenes mainly consist of man-made objects, such as furniture, which\noften exhibit rectilinear geometry. This property allows indoor scenes to be\nrepresented using combinations of polycuboids, providing a compact\nrepresentation that benefits downstream applications like furniture\nrearrangement. Our framework takes a noisy point cloud as input and first\ndetects six types of cuboid faces using a transformer network. Then, a graph\nneural network is used to validate the spatial relationships of the detected\nfaces to form potential polycuboids. Finally, each polycuboid instance is\nreconstructed by forming a set of boxes based on the aggregated face labels. To\ntrain our networks, we introduce a synthetic dataset encompassing a diverse\nrange of cuboid and polycuboid shapes that reflect the characteristics of\nindoor scenes. Our framework generalizes well to real-world indoor scene\ndatasets, including Replica, ScanNet, and scenes captured with an iPhone. The\nversatility of our method is demonstrated through practical applications, such\nas virtual room tours and scene editing.\n","authors":["Gahye Lee","Hyejeong Yoon","Jungeon Kim","Seungyong Lee"],"pdf_url":"https://arxiv.org/pdf/2503.14912v2.pdf","comment":"Accepted to 3DV 2025. For project page, see this\n  https://waldstein94.github.io/deep-polycuboid-fitting/"},{"id":"http://arxiv.org/abs/2503.18652v1","updated":"2025-03-24T13:17:41Z","published":"2025-03-24T13:17:41Z","title":"Robust face recognition based on the wing loss and the $\\ell_1$\n  regularization","summary":"  In recent years, sparse sampling techniques based on regression analysis have\nwitnessed extensive applications in face recognition research. Presently,\nnumerous sparse sampling models based on regression analysis have been explored\nby various researchers. Nevertheless, the recognition rates of the majority of\nthese models would be significantly decreased when confronted with highly\noccluded and highly damaged face images. In this paper, a new wing-constrained\nsparse coding model(WCSC) and its weighted version(WWCSC) are introduced, so as\nto deal with the face recognition problem in complex circumstances, where the\nalternating direction method of multipliers (ADMM) algorithm is employed to\nsolve the corresponding minimization problems. In addition, performances of the\nproposed method are examined based on the four well-known facial databases,\nnamely the ORL facial database, the Yale facial database, the AR facial\ndatabase and the FERET facial database. Also, compared to the other methods in\nthe literatures, the WWCSC has a very high recognition rate even in complex\nsituations where face images have high occlusion or high damage, which\nillustrates the robustness of the WWCSC method in facial recognition.\n","authors":["Yaoyao Yun","Jianwen Xu"],"pdf_url":"https://arxiv.org/pdf/2503.18652v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.18642v1","updated":"2025-03-24T13:09:47Z","published":"2025-03-24T13:09:47Z","title":"Rethinking Glaucoma Calibration: Voting-Based Binocular and Metadata\n  Integration","summary":"  Glaucoma is an incurable ophthalmic disease that damages the optic nerve,\nleads to vision loss, and ranks among the leading causes of blindness\nworldwide. Diagnosing glaucoma typically involves fundus photography, optical\ncoherence tomography (OCT), and visual field testing. However, the high cost of\nOCT often leads to reliance on fundus photography and visual field testing,\nboth of which exhibit inherent inter-observer variability. This stems from\nglaucoma being a multifaceted disease that influenced by various factors. As a\nresult, glaucoma diagnosis is highly subjective, emphasizing the necessity of\ncalibration, which aligns predicted probabilities with actual disease\nlikelihood. Proper calibration is essential to prevent overdiagnosis or\nmisdiagnosis, which are critical concerns for high-risk diseases. Although AI\nhas significantly improved diagnostic accuracy, overconfidence in models have\nworsen calibration performance. Recent study has begun focusing on calibration\nfor glaucoma. Nevertheless, previous study has not fully considered glaucoma's\nsystemic nature and the high subjectivity in its diagnostic process. To\novercome these limitations, we propose V-ViT (Voting-based ViT), a novel\nframework that enhances calibration by incorporating disease-specific\ncharacteristics. V-ViT integrates binocular data and metadata, reflecting the\nmulti-faceted nature of glaucoma diagnosis. Additionally, we introduce a MC\ndropout-based Voting System to address high subjectivity. Our approach achieves\nstate-of-the-art performance across all metrics, including accuracy,\ndemonstrating that our proposed methods are effective in addressing calibration\nissues. We validate our method using a custom dataset including binocular data.\n","authors":["Taejin Jeong","Joohyeok Kim","Jaehoon Joo","Yeonwoo Jung","Hyeonmin Kim","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2503.18642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18640v1","updated":"2025-03-24T13:05:05Z","published":"2025-03-24T13:05:05Z","title":"LLGS: Unsupervised Gaussian Splatting for Image Enhancement and\n  Reconstruction in Pure Dark Environment","summary":"  3D Gaussian Splatting has shown remarkable capabilities in novel view\nrendering tasks and exhibits significant potential for multi-view\noptimization.However, the original 3D Gaussian Splatting lacks color\nrepresentation for inputs in low-light environments. Simply using enhanced\nimages as inputs would lead to issues with multi-view consistency, and current\nsingle-view enhancement systems rely on pre-trained data, lacking scene\ngeneralization. These problems limit the application of 3D Gaussian Splatting\nin low-light conditions in the field of robotics, including high-fidelity\nmodeling and feature matching. To address these challenges, we propose an\nunsupervised multi-view stereoscopic system based on Gaussian Splatting, called\nLow-Light Gaussian Splatting (LLGS). This system aims to enhance images in\nlow-light environments while reconstructing the scene. Our method introduces a\ndecomposable Gaussian representation called M-Color, which separately\ncharacterizes color information for targeted enhancement. Furthermore, we\npropose an unsupervised optimization method with zero-knowledge priors, using\ndirection-based enhancement to ensure multi-view consistency. Experiments\nconducted on real-world datasets demonstrate that our system outperforms\nstate-of-the-art methods in both low-light enhancement and 3D Gaussian\nSplatting.\n","authors":["Haoran Wang","Jingwei Huang","Lu Yang","Tianchen Deng","Gaojing Zhang","Mingrui Li"],"pdf_url":"https://arxiv.org/pdf/2503.18640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01376v2","updated":"2025-03-24T13:02:03Z","published":"2024-10-02T09:44:54Z","title":"Learning Physics From Video: Unsupervised Physical Parameter Estimation\n  for Continuous Dynamical Systems","summary":"  Extracting physical dynamical system parameters from recorded observations is\nkey in natural science. Current methods for automatic parameter estimation from\nvideo train supervised deep networks on large datasets. Such datasets require\nlabels, which are difficult to acquire. While some unsupervised\ntechniques--which depend on frame prediction--exist, they suffer from long\ntraining times, initialization instabilities, only consider motion-based\ndynamical systems, and are evaluated mainly on synthetic data. In this work, we\npropose an unsupervised method to estimate the physical parameters of known,\ncontinuous governing equations from single videos suitable for different\ndynamical systems beyond motion and robust to initialization. Moreover, we\nremove the need for frame prediction by implementing a KL-divergence-based loss\nfunction in the latent space, which avoids convergence to trivial solutions and\nreduces model size and compute. We first evaluate our model on synthetic data,\nas commonly done. After which, we take the field closer to reality by recording\nDelfys75: our own real-world dataset of 75 videos for five different types of\ndynamical systems to evaluate our method and others. Our method compares\nfavorably to others. %, yet, and real-world video datasets and demonstrate\nimproved parameter estimation accuracy compared to existing methods. Code and\ndata are available\nonline:https://github.com/Alejandro-neuro/Learning_physics_from_video.\n","authors":["Alejandro Castañeda Garcia","Jan van Gemert","Daan Brinks","Nergis Tömen"],"pdf_url":"https://arxiv.org/pdf/2410.01376v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18637v1","updated":"2025-03-24T13:00:25Z","published":"2025-03-24T13:00:25Z","title":"Unbiasing through Textual Descriptions: Mitigating Representation Bias\n  in Video Benchmarks","summary":"  We propose a new \"Unbiased through Textual Description (UTD)\" video benchmark\nbased on unbiased subsets of existing video classification and retrieval\ndatasets to enable a more robust assessment of video understanding\ncapabilities. Namely, we tackle the problem that current video benchmarks may\nsuffer from different representation biases, e.g., object bias or single-frame\nbias, where mere recognition of objects or utilization of only a single frame\nis sufficient for correct prediction. We leverage VLMs and LLMs to analyze and\ndebias benchmarks from such representation biases. Specifically, we generate\nframe-wise textual descriptions of videos, filter them for specific information\n(e.g. only objects) and leverage them to examine representation biases across\nthree dimensions: 1) concept bias - determining if a specific concept (e.g.,\nobjects) alone suffice for prediction; 2) temporal bias - assessing if temporal\ninformation contributes to prediction; and 3) common sense vs. dataset bias -\nevaluating whether zero-shot reasoning or dataset correlations contribute to\nprediction. We conduct a systematic analysis of 12 popular video classification\nand retrieval datasets and create new object-debiased test splits for these\ndatasets. Moreover, we benchmark 30 state-of-the-art video models on original\nand debiased splits and analyze biases in the models. To facilitate the future\ndevelopment of more robust video understanding benchmarks and models, we\nrelease: \"UTD-descriptions\", a dataset with our rich structured descriptions\nfor each dataset, and \"UTD-splits\", a dataset of object-debiased test splits.\n","authors":["Nina Shvetsova","Arsha Nagrani","Bernt Schiele","Hilde Kuehne","Christian Rupprecht"],"pdf_url":"https://arxiv.org/pdf/2503.18637v1.pdf","comment":"To be published at CVPR 2025, project webpage\n  https://utd-project.github.io/"},{"id":"http://arxiv.org/abs/2503.18635v1","updated":"2025-03-24T12:57:23Z","published":"2025-03-24T12:57:23Z","title":"OCCO: LVM-guided Infrared and Visible Image Fusion Framework based on\n  Object-aware and Contextual COntrastive Learning","summary":"  Image fusion is a crucial technique in the field of computer vision, and its\ngoal is to generate high-quality fused images and improve the performance of\ndownstream tasks. However, existing fusion methods struggle to balance these\ntwo factors. Achieving high quality in fused images may result in lower\nperformance in downstream visual tasks, and vice versa. To address this\ndrawback, a novel LVM (large vision model)-guided fusion framework with\nObject-aware and Contextual COntrastive learning is proposed, termed as OCCO.\nThe pre-trained LVM is utilized to provide semantic guidance, allowing the\nnetwork to focus solely on fusion tasks while emphasizing learning salient\nsemantic features in form of contrastive learning. Additionally, a novel\nfeature interaction fusion network is also designed to resolve information\nconflicts in fusion images caused by modality differences. By learning the\ndistinction between positive samples and negative samples in the latent feature\nspace (contextual space), the integrity of target information in fused image is\nimproved, thereby benefiting downstream performance. Finally, compared with\neight state-of-the-art methods on four datasets, the effectiveness of the\nproposed method is validated, and exceptional performance is also demonstrated\non downstream visual task.\n","authors":["Hui Li","Congcong Bian","Zeyang Zhang","Xiaoning Song","Xi Li","Xiao-Jun Wu"],"pdf_url":"https://arxiv.org/pdf/2503.18635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04282v2","updated":"2025-03-24T12:53:56Z","published":"2024-12-05T16:03:37Z","title":"Learnable Infinite Taylor Gaussian for Dynamic View Rendering","summary":"  Capturing the temporal evolution of Gaussian properties such as position,\nrotation, and scale is a challenging task due to the vast number of\ntime-varying parameters and the limited photometric data available, which\ngenerally results in convergence issues, making it difficult to find an optimal\nsolution. While feeding all inputs into an end-to-end neural network can\neffectively model complex temporal dynamics, this approach lacks explicit\nsupervision and struggles to generate high-quality transformation fields. On\nthe other hand, using time-conditioned polynomial functions to model Gaussian\ntrajectories and orientations provides a more explicit and interpretable\nsolution, but requires significant handcrafted effort and lacks\ngeneralizability across diverse scenes. To overcome these limitations, this\npaper introduces a novel approach based on a learnable infinite Taylor Formula\nto model the temporal evolution of Gaussians. This method offers both the\nflexibility of an implicit network-based approach and the interpretability of\nexplicit polynomial functions, allowing for more robust and generalizable\nmodeling of Gaussian dynamics across various dynamic scenes. Extensive\nexperiments on dynamic novel view rendering tasks are conducted on public\ndatasets, demonstrating that the proposed method achieves state-of-the-art\nperformance in this domain. More information is available on our project\npage(https://ellisonking.github.io/TaylorGaussian).\n","authors":["Bingbing Hu","Yanyan Li","Rui Xie","Bo Xu","Haoye Dong","Junfeng Yao","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2412.04282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18631v1","updated":"2025-03-24T12:49:47Z","published":"2025-03-24T12:49:47Z","title":"Robust Lane Detection with Wavelet-Enhanced Context Modeling and\n  Adaptive Sampling","summary":"  Lane detection is critical for autonomous driving and ad-vanced driver\nassistance systems (ADAS). While recent methods like CLRNet achieve strong\nperformance, they struggle under adverse con-ditions such as extreme weather,\nillumination changes, occlusions, and complex curves. We propose a\nWavelet-Enhanced Feature Pyramid Net-work (WE-FPN) to address these challenges.\nA wavelet-based non-local block is integrated before the feature pyramid to\nimprove global context modeling, especially for occluded and curved lanes.\nAdditionally, we de-sign an adaptive preprocessing module to enhance lane\nvisibility under poor lighting. An attention-guided sampling strategy further\nreffnes spa-tial features, boosting accuracy on distant and curved lanes.\nExperiments on CULane and TuSimple demonstrate that our approach signiffcantly\noutperforms baselines in challenging scenarios, achieving better robust-ness\nand accuracy in real-world driving conditions.\n","authors":["Kunyang Li","Ming Hou"],"pdf_url":"https://arxiv.org/pdf/2503.18631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07101v2","updated":"2025-03-24T12:47:59Z","published":"2025-01-13T07:26:37Z","title":"SAMKD: Spatial-aware Adaptive Masking Knowledge Distillation for Object\n  Detection","summary":"  Most of recent attention-guided feature masking distillation methods perform\nknowledge transfer via global teacher attention maps without delving into\nfine-grained clues. Instead, performing distillation at finer granularity is\nconducive to uncovering local details supplementary to global knowledge\ntransfer and reconstructing comprehensive student features. In this study, we\npropose a Spatial-aware Adaptive Masking Knowledge Distillation (SAMKD)\nframework for accurate object detection. Different from previous feature\ndistillation methods which mainly perform single-scale feature masking, we\ndevelop spatially hierarchical feature masking distillation scheme, such that\nthe object-aware locality is encoded during coarse-to-fine distillation process\nfor improved feature reconstruction. In addition, our spatial-aware feature\ndistillation strategy is combined with a masking logit distillation scheme in\nwhich region-specific feature difference between teacher and student networks\nis utilized to adaptively guide the distillation process. Thus, it can help the\nstudent model to better learn from the teacher counterpart with improved\nknowledge transfer and reduced gap. Extensive experiments for detection task\ndemonstrate the superiority of our method. For example, when FCOS is used as\nteacher detector with ResNet101 backbone, our method improves the student\nnetwork from 35.3\\% to 38.8\\% mAP, outperforming state-of-the-art distillation\nmethods including MGD, FreeKD and DMKD.\n","authors":["Zhourui Zhang","Jun Li","Jiayan Li","Jianhua Xu"],"pdf_url":"https://arxiv.org/pdf/2501.07101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13193v2","updated":"2025-03-24T12:45:56Z","published":"2024-12-17T18:59:46Z","title":"GaussTR: Foundation Model-Aligned Gaussian Transformer for\n  Self-Supervised 3D Spatial Understanding","summary":"  3D Semantic Occupancy Prediction is fundamental for spatial understanding,\nyet existing approaches face challenges in scalability and generalization due\nto their reliance on extensive labeled data and computationally intensive\nvoxel-wise representations. In this paper, we introduce GaussTR, a novel\nGaussian-based Transformer framework that unifies sparse 3D modeling with\nfoundation model alignment through Gaussian representations to advance 3D\nspatial understanding. GaussTR predicts sparse sets of Gaussians in a\nfeed-forward manner to represent 3D scenes. By splatting the Gaussians into 2D\nviews and aligning the rendered features with foundation models, GaussTR\nfacilitates self-supervised 3D representation learning and enables\nopen-vocabulary semantic occupancy prediction without requiring explicit\nannotations. Empirical experiments on the Occ3D-nuScenes dataset demonstrate\nGaussTR's state-of-the-art zero-shot performance of 12.27 mIoU, along with a\n40% reduction in training time. These results highlight the efficacy of GaussTR\nfor scalable and holistic 3D spatial understanding, with promising implications\nin autonomous driving and embodied agents. The code is available at\nhttps://github.com/hustvl/GaussTR.\n","authors":["Haoyi Jiang","Liu Liu","Tianheng Cheng","Xinjie Wang","Tianwei Lin","Zhizhong Su","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13193v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18629v1","updated":"2025-03-24T12:45:52Z","published":"2025-03-24T12:45:52Z","title":"Towards Human-Understandable Multi-Dimensional Concept Discovery","summary":"  Concept-based eXplainable AI (C-XAI) aims to overcome the limitations of\ntraditional saliency maps by converting pixels into human-understandable\nconcepts that are consistent across an entire dataset. A crucial aspect of\nC-XAI is completeness, which measures how well a set of concepts explains a\nmodel's decisions. Among C-XAI methods, Multi-Dimensional Concept Discovery\n(MCD) effectively improves completeness by breaking down the CNN latent space\ninto distinct and interpretable concept subspaces. However, MCD's explanations\ncan be difficult for humans to understand, raising concerns about their\npractical utility. To address this, we propose Human-Understandable\nMulti-dimensional Concept Discovery (HU-MCD). HU-MCD uses the Segment Anything\nModel for concept identification and implements a CNN-specific input masking\ntechnique to reduce noise introduced by traditional masking methods. These\nchanges to MCD, paired with the completeness relation, enable HU-MCD to enhance\nconcept understandability while maintaining explanation faithfulness. Our\nexperiments, including human subject studies, show that HU-MCD provides more\nprecise and reliable explanations than existing C-XAI methods. The code is\navailable at https://github.com/grobruegge/hu-mcd.\n","authors":["Arne Grobrügge","Niklas Kühl","Gerhard Satzger","Philipp Spitzer"],"pdf_url":"https://arxiv.org/pdf/2503.18629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18627v1","updated":"2025-03-24T12:43:11Z","published":"2025-03-24T12:43:11Z","title":"Dig2DIG: Dig into Diffusion Information Gains for Image Fusion","summary":"  Image fusion integrates complementary information from multi-source images to\ngenerate more informative results. Recently, the diffusion model, which\ndemonstrates unprecedented generative potential, has been explored in image\nfusion. However, these approaches typically incorporate predefined multimodal\nguidance into diffusion, failing to capture the dynamically changing\nsignificance of each modality, while lacking theoretical guarantees. To address\nthis issue, we reveal a significant spatio-temporal imbalance in image\ndenoising; specifically, the diffusion model produces dynamic information gains\nin different image regions with denoising steps. Based on this observation, we\nDig into the Diffusion Information Gains (Dig2DIG) and theoretically derive a\ndiffusion-based dynamic image fusion framework that provably reduces the upper\nbound of the generalization error. Accordingly, we introduce diffusion\ninformation gains (DIG) to quantify the information contribution of each\nmodality at different denoising steps, thereby providing dynamic guidance\nduring the fusion process. Extensive experiments on multiple fusion scenarios\nconfirm that our method outperforms existing diffusion-based approaches in\nterms of both fusion quality and inference efficiency.\n","authors":["Bing Cao","Baoshuo Cai","Changqing Zhang","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2503.18627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18626v1","updated":"2025-03-24T12:41:40Z","published":"2025-03-24T12:41:40Z","title":"Generative Dataset Distillation using Min-Max Diffusion Model","summary":"  In this paper, we address the problem of generative dataset distillation that\nutilizes generative models to synthesize images. The generator may produce any\nnumber of images under a preserved evaluation time. In this work, we leverage\nthe popular diffusion model as the generator to compute a surrogate dataset,\nboosted by a min-max loss to control the dataset's diversity and\nrepresentativeness during training. However, the diffusion model is\ntime-consuming when generating images, as it requires an iterative generation\nprocess. We observe a critical trade-off between the number of image samples\nand the image quality controlled by the diffusion steps and propose Diffusion\nStep Reduction to achieve optimal performance. This paper details our\ncomprehensive method and its performance. Our model achieved $2^{nd}$ place in\nthe generative track of \\href{https://www.dd-challenge.com/#/}{The First\nDataset Distillation Challenge of ECCV2024}, demonstrating its superior\nperformance.\n","authors":["Junqiao Fan","Yunjiao Zhou","Min Chang Jordan Ren","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2503.18626v1.pdf","comment":"The paper is accepted as the ECCV2024 workshop paper and achieved\n  second place in the generative track of The First Dataset Distillation\n  Challenge of ECCV2024, https://www.dd-challenge.com/#/"},{"id":"http://arxiv.org/abs/2503.18623v1","updated":"2025-03-24T12:36:24Z","published":"2025-03-24T12:36:24Z","title":"Training-Free Personalization via Retrieval and Reasoning on\n  Fingerprints","summary":"  Vision Language Models (VLMs) have lead to major improvements in multimodal\nreasoning, yet they still struggle to understand user-specific concepts.\nExisting personalization methods address this limitation but heavily rely on\ntraining procedures, that can be either costly or unpleasant to individual\nusers. We depart from existing work, and for the first time explore the\ntraining-free setting in the context of personalization. We propose a novel\nmethod, Retrieval and Reasoning for Personalization (R2P), leveraging internal\nknowledge of VLMs. First, we leverage VLMs to extract the concept fingerprint,\ni.e., key attributes uniquely defining the concept within its semantic class.\nWhen a query arrives, the most similar fingerprints are retrieved and scored\nvia chain-of-thought-reasoning. To reduce the risk of hallucinations, the\nscores are validated through cross-modal verification at the attribute level:\nin case of a discrepancy between the scores, R2P refines the concept\nassociation via pairwise multimodal matching, where the retrieved fingerprints\nand their images are directly compared with the query. We validate R2P on two\npublicly available benchmarks and a newly introduced dataset, Personal Concepts\nwith Visual Ambiguity (PerVA), for concept identification highlighting\nchallenges in visual ambiguity. R2P consistently outperforms state-of-the-art\napproaches on various downstream tasks across all benchmarks. Code will be\navailable upon acceptance.\n","authors":["Deepayan Das","Davide Talon","Yiming Wang","Massimiliano Mancini","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2503.18623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02452v2","updated":"2025-03-24T12:34:02Z","published":"2025-02-04T16:19:20Z","title":"Personalization Toolkit: Training Free Personalization of Large Vision\n  Language Models","summary":"  Large Vision Language Models (LVLMs) have significant potential to provide\npersonalized assistance by adapting to the unique needs and preferences of\nindividual users. The personalization of LVLMs has emerged as a field that\nfocuses on customizing models to recognize specific object instances and\nprovide tailored responses. However, current methodologies depend on\ntime-consuming test-time training for each user and object, which proves to be\nimpractical. This paper introduces a novel, training-free approach to LVLM\npersonalization by leveraging pre-trained vision foundation models to extract\ndistinct features, retrieval-augmented generation (RAG) techniques to recognize\ninstances in the visual input, and visual prompting methods. Our model-agnostic\nvision toolkit enables flexible and efficient personalization without the need\nfor extensive retraining. We demonstrate state-of-the-art results, surpassing\nconventional training-based approaches, and set a new benchmark for LVLM\npersonalization.\n","authors":["Soroush Seifi","Vaggelis Dorovatas","Daniel Olmeda Reino","Rahaf Aljundi"],"pdf_url":"https://arxiv.org/pdf/2502.02452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13999v2","updated":"2025-03-24T12:24:58Z","published":"2025-03-18T08:06:05Z","title":"BI-RADS prediction of mammographic masses using uncertainty information\n  extracted from a Bayesian Deep Learning model","summary":"  The BI_RADS score is a probabilistic reporting tool used by radiologists to\nexpress the level of uncertainty in predicting breast cancer based on some\nmorphological features in mammography images. There is a significant\nvariability in describing masses which sometimes leads to BI_RADS\nmisclassification. Using a BI_RADS prediction system is required to support the\nfinal radiologist decisions. In this study, the uncertainty information\nextracted by a Bayesian deep learning model is utilized to predict the BI_RADS\nscore. The investigation results based on the pathology information demonstrate\nthat the f1-scores of the predictions of the radiologist are 42.86%, 48.33% and\n48.28%, meanwhile, the f1-scores of the model performance are 73.33%, 59.60%\nand 59.26% in the BI_RADS 2, 3 and 5 dataset samples, respectively. Also, the\nmodel can distinguish malignant from benign samples in the BI_RADS 0 category\nof the used dataset with an accuracy of 75.86% and correctly identify all\nmalignant samples as BI_RADS 5. The Grad-CAM visualization shows the model pays\nattention to the morphological features of the lesions. Therefore, this study\nshows the uncertainty-aware Bayesian Deep Learning model can report his\nuncertainty about the malignancy of a lesion based on morphological features,\nlike a radiologist.\n","authors":["Mohaddeseh Chegini","Ali Mahloojifar"],"pdf_url":"https://arxiv.org/pdf/2503.13999v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05843v3","updated":"2025-03-24T12:22:37Z","published":"2025-02-09T10:30:54Z","title":"From Objects to Events: Unlocking Complex Visual Understanding in Object\n  Detectors via LLM-guided Symbolic Reasoning","summary":"  Our key innovation lies in bridging the semantic gap between object detection\nand event understanding without requiring expensive task-specific training. The\nproposed plug-and-play framework interfaces with any open-vocabulary detector\nwhile extending their inherent capabilities across architectures. At its core,\nour approach combines (i) a symbolic regression mechanism exploring\nrelationship patterns among detected entities and (ii) a LLM-guided\nstrategically guiding the search toward meaningful expressions. These\ndiscovered symbolic rules transform low-level visual perception into\ninterpretable event understanding, providing a transparent reasoning path from\nobjects to events with strong transferability across domains.We compared our\ntraining-free framework against specialized event recognition systems across\ndiverse application domains. Experiments demonstrate that our framework\nenhances multiple object detector architectures to recognize complex events\nsuch as illegal fishing activities (75% AUROC, +8.36% improvement),\nconstruction safety violations (+15.77%), and abnormal crowd behaviors\n(+23.16%). The code will be released soon.\n","authors":["Yuhui Zeng","Haoxiang Wu","Wenjie Nie","Xiawu Zheng","Guangyao Chen","Yunhang Shen","Jun Peng","Yonghong Tian","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2502.05843v3.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.15816v2","updated":"2025-03-24T12:21:44Z","published":"2025-03-20T03:03:46Z","title":"A Vision Centric Remote Sensing Benchmark","summary":"  Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision-language tasks but their remote sensing (RS) counterpart are relatively\nunder explored. Unlike natural images, RS imagery presents unique challenges\nthat current MLLMs struggle to handle, particularly in visual grounding and\nspatial reasoning. This study investigates the limitations of CLIP-based MLLMs\nin RS, highlighting their failure to differentiate visually distinct yet\nsemantically similar RS images. To address this, we introduce a remote sensing\nmultimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs\nin RS tasks by identifying the CLIP-blind pairs, where CLIP-based models\nincorrectly assign high similarity scores to visually distinct RS images.\nThrough a visual question answering (VQA) evaluation, we analyze the\nperformance of state-of-the-art MLLMs, revealing significant limitations in RS\nspecific representation learning. The results provide valuable insights into\nthe weaknesses of CLIP-based visual encoding and offer a foundation for future\nresearch to develop more effective MLLMs tailored for remote sensing\napplications.\n","authors":["Abduljaleel Adejumo","Faegheh Yeganli","Clifford Broni-bediako","Aoran Xiao","Naoto Yokoya","Mennatullah Siam"],"pdf_url":"https://arxiv.org/pdf/2503.15816v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2503.15910v2","updated":"2025-03-24T12:16:42Z","published":"2025-03-20T07:40:24Z","title":"No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR\n  Semantic Segmentation in Adverse Weather","summary":"  Existing domain generalization methods for LiDAR semantic segmentation under\nadverse weather struggle to accurately predict \"things\" categories compared to\n\"stuff\" categories. In typical driving scenes, \"things\" categories can be\ndynamic and associated with higher collision risks, making them crucial for\nsafe navigation and planning. Recognizing the importance of \"things\"\ncategories, we identify their performance drop as a serious bottleneck in\nexisting approaches. We observed that adverse weather induces degradation of\nsemantic-level features and both corruption of local features, leading to a\nmisprediction of \"things\" as \"stuff\". To mitigate these corruptions, we suggest\nour method, NTN - segmeNt Things for No-accident. To address semantic-level\nfeature corruption, we bind each point feature to its superclass, preventing\nthe misprediction of things classes into visually dissimilar categories.\nAdditionally, to enhance robustness against local corruption caused by adverse\nweather, we define each LiDAR beam as a local region and propose a\nregularization term that aligns the clean data with its corrupted counterpart\nin feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU\ngain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the\nSemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9\nmIoU improvement on \"things\" classes, respectively, highlighting its\neffectiveness.\n","authors":["Junsung Park","Hwijeong Lee","Inha Kang","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2503.15910v2.pdf","comment":"18 pages, accepted in CVPR 2025"},{"id":"http://arxiv.org/abs/2410.20972v2","updated":"2025-03-24T12:16:17Z","published":"2024-10-28T12:43:48Z","title":"Attention Overlap Is Responsible for The Entity Missing Problem in\n  Text-to-image Diffusion Models!","summary":"  Text-to-image diffusion models, such as Stable Diffusion and DALL-E, are\ncapable of generating high-quality, diverse, and realistic images from textual\nprompts. However, they sometimes struggle to accurately depict specific\nentities described in prompts, a limitation known as the entity missing problem\nin compositional generation. While prior studies suggested that adjusting\ncross-attention maps during the denoising process could alleviate this problem,\nthey did not systematically investigate which objective functions could best\naddress it. This study examines three potential causes of the entity-missing\nproblem, focusing on cross-attention dynamics: (1) insufficient attention\nintensity for certain entities, (2) overly broad attention spread, and (3)\nexcessive overlap between attention maps of different entities. We found that\nreducing overlap in attention maps between entities can effectively minimize\nthe rate of entity missing. Specifically, we hypothesize that tokens related to\nspecific entities compete for attention on certain image regions during the\ndenoising process, which can lead to divided attention across tokens and\nprevent accurate representation of each entity. To address this issue, we\nintroduced four loss functions, Intersection over Union (IoU), center-of-mass\n(CoM) distance, Kullback-Leibler (KL) divergence, and clustering compactness\n(CC) to regulate attention overlap during denoising steps without the need for\nretraining. Experimental results across a wide variety of benchmarks reveal\nthat these proposed training-free methods significantly improve compositional\naccuracy, outperforming previous approaches in visual question answering (VQA),\ncaptioning scores, CLIP similarity, and human evaluations. Notably, these\nmethods improved human evaluation scores by 9% over the best baseline,\ndemonstrating substantial improvements in compositional alignment.\n","authors":["Arash Marioriyad","Mohammadali Banayeeanzade","Reza Abbasi","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2410.20972v2.pdf","comment":"TMLR - 2025"},{"id":"http://arxiv.org/abs/2412.04680v2","updated":"2025-03-24T11:51:37Z","published":"2024-12-06T00:38:36Z","title":"Superpixel Tokenization for Vision Transformers: Preserving Semantic\n  Integrity in Visual Tokens","summary":"  Transformers, a groundbreaking architecture proposed for Natural Language\nProcessing (NLP), have also achieved remarkable success in Computer Vision. A\ncornerstone of their success lies in the attention mechanism, which models\nrelationships among tokens. While the tokenization process in NLP inherently\nensures that a single token does not contain multiple semantics, the\ntokenization of Vision Transformer (ViT) utilizes tokens from uniformly\npartitioned square image patches, which may result in an arbitrary mixing of\nvisual concepts in a token. In this work, we propose to substitute the\ngrid-based tokenization in ViT with superpixel tokenization, which employs\nsuperpixels to generate a token that encapsulates a sole visual concept.\nUnfortunately, the diverse shapes, sizes, and locations of superpixels make\nintegrating superpixels into ViT tokenization rather challenging. Our\ntokenization pipeline, comprised of pre-aggregate extraction and\nsuperpixel-aware aggregation, overcomes the challenges that arise in superpixel\ntokenization. Extensive experiments demonstrate that our approach, which\nexhibits strong compatibility with existing frameworks, enhances the accuracy\nand robustness of ViT on various downstream tasks.\n","authors":["Jaihyun Lew","Soohyuk Jang","Jaehoon Lee","Seungryong Yoo","Eunji Kim","Saehyung Lee","Jisoo Mok","Siwon Kim","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2412.04680v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18589v1","updated":"2025-03-24T11:46:58Z","published":"2025-03-24T11:46:58Z","title":"Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling","summary":"  Multi-agent trajectory modeling has primarily focused on forecasting future\nstates, often overlooking broader tasks like trajectory completion, which are\ncrucial for real-world applications such as correcting tracking data. Existing\nmethods also generally predict agents' states without offering any state-wise\nmeasure of uncertainty. Moreover, popular multi-modal sampling methods lack any\nerror probability estimates for each generated scene under the same prior\nobservations, making it difficult to rank the predictions during inference\ntime. We introduce U2Diff, a \\textbf{unified} diffusion model designed to\nhandle trajectory completion while providing state-wise \\textbf{uncertainty}\nestimates jointly. This uncertainty estimation is achieved by augmenting the\nsimple denoising loss with the negative log-likelihood of the predicted noise\nand propagating latent space uncertainty to the real state space. Additionally,\nwe incorporate a Rank Neural Network in post-processing to enable \\textbf{error\nprobability} estimation for each generated mode, demonstrating a strong\ncorrelation with the error relative to ground truth. Our method outperforms the\nstate-of-the-art solutions in trajectory completion and forecasting across four\nchallenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),\nhighlighting the effectiveness of uncertainty and error probability estimation.\nVideo at https://youtu.be/ngw4D4eJToE\n","authors":["Guillem Capellera","Antonio Rubio","Luis Ferraz","Antonio Agudo"],"pdf_url":"https://arxiv.org/pdf/2503.18589v1.pdf","comment":"Accepted to CVPR 2025 conference"},{"id":"http://arxiv.org/abs/2406.10819v2","updated":"2025-03-24T11:46:14Z","published":"2024-06-16T06:56:53Z","title":"GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented\n  Understanding","summary":"  Recently, Multimodal Large Language Models (MLLMs) have been used as agents\nto control keyboard and mouse inputs by directly perceiving the Graphical User\nInterface (GUI) and generating corresponding commands. However, current agents\nprimarily demonstrate strong understanding capabilities in static environments\nand are mainly applied to relatively simple domains, such as Web or mobile\ninterfaces. We argue that a robust GUI agent should be capable of perceiving\ntemporal information on the GUI, including dynamic Web content and multi-step\ntasks. Additionally, it should possess a comprehensive understanding of various\nGUI scenarios, including desktop software and multi-window interactions. To\nthis end, this paper introduces a new dataset, termed GUI-World, which features\nmeticulously crafted Human-MLLM annotations, extensively covering six GUI\nscenarios and eight types of GUI-oriented questions in three formats. We\nevaluate the capabilities of current state-of-the-art MLLMs, including Image\nLLMs and Video LLMs, in understanding various types of GUI content, especially\ndynamic and sequential content. Our findings reveal that current models\nstruggle with dynamic GUI content without manually annotated keyframes or\noperation history. On the other hand, Video LLMs fall short in all GUI-oriented\ntasks given the sparse GUI video dataset. Therefore, we take the initial step\nof leveraging a fine-tuned Video LLM, GUI-Vid, as a GUI-oriented assistant,\ndemonstrating an improved understanding of various GUI tasks. However, due to\nthe limitations in the performance of base LLMs, we conclude that using video\nLLMs as GUI agents remains a significant challenge. We believe our work\nprovides valuable insights for future research in dynamic GUI content\nunderstanding. All the dataset and code are publicly available at:\nhttps://gui-world.github.io.\n","authors":["Dongping Chen","Yue Huang","Siyuan Wu","Jingyu Tang","Liuyi Chen","Yilin Bai","Zhigang He","Chenlong Wang","Huichi Zhou","Yiqiang Li","Tianshuo Zhou","Yue Yu","Chujie Gao","Qihui Zhang","Yi Gui","Zhen Li","Yao Wan","Pan Zhou","Jianfeng Gao","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2406.10819v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2503.12836v2","updated":"2025-03-24T11:44:55Z","published":"2025-03-17T05:32:15Z","title":"CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D\nreconstruction and novel view synthesis, leading to its widespread commercial\nuse. Consequently, copyright protection via watermarking has become critical.\nHowever, because 3DGS relies on millions of Gaussians, which require gigabytes\nof storage, efficient transfer and storage require compression. Existing 3DGS\nwatermarking methods are vulnerable to quantization-based compression, often\nresulting in the loss of the embedded watermark. To address this challenge, we\npropose a novel watermarking method that ensures watermark robustness after\nmodel compression while maintaining high rendering quality. In detail, we\nincorporate a quantization distortion layer that simulates compression during\ntraining, preserving the watermark under quantization-based compression. Also,\nwe propose a learnable watermark embedding feature that embeds the watermark\ninto the anchor feature, ensuring structural consistency and seamless\nintegration into the 3D scene. Furthermore, we present a frequency-aware anchor\ngrowing mechanism to enhance image quality in high-frequency regions by\neffectively identifying Guassians within these regions. Experimental results\nconfirm that our method preserves the watermark and maintains superior image\nquality under high compression, validating it as a promising approach for a\nsecure 3DGS model.\n","authors":["Sumin In","Youngdong Jang","Utae Jeong","MinHyuk Jang","Hyeongcheol Park","Eunbyung Park","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2503.12836v2.pdf","comment":"23 pages, 17 figures"},{"id":"http://arxiv.org/abs/2503.18583v1","updated":"2025-03-24T11:41:21Z","published":"2025-03-24T11:41:21Z","title":"Adapting Video Diffusion Models for Time-Lapse Microscopy","summary":"  We present a domain adaptation of video diffusion models to generate highly\nrealistic time-lapse microscopy videos of cell division in HeLa cells. Although\nstate-of-the-art generative video models have advanced significantly for\nnatural videos, they remain underexplored in microscopy domains. To address\nthis gap, we fine-tune a pretrained video diffusion model on\nmicroscopy-specific sequences, exploring three conditioning strategies: (1)\ntext prompts derived from numeric phenotypic measurements (e.g., proliferation\nrates, migration speeds, cell-death frequencies), (2) direct numeric embeddings\nof phenotype scores, and (3) image-conditioned generation, where an initial\nmicroscopy frame is extended into a complete video sequence. Evaluation using\nbiologically meaningful morphological, proliferation, and migration metrics\ndemonstrates that fine-tuning substantially improves realism and accurately\ncaptures critical cellular behaviors such as mitosis and migration. Notably,\nthe fine-tuned model also generalizes beyond the training horizon, generating\ncoherent cell dynamics even in extended sequences. However, precisely\ncontrolling specific phenotypic characteristics remains challenging,\nhighlighting opportunities for future work to enhance conditioning methods. Our\nresults demonstrate the potential for domain-specific fine-tuning of generative\nvideo models to produce biologically plausible synthetic microscopy data,\nsupporting applications such as in-silico hypothesis testing and data\naugmentation.\n","authors":["Alexander Holmberg","Nils Mechtel","Wei Ouyang"],"pdf_url":"https://arxiv.org/pdf/2503.18583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18578v1","updated":"2025-03-24T11:35:56Z","published":"2025-03-24T11:35:56Z","title":"Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding","summary":"  Modern vision-language models (VLMs) develop patch embedding and convolution\nbackbone within vector space, especially Euclidean ones, at the very founding.\nWhen expanding VLMs to a galaxy scale for understanding astronomical phenomena,\nthe integration of spherical space for planetary orbits and hyperbolic spaces\nfor black holes raises two formidable challenges. a) The current pre-training\nmodel is confined to Euclidean space rather than a comprehensive geometric\nembedding. b) The predominant architecture lacks suitable backbones for\nanisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a\ngeometry-aware VLM, for the universe-level vision understanding tasks. We\nproposed the geometry prompt that generates geometry tokens by random walks\nacross diverse spaces on a multi-scale physical graph, along with a geometry\nadapter that compresses and reshapes the space anisotropy in a\nmixture-of-experts manner. Extensive experiments demonstrate the effectiveness\nof our approach, with Galaxy-Walker achieving state-of-the-art performance in\nboth galaxy property estimation ($R^2$ scores up to $0.91$) and morphology\nclassification tasks (up to $+0.17$ F1 improvement in challenging features),\nsignificantly outperforming both domain-specific models and general-purpose\nVLMs.\n","authors":["Tianyu Chen","Xingcheng Fu","Yisen Gao","Haodong Qian","Yuecen Wei","Kun Yan","Haoyi Zhou","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2503.18578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12799v2","updated":"2025-03-24T11:30:58Z","published":"2025-03-17T04:07:47Z","title":"Grounded Chain-of-Thought for Multimodal Large Language Models","summary":"  Despite great progress, existing multimodal large language models (MLLMs) are\nprone to visual hallucination, greatly impeding their trustworthy applications.\nIn this paper, we study this problem from the perspective of visual-spatial\nreasoning, and propose a new learning task for MLLMs, termed Grounded\nChain-of-Thought (GCoT). Different from recent visual CoT studies, which focus\nmore on visual knowledge reasoning, GCoT is keen to helping MLLMs to recognize\nand ground the relevant visual cues step by step, thereby predicting the\ncorrect answer with grounding coordinates as the intuitive basis. To facilitate\nthis task, we also carefully design and construct a dataset called multimodal\ngrounded chain-of-thought (MM-GCoT) consisting of 24,022 GCoT examples for\n5,033 images. Besides, a comprehensive consistency evaluation system is also\nintroduced, including the metrics of answer accuracy, grounding accuracy and\nanswer-grounding consistency. We further design and conduct a bunch of\nexperiments on 12 advanced MLLMs, and reveal some notable findings: i. most\nMLLMs performs poorly on the consistency evaluation, indicating obvious visual\nhallucination; ii. visual hallucination is not directly related to the\nparameter size and general multimodal performance, i.e., a larger and stronger\nMLLM is not less affected by this issue. Lastly, we also demonstrate that the\nproposed dataset can help existing MLLMs to well cultivate their GCoT\ncapability and reduce the inconsistent answering significantly. Moreover, their\nGCoT can be also generalized to exiting multimodal tasks, such as open-world QA\nand REC.\n","authors":["Qiong Wu","Xiangcong Yang","Yiyi Zhou","Chenxin Fang","Baiyang Song","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2503.12799v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12138v4","updated":"2025-03-24T11:30:32Z","published":"2025-02-17T18:54:05Z","title":"FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views","summary":"  We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https://zhanghe3z.github.io/FLARE/\n","authors":["Shangzhan Zhang","Jianyuan Wang","Yinghao Xu","Nan Xue","Christian Rupprecht","Xiaowei Zhou","Yujun Shen","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2502.12138v4.pdf","comment":"CVPR 2025. Website: https://zhanghe3z.github.io/FLARE/"},{"id":"http://arxiv.org/abs/2503.18567v1","updated":"2025-03-24T11:22:27Z","published":"2025-03-24T11:22:27Z","title":"Advancing Cross-Organ Domain Generalization with Test-Time Style\n  Transfer and Diversity Enhancement","summary":"  Deep learning has made significant progress in addressing challenges in\nvarious fields including computational pathology (CPath). However, due to the\ncomplexity of the domain shift problem, the performance of existing models will\ndegrade, especially when it comes to multi-domain or cross-domain tasks. In\nthis paper, we propose a Test-time style transfer (T3s) that uses a\nbidirectional mapping mechanism to project the features of the source and\ntarget domains into a unified feature space, enhancing the generalization\nability of the model. To further increase the style expression space, we\nintroduce a Cross-domain style diversification module (CSDM) to ensure the\northogonality between style bases. In addition, data augmentation and low-rank\nadaptation techniques are used to improve feature alignment and sensitivity,\nenabling the model to adapt to multi-domain inputs effectively. Our method has\ndemonstrated effectiveness on three unseen datasets.\n","authors":["Biwen Meng","Xi Long","Wanrong Yang","Ruochen Liu","Yi Tian","Yalin Zheng","Jingxin Liu"],"pdf_url":"https://arxiv.org/pdf/2503.18567v1.pdf","comment":"2025 IEEE International Symposium on Biomedical Imaging (ISBI)"},{"id":"http://arxiv.org/abs/2412.03240v2","updated":"2025-03-24T11:21:17Z","published":"2024-12-04T11:42:17Z","title":"Task-driven Image Fusion with Learnable Fusion Loss","summary":"  Multi-modal image fusion aggregates information from multiple sensor sources,\nachieving superior visual quality and perceptual features compared to\nsingle-source images, often improving downstream tasks. However, current fusion\nmethods for downstream tasks still use predefined fusion objectives that\npotentially mismatch the downstream tasks, limiting adaptive guidance and\nreducing model flexibility. To address this, we propose Task-driven Image\nFusion (TDFusion), a fusion framework incorporating a learnable fusion loss\nguided by task loss. Specifically, our fusion loss includes learnable\nparameters modeled by a neural network called the loss generation module. This\nmodule is supervised by the downstream task loss in a meta-learning manner. The\nlearning objective is to minimize the task loss of fused images after\noptimizing the fusion module with the fusion loss. Iterative updates between\nthe fusion module and the loss module ensure that the fusion network evolves\ntoward minimizing task loss, guiding the fusion process toward the task\nobjectives. TDFusion's training relies entirely on the downstream task loss,\nmaking it adaptable to any specific task. It can be applied to any architecture\nof fusion and task networks. Experiments demonstrate TDFusion's performance\nthrough fusion experiments conducted on four different datasets, in addition to\nevaluations on semantic segmentation and object detection tasks.\n","authors":["Haowen Bai","Jiangshe Zhang","Zixiang Zhao","Yichen Wu","Lilun Deng","Yukun Cui","Tao Feng","Shuang Xu"],"pdf_url":"https://arxiv.org/pdf/2412.03240v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18559v1","updated":"2025-03-24T11:13:33Z","published":"2025-03-24T11:13:33Z","title":"AMD-Hummingbird: Towards an Efficient Text-to-Video Model","summary":"  Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.\n","authors":["Takashi Isobe","He Cui","Dong Zhou","Mengmeng Ge","Dong Li","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2503.18559v1.pdf","comment":"Homepage:\n  https://www.amd.com/en/developer/resources/technical-articles/amd-hummingbird-0-9b-text-to-video-diffusion-model-with-4-step-inferencing.html|\n  GitHub: https://github.com/AMD-AIG-AIMA/AMD-Hummingbird-T2V"},{"id":"http://arxiv.org/abs/2503.18557v1","updated":"2025-03-24T11:10:52Z","published":"2025-03-24T11:10:52Z","title":"LeanStereo: A Leaner Backbone based Stereo Network","summary":"  Recently, end-to-end deep networks based stereo matching methods, mainly\nbecause of their performance, have gained popularity. However, this improvement\nin performance comes at the cost of increased computational and memory\nbandwidth requirements, thus necessitating specialized hardware (GPUs); even\nthen, these methods have large inference times compared to classical methods.\nThis limits their applicability in real-world applications. Although we desire\nhigh accuracy stereo methods albeit with reasonable inference time. To this\nend, we propose a fast end-to-end stereo matching method. Majority of this\nspeedup comes from integrating a leaner backbone. To recover the performance\nlost because of a leaner backbone, we propose to use learned attention weights\nbased cost volume combined with LogL1 loss for stereo matching. Using LogL1\nloss not only improves the overall performance of the proposed network but also\nleads to faster convergence. We do a detailed empirical evaluation of different\ndesign choices and show that our method requires 4x less operations and is also\nabout 9 to 14x faster compared to the state of the art methods like ACVNet [1],\nLEAStereo [2] and CFNet [3] while giving comparable performance.\n","authors":["Rafia Rahim","Samuel Woerz","Andreas Zell"],"pdf_url":"https://arxiv.org/pdf/2503.18557v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.08561v2","updated":"2025-03-24T11:10:32Z","published":"2025-03-11T15:50:20Z","title":"ComicsPAP: understanding comic strips by picking the correct panel","summary":"  Large multimodal models (LMMs) have made impressive strides in image\ncaptioning, VQA, and video comprehension, yet they still struggle with the\nintricate temporal and spatial cues found in comics. To address this gap, we\nintroduce ComicsPAP, a large-scale benchmark designed for comic strip\nunderstanding. Comprising over 100k samples and organized into 5 subtasks under\na Pick-a-Panel framework, ComicsPAP demands models to identify the missing\npanel in a sequence. Our evaluations, conducted under both multi-image and\nsingle-image protocols, reveal that current state-of-the-art LMMs perform near\nchance on these tasks, underscoring significant limitations in capturing\nsequential and contextual dependencies. To close the gap, we adapted LMMs for\ncomic strip understanding, obtaining better results on ComicsPAP than 10x\nbigger models, demonstrating that ComicsPAP offers a robust resource to drive\nfuture research in multimodal comic comprehension.\n","authors":["Emanuele Vivoli","Artemis Llabrés","Mohamed Ali Souibgui","Marco Bertini","Ernest Valveny Llobet","Dimosthenis Karatzas"],"pdf_url":"https://arxiv.org/pdf/2503.08561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18556v1","updated":"2025-03-24T11:09:06Z","published":"2025-03-24T11:09:06Z","title":"Instruction-Aligned Visual Attention for Mitigating Hallucinations in\n  Large Vision-Language Models","summary":"  Despite the significant success of Large Vision-Language models(LVLMs), these\nmodels still suffer hallucinations when describing images, generating answers\nthat include non-existent objects. It is reported that these models tend to\nover-focus on certain irrelevant image tokens that do not contain critical\ninformation for answering the question and distort the output. To address this,\nwe propose an Instruction-Aligned Visual Attention(IAVA) approach, which\nidentifies irrelevant tokens by comparing changes in attention weights under\ntwo different instructions. By applying contrastive decoding, we dynamically\nadjust the logits generated from original image tokens and irrelevant image\ntokens, reducing the model's over-attention to irrelevant information. The\nexperimental results demonstrate that IAVA consistently outperforms existing\ndecoding techniques on benchmarks such as MME, POPE, and TextVQA in mitigating\nobject hallucinations. Our IAVA approach is available online at\nhttps://github.com/Lee-lab558/IAVA.\n","authors":["Bin Li","Dehong Gao","Yeyuan Wang","Linbo Jin","Shanqing Yu","Xiaoyan Cai","Libin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.18556v1.pdf","comment":"Accepted by ICME2025"},{"id":"http://arxiv.org/abs/2503.18553v1","updated":"2025-03-24T11:06:04Z","published":"2025-03-24T11:06:04Z","title":"ATARS: An Aerial Traffic Atomic Activity Recognition and Temporal\n  Segmentation Dataset","summary":"  Traffic Atomic Activity which describes traffic patterns for topological\nintersection dynamics is a crucial topic for the advancement of intelligent\ndriving systems. However, existing atomic activity datasets are collected from\nan egocentric view, which cannot support the scenarios where traffic activities\nin an entire intersection are required. Moreover, existing datasets only\nprovide video-level atomic activity annotations, which require exhausting\nefforts to manually trim the videos for recognition and limit their\napplications to untrimmed videos. To bridge this gap, we introduce the Aerial\nTraffic Atomic Activity Recognition and Segmentation (ATARS) dataset, the first\naerial dataset designed for multi-label atomic activity analysis. We offer\natomic activity labels for each frame, which accurately record the intervals\nfor traffic activities. Moreover, we propose a novel task, Multi-label Temporal\nAtomic Activity Recognition, enabling the study of accurate temporal\nlocalization for atomic activity and easing the burden of manual video trimming\nfor recognition. We conduct extensive experiments to evaluate existing\nstate-of-the-art models on both atomic activity recognition and temporal atomic\nactivity segmentation. The results highlight the unique challenges of our ATARS\ndataset, such as recognizing extremely small objects' activities. We further\nprovide comprehensive discussion analyzing these challenges and offer valuable\ninsights for future direction to improve recognizing atomic activity in aerial\nview. Our source code and dataset are available at\nhttps://github.com/magecliff96/ATARS/\n","authors":["Zihao Chen","Hsuanyu Wu","Chi-Hsi Kung","Yi-Ting Chen","Yan-Tsung Peng"],"pdf_url":"https://arxiv.org/pdf/2503.18553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18552v1","updated":"2025-03-24T11:05:41Z","published":"2025-03-24T11:05:41Z","title":"EvAnimate: Event-conditioned Image-to-Video Generation for Human\n  Animation","summary":"  Conditional human animation transforms a static reference image into a\ndynamic sequence by applying motion cues such as poses. These motion cues are\ntypically derived from video data but are susceptible to limitations including\nlow temporal resolution, motion blur, overexposure, and inaccuracies under\nlow-light conditions. In contrast, event cameras provide data streams with\nexceptionally high temporal resolution, a wide dynamic range, and inherent\nresistance to motion blur and exposure issues. In this work, we propose\nEvAnimate, a framework that leverages event streams as motion cues to animate\nstatic human images. Our approach employs a specialized event representation\nthat transforms asynchronous event streams into 3-channel slices with\ncontrollable slicing rates and appropriate slice density, ensuring\ncompatibility with diffusion models. Subsequently, a dual-branch architecture\ngenerates high-quality videos by harnessing the inherent motion dynamics of the\nevent streams, thereby enhancing both video quality and temporal consistency.\nSpecialized data augmentation strategies further enhance cross-person\ngeneralization. Finally, we establish a new benchmarking, including simulated\nevent data for training and validation, and a real-world event dataset\ncapturing human actions under normal and extreme scenarios. The experiment\nresults demonstrate that EvAnimate achieves high temporal fidelity and robust\nperformance in scenarios where traditional video-derived cues fall short.\n","authors":["Qiang Qu","Ming Li","Xiaoming Chen","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2503.18552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18548v1","updated":"2025-03-24T11:00:00Z","published":"2025-03-24T11:00:00Z","title":"Benchmarking Post-Hoc Unknown-Category Detection in Food Recognition","summary":"  Food recognition models often struggle to distinguish between seen and unseen\nsamples, frequently misclassifying samples from unseen categories by assigning\nthem an in-distribution (ID) label. This misclassification presents significant\nchallenges when deploying these models in real-world applications, particularly\nwithin automatic dietary assessment systems, where incorrect labels can lead to\ncascading errors throughout the system. Ideally, such models should prompt the\nuser when an unknown sample is encountered, allowing for corrective action.\nGiven no prior research exploring food recognition in real-world settings, in\nthis work we conduct an empirical analysis of various post-hoc\nout-of-distribution (OOD) detection methods for fine-grained food recognition.\nOur findings indicate that virtual logit matching (ViM) performed the best\noverall, likely due to its combination of logits and feature-space\nrepresentations. Additionally, our work reinforces prior notions in the OOD\ndomain, noting that models with higher ID accuracy performed better across the\nevaluated OOD detection methods. Furthermore, transformer-based architectures\nconsistently outperformed convolution-based models in detecting OOD samples\nacross various methods.\n","authors":["Lubnaa Abdur Rahman","Ioannis Papathanail","Lorenzo Brigato","Stavroula Mougiakakou"],"pdf_url":"https://arxiv.org/pdf/2503.18548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18544v1","updated":"2025-03-24T10:56:57Z","published":"2025-03-24T10:56:57Z","title":"Distilling Stereo Networks for Performant and Efficient Leaner Networks","summary":"  Knowledge distillation has been quite popular in vision for tasks like\nclassification and segmentation however not much work has been done for\ndistilling state-of-the-art stereo matching methods despite their range of\napplications. One of the reasons for its lack of use in stereo matching\nnetworks is due to the inherent complexity of these networks, where a typical\nnetwork is composed of multiple two- and three-dimensional modules. In this\nwork, we systematically combine the insights from state-of-the-art stereo\nmethods with general knowledge-distillation techniques to develop a joint\nframework for stereo networks distillation with competitive results and faster\ninference. Moreover, we show, via a detailed empirical analysis, that\ndistilling knowledge from the stereo network requires careful design of the\ncomplete distillation pipeline starting from backbone to the right selection of\ndistillation points and corresponding loss functions. This results in the\nstudent networks that are not only leaner and faster but give excellent\nperformance . For instance, our student network while performing better than\nthe performance oriented methods like PSMNet [1], CFNet [2], and LEAStereo [3])\non benchmark SceneFlow dataset, is 8x, 5x, and 8x faster respectively.\nFurthermore, compared to speed oriented methods having inference time less than\n100ms, our student networks perform better than all the tested methods. In\naddition, our student network also shows better generalization capabilities\nwhen tested on unseen datasets like ETH3D and Middlebury.\n","authors":["Rafia Rahim","Samuel Woerz","Andreas Zell"],"pdf_url":"https://arxiv.org/pdf/2503.18544v1.pdf","comment":"8 pages, 3 figures. Published in: 2023 International Joint Conference\n  on Neural Networks (IJCNN)"},{"id":"http://arxiv.org/abs/2503.18541v1","updated":"2025-03-24T10:51:28Z","published":"2025-03-24T10:51:28Z","title":"UniPCGC: Towards Practical Point Cloud Geometry Compression via an\n  Efficient Unified Approach","summary":"  Learning-based point cloud compression methods have made significant progress\nin terms of performance. However, these methods still encounter challenges\nincluding high complexity, limited compression modes, and a lack of support for\nvariable rate, which restrict the practical application of these methods. In\norder to promote the development of practical point cloud compression, we\npropose an efficient unified point cloud geometry compression framework, dubbed\nas UniPCGC. It is a lightweight framework that supports lossy compression,\nlossless compression, variable rate and variable complexity. First, we\nintroduce the Uneven 8-Stage Lossless Coder (UELC) in the lossless mode, which\nallocates more computational complexity to groups with higher coding\ndifficulty, and merges groups with lower coding difficulty. Second, Variable\nRate and Complexity Module (VRCM) is achieved in the lossy mode through joint\nadoption of a rate modulation module and dynamic sparse convolution. Finally,\nthrough the dynamic combination of UELC and VRCM, we achieve lossy compression,\nlossless compression, variable rate and complexity within a unified framework.\nCompared to the previous state-of-the-art method, our method achieves a\ncompression ratio (CR) gain of 8.1\\% on lossless compression, and a Bjontegaard\nDelta Rate (BD-Rate) gain of 14.02\\% on lossy compression, while also\nsupporting variable rate and variable complexity.\n","authors":["Kangli Wang","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2503.18541v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2503.18540v1","updated":"2025-03-24T10:49:55Z","published":"2025-03-24T10:49:55Z","title":"HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for\n  Building-Level Remote Sensing Applications","summary":"  Recent advances in self-supervised learning have led to the development of\nfoundation models that have significantly advanced performance in various\ncomputer vision tasks. However, despite their potential, these models often\noverlook the crucial role of high-resolution digital surface models (DSMs) in\nunderstanding urban environments, particularly for building-level analysis,\nwhich is essential for applications like digital twins. To address this gap, we\nintroduce HiRes-FusedMIM, a novel pre-trained model specifically designed to\nleverage the rich information contained within high-resolution RGB and DSM\ndata. HiRes-FusedMIM utilizes a dual-encoder simple masked image modeling\n(SimMIM) architecture with a multi-objective loss function that combines\nreconstruction and contrastive objectives, enabling it to learn powerful, joint\nrepresentations from both modalities. We conducted a comprehensive evaluation\nof HiRes-FusedMIM on a diverse set of downstream tasks, including\nclassification, semantic segmentation, and instance segmentation. Our results\ndemonstrate that: 1) HiRes-FusedMIM outperforms previous state-of-the-art\ngeospatial methods on several building-related datasets, including WHU Aerial\nand LoveDA, demonstrating its effectiveness in capturing and leveraging\nfine-grained building information; 2) Incorporating DSMs during pre-training\nconsistently improves performance compared to using RGB data alone,\nhighlighting the value of elevation information for building-level analysis; 3)\nThe dual-encoder architecture of HiRes-FusedMIM, with separate encoders for RGB\nand DSM data, significantly outperforms a single-encoder model on the Vaihingen\nsegmentation task, indicating the benefits of learning specialized\nrepresentations for each modality. To facilitate further research and\napplications in this direction, we will publicly release the trained model\nweights.\n","authors":["Guneet Mutreja","Philipp Schuegraf","Ksenia Bittner"],"pdf_url":"https://arxiv.org/pdf/2503.18540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18536v1","updated":"2025-03-24T10:42:48Z","published":"2025-03-24T10:42:48Z","title":"DiN: Diffusion Model for Robust Medical VQA with Semantic Noisy Labels","summary":"  Medical Visual Question Answering (Med-VQA) systems benefit the\ninterpretation of medical images containing critical clinical information.\nHowever, the challenge of noisy labels and limited high-quality datasets\nremains underexplored. To address this, we establish the first benchmark for\nnoisy labels in Med-VQA by simulating human mislabeling with semantically\ndesigned noise types. More importantly, we introduce the DiN framework, which\nleverages a diffusion model to handle noisy labels in Med-VQA. Unlike the\ndominant classification-based VQA approaches that directly predict answers, our\nAnswer Diffuser (AD) module employs a coarse-to-fine process, refining answer\ncandidates with a diffusion model for improved accuracy. The Answer Condition\nGenerator (ACG) further enhances this process by generating task-specific\nconditional information via integrating answer embeddings with fused\nimage-question features. To address label noise, our Noisy Label\nRefinement(NLR) module introduces a robust loss function and dynamic answer\nadjustment to further boost the performance of the AD module.\n","authors":["Erjian Guo","Zhen Zhao","Zicheng Wang","Tong Chen","Yunyi Liu","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.18536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18528v1","updated":"2025-03-24T10:35:11Z","published":"2025-03-24T10:35:11Z","title":"k-NN as a Simple and Effective Estimator of Transferability","summary":"  How well can one expect transfer learning to work in a new setting where the\ndomain is shifted, the task is different, and the architecture changes? Many\ntransfer learning metrics have been proposed to answer this question. But how\naccurate are their predictions in a realistic new setting? We conducted an\nextensive evaluation involving over 42,000 experiments comparing 23\ntransferability metrics across 16 different datasets to assess their ability to\npredict transfer performance. Our findings reveal that none of the existing\nmetrics perform well across the board. However, we find that a simple k-nearest\nneighbor evaluation -- as is commonly used to evaluate feature quality for\nself-supervision -- not only surpasses existing metrics, but also offers better\ncomputational efficiency and ease of implementation.\n","authors":["Moein Sorkhei","Christos Matsoukas","Johan Fredin Haslum","Kevin Smith"],"pdf_url":"https://arxiv.org/pdf/2503.18528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18527v1","updated":"2025-03-24T10:34:07Z","published":"2025-03-24T10:34:07Z","title":"AIM2PC: Aerial Image to 3D Building Point Cloud Reconstruction","summary":"  Three-dimensional urban reconstruction of buildings from single-view images\nhas attracted significant attention over the past two decades. However, recent\nmethods primarily focus on rooftops from aerial images, often overlooking\nessential geometrical details. Additionally, there is a notable lack of\ndatasets containing complete 3D point clouds for entire buildings, along with\nchallenges in obtaining reliable camera pose information for aerial images.\nThis paper addresses these challenges by presenting a novel methodology, AIM2PC\n, which utilizes our generated dataset that includes complete 3D point clouds\nand determined camera poses. Our approach takes features from a single aerial\nimage as input and concatenates them with essential additional conditions, such\nas binary masks and Sobel edge maps, to enable more edge-aware reconstruction.\nBy incorporating a point cloud diffusion model based on Centered denoising\nDiffusion Probabilistic Models (CDPM), we project these concatenated features\nonto the partially denoised point cloud using our camera poses at each\ndiffusion step. The proposed method is able to reconstruct the complete 3D\nbuilding point cloud, including wall information and demonstrates superior\nperformance compared to existing baseline techniques. To allow further\ncomparisons with our methodology the dataset has been made available at\nhttps://github.com/Soulaimene/AIM2PCDataset\n","authors":["Soulaimene Turki","Daniel Panangian","Houda Chaabouni-Chouayakh","Ksenia Bittner"],"pdf_url":"https://arxiv.org/pdf/2503.18527v1.pdf","comment":"Accepted to ISPRS Geospatial Week 2025"},{"id":"http://arxiv.org/abs/2412.03214v3","updated":"2025-03-24T10:29:33Z","published":"2024-12-04T11:05:01Z","title":"Continual Low-Rank Scaled Dot-product Attention","summary":"  Transformers are widely used for their ability to capture data relations in\nsequence processing, with great success for a wide range of static tasks.\nHowever, the computational and memory footprint of their main component, i.e.,\nthe Scaled Dot-product Attention, is commonly overlooked. This makes their\nadoption in applications involving stream data processing with constraints in\nresponse latency, computational and memory resources infeasible. Some works\nhave proposed methods to lower the computational cost of Transformers, i.e.\nlow-rank approximations, sparsity in attention, and efficient formulations for\nContinual Inference. In this paper, we introduce a new formulation of the\nScaled Dot-product Attention based on the Nystr\\\"om approximation that is\nsuitable for Continual Inference. In experiments on Online Audio Classification\nand Online Action Detection tasks, the proposed Continual Scaled Dot-product\nAttention can lower the number of operations by up to three orders of magnitude\ncompared to the original Transformers while retaining the predictive\nperformance of competing models.\n","authors":["Ginés Carreto Picón","Illia Oleksiienko","Lukas Hedegaard","Arian Bakhtiarnia","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2412.03214v3.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.11774v3","updated":"2025-03-24T10:25:29Z","published":"2024-10-15T16:55:10Z","title":"Fractal Calibration for long-tailed object detection","summary":"  Real-world datasets follow an imbalanced distribution, which poses\nsignificant challenges in rare-category object detection. Recent studies tackle\nthis problem by developing re-weighting and re-sampling methods, that utilise\nthe class frequencies of the dataset. However, these techniques focus solely on\nthe frequency statistics and ignore the distribution of the classes in image\nspace, missing important information. In contrast to them, we propose FRActal\nCALibration (FRACAL): a novel post-calibration method for long-tailed object\ndetection. FRACAL devises a logit adjustment method that utilises the fractal\ndimension to estimate how uniformly classes are distributed in image space.\nDuring inference, it uses the fractal dimension to inversely downweight the\nprobabilities of uniformly spaced class predictions achieving balance in two\naxes: between frequent and rare categories, and between uniformly spaced and\nsparsely spaced classes. FRACAL is a post-processing method and it does not\nrequire any training, also it can be combined with many off-the-shelf models\nsuch as one-stage sigmoid detectors and two-stage instance segmentation models.\nFRACAL boosts the rare class performance by up to 8.6% and surpasses all\nprevious methods on LVIS dataset, while showing good generalisation to other\ndatasets such as COCO, V3Det and OpenImages. We provide the code at\nhttps://github.com/kostas1515/FRACAL.\n","authors":["Konstantinos Panagiotis Alexandridis","Ismail Elezi","Jiankang Deng","Anh Nguyen","Shan Luo"],"pdf_url":"https://arxiv.org/pdf/2410.11774v3.pdf","comment":"CVPR2025 (camera-ready)"},{"id":"http://arxiv.org/abs/2411.07747v5","updated":"2025-03-24T10:22:01Z","published":"2024-11-12T12:18:18Z","title":"Constraint-Aware Feature Learning for Parametric Point Cloud","summary":"  Parametric point clouds are sampled from CAD shapes and are becoming\nincreasingly common in industrial manufacturing. Most existing CAD-specific\ndeep learning methods only focus on geometric features, while overlooking\nconstraints which are inherent and important in CAD shapes. This limits their\nability to discern CAD shapes with similar appearance but different\nconstraints. To tackle this challenge, we first analyze the constraint\nimportance via a simple validation experiment. Then, we introduce a deep\nlearning-friendly constraints representation with three vectorized components,\nand design a constraint-aware feature learning network (CstNet), which includes\ntwo stages. Stage 1 extracts constraint feature from B-Rep data or point cloud\nbased on shape local information. It enables better generalization ability to\nunseen dataset after model pre-training. Stage 2 employs attention layers to\nadaptively adjust the weights of three constraints' components. It facilitates\nthe effective utilization of constraints. In addition, we built the first\nmulti-modal parametric-purpose dataset, i.e. Param20K, comprising about 20K\nshape instances of 75 classes. On this dataset, we performed the classification\nand rotation robustness experiments, and CstNet achieved 3.52\\% and 26.17\\%\nabsolute improvements in instance accuracy over the state-of-the-art methods,\nrespectively. To the best of our knowledge, CstNet is the first\nconstraint-aware deep learning method tailored for parametric point cloud\nanalysis in CAD domain.\n","authors":["Xi Cheng","Ruiqi Lei","Di Huang","Zhichao Liao","Fengyuan Piao","Yan Chen","Pingfa Feng","Long Zeng"],"pdf_url":"https://arxiv.org/pdf/2411.07747v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11335v2","updated":"2025-03-24T10:10:38Z","published":"2025-03-14T12:03:29Z","title":"APLA: A Simple Adaptation Method for Vision Transformers","summary":"  Existing adaptation techniques typically require architectural modifications\nor added parameters, leading to high computational costs and complexity. We\nintroduce Attention Projection Layer Adaptation (APLA), a simple approach to\nadapt vision transformers (ViTs) without altering the architecture or adding\nparameters. Through a systematic analysis, we find that the layer immediately\nafter the attention mechanism is crucial for adaptation. By updating only this\nprojection layer, or even just a random subset of this layer's weights, APLA\nachieves state-of-the-art performance while reducing GPU memory usage by up to\n52.63% and training time by up to 43.0%, with no extra cost at inference.\nAcross 46 datasets covering a variety of tasks including scene classification,\nmedical imaging, satellite imaging, and fine-grained classification, APLA\nconsistently outperforms 17 other leading adaptation methods, including full\nfine-tuning, on classification, segmentation, and detection tasks. The code is\navailable at https://github.com/MoeinSorkhei/APLA.\n","authors":["Moein Sorkhei","Emir Konuk","Kevin Smith","Christos Matsoukas"],"pdf_url":"https://arxiv.org/pdf/2503.11335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18513v1","updated":"2025-03-24T10:07:46Z","published":"2025-03-24T10:07:46Z","title":"LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene","summary":"  Humans perceive and comprehend their surroundings through information\nspanning multiple frequencies. In immersive scenes, people naturally scan their\nenvironment to grasp its overall structure while examining fine details of\nobjects that capture their attention. However, current NeRF frameworks\nprimarily focus on modeling either high-frequency local views or the broad\nstructure of scenes with low-frequency information, which is limited to\nbalancing both. We introduce FA-NeRF, a novel frequency-aware framework for\nview synthesis that simultaneously captures the overall scene structure and\nhigh-definition details within a single NeRF model. To achieve this, we propose\na 3D frequency quantification method that analyzes the scene's frequency\ndistribution, enabling frequency-aware rendering. Our framework incorporates a\nfrequency grid for fast convergence and querying, a frequency-aware feature\nre-weighting strategy to balance features across different frequency contents.\nExtensive experiments show that our method significantly outperforms existing\napproaches in modeling entire scenes while preserving fine details.\n","authors":["Xiaoyu Zhang","Weihong Pan","Chong Bao","Xiyu Zhang","Xiaojun Xiang","Hanqing Jiang","Hujun Bao"],"pdf_url":"https://arxiv.org/pdf/2503.18513v1.pdf","comment":"8 pages, 6 figures. Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18512v1","updated":"2025-03-24T10:07:16Z","published":"2025-03-24T10:07:16Z","title":"Uncertainty-guided Perturbation for Image Super-Resolution Diffusion\n  Model","summary":"  Diffusion-based image super-resolution methods have demonstrated significant\nadvantages over GAN-based approaches, particularly in terms of perceptual\nquality. Building upon a lengthy Markov chain, diffusion-based methods possess\nremarkable modeling capacity, enabling them to achieve outstanding performance\nin real-world scenarios. Unlike previous methods that focus on modifying the\nnoise schedule or sampling process to enhance performance, our approach\nemphasizes the improved utilization of LR information. We find that different\nregions of the LR image can be viewed as corresponding to different timesteps\nin a diffusion process, where flat areas are closer to the target HR\ndistribution but edge and texture regions are farther away. In these flat\nareas, applying a slight noise is more advantageous for the reconstruction. We\nassociate this characteristic with uncertainty and propose to apply uncertainty\nestimate to guide region-specific noise level control, a technique we refer to\nas Uncertainty-guided Noise Weighting. Pixels with lower uncertainty (i.e.,\nflat regions) receive reduced noise to preserve more LR information, therefore\nimproving performance. Furthermore, we modify the network architecture of\nprevious methods to develop our Uncertainty-guided Perturbation\nSuper-Resolution (UPSR) model. Extensive experimental results demonstrate that,\ndespite reduced model size and training overhead, the proposed UWSR method\noutperforms current state-of-the-art methods across various datasets, both\nquantitatively and qualitatively.\n","authors":["Leheng Zhang","Weiyi You","Kexuan Shi","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2503.18512v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18507v1","updated":"2025-03-24T10:02:22Z","published":"2025-03-24T10:02:22Z","title":"Can Text-to-Video Generation help Video-Language Alignment?","summary":"  Recent video-language alignment models are trained on sets of videos, each\nwith an associated positive caption and a negative caption generated by large\nlanguage models. A problem with this procedure is that negative captions may\nintroduce linguistic biases, i.e., concepts are seen only as negatives and\nnever associated with a video. While a solution would be to collect videos for\nthe negative captions, existing databases lack the fine-grained variations\nneeded to cover all possible negatives. In this work, we study whether\nsynthetic videos can help to overcome this issue. Our preliminary analysis with\nmultiple generators shows that, while promising on some tasks, synthetic videos\nharm the performance of the model on others. We hypothesize this issue is\nlinked to noise (semantic and visual) in the generated videos and develop a\nmethod, SynViTA, that accounts for those. SynViTA dynamically weights the\ncontribution of each synthetic video based on how similar its target caption is\nw.r.t. the real counterpart. Moreover, a semantic consistency loss makes the\nmodel focus on fine-grained differences across captions, rather than\ndifferences in video appearance. Experiments show that, on average, SynViTA\nimproves over existing methods on VideoCon test sets and SSv2-Temporal,\nSSv2-Events, and ATP-Hard benchmarks, being a first promising step for using\nsynthetic videos when learning video-language models.\n","authors":["Luca Zanella","Massimiliano Mancini","Willi Menapace","Sergey Tulyakov","Yiming Wang","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2503.18507v1.pdf","comment":"CVPR 2025. Project website at https://lucazanella.github.io/synvita/"},{"id":"http://arxiv.org/abs/2406.20085v3","updated":"2025-03-24T09:58:24Z","published":"2024-06-28T17:53:18Z","title":"Auto Cherry-Picker: Learning from High-quality Generative Data Driven by\n  Language","summary":"  Diffusion models can generate realistic and diverse images, potentially\nfacilitating data availability for data-intensive perception tasks. However,\nleveraging these models to boost performance on downstream tasks with synthetic\ndata poses several challenges, including aligning with real data distribution,\nscaling synthetic sample volumes, and ensuring their quality. To bridge these\ngaps, we present \\textbf{A}uto \\textbf{C}herry-\\textbf{P}icker (ACP), a novel\nframework that generates high-quality cross-modality training samples at scale\nto augment perception and multi-modal training. ACP first uses LLMs to sample\ndescriptions and layouts based on object combinations from real data priors,\neliminating the need for ground truth image captions or annotations. Next, we\nuse an off-the-shelf controllable diffusion model to generate multiple images.\nThen, the generated data are refined using a comprehensively designed metric,\nComposite Layout and Image Score (CLIS), to ensure quality. Our customized\nsynthetic high-quality samples boost performance in various scenarios,\nespecially in addressing challenges associated with long-tailed distribution\nand imbalanced datasets. Experiment results on downstream tasks demonstrate\nthat ACP can significantly improve the performance of existing models. In\naddition, we find a positive correlation between CLIS and performance gains in\ndownstream tasks. This finding shows the potential for evaluation metrics as\nthe role for various visual perception and MLLM tasks.\n","authors":["Yicheng Chen","Xiangtai Li","Yining Li","Yanhong Zeng","Jianzong Wu","Xiangyu Zhao","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.20085v3.pdf","comment":"Accepted to CVPR2025"},{"id":"http://arxiv.org/abs/2411.19715v2","updated":"2025-03-24T09:41:55Z","published":"2024-11-29T14:02:11Z","title":"Forensics Adapter: Adapting CLIP for Generalizable Face Forgery\n  Detection","summary":"  We describe the Forensics Adapter, an adapter network designed to transform\nCLIP into an effective and generalizable face forgery detector. Although CLIP\nis highly versatile, adapting it for face forgery detection is non-trivial as\nforgery-related knowledge is entangled with a wide range of unrelated\nknowledge. Existing methods treat CLIP merely as a feature extractor, lacking\ntask-specific adaptation, which limits their effectiveness. To address this, we\nintroduce an adapter to learn face forgery traces -- the blending boundaries\nunique to forged faces, guided by task-specific objectives. Then we enhance the\nCLIP visual tokens with a dedicated interaction strategy that communicates\nknowledge across CLIP and the adapter. Since the adapter is alongside CLIP, its\nversatility is highly retained, naturally ensuring strong generalizability in\nface forgery detection. With only 5.7M trainable parameters, our method\nachieves a significant performance boost, improving by approximately 7% on\naverage across five standard datasets. We believe the proposed method can serve\nas a baseline for future CLIP-based face forgery detection methods. The code is\navailable at https://github.com/OUC-VAS/ForensicsAdapter.\n","authors":["Xinjie Cui","Yuezun Li","Ao Luo","Jiaran Zhou","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2411.19715v2.pdf","comment":"CVPR 2025"}]},"2025-03-23T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2503.18243v1","updated":"2025-03-23T23:47:46Z","published":"2025-03-23T23:47:46Z","title":"A Robot-Led Intervention for Emotion Regulation: From Expression to\n  Reappraisal","summary":"  Emotion regulation is a crucial skill for managing emotions in everyday life,\nyet finding a constructive and accessible method to support these processes\nremains challenging due to their cognitive demands. In this study, we explore\nhow regular interactions with a social robot, conducted in a structured yet\nfamiliar environment within university halls and departments, can provide\neffective support for emotion regulation through cognitive reappraisal.\nTwenty-one students participated in a five-session study at a university hall\nor department, where the robot facilitated structured conversations,\nencouraging the students to reinterpret emotionally charged situations that\nthey shared with the robot. Quantitative and qualitative results indicate\nsignificant improvements in emotion self-regulation, with participants\nreporting better understanding and control of their emotions. The intervention\nled to significant changes in constructive emotion regulation tendencies and\npositive effects on mood and sentiment after each session. The findings also\ndemonstrate that repeated interactions with the robot encouraged greater\nemotional expressiveness, including longer speech disclosures, increased use of\naffective language, and heightened facial arousal. Notably, expressiveness\nfollowed structured patterns aligned with the reappraisal process, with\nexpression peaking during key reappraisal moments, particularly when\nparticipants were prompted to reinterpret negative experiences. The qualitative\nfeedback further highlighted how the robot fostered introspection and provided\na supportive space for discussing emotions, enabling participants to confront\nlong-avoided emotional challenges. These findings demonstrate the potential of\nrobots to effectively assist in emotion regulation in familiar environments,\noffering both emotional support and cognitive guidance.\n","authors":["Guy Laban","Julie Wang","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2503.18243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18221v1","updated":"2025-03-23T21:48:26Z","published":"2025-03-23T21:48:26Z","title":"Decentralized Navigation of a Cable-Towed Load using Quadrupedal Robot\n  Team via MARL","summary":"  This work addresses the challenge of enabling a team of quadrupedal robots to\ncollaboratively tow a cable-connected load through cluttered and unstructured\nenvironments while avoiding obstacles. Leveraging cables allows the multi-robot\nsystem to navigate narrow spaces by maintaining slack when necessary. However,\nthis introduces hybrid physical interactions due to alternating taut and slack\nstates, with computational complexity that scales exponentially as the number\nof agents increases. To tackle these challenges, we developed a scalable and\ndecentralized system capable of dynamically coordinating a variable number of\nquadrupedal robots while managing the hybrid physical interactions inherent in\nthe load-towing task. At the core of this system is a novel multi-agent\nreinforcement learning (MARL)-based planner, designed for decentralized\ncoordination. The MARL-based planner is trained using a centralized training\nwith decentralized execution (CTDE) framework, enabling each robot to make\ndecisions autonomously using only local (ego) observations. To accelerate\nlearning and ensure effective collaboration across varying team sizes, we\nintroduce a tailored training curriculum for MARL. Experimental results\nhighlight the flexibility and scalability of the framework, demonstrating\nsuccessful deployment with one to four robots in real-world scenarios and up to\ntwelve robots in simulation. The decentralized planner maintains consistent\ninference times, regardless of the team size. Additionally, the proposed system\ndemonstrates robustness to environment perturbations and adaptability to\nvarying load weights. This work represents a step forward in achieving flexible\nand efficient multi-legged robotic collaboration in complex and real-world\nenvironments.\n","authors":["Wen-Tse Chen","Minh Nguyen","Zhongyu Li","Guo Ning Sue","Koushil Sreenath"],"pdf_url":"https://arxiv.org/pdf/2503.18221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01227v3","updated":"2025-03-23T20:33:31Z","published":"2023-12-02T21:10:06Z","title":"Distributed Bayesian Estimation in Sensor Networks: Consensus on\n  Marginal Densities","summary":"  In this paper, we aim to design and analyze distributed Bayesian estimation\nalgorithms for sensor networks. The challenges we address are to (i) derive a\ndistributed provably-correct algorithm in the functional space of probability\ndistributions over continuous variables, and (ii) leverage these results to\nobtain new distributed estimators restricted to subsets of variables observed\nby individual agents. This relates to applications such as cooperative\nlocalization and federated learning, where the data collected at any agent\ndepends on a subset of all variables of interest. We present Bayesian density\nestimation algorithms using data from non-linear likelihoods at agents in\ncentralized, distributed, and marginal distributed settings. After setting up a\ndistributed estimation objective, we prove almost-sure convergence to the\noptimal set of pdfs at each agent. Then, we prove the same for a storage-aware\nalgorithm estimating densities only over relevant variables at each agent.\nFinally, we present a Gaussian version of these algorithms and implement it in\na mapping problem using variational inference to handle non-linear likelihood\nmodels associated with LiDAR sensing.\n","authors":["Parth Paritosh","Nikolay Atanasov","Sonia Martinez"],"pdf_url":"https://arxiv.org/pdf/2312.01227v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18192v1","updated":"2025-03-23T20:22:14Z","published":"2025-03-23T20:22:14Z","title":"Extended Visibility of Autonomous Vehicles via Optimized Cooperative\n  Perception under Imperfect Communication","summary":"  Autonomous Vehicles (AVs) rely on individual perception systems to navigate\nsafely. However, these systems face significant challenges in adverse weather\nconditions, complex road geometries, and dense traffic scenarios. Cooperative\nPerception (CP) has emerged as a promising approach to extending the perception\nquality of AVs by jointly processing shared camera feeds and sensor readings\nacross multiple vehicles. This work presents a novel CP framework designed to\noptimize vehicle selection and networking resource utilization under imperfect\ncommunications. Our optimized CP formation considers critical factors such as\nthe helper vehicles' spatial position, visual range, motion blur, and available\ncommunication budgets. Furthermore, our resource optimization module allocates\ncommunication channels while adjusting power levels to maximize data flow\nefficiency between the ego and helper vehicles, considering realistic models of\nmodern vehicular communication systems, such as LTE and 5G NR-V2X. We validate\nour approach through extensive experiments on pedestrian detection in\nchallenging scenarios, using synthetic data generated by the CARLA simulator.\nThe results demonstrate that our method significantly improves upon the\nperception quality of individual AVs with about 10% gain in detection accuracy.\nThis substantial gain uncovers the unleashed potential of CP to enhance AV\nsafety and performance in complex situations.\n","authors":["Ahmad Sarlak","Rahul Amin","Abolfazl Razi"],"pdf_url":"https://arxiv.org/pdf/2503.18192v1.pdf","comment":"55 pages, 13 figures, 3 tables, Elsevier Journal"},{"id":"http://arxiv.org/abs/2503.18187v1","updated":"2025-03-23T19:54:08Z","published":"2025-03-23T19:54:08Z","title":"Joint State-Parameter Observer-Based Robust Control of a UAV for Heavy\n  Load Transportation","summary":"  This paper proposes a joint state-parameter observer-based controller for\ntrajectory tracking of an octocopter unmanned aerial vehicle (OUAV), for\ntransportation of a heavy load with unknown mass and size. The multi-body\ndynamic model of the OUAV with a rigidly attached load is obtained, effectively\nconsidering the effects of the load parameters into the dynamics of the system.\nA robust nonlinear W-infinity control strategy is designed for optimal\ntrajectory tracking of the OUAV, with information of the states and load\nparameters provided by a joint estimation unscented Kalman filter. The\neffectiveness of the proposed strategy is corroborated by numerical results.\n","authors":["Brenner S. Rego","Daniel N. Cardoso","Marco. H. Terra","Guilherme V. Raffo"],"pdf_url":"https://arxiv.org/pdf/2503.18187v1.pdf","comment":"12 pages, 3 figures. This is a preprint of a paper presented at the\n  2023 Conference on Climbing and Walking Robots (CLAWAR) and published later\n  by Springer Nature Switzerland"},{"id":"http://arxiv.org/abs/2409.04965v2","updated":"2025-03-23T19:45:11Z","published":"2024-09-08T04:04:21Z","title":"Socially-Aware Robot Navigation Enhanced by Bidirectional Natural\n  Language Conversations Using Large Language Models","summary":"  Robot navigation is crucial across various domains, yet traditional methods\nfocus on efficiency and obstacle avoidance, often overlooking human behavior in\nshared spaces. With the rise of service robots, socially aware navigation has\ngained prominence. However, existing approaches primarily predict pedestrian\nmovements or issue alerts, lacking true human-robot interaction. We introduce\nHybrid Soft Actor-Critic with Large Language Model (HSAC-LLM), a novel\nframework for socially aware navigation. By integrating deep reinforcement\nlearning with large language models, HSAC-LLM enables bidirectional natural\nlanguage interactions, predicting both continuous and discrete navigation\nactions. When potential collisions arise, the robot proactively communicates\nwith pedestrians to determine avoidance strategies. Experiments in 2D\nsimulation, Gazebo, and real-world environments demonstrate that HSAC-LLM\noutperforms state-of-the-art DRL methods in interaction, navigation, and\nobstacle avoidance. This paradigm advances effective human-robot interactions\nin dynamic settings. Videos are available at https://hsacllm.github.io/.\n","authors":["Congcong Wen","Yifan Liu","Geeta Chandra Raju Bethala","Shuaihang Yuan","Hao Huang","Yu Hao","Mengyu Wang","Yu-Shen Liu","Anthony Tzes","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2409.04965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13542v2","updated":"2025-03-23T17:57:04Z","published":"2024-05-22T11:26:09Z","title":"Towards Safe Mid-Air Drone Interception: Strategies for Tracking &\n  Capture","summary":"  A unique approach for the mid-air autonomous aerial interception of\nnon-cooperating UAV by a flying robot equipped with a net is presented in this\npaper. A novel interception guidance method dubbed EPN is proposed, designed to\ncatch agile maneuvering targets while relying on onboard state estimation and\ntracking. The proposed method is compared with state-of-the-art approaches in\nsimulations using 100 different trajectories of the target with varying\ncomplexity comprising almost 14 hours of flight data, and EPN demonstrates the\nshortest response time and the highest number of interceptions, which are key\nparameters of agile interception. To enable robust transfer from theory and\nsimulation to a real-world implementation, we aim to avoid overfitting to\nspecific assumptions about the target, and to tackle interception of a target\nfollowing an unknown general trajectory. Furthermore, we identify several often\noverlooked problems related to tracking and estimation of the target's state\nthat can have a significant influence on the overall performance of the system.\nWe propose the use of a novel state estimation filter based on the IMM filter\nand a new measurement model. Simulated experiments show that the proposed\nsolution provides significant improvements in estimation accuracy over the\ncommonly employed KF approaches when considering general trajectories. Based on\nthese results, we employ the proposed filtering and guidance methods to\nimplement a complete autonomous interception system, which is thoroughly\nevaluated in realistic simulations and tested in real-world experiments with a\nmaneuvering target going far beyond the performance of any state-of-the-art\nsolution.\n","authors":["Michal Pliska","Matouš Vrba","Tomáš Báča","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2405.13542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18108v1","updated":"2025-03-23T15:27:43Z","published":"2025-03-23T15:27:43Z","title":"Unraveling the Effects of Synthetic Data on End-to-End Autonomous\n  Driving","summary":"  End-to-end (E2E) autonomous driving (AD) models require diverse, high-quality\ndata to perform well across various driving scenarios. However, collecting\nlarge-scale real-world data is expensive and time-consuming, making\nhigh-fidelity synthetic data essential for enhancing data diversity and model\nrobustness. Existing driving simulators for synthetic data generation have\nsignificant limitations: game-engine-based simulators struggle to produce\nrealistic sensor data, while NeRF-based and diffusion-based methods face\nefficiency challenges. Additionally, recent simulators designed for closed-loop\nevaluation provide limited interaction with other vehicles, failing to simulate\ncomplex real-world traffic dynamics. To address these issues, we introduce\nSceneCrafter, a realistic, interactive, and efficient AD simulator based on 3D\nGaussian Splatting (3DGS). SceneCrafter not only efficiently generates\nrealistic driving logs across diverse traffic scenarios but also enables robust\nclosed-loop evaluation of end-to-end models. Experimental results demonstrate\nthat SceneCrafter serves as both a reliable evaluation platform and a efficient\ndata generator that significantly improves end-to-end model generalization.\n","authors":["Junhao Ge","Zuhong Liu","Longteng Fan","Yifan Jiang","Jiaqi Su","Yiming Li","Zhejun Zhang","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2503.18108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11911v2","updated":"2025-03-23T14:46:55Z","published":"2024-11-17T16:36:09Z","title":"ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode\n  Modeling","summary":"  Anticipating the multimodality of future events lays the foundation for safe\nautonomous driving. However, multimodal motion prediction for traffic agents\nhas been clouded by the lack of multimodal ground truth. Existing works\npredominantly adopt the winner-take-all training strategy to tackle this\nchallenge, yet still suffer from limited trajectory diversity and uncalibrated\nmode confidence. While some approaches address these limitations by generating\nexcessive trajectory candidates, they necessitate a post-processing stage to\nidentify the most representative modes, a process lacking universal principles\nand compromising trajectory accuracy. We are thus motivated to introduce\nModeSeq, a new multimodal prediction paradigm that models modes as sequences.\nUnlike the common practice of decoding multiple plausible trajectories in one\nshot, ModeSeq requires motion decoders to infer the next mode step by step,\nthereby more explicitly capturing the correlation between modes and\nsignificantly enhancing the ability to reason about multimodality. Leveraging\nthe inductive bias of sequential mode prediction, we also propose the\nEarly-Match-Take-All (EMTA) training strategy to diversify the trajectories\nfurther. Without relying on dense mode prediction or heuristic post-processing,\nModeSeq considerably improves the diversity of multimodal output while\nattaining satisfactory trajectory accuracy, resulting in balanced performance\non motion prediction benchmarks. Moreover, ModeSeq naturally emerges with the\ncapability of mode extrapolation, which supports forecasting more behavior\nmodes when the future is highly uncertain.\n","authors":["Zikang Zhou","Hengjian Zhou","Haibo Hu","Zihao Wen","Jianping Wang","Yung-Hui Li","Yu-Kai Huang"],"pdf_url":"https://arxiv.org/pdf/2411.11911v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18073v1","updated":"2025-03-23T13:45:39Z","published":"2025-03-23T13:45:39Z","title":"PanopticSplatting: End-to-End Panoptic Gaussian Splatting","summary":"  Open-vocabulary panoptic reconstruction is a challenging task for\nsimultaneous scene reconstruction and understanding. Recently, methods have\nbeen proposed for 3D scene understanding based on Gaussian splatting. However,\nthese methods are multi-staged, suffering from the accumulated errors and the\ndependence of hand-designed components. To streamline the pipeline and achieve\nglobal optimization, we propose PanopticSplatting, an end-to-end system for\nopen-vocabulary panoptic reconstruction. Our method introduces query-guided\nGaussian segmentation with local cross attention, lifting 2D instance masks\nwithout cross-frame association in an end-to-end way. The local cross attention\nwithin view frustum effectively reduces the training memory, making our model\nmore accessible to large scenes with more Gaussians and objects. In addition,\nto address the challenge of noisy labels in 2D pseudo masks, we propose label\nblending to promote consistent 3D segmentation with less noisy floaters, as\nwell as label warping on 2D predictions which enhances multi-view coherence and\nsegmentation accuracy. Our method demonstrates strong performances in 3D scene\npanoptic reconstruction on the ScanNet-V2 and ScanNet++ datasets, compared with\nboth NeRF-based and Gaussian-based panoptic reconstruction methods. Moreover,\nPanopticSplatting can be easily generalized to numerous variants of Gaussian\nsplatting, and we demonstrate its robustness on different Gaussian base models.\n","authors":["Yuxuan Xie","Xuan Yu","Changjian Jiang","Sitong Mao","Shunbo Zhou","Rui Fan","Rong Xiong","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2503.18073v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.18065v1","updated":"2025-03-23T13:18:17Z","published":"2025-03-23T13:18:17Z","title":"Unseen from Seen: Rewriting Observation-Instruction Using Foundation\n  Models for Augmenting Vision-Language Navigation","summary":"  Data scarcity is a long-standing challenge in the Vision-Language Navigation\n(VLN) field, which extremely hinders the generalization of agents to unseen\nenvironments. Previous works primarily rely on additional simulator data or\nweb-collected images/videos to improve the generalization. However, the\nsimulator environments still face limited diversity, and the web-collected data\noften requires extensive labor to remove the noise. In this paper, we propose a\nRewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates\nthe unseen observation-instruction pairs via rewriting human-annotated training\ndata. Benefiting from our rewriting mechanism, new observation-instruction can\nbe obtained in both simulator-free and labor-saving manners to promote\ngeneralization. Specifically, we first introduce Object-Enriched Observation\nRewriting, where we combine Vision-Language Models (VLMs) and Large Language\nModels (LLMs) to derive rewritten object-enriched scene descriptions, enabling\nobservation synthesis with diverse objects and spatial layouts via\nText-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast\nInstruction Rewriting, which generates observation-aligned rewritten\ninstructions by requiring LLMs to reason the difference between original and\nnew observations. We further develop a mixing-then-focusing training strategy\nwith a random observation cropping scheme, effectively enhancing data\ndistribution diversity while suppressing augmentation data noise during\ntraining. Experiments on both the discrete environments (R2R, REVERIE, and R4R\ndatasets) and continuous environments (R2R-CE dataset) show the superior\nperformance and impressive generalization ability of our method. Code is\navailable at https://github.com/SaDil13/VLN-RAM.\n","authors":["Ziming Wei","Bingqian Lin","Yunshuang Nie","Jiaqi Chen","Shikui Ma","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2503.18065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18051v1","updated":"2025-03-23T12:38:02Z","published":"2025-03-23T12:38:02Z","title":"Assist-as-needed Hip Exoskeleton Control for Gait Asymmetry Correction\n  via Human-in-the-loop Optimization","summary":"  Gait asymmetry is a significant clinical characteristic of hemiplegic gait\nthat most stroke survivors suffer, leading to limited mobility and long-term\nnegative impacts on their quality of life. Although a variety of exoskeleton\ncontrols have been developed for robot-assisted gait rehabilitation, little\nattention has been paid to correcting the gait asymmetry of stroke patients\nfollowing the assist-as-need (AAN) principle, and it is still challenging to\nproperly share control between the exoskeleton and stroke patients with partial\nmotor control. In view of this, this article proposes an AAN hip exoskeleton\ncontrol with human-in-the-loop optimization to correct gait asymmetry in stroke\npatients. To realize the AAN concept, an objective function was designed for\nreal-time evaluation of the subject's gait performance and active\nparticipation, which considers the variability of natural human movement and\nguides the online tuning of control parameters on a subject-specific basis. In\nthis way, patients were stimulated to contribute as much as possible to\nmovement, thus maximizing the efficiency and outcomes of post-stroke gait\nrehabilitation. Finally, an experimental study was conducted to verify the\nfeasibility and effectiveness of the proposed AAN control on healthy subjects\nwith artificial gait impairment. For the first time, the common hypothesis that\nAAN controls can improve human active participation was validated from the\nbiomechanics viewpoint.\n","authors":["Yuepeng Qian","Jingfeng Xiong","Haoyong Yu","Chenglong Fu"],"pdf_url":"https://arxiv.org/pdf/2503.18051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04565v2","updated":"2025-03-23T11:58:13Z","published":"2025-03-06T15:53:42Z","title":"Omnidirectional Multi-Object Tracking","summary":"  Panoramic imagery, with its 360{\\deg} field of view, offers comprehensive\ninformation to support Multi-Object Tracking (MOT) in capturing spatial and\ntemporal relationships of surrounding objects. However, most MOT algorithms are\ntailored for pinhole images with limited views, impairing their effectiveness\nin panoramic settings. Additionally, panoramic image distortions, such as\nresolution loss, geometric deformation, and uneven lighting, hinder direct\nadaptation of existing MOT methods, leading to significant performance\ndegradation. To address these challenges, we propose OmniTrack, an\nomnidirectional MOT framework that incorporates Tracklet Management to\nintroduce temporal cues, FlexiTrack Instances for object localization and\nassociation, and the CircularStatE Module to alleviate image and geometric\ndistortions. This integration enables tracking in panoramic field-of-view\nscenarios, even under rapid sensor motion. To mitigate the lack of panoramic\nMOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic\ndataset collected by a quadruped robot, featuring diverse challenges such as\npanoramic fields of view, intense motion, and complex environments. Extensive\nexperiments on the public JRDB dataset and the newly introduced QuadTrack\nbenchmark demonstrate the state-of-the-art performance of the proposed\nframework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an\nimprovement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the\nbaseline by 6.81%. The established dataset and source code are available at\nhttps://github.com/xifen523/OmniTrack.\n","authors":["Kai Luo","Hao Shi","Sheng Wu","Fei Teng","Mengfei Duan","Chang Huang","Yuhang Wang","Kaiwei Wang","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2503.04565v2.pdf","comment":"Accepted to CVPR 2025. The established dataset and source code are\n  available at https://github.com/xifen523/OmniTrack"},{"id":"http://arxiv.org/abs/2503.17985v1","updated":"2025-03-23T08:38:13Z","published":"2025-03-23T08:38:13Z","title":"Optimizing Navigation And Chemical Application in Precision Agriculture\n  With Deep Reinforcement Learning And Conditional Action Tree","summary":"  This paper presents a novel reinforcement learning (RL)-based planning scheme\nfor optimized robotic management of biotic stresses in precision agriculture.\nThe framework employs a hierarchical decision-making structure with conditional\naction masking, where high-level actions direct the robot's exploration, while\nlow-level actions optimize its navigation and efficient chemical spraying in\naffected areas. The key objectives of optimization include improving the\ncoverage of infected areas with limited battery power and reducing chemical\nusage, thus preventing unnecessary spraying of healthy areas of the field. Our\nnumerical experimental results demonstrate that the proposed method,\nHierarchical Action Masking Proximal Policy Optimization (HAM-PPO),\nsignificantly outperforms baseline practices, such as LawnMower navigation +\nindiscriminate spraying (Carpet Spray), in terms of yield recovery and resource\nefficiency. HAM-PPO consistently achieves higher yield recovery percentages and\nlower chemical costs across a range of infection scenarios. The framework also\nexhibits robustness to observation noise and generalizability under diverse\nenvironmental conditions, adapting to varying infection ranges and spatial\ndistribution patterns.\n","authors":["Mahsa Khosravi","Zhanhong Jiang","Joshua R Waite","Sarah Jonesc","Hernan Torres","Arti Singh","Baskar Ganapathysubramanian","Asheesh Kumar Singh","Soumik Sarkar"],"pdf_url":"https://arxiv.org/pdf/2503.17985v1.pdf","comment":"32 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.06960v2","updated":"2025-03-23T08:34:06Z","published":"2025-03-10T06:18:31Z","title":"A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning","summary":"  Pre-trained vision models (PVMs) are fundamental to modern robotics, yet\ntheir optimal configuration remains unclear. Through systematic evaluation, we\nfind that while DINO and iBOT outperform MAE across visuomotor control and\nperception tasks, they struggle when trained on non-(single-)object-centric\n(NOC) data--a limitation strongly correlated with their diminished ability to\nlearn object-centric representations. This investigation indicates that the\nability to form object-centric representations from the non-object-centric\nrobotics dataset is the key to success for PVMs. Motivated by this discovery,\nwe designed SlotMIM, a method that induces object-centric representations by\nintroducing a semantic bottleneck to reduce the number of prototypes to\nencourage the emergence of objectness as well as cross-view consistency\nregularization for encouraging multiview invariance. Our experiments encompass\npre-training on object-centric, scene-centric, web-crawled, and ego-centric\ndata. Across all settings, our approach learns transferrable representations\nand achieves significant improvements over prior work in image recognition,\nscene understanding, and robot learning evaluations. When scaled up with\nmillion-scale datasets, our method also demonstrates superior data efficiency\nand scalability. Our code and models are publicly available at\nhttps://github.com/CVMI-Lab/SlotMIM.\n","authors":["Xin Wen","Bingchen Zhao","Yilun Chen","Jiangmiao Pang","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2503.06960v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2411.09627v2","updated":"2025-03-23T08:02:09Z","published":"2024-11-14T17:54:43Z","title":"One-Shot Manipulation Strategy Learning by Making Contact Analogies","summary":"  We present a novel approach, MAGIC (manipulation analogies for generalizable\nintelligent contacts), for one-shot learning of manipulation strategies with\nfast and extensive generalization to novel objects. By leveraging a reference\naction trajectory, MAGIC effectively identifies similar contact points and\nsequences of actions on novel objects to replicate a demonstrated strategy,\nsuch as using different hooks to retrieve distant objects of different shapes\nand sizes. Our method is based on a two-stage contact-point matching process\nthat combines global shape matching using pretrained neural features with local\ncurvature analysis to ensure precise and physically plausible contact points.\nWe experiment with three tasks including scooping, hanging, and hooking\nobjects. MAGIC demonstrates superior performance over existing methods,\nachieving significant improvements in runtime speed and generalization to\ndifferent object categories. Website: https://magic-2024.github.io/ .\n","authors":["Yuyao Liu","Jiayuan Mao","Joshua Tenenbaum","Tomás Lozano-Pérez","Leslie Pack Kaelbling"],"pdf_url":"https://arxiv.org/pdf/2411.09627v2.pdf","comment":"ICRA 2025; CoRL LEAP Workshop, 2024"},{"id":"http://arxiv.org/abs/2409.08493v2","updated":"2025-03-23T07:53:53Z","published":"2024-09-13T02:37:28Z","title":"Intelligent LiDAR Navigation: Leveraging External Information and\n  Semantic Maps with LLM as Copilot","summary":"  Traditional robot navigation systems primarily utilize occupancy grid maps\nand laser-based sensing technologies, as demonstrated by the popular move_base\npackage in ROS. Unlike robots, humans navigate not only through spatial\nawareness and physical distances but also by integrating external information,\nsuch as elevator maintenance updates from public notification boards and\nexperiential knowledge, like the need for special access through certain doors.\nWith the development of Large Language Models (LLMs), which possesses text\nunderstanding and intelligence close to human performance, there is now an\nopportunity to infuse robot navigation systems with a level of understanding\nakin to human cognition. In this study, we propose using osmAG (Area Graph in\nOpensStreetMap textual format), an innovative semantic topometric hierarchical\nmap representation, to bridge the gap between the capabilities of ROS move_base\nand the contextual understanding offered by LLMs. Our methodology employs LLMs\nas an actual copilot in robot navigation, enabling the integration of a broader\nrange of informational inputs while maintaining the robustness of traditional\nrobotic navigation systems. Our code, demo, map, experiment results can be\naccessed at\nhttps://github.com/xiexiexiaoxiexie/Intelligent-LiDAR-Navigation-LLM-as-Copilot.\n","authors":["Fujing Xie","Jiajie Zhang","Sören Schwertfeger"],"pdf_url":"https://arxiv.org/pdf/2409.08493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17973v1","updated":"2025-03-23T07:49:19Z","published":"2025-03-23T07:49:19Z","title":"PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable\n  Objects from Videos","summary":"  Creating a physical digital twin of a real-world object has immense potential\nin robotics, content creation, and XR. In this paper, we present PhysTwin, a\nnovel framework that uses sparse videos of dynamic objects under interaction to\nproduce a photo- and physically realistic, real-time interactive virtual\nreplica. Our approach centers on two key components: (1) a physics-informed\nrepresentation that combines spring-mass models for realistic physical\nsimulation, generative shape models for geometry, and Gaussian splats for\nrendering; and (2) a novel multi-stage, optimization-based inverse modeling\nframework that reconstructs complete geometry, infers dense physical\nproperties, and replicates realistic appearance from videos. Our method\nintegrates an inverse physics framework with visual perception cues, enabling\nhigh-fidelity reconstruction even from partial, occluded, and limited\nviewpoints. PhysTwin supports modeling various deformable objects, including\nropes, stuffed animals, cloth, and delivery packages. Experiments show that\nPhysTwin outperforms competing methods in reconstruction, rendering, future\nprediction, and simulation under novel interactions. We further demonstrate its\napplications in interactive real-time simulation and model-based robotic motion\nplanning.\n","authors":["Hanxiao Jiang","Hao-Yu Hsu","Kaifeng Zhang","Hsin-Ni Yu","Shenlong Wang","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2503.17973v1.pdf","comment":"Project Page: https://jianghanxiao.github.io/phystwin-web/"},{"id":"http://arxiv.org/abs/2408.10162v3","updated":"2025-03-23T05:46:48Z","published":"2024-08-19T17:16:35Z","title":"Physics-Aware Combinatorial Assembly Sequence Planning using Data-free\n  Action Masking","summary":"  Combinatorial assembly uses standardized unit primitives to build objects\nthat satisfy user specifications. This paper studies assembly sequence planning\n(ASP) for physical combinatorial assembly. Given the shape of the desired\nobject, the goal is to find a sequence of actions for placing unit primitives\nto build the target object. In particular, we aim to ensure the planned\nassembly sequence is physically executable. However, ASP for combinatorial\nassembly is particularly challenging due to its combinatorial nature. To\naddress the challenge, we employ deep reinforcement learning to learn a\nconstruction policy for placing unit primitives sequentially to build the\ndesired object. Specifically, we design an online physics-aware action mask\nthat filters out invalid actions, which effectively guides policy learning and\nensures violation-free deployment. In the end, we apply the proposed method to\nLego assembly with more than 250 3D structures. The experiment results\ndemonstrate that the proposed method plans physically valid assembly sequences\nto build all structures, achieving a $100\\%$ success rate, whereas the best\ncomparable baseline fails more than $40$ structures. Our implementation is\navailable at\n\\url{https://github.com/intelligent-control-lab/PhysicsAwareCombinatorialASP}.\n","authors":["Ruixuan Liu","Alan Chen","Weiye Zhao","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2408.10162v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15959v6","updated":"2025-03-23T05:03:59Z","published":"2024-10-21T12:43:54Z","title":"Diffusion Transformer Policy","summary":"  Recent large vision-language-action models pretrained on diverse robot\ndatasets have demonstrated the potential for generalizing to new environments\nwith a few in-domain data. However, those approaches usually predict individual\ndiscretized or continuous action by a small action head, which limits the\nability in handling diverse action spaces. In contrast, we model the continuous\naction sequence with a large multi-modal diffusion transformer, dubbed as\nDiffusion Transformer Policy, in which we directly denoise action chunks by a\nlarge transformer model rather than a small action head for action embedding.\nBy leveraging the scaling capability of transformers, the proposed approach can\neffectively model continuous end-effector actions across large diverse robot\ndatasets, and achieve better generalization performance. Extensive experiments\ndemonstrate the effectiveness and generalization of Diffusion Transformer\nPolicy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world\nFranka arm, achieving consistent better performance on Real-to-Sim benchmark\nSimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo.\nSpecifically, without bells and whistles, the proposed approach achieves\nstate-of-the-art performance with only a single third-view camera stream in the\nCalvin task ABC->D, improving the average number of tasks completed in a row of\n5 to 3.6, and the pretraining stage significantly facilitates the success\nsequence length on the Calvin by over 1.2.\n","authors":["Zhi Hou","Tianyi Zhang","Yuwen Xiong","Hengjun Pu","Chengyang Zhao","Ronglei Tong","Yu Qiao","Jifeng Dai","Yuntao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15959v6.pdf","comment":"preprint; New Project Page: https://robodita.github.io; revert\n  unsuitable replacement"},{"id":"http://arxiv.org/abs/2409.15633v2","updated":"2025-03-23T03:53:07Z","published":"2024-09-24T00:33:43Z","title":"Intent Prediction-Driven Model Predictive Control for UAV Planning and\n  Navigation in Dynamic Environments","summary":"  Aerial robots can enhance construction site productivity by autonomously\nhandling inspection and mapping tasks. However, ensuring safe navigation near\nhuman workers remains challenging. While navigation in static environments has\nbeen well studied, navigating dynamic environments remains open due to\nchallenges in perception and planning. Payload limitations restrict the robots\nto using cameras with limited fields of view, resulting in unreliable\nperception and tracking during collision avoidance. Moreover, the rapidly\nchanging conditions of dynamic environments can quickly make the generated\noptimal trajectory outdated.To address these challenges, this paper presents a\ncomprehensive navigation framework that integrates perception, intent\nprediction, and planning. Our perception module detects and tracks dynamic\nobstacles efficiently and handles tracking loss and occlusion during collision\navoidance. The proposed intent prediction module employs a Markov Decision\nProcess (MDP) to forecast potential actions of dynamic obstacles with the\npossible future trajectories. Finally, a novel intent-based planning algorithm,\nleveraging model predictive control (MPC), is applied to generate navigation\ntrajectories. Simulation and physical experiments demonstrate that our method\nimproves the safety of navigation by achieving the fewest collisions compared\nto benchmarks.\n","authors":["Zhefan Xu","Hanyu Jin","Xinming Han","Haoyu Shen","Kenji Shimada"],"pdf_url":"https://arxiv.org/pdf/2409.15633v2.pdf","comment":"8 pages, 7 figures, 2 tables, experiment video:\n  https://youtu.be/4xsEeMB9WPY, GitHub: https://github.com/Zhefan-Xu/Intent-MPC"},{"id":"http://arxiv.org/abs/2411.18562v4","updated":"2025-03-23T03:51:01Z","published":"2024-11-27T18:03:26Z","title":"DexDiffuser: Interaction-aware Diffusion Planning for Adaptive Dexterous\n  Manipulation","summary":"  Dexterous manipulation with contact-rich interactions is crucial for advanced\nrobotics. While recent diffusion-based planning approaches show promise for\nsimple manipulation tasks, they often produce unrealistic ghost states (e.g.,\nthe object automatically moves without hand contact) or lack adaptability when\nhandling complex sequential interactions. In this work, we introduce\nDexDiffuser, an interaction-aware diffusion planning framework for adaptive\ndexterous manipulation. DexDiffuser models joint state-action dynamics through\na dual-phase diffusion process which consists of pre-interaction contact\nalignment and post-contact goal-directed control, enabling goal-adaptive\ngeneralizable dexterous manipulation. Additionally, we incorporate dynamics\nmodel-based dual guidance and leverage large language models for automated\nguidance function generation, enhancing generalizability for physical\ninteractions and facilitating diverse goal adaptation through language cues.\nExperiments on physical interaction tasks such as door opening, pen and block\nre-orientation, object relocation, and hammer striking demonstrate\nDexDiffuser's effectiveness on goals outside training distributions, achieving\nover twice the average success rate (59.2% vs. 29.5%) compared to existing\nmethods. Our framework achieves an average of 70.7% success rate on goal\nadaptive dexterous tasks, highlighting its robustness and flexibility in\ncontact-rich manipulation.\n","authors":["Zhixuan Liang","Yao Mu","Yixiao Wang","Tianxing Chen","Wenqi Shao","Wei Zhan","Masayoshi Tomizuka","Ping Luo","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2411.18562v4.pdf","comment":"Camera ready version. 27 pages. Project page:\n  https://dexdiffuser.github.io/"},{"id":"http://arxiv.org/abs/2503.17902v1","updated":"2025-03-23T02:29:21Z","published":"2025-03-23T02:29:21Z","title":"Adaptive Koopman Model Predictive Control of Simple Serial Robots","summary":"  Approximating nonlinear systems as linear ones is a common workaround to\napply control tools tailored for linear systems. This motivates our present\nwork where we developed a data-driven model predictive controller (MPC) based\non the Koopman operator framework, allowing the embedding of nonlinear dynamics\nin a higher dimensional, but linear function space. The controller, termed\nadaptive Koopman model predictive control (KMPC), uses online closed-loop\nfeedback to learn and incrementally update a linear representation of nonlinear\nsystem dynamics, without the prior knowledge of a model. Adaptive KMPC differs\nfrom most other Koopman-based control frameworks that aim to identify\nhigh-validity-range models in advance and then enter closed-loop control\nwithout further model adaptations. To validate the controller, trajectory\ntracking experiments are conducted with 1R and 2R robots under force\ndisturbances and changing model parameters. We compare the controller to\nclassical linearization MPC and Koopman-based MPC without model updates,\ndenoted static KMPC. The results show that adaptive KMPC can, opposed to static\nKMPC, generalize over unforeseen force disturbances and can, opposed to\nlinearization MPC, handle varying dynamic parameters, while using a small set\nof basis functions to approximate the Koopman operator.\n","authors":["Adriano del Río","Christoph Stoeffler"],"pdf_url":"https://arxiv.org/pdf/2503.17902v1.pdf","comment":"Preprint submitted to IROS 2025; See supplementary material at\n  https://github.com/adrianodelr/adaptive-koopman-mpc"},{"id":"http://arxiv.org/abs/2503.18988v1","updated":"2025-03-23T09:11:04Z","published":"2025-03-23T09:11:04Z","title":"SG-Tailor: Inter-Object Commonsense Relationship Reasoning for Scene\n  Graph Manipulation","summary":"  Scene graphs capture complex relationships among objects, serving as strong\npriors for content generation and manipulation. Yet, reasonably manipulating\nscene graphs -- whether by adding nodes or modifying edges -- remains a\nchallenging and untouched task. Tasks such as adding a node to the graph or\nreasoning about a node's relationships with all others are computationally\nintractable, as even a single edge modification can trigger conflicts due to\nthe intricate interdependencies within the graph. To address these challenges,\nwe introduce SG-Tailor, an autoregressive model that predicts the conflict-free\nrelationship between any two nodes. SG-Tailor not only infers inter-object\nrelationships, including generating commonsense edges for newly added nodes but\nalso resolves conflicts arising from edge modifications to produce coherent,\nmanipulated graphs for downstream tasks. For node addition, the model queries\nthe target node and other nodes from the graph to predict the appropriate\nrelationships. For edge modification, SG-Tailor employs a Cut-And-Stitch\nstrategy to solve the conflicts and globally adjust the graph. Extensive\nexperiments demonstrate that SG-Tailor outperforms competing methods by a large\nmargin and can be seamlessly integrated as a plug-in module for scene\ngeneration and robotic manipulation tasks.\n","authors":["Haoliang Shang","Hanyu Wu","Guangyao Zhai","Boyang Sun","Fangjinhua Wang","Federico Tombari","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2503.18988v1.pdf","comment":"The code will be available at https://github.com/josef5838/SG-Tailor"}]},"2025-03-22T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2503.06953v2","updated":"2025-03-22T21:16:19Z","published":"2025-03-10T05:59:01Z","title":"MERLION: Marine ExploRation with Language guIded Online iNformative\n  Visual Sampling and Enhancement","summary":"  Autonomous and targeted underwater visual monitoring and exploration using\nAutonomous Underwater Vehicles (AUVs) can be a challenging task due to both\nonline and offline constraints. The online constraints comprise limited onboard\nstorage capacity and communication bandwidth to the surface, whereas the\noffline constraints entail the time and effort required for the selection of\ndesired key frames from the video data. An example use case of targeted\nunderwater visual monitoring is finding the most interesting visual frames of\nfish in a long sequence of an AUV's visual experience. This challenge of\ntargeted informative sampling is further aggravated in murky waters with poor\nvisibility. In this paper, we present MERLION, a novel framework that provides\nsemantically aligned and visually enhanced summaries for murky underwater\nmarine environment monitoring and exploration. Specifically, our framework\nintegrates (a) an image-text model for semantically aligning the visual samples\nto the users' needs, (b) an image enhancement model for murky water visual data\nand (c) an informative sampler for summarizing the monitoring experience. We\nvalidate our proposed MERLION framework on real-world data with user studies\nand present qualitative and quantitative results using our evaluation metric\nand show improved results compared to the state-of-the-art approaches. We have\nopen-sourced the code for MERLION at the following link\nhttps://github.com/MARVL-Lab/MERLION.git.\n","authors":["Shrutika Vishal Thengane","Marcel Bartholomeus Prasetyo","Yu Xiang Tan","Malika Meghjani"],"pdf_url":"https://arxiv.org/pdf/2503.06953v2.pdf","comment":"In proceedings of IEEE ICRA 2025"},{"id":"http://arxiv.org/abs/2503.16340v2","updated":"2025-03-22T20:22:39Z","published":"2025-03-20T16:57:15Z","title":"Deep learning framework for action prediction reveals multi-timescale\n  locomotor control","summary":"  Modeling movement in real-world tasks is a fundamental scientific goal for\nmotor control, biomechanics, and rehabilitation engineering. However, existing\nmodels and their simplifying assumptions such as linear and fixed timescale\nmappings do not generalize to real-world contexts. Here, we develop a deep\nlearning-based framework for action prediction with architecture-dependent\ntrial embedding, outperforming traditional models across multiple contexts\n(walking and running, treadmill and overground, varying terrains) and input\nmodalities (multiple body states, gaze). We find that neural network\narchitectures with flexible input history-dependence like GRU and Transformer\nperform best overall. By quantifying the model's predictions relative to an\nautoregressive baseline, we identify context- and modality-dependent\ntimescales. There is greater reliance on fast-timescale predictions in complex\nterrain, gaze predictions precede body state predictions, and full-body state\npredictions precede center-of-mass-relevant predictions. This deep learning\nframework for action prediction provides quantifiable insights into the control\nof complex movements and can be extended to other actions, contexts, and\npopulations.\n","authors":["Wei-Chen Wang","Antoine De Comite","Alexandra Voloshina","Monica Daley","Nidhi Seethapathi"],"pdf_url":"https://arxiv.org/pdf/2503.16340v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17846v1","updated":"2025-03-22T19:45:12Z","published":"2025-03-22T19:45:12Z","title":"Smart Ankleband for Plug-and-Play Hand-Prosthetic Control","summary":"  Building robotic prostheses requires the creation of a sensor-based interface\ndesigned to provide the robotic hand with the control required to perform hand\ngestures. Traditional Electromyography (EMG) based prosthetics and emerging\nalternatives often face limitations such as muscle-activation limitations, high\ncost, and complex-calibration procedures. In this paper, we present a low-cost\nrobotic system composed of a smart ankleband for intuitive, calibration-free\ncontrol of a robotic hand, and a robotic prosthetic hand that executes actions\ncorresponding to leg gestures. The ankleband integrates an Inertial Measurement\nUnit (IMU) sensor with a lightweight temporal neural network to infer\nuser-intended leg gestures from motion data. Our system represents a\nsignificant step towards higher adoption rates of robotic prostheses among arm\namputees, as it enables one to operate a prosthetic hand using a low-cost,\nlow-power, and calibration-free solution. To evaluate our work, we collected\ndata from 10 subjects and tested our prototype ankleband with a robotic hand on\nan individual with upper-limb amputations. Our results demonstrate that this\nsystem empowers users to perform daily tasks more efficiently, requiring few\ncompensatory movements.\n","authors":["Dean Zadok","Oren Salzman","Alon Wolf","Alex M. Bronstein"],"pdf_url":"https://arxiv.org/pdf/2503.17846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17733v1","updated":"2025-03-22T11:26:47Z","published":"2025-03-22T11:26:47Z","title":"GS-LTS: 3D Gaussian Splatting-Based Adaptive Modeling for Long-Term\n  Service Robots","summary":"  3D Gaussian Splatting (3DGS) has garnered significant attention in robotics\nfor its explicit, high fidelity dense scene representation, demonstrating\nstrong potential for robotic applications. However, 3DGS-based methods in\nrobotics primarily focus on static scenes, with limited attention to the\ndynamic scene changes essential for long-term service robots. These robots\ndemand sustained task execution and efficient scene updates-challenges current\napproaches fail to meet. To address these limitations, we propose GS-LTS\n(Gaussian Splatting for Long-Term Service), a 3DGS-based system enabling indoor\nrobots to manage diverse tasks in dynamic environments over time. GS-LTS\ndetects scene changes (e.g., object addition or removal) via single-image\nchange detection, employs a rule-based policy to autonomously collect\nmulti-view observations, and efficiently updates the scene representation\nthrough Gaussian editing. Additionally, we propose a simulation-based benchmark\nthat automatically generates scene change data as compact configuration\nscripts, providing a standardized, user-friendly evaluation benchmark.\nExperimental results demonstrate GS-LTS's advantages in reconstruction,\nnavigation, and superior scene updates-faster and higher quality than the image\ntraining baseline-advancing 3DGS for long-term robotic operations. Code and\nbenchmark are available at: https://vipl-vsu.github.io/3DGS-LTS.\n","authors":["Bin Fu","Jialin Li","Bin Zhang","Ruiping Wang","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2503.17733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17730v1","updated":"2025-03-22T11:04:42Z","published":"2025-03-22T11:04:42Z","title":"Aportes para el cumplimiento del Reglamento (UE) 2024/1689 en robótica\n  y sistemas autónomos","summary":"  Cybersecurity in robotics stands out as a key aspect within Regulation (EU)\n2024/1689, also known as the Artificial Intelligence Act, which establishes\nspecific guidelines for intelligent and automated systems. A fundamental\ndistinction in this regulatory framework is the difference between robots with\nArtificial Intelligence (AI) and those that operate through automation systems\nwithout AI, since the former are subject to stricter security requirements due\nto their learning and autonomy capabilities. This work analyzes cybersecurity\ntools applicable to advanced robotic systems, with special emphasis on the\nprotection of knowledge bases in cognitive architectures. Furthermore, a list\nof basic tools is proposed to guarantee the security, integrity, and resilience\nof these systems, and a practical case is presented, focused on the analysis of\nrobot knowledge management, where ten evaluation criteria are defined to ensure\ncompliance with the regulation and reduce risks in human-robot interaction\n(HRI) environments.\n","authors":["Francisco J. Rodríguez Lera","Yoana Pita Lorenzo","David Sobrín Hidalgo","Laura Fernández Becerra","Irene González Fernández","Jose Miguel Guerrero Hernández"],"pdf_url":"https://arxiv.org/pdf/2503.17730v1.pdf","comment":"9 pages, 1 figure, in Spanish"},{"id":"http://arxiv.org/abs/2403.07376v2","updated":"2025-03-22T11:04:36Z","published":"2024-03-12T07:27:02Z","title":"NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning\n  Disentangled Reasoning","summary":"  Vision-and-Language Navigation (VLN), as a crucial research problem of\nEmbodied AI, requires an embodied agent to navigate through complex 3D\nenvironments following natural language instructions. Recent research has\nhighlighted the promising capacity of large language models (LLMs) in VLN by\nimproving navigational reasoning accuracy and interpretability. However, their\npredominant use in an offline manner usually suffers from substantial domain\ngap between the VLN task and the LLM training corpus. This paper introduces a\nnovel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill\nparameter-efficient in-domain training to enable self-guided navigational\ndecision, leading to a significant mitigation of the domain gap in a\ncost-effective manner. Specifically, at each timestep, the LLM is prompted to\nforecast the navigational chain-of-thought by: 1) acting as a world model to\nimagine the next observation according to the instruction, 2) selecting the\ncandidate observation that best aligns with the imagination, and 3) determining\nthe action based on the reasoning from the prior steps. Through constructing\nformalized labels for training, the LLM can learn to generate desired and\nreasonable chain-of-thought outputs for improving the action decision.\nExperimental results across various training settings and popular VLN\nbenchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room\n(R4R)) show the significant superiority of NavCoT over the direct action\nprediction variants. Through simple parameter-efficient finetuning, our NavCoT\noutperforms a recent GPT4-based approach with ~7% relative improvement on the\nR2R dataset. We believe that NavCoT will help unlock more task-adaptive and\nscalable LLM-based embodied agents, which are helpful for developing real-world\nrobotics applications. Code is available at\nhttps://github.com/expectorlin/NavCoT.\n","authors":["Bingqian Lin","Yunshuang Nie","Ziming Wei","Jiaqi Chen","Shikui Ma","Jianhua Han","Hang Xu","Xiaojun Chang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2403.07376v2.pdf","comment":"Accepted by TPAMI 2025"},{"id":"http://arxiv.org/abs/2503.17711v1","updated":"2025-03-22T09:53:31Z","published":"2025-03-22T09:53:31Z","title":"Adaptive Perching and Grasping by Aerial Robot with Light-weight and\n  High Grip-force Tendon-driven Three-fingered Hand using Single Actuator","summary":"  In previous research, various types of aerial robots equipped with perching\nmechanisms have been developed to extend operational time. However, most\nexisting perching methods adopt either an upward or downward approach, making\nit difficult to perch near walls with surrounding obstacles. Additionally,\nperching hands are typically designed solely for attachment to objects and lack\nadditional functionality, imposing a payload burden during flight. To address\nthese issues, this paper proposes a lightweight robotic hand, the \"Tri-force\nhand\", capable of both perching and object grasping, as well as a new perching\nmethod called \"Pendulum-perching\". The Tri-force hand is a tendon-driven,\nthree-fingered hand utilizing a spherical joint and a two-dimensional\ndifferential plate, enabling passive actuation with a single actuator. Each\nfinger module, designed with controllable semi-tendon drive, can conform to\narbitrary shapes within its operating range, allowing both perching and\nadaptive object grasping. By integrating this hand into a fully actuated aerial\nrobot, the system can perform multi-directional approaches from the side and\nlanding using gravity. This approach is similar to Crush-perching seen in\nresearches with fixed-wing aerial robots, but it differs in its superior\ncontrol over approach speed and direction, as well as its ability to achieve\nstable detachment and re-launch. In experiments, the fabricated Tri-force hand\ndemonstrated the ability to withstand a total weight of up to 27.5 kg, grasp\nvarious objects ranging from simple to complex-shaped tools, and achieve a high\nsuccess rate in both perching and takeoff.\n","authors":["Hisaaki Iida","Junichiro Sugihara","Kazuki Sugihara","Haruki Kozuka","Jinjie Li","Keisuke Nagato","Moju Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.17711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17703v1","updated":"2025-03-22T09:03:31Z","published":"2025-03-22T09:03:31Z","title":"RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action\n  Issue Detection, Explanation and Recovery","summary":"  As robots increasingly operate in dynamic human-centric environments,\nimproving their ability to detect, explain, and recover from action-related\nissues becomes crucial. Traditional model-based and data-driven techniques lack\nadaptability, while more flexible generative AI methods struggle with grounding\nextracted information to real-world constraints. We introduce RAIDER, a novel\nagent that integrates Large Language Models (LLMs) with grounded tools for\nadaptable and efficient issue detection and explanation. Using a unique\n\"Ground, Ask& Answer, Issue\" procedure, RAIDER dynamically generates\ncontext-aware precondition questions and selects appropriate tools for\nresolution, achieving targeted information gathering. Our results within a\nsimulated household environment surpass methods relying on predefined models,\nfull scene descriptions, or standalone trained models. Additionally, RAIDER's\nexplanations enhance recovery success, including cases requiring human\ninteraction. Its modular architecture, featuring self-correction mechanisms,\nenables straightforward adaptation to diverse scenarios, as demonstrated in a\nreal-world human-assistive task. This showcases RAIDER's potential as a\nversatile agentic AI solution for robotic issue detection and explanation,\nwhile addressing the problem of grounding generative AI for its effective\napplication in embodied agents. Project website:\nhttps://raider-llmagent.github.io/\n","authors":["Silvia Izquierdo-Badiola","Carlos Rizzo","Guillem Alenyà"],"pdf_url":"https://arxiv.org/pdf/2503.17703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17697v1","updated":"2025-03-22T08:39:01Z","published":"2025-03-22T08:39:01Z","title":"Sense4FL: Vehicular Crowdsensing Enhanced Federated Learning for\n  Autonomous Driving","summary":"  To accommodate constantly changing road conditions, real-time model training\nis essential for autonomous driving (AD). Federated learning (FL) serves as a\npromising paradigm to enable autonomous vehicles to train models\ncollaboratively with their onboard computing resources. However, existing\nvehicle selection schemes for FL all assume predetermined and\nlocation-independent vehicles' datasets, neglecting the fact that vehicles\ncollect training data along their routes, thereby resulting in suboptimal\nvehicle selection. To improve the perception quality in AD for a region, we\npropose Sense4FL, a vehicular crowdsensing-enhanced FL framework featuring\ntrajectory-dependent vehicular training data collection. To this end, we first\nderive the convergence bound of FL by considering the impact of both vehicles'\nuncertain trajectories and uploading probabilities, from which we discover that\nminimizing the training loss is equivalent to minimizing a weighted sum of\nlocal and global earth mover's distance (EMD) between vehicles' collected data\ndistribution and global data distribution. Based on this observation, we\nformulate the trajectory-dependent vehicle selection and data collection\nproblem for FL in AD. Given that the problem is NP-hard, we develop an\nefficient algorithm to find the solution with an approximation guarantee.\nExtensive simulation results have demonstrated the effectiveness of our\napproach in improving object detection performance compared with existing\nbenchmarks.\n","authors":["Yanan Ma","Senkang Hu","Zhengru Fang","Yun Ji","Yiqin Deng","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2503.17697v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.17678v1","updated":"2025-03-22T07:16:54Z","published":"2025-03-22T07:16:54Z","title":"Computationally and Sample Efficient Safe Reinforcement Learning Using\n  Adaptive Conformal Prediction","summary":"  Safety is a critical concern in learning-enabled autonomous systems\nespecially when deploying these systems in real-world scenarios. An important\nchallenge is accurately quantifying the uncertainty of unknown models to\ngenerate provably safe control policies that facilitate the gathering of\ninformative data, thereby achieving both safe and optimal policies.\nAdditionally, the selection of the data-driven model can significantly impact\nboth the real-time implementation and the uncertainty quantification process.\nIn this paper, we propose a provably sample efficient episodic safe learning\nframework that remains robust across various model choices with quantified\nuncertainty for online control tasks. Specifically, we first employ Quadrature\nFourier Features (QFF) for kernel function approximation of Gaussian Processes\n(GPs) to enable efficient approximation of unknown dynamics. Then the Adaptive\nConformal Prediction (ACP) is used to quantify the uncertainty from online\nobservations and combined with the Control Barrier Functions (CBF) to\ncharacterize the uncertainty-aware safe control constraints under learned\ndynamics. Finally, an optimism-based exploration strategy is integrated with\nACP-based CBFs for safe exploration and near-optimal safe nonlinear control.\nTheoretical proofs and simulation results are provided to demonstrate the\neffectiveness and efficiency of the proposed framework.\n","authors":["Hao Zhou","Yanze Zhang","Wenhao Luo"],"pdf_url":"https://arxiv.org/pdf/2503.17678v1.pdf","comment":"7 pages, accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2503.10028v4","updated":"2025-03-22T04:34:53Z","published":"2025-03-13T04:14:09Z","title":"LEVA: A high-mobility logistic vehicle with legged suspension","summary":"  The autonomous transportation of materials over challenging terrain is a\nchallenge with major economic implications and remains unsolved. This paper\nintroduces LEVA, a high-payload, high-mobility robot designed for autonomous\nlogistics across varied terrains, including those typical in agriculture,\nconstruction, and search and rescue operations. LEVA uniquely integrates an\nadvanced legged suspension system using parallel kinematics. It is capable of\ntraversing stairs using a rl controller, has steerable wheels, and includes a\nspecialized box pickup mechanism that enables autonomous payload loading as\nwell as precise and reliable cargo transportation of up to 85 kg across uneven\nsurfaces, steps and inclines while maintaining a cot of as low as 0.15. Through\nextensive experimental validation, LEVA demonstrates its off-road capabilities\nand reliability regarding payload loading and transport.\n","authors":["Marco Arnold","Lukas Hildebrandt","Kaspar Janssen","Efe Ongan","Pascal Bürge","Ádám Gyula Gábriel","James Kennedy","Rishi Lolla","Quanisha Oppliger","Micha Schaaf","Joseph Church","Michael Fritsche","Victor Klemm","Turcan Tuna","Giorgio Valsecchi","Cedric Weibel","Michael Wüthrich","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2503.10028v4.pdf","comment":"Accepted for publication at the 2025 IEEE International Conference on\n  Robotics and Automation (ICRA). This is the author's preprint version. 6\n  pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2503.05818v2","updated":"2025-03-22T04:22:47Z","published":"2025-03-04T18:45:20Z","title":"Closing the Intent-to-Behavior Gap via Fulfillment Priority Logic","summary":"  Practitioners designing reinforcement learning policies face a fundamental\nchallenge: translating intended behavioral objectives into representative\nreward functions. This challenge stems from behavioral intent requiring\nsimultaneous achievement of multiple competing objectives, typically addressed\nthrough labor-intensive linear reward composition that yields brittle results.\nConsider the ubiquitous robotics scenario where performance maximization\ndirectly conflicts with energy conservation. Such competitive dynamics are\nresistant to simple linear reward combinations. In this paper, we present the\nconcept of objective fulfillment upon which we build Fulfillment Priority Logic\n(FPL). FPL allows practitioners to define logical formula representing their\nintentions and priorities within multi-objective reinforcement learning. Our\nnovel Balanced Policy Gradient algorithm leverages FPL specifications to\nachieve up to 500\\% better sample efficiency compared to Soft Actor Critic.\nNotably, this work constitutes the first implementation of non-linear utility\nscalarization design, specifically for continuous control problems.\n","authors":["Bassel El Mabsout","Abdelrahman AbdelGawad","Renato Mancuso"],"pdf_url":"https://arxiv.org/pdf/2503.05818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17626v1","updated":"2025-03-22T03:01:25Z","published":"2025-03-22T03:01:25Z","title":"Transferable Latent-to-Latent Locomotion Policy for Efficient and\n  Versatile Motion Control of Diverse Legged Robots","summary":"  Reinforcement learning (RL) has demonstrated remarkable capability in\nacquiring robot skills, but learning each new skill still requires substantial\ndata collection for training. The pretrain-and-finetune paradigm offers a\npromising approach for efficiently adapting to new robot entities and tasks.\nInspired by the idea that acquired knowledge can accelerate learning new tasks\nwith the same robot and help a new robot master a trained task, we propose a\nlatent training framework where a transferable latent-to-latent locomotion\npolicy is pretrained alongside diverse task-specific observation encoders and\naction decoders. This policy in latent space processes encoded latent\nobservations to generate latent actions to be decoded, with the potential to\nlearn general abstract motion skills. To retain essential information for\ndecision-making and control, we introduce a diffusion recovery module that\nminimizes information reconstruction loss during pretrain stage. During\nfine-tune stage, the pretrained latent-to-latent locomotion policy remains\nfixed, while only the lightweight task-specific encoder and decoder are\noptimized for efficient adaptation. Our method allows a robot to leverage its\nown prior experience across different tasks as well as the experience of other\nmorphologically diverse robots to accelerate adaptation. We validate our\napproach through extensive simulations and real-world experiments,\ndemonstrating that the pretrained latent-to-latent locomotion policy\neffectively generalizes to new robot entities and tasks with improved\nefficiency.\n","authors":["Ziang Zheng","Guojian Zhan","Bin Shuai","Shengtao Qin","Jiangtao Li","Tao Zhang","Shengbo Eben Li"],"pdf_url":"https://arxiv.org/pdf/2503.17626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14537v2","updated":"2025-03-22T02:26:04Z","published":"2025-03-17T13:56:03Z","title":"Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive\n  Survey","summary":"  Learning-based 3D reconstruction has emerged as a transformative technique in\nautonomous driving, enabling precise modeling of both dynamic and static\nenvironments through advanced neural representations. Despite data\naugmentation, 3D reconstruction inspires pioneering solution for vital tasks in\nthe field of autonomous driving, such as scene understanding and closed-loop\nsimulation. We investigates the details of 3D reconstruction and conducts a\nmulti-perspective, in-depth analysis of recent advancements. Specifically, we\nfirst provide a systematic introduction of preliminaries, including data\nmodalities, benchmarks and technical preliminaries of learning-based 3D\nreconstruction, facilitating instant identification of suitable methods\naccording to sensor suites. Then, we systematically review learning-based 3D\nreconstruction methods in autonomous driving, categorizing approaches by\nsubtasks and conducting multi-dimensional analysis and summary to establish a\ncomprehensive technical reference. The development trends and existing\nchallenges are summarized in the context of learning-based 3D reconstruction in\nautonomous driving. We hope that our review will inspire future researches.\n","authors":["Liewen Liao","Weihao Yan","Ming Yang","Songan Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17615v1","updated":"2025-03-22T02:21:11Z","published":"2025-03-22T02:21:11Z","title":"Feature Selection Based on Reinforcement Learning and Hazard State\n  Classification for Magnetic Adhesion Wall-Climbing Robots","summary":"  Magnetic adhesion tracked wall-climbing robots face potential risks of\noverturning during high-altitude operations, making their stability crucial for\nensuring safety. This study presents a dynamic feature selection method based\non Proximal Policy Optimization (PPO) reinforcement learning, combined with\ntypical machine learning models, aimed at improving the classification accuracy\nof hazardous states under complex operating conditions. Firstly, this work\ninnovatively employs a fiber rod-based MEMS attitude sensor to collect\nvibration data from the robot and extract high-dimensional feature vectors in\nboth time and frequency domains. Then, a reinforcement learning model is used\nto dynamically select the optimal feature subset, reducing feature redundancy\nand enhancing classification accuracy. Finally, a CNN-LSTM deep learning model\nis employed for classification and recognition. Experimental results\ndemonstrate that the proposed method significantly improves the robot's ability\nto assess hazardous states across various operational scenarios, providing\nreliable technical support for robotic safety monitoring.\n","authors":["Zhen Ma","He Xu","Jielong Dou","Yi Qin","Xueyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.17615v1.pdf","comment":"21 pages, 11 figures, manuscript for Journal of Autonomous Robots"},{"id":"http://arxiv.org/abs/2503.17589v1","updated":"2025-03-22T00:15:34Z","published":"2025-03-22T00:15:34Z","title":"Extending First-order Motion Planners to Second-order Dynamics","summary":"  This paper extends first-order motion planners to robots governed by\nsecond-order dynamics. Two control schemes are proposed based on the knowledge\nof a scalar function whose negative gradient aligns with a given first-order\nmotion planner. When such a function is known, the first-order motion planner\nis combined with a damping velocity vector with a dynamic gain to extend the\nsafety and convergence guarantees of the first-order motion planner to\nsecond-order systems. If no such function is available, we propose an\nalternative control scheme ensuring that the error between the robot's velocity\nand the first-order motion planner converges to zero. The theoretical\ndevelopments are supported by simulation results demonstrating the\neffectiveness of the proposed approaches.\n","authors":["Mayur Sawant","Abdelhamid Tayebi"],"pdf_url":"https://arxiv.org/pdf/2503.17589v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2102.12145v5","updated":"2025-03-22T08:50:38Z","published":"2021-02-24T09:11:31Z","title":"GDRNPP: A Geometry-guided and Fully Learning-based Object Pose Estimator","summary":"  6D pose estimation of rigid objects is a long-standing and challenging task\nin computer vision. Recently, the emergence of deep learning reveals the\npotential of Convolutional Neural Networks (CNNs) to predict reliable 6D poses.\nGiven that direct pose regression networks currently exhibit suboptimal\nperformance, most methods still resort to traditional techniques to varying\ndegrees. For example, top-performing methods often adopt an indirect strategy\nby first establishing 2D-3D or 3D-3D correspondences followed by applying the\nRANSAC-based PnP or Kabsch algorithms, and further employing ICP for\nrefinement. Despite the performance enhancement, the integration of traditional\ntechniques makes the networks time-consuming and not end-to-end trainable.\nOrthogonal to them, this paper introduces a fully learning-based object pose\nestimator. In this work, we first perform an in-depth investigation of both\ndirect and indirect methods and propose a simple yet effective Geometry-guided\nDirect Regression Network (GDRN) to learn the 6D pose from monocular images in\nan end-to-end manner. Afterwards, we introduce a geometry-guided pose\nrefinement module, enhancing pose accuracy when extra depth data is available.\nGuided by the predicted coordinate map, we build an end-to-end differentiable\narchitecture that establishes robust and accurate 3D-3D correspondences between\nthe observed and rendered RGB-D images to refine the pose. Our enhanced pose\nestimation pipeline GDRNPP (GDRN Plus Plus) conquered the leaderboard of the\nBOP Challenge for two consecutive years, becoming the first to surpass all\nprior methods that relied on traditional techniques in both accuracy and speed.\nThe code and models are available at\nhttps://github.com/shanice-l/gdrnpp_bop2022.\n","authors":["Xingyu Liu","Ruida Zhang","Chenyangguang Zhang","Gu Wang","Jiwen Tang","Zhigang Li","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2102.12145v5.pdf","comment":"accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI), code: https://github.com/shanice-l/gdrnpp_bop2022"}]},"2025-03-25T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2503.18945v2","updated":"2025-03-25T15:31:25Z","published":"2025-03-24T17:59:51Z","title":"Aether: Geometric-Aware Unified World Modeling","summary":"  The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance is\ncomparable with or even better than that of domain-specific models.\nAdditionally, Aether employs camera trajectories as geometry-informed action\nspaces, enabling effective action-conditioned prediction and visual planning.\nWe hope our work inspires the community to explore new frontiers in\nphysically-reasonable world modeling and its applications.\n","authors":[" Aether Team","Haoyi Zhu","Yifan Wang","Jianjun Zhou","Wenzheng Chang","Yang Zhou","Zizun Li","Junyi Chen","Chunhua Shen","Jiangmiao Pang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2503.18945v2.pdf","comment":"Project Page: https://aether-world.github.io/"},{"id":"http://arxiv.org/abs/2503.18673v2","updated":"2025-03-25T06:18:47Z","published":"2025-03-24T13:46:21Z","title":"Any6D: Model-free 6D Pose Estimation of Novel Objects","summary":"  We introduce Any6D, a model-free framework for 6D object pose estimation that\nrequires only a single RGB-D anchor image to estimate both the 6D pose and size\nof unknown objects in novel scenes. Unlike existing methods that rely on\ntextured 3D models or multiple viewpoints, Any6D leverages a joint object\nalignment process to enhance 2D-3D alignment and metric scale estimation for\nimproved pose accuracy. Our approach integrates a render-and-compare strategy\nto generate and refine pose hypotheses, enabling robust performance in\nscenarios with occlusions, non-overlapping views, diverse lighting conditions,\nand large cross-environment variations. We evaluate our method on five\nchallenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,\ndemonstrating its effectiveness in significantly outperforming state-of-the-art\nmethods for novel object pose estimation. Project page:\nhttps://taeyeop.com/any6d\n","authors":["Taeyeop Lee","Bowen Wen","Minjun Kang","Gyuree Kang","In So Kweon","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2503.18673v2.pdf","comment":"CVPR 2025, Project Page: https://taeyeop.com/any6d"},{"id":"http://arxiv.org/abs/2503.16340v3","updated":"2025-03-25T04:50:17Z","published":"2025-03-20T16:57:15Z","title":"Deep learning framework for action prediction reveals multi-timescale\n  locomotor control","summary":"  Modeling movement in real-world tasks is a fundamental goal for motor\ncontrol, biomechanics, and rehabilitation engineering. However, widely used\ndata-driven models of essential tasks like locomotion make simplifying\nassumptions such as linear and fixed timescale mappings between past inputs and\nfuture actions, which do not generalize to real-world contexts. Here, we\ndevelop a deep learning-based framework for action prediction with\narchitecture-dependent trial embeddings, outperforming traditional models\nacross contexts (walking and running, treadmill and overground, varying\nterrains) and input modalities (multiple body states, gaze). We find that\nneural network architectures with flexible input history-dependence like GRU\nand Transformer perform best overall. By quantifying the model's predictions\nrelative to an autoregressive baseline, we identify context- and\nmodality-dependent timescales. These analyses reveal that there is greater\nreliance on fast-timescale predictions in complex terrain, gaze predicts future\nfoot placement before body states, and the full-body state predictions precede\nthose by center-of-mass-relevant states. This deep learning framework for\naction prediction provides quantifiable insights into the control of real-world\nlocomotion and can be extended to other actions, contexts, and populations.\n","authors":["Wei-Chen Wang","Antoine De Comite","Alexandra Voloshina","Monica Daley","Nidhi Seethapathi"],"pdf_url":"https://arxiv.org/pdf/2503.16340v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15928v4","updated":"2025-03-25T23:41:23Z","published":"2023-10-24T15:26:57Z","title":"AO-Grasp: Articulated Object Grasp Generation","summary":"  We introduce AO-Grasp, a grasp proposal method that generates 6 DoF grasps\nthat enable robots to interact with articulated objects, such as opening and\nclosing cabinets and appliances. AO-Grasp consists of two main contributions:\nthe AO-Grasp Model and the AO-Grasp Dataset. Given a segmented partial point\ncloud of a single articulated object, the AO-Grasp Model predicts the best\ngrasp points on the object with an Actionable Grasp Point Predictor. Then, it\nfinds corresponding grasp orientations for each of these points, resulting in\nstable and actionable grasp proposals. We train the AO-Grasp Model on our new\nAO-Grasp Dataset, which contains 78K actionable parallel-jaw grasps on\nsynthetic articulated objects. In simulation, AO-Grasp achieves a 45.0 % grasp\nsuccess rate, whereas the highest performing baseline achieves a 35.0% success\nrate. Additionally, we evaluate AO-Grasp on 120 real-world scenes of objects\nwith varied geometries, articulation axes, and joint states, where AO-Grasp\nproduces successful grasps on 67.5% of scenes, while the baseline only produces\nsuccessful grasps on 33.3% of scenes. To the best of our knowledge, AO-Grasp is\nthe first method for generating 6 DoF grasps on articulated objects directly\nfrom partial point clouds without requiring part detection or hand-designed\ngrasp heuristics. Project website: https://stanford-iprl-lab.github.io/ao-grasp\n","authors":["Carlota Parés Morlans","Claire Chen","Yijia Weng","Michelle Yi","Yuying Huang","Nick Heppert","Linqi Zhou","Leonidas Guibas","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2310.15928v4.pdf","comment":"Project website: https://stanford-iprl-lab.github.io/ao-grasp"},{"id":"http://arxiv.org/abs/2403.17246v2","updated":"2025-03-25T23:39:13Z","published":"2024-03-25T22:47:13Z","title":"TwoStep: Multi-agent Task Planning using Classical Planners and Large\n  Language Models","summary":"  Classical planning formulations like the Planning Domain Definition Language\n(PDDL) admit action sequences guaranteed to achieve a goal state given an\ninitial state if any are possible. However, reasoning problems defined in PDDL\ndo not capture temporal aspects of action taking, such as concurrent actions\nbetween two agents when there are no conflicting conditions, without\nsignificant modification and definition to existing PDDL domains. A human\nexpert aware of such constraints can decompose a goal into subgoals, each\nreachable through single agent planning, to take advantage of simultaneous\nactions. In contrast to classical planning, large language models (LLMs)\ndirectly used for inferring plan steps rarely guarantee execution success, but\nare capable of leveraging commonsense reasoning to assemble action sequences.\nWe combine the strengths of both classical planning and LLMs by approximating\nhuman intuitions for multi-agent planning goal decomposition. We demonstrate\nthat LLM-based goal decomposition leads to faster planning times than solving\nmulti-agent PDDL problems directly while simultaneously achieving fewer plan\nexecution steps than a single agent plan alone, as well as most multiagent\nplans, while guaranteeing execution success. Additionally, we find that\nLLM-based approximations of subgoals result in similar multi-agent execution\nlengths to those specified by human experts. Website and resources at\nhttps://glamor-usc.github.io/twostep\n","authors":["David Bai","Ishika Singh","David Traum","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2403.17246v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2503.20105v1","updated":"2025-03-25T23:02:13Z","published":"2025-03-25T23:02:13Z","title":"Direct Post-Training Preference Alignment for Multi-Agent Motion\n  Generation Models Using Implicit Feedback from Pre-training Demonstrations","summary":"  Recent advancements in LLMs have revolutionized motion generation models in\nembodied applications. While LLM-type auto-regressive motion generation models\nbenefit from training scalability, there remains a discrepancy between their\ntoken prediction objectives and human preferences. As a result, models\npre-trained solely with token-prediction objectives often generate behaviors\nthat deviate from what humans would prefer, making post-training preference\nalignment crucial for producing human-preferred motions. Unfortunately,\npost-training alignment requires extensive preference rankings of motions\ngenerated by the pre-trained model, which are costly to annotate, especially in\nmulti-agent settings. Recently, there has been growing interest in leveraging\npre-training demonstrations to scalably generate preference data for\npost-training alignment. However, these methods often adopt an adversarial\nassumption, treating all pre-trained model-generated samples as unpreferred\nexamples. This adversarial approach overlooks the valuable signal provided by\npreference rankings among the model's own generations, ultimately reducing\nalignment effectiveness and potentially leading to misaligned behaviors. In\nthis work, instead of treating all generated samples as equally bad, we\nleverage implicit preferences encoded in pre-training demonstrations to\nconstruct preference rankings among the pre-trained model's generations,\noffering more nuanced preference alignment guidance with zero human cost. We\napply our approach to large-scale traffic simulation and demonstrate its\neffectiveness in improving the realism of pre-trained model's generated\nbehaviors, making a lightweight 1M motion generation model comparable to SOTA\nlarge imitation-based models by relying solely on implicit feedback from\npre-training demonstrations, without additional post-training human preference\nannotations or high computational costs.\n","authors":["Ran Tian","Kratarth Goel"],"pdf_url":"https://arxiv.org/pdf/2503.20105v1.pdf","comment":"ICLR 2025 Spotlight"},{"id":"http://arxiv.org/abs/2503.20102v1","updated":"2025-03-25T22:52:46Z","published":"2025-03-25T22:52:46Z","title":"Extendable Long-Horizon Planning via Hierarchical Multiscale Diffusion","summary":"  This paper tackles a novel problem, extendable long-horizon planning-enabling\nagents to plan trajectories longer than those in training data without\ncompounding errors. To tackle this, we propose the Hierarchical Multiscale\nDiffuser (HM-Diffuser) and Progressive Trajectory Extension (PTE), an\naugmentation method that iteratively generates longer trajectories by stitching\nshorter ones. HM-Diffuser trains on these extended trajectories using a\nhierarchical structure, efficiently handling tasks across multiple temporal\nscales. Additionally, we introduce Adaptive Plan Pondering and the Recursive\nHM-Diffuser, which consolidate hierarchical layers into a single model to\nprocess temporal scales recursively. Experimental results demonstrate the\neffectiveness of our approach, advancing diffusion-based planners for scalable\nlong-horizon planning.\n","authors":["Chang Chen","Hany Hamed","Doojin Baek","Taegu Kang","Yoshua Bengio","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2503.20102v1.pdf","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2503.20066v1","updated":"2025-03-25T21:01:05Z","published":"2025-03-25T21:01:05Z","title":"Learning Scene-Level Signed Directional Distance Function with\n  Ellipsoidal Priors and Neural Residuals","summary":"  Dense geometric environment representations are critical for autonomous\nmobile robot navigation and exploration. Recent work shows that implicit\ncontinuous representations of occupancy, signed distance, or radiance learned\nusing neural networks offer advantages in reconstruction fidelity, efficiency,\nand differentiability over explicit discrete representations based on meshes,\npoint clouds, and voxels. In this work, we explore a directional formulation of\nsigned distance, called signed directional distance function (SDDF). Unlike\nsigned distance function (SDF) and similar to neural radiance fields (NeRF),\nSDDF has a position and viewing direction as input. Like SDF and unlike NeRF,\nSDDF directly provides distance to the observed surface along the direction,\nrather than integrating along the view ray, allowing efficient view synthesis.\nTo learn and predict scene-level SDDF efficiently, we develop a differentiable\nhybrid representation that combines explicit ellipsoid priors and implicit\nneural residuals. This approach allows the model to effectively handle large\ndistance discontinuities around obstacle boundaries while preserving the\nability for dense high-fidelity prediction. We show that SDDF is competitive\nwith the state-of-the-art neural implicit scene models in terms of\nreconstruction accuracy and rendering efficiency, while allowing differentiable\nview prediction for robot trajectory optimization.\n","authors":["Zhirui Dai","Hojoon Shin","Yulun Tian","Ki Myung Brian Lee","Nikolay Atanasov"],"pdf_url":"https://arxiv.org/pdf/2503.20066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11962v2","updated":"2025-03-25T20:33:14Z","published":"2024-09-18T13:12:32Z","title":"Reactive Collision Avoidance for Safe Agile Navigation","summary":"  Reactive collision avoidance is essential for agile robots navigating complex\nand dynamic environments, enabling real-time obstacle response. However, this\ntask is inherently challenging because it requires a tight integration of\nperception, planning, and control, which traditional methods often handle\nseparately, resulting in compounded errors and delays. This paper introduces a\nnovel approach that unifies these tasks into a single reactive framework using\nsolely onboard sensing and computing. Our method combines nonlinear model\npredictive control with adaptive control barrier functions, directly linking\nperception-driven constraints to real-time planning and control. Constraints\nare determined by using a neural network to refine noisy RGB-D data, enhancing\ndepth accuracy, and selecting points with the minimum time-to-collision to\nprioritize the most immediate threats. To maintain a balance between safety and\nagility, a heuristic dynamically adjusts the optimization process, preventing\noverconstraints in real time. Extensive experiments with an agile quadrotor\ndemonstrate effective collision avoidance across diverse indoor and outdoor\nenvironments, without requiring environment-specific tuning or explicit\nmapping.\n","authors":["Alessandro Saviolo","Niko Picello","Jeffrey Mao","Rishabh Verma","Giuseppe Loianno"],"pdf_url":"https://arxiv.org/pdf/2409.11962v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03920v3","updated":"2025-03-25T19:58:16Z","published":"2024-09-05T21:29:02Z","title":"Asymptotically-Optimal Multi-Query Path Planning for a Polygonal Robot","summary":"  Shortest-path roadmaps, also known as reduced visibility graphs, provides a\nhighly efficient multi-query method for computing optimal paths in\ntwo-dimensional environments. Combined with Minkowski sum computations,\nshortest-path roadmaps can compute optimal paths for a translating robot in 2D.\nIn this study, we explore the intuitive idea of stacking up a set of reduced\nvisibility graphs at different orientations for a polygonal holonomic robot to\nsupport the fast computation of near-optimal paths, allowing simultaneous 2D\ntranslation and rotation. The resulting algorithm, rotation-stacked visibility\ngraph (RVG), is shown to be resolution-complete and asymptotically optimal.\nExtensive computational experiments show RVG significantly outperforms\nstate-of-the-art single- and multi-query sampling-based methods on both\ncomputation time and solution optimality fronts.\n","authors":["Duo Zhang","Zihe Ye","Jingjin Yu"],"pdf_url":"https://arxiv.org/pdf/2409.03920v3.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2410.14337v2","updated":"2025-03-25T19:43:29Z","published":"2024-10-18T09:50:05Z","title":"Perception of Emotions in Human and Robot Faces: Is the Eye Region\n  Enough?","summary":"  The increased interest in developing next-gen social robots has raised\nquestions about the factors affecting the perception of robot emotions. This\nstudy investigates the impact of robot appearances (humanlike, mechanical) and\nface regions (full-face, eye-region) on human perception of robot emotions. A\nbetween-subjects user study (N = 305) was conducted where participants were\nasked to identify the emotions being displayed in videos of robot faces, as\nwell as a human baseline. Our findings reveal three important insights for\neffective social robot face design in Human-Robot Interaction (HRI): Firstly,\nrobots equipped with a back-projected, fully animated face - regardless of\nwhether they are more human-like or more mechanical-looking - demonstrate a\ncapacity for emotional expression comparable to that of humans. Secondly, the\nrecognition accuracy of emotional expressions in both humans and robots\ndeclines when only the eye region is visible. Lastly, within the constraint of\nonly the eye region being visible, robots with more human-like features\nsignificantly enhance emotion recognition.\n","authors":["Chinmaya Mishra","Gabriel Skantze","Peter Hagoort","Rinus Verdonschot"],"pdf_url":"https://arxiv.org/pdf/2410.14337v2.pdf","comment":"Accepted for publication at the 16th International Conference on\n  Social Robotics, Odense, Denmark (ICSR 2024)"},{"id":"http://arxiv.org/abs/2410.03132v5","updated":"2025-03-25T19:16:05Z","published":"2024-10-04T04:07:15Z","title":"Autoregressive Action Sequence Learning for Robotic Manipulation","summary":"  Designing a universal policy architecture that performs well across diverse\nrobots and task configurations remains a key challenge. In this work, we\naddress this by representing robot actions as sequential data and generating\nactions through autoregressive sequence modeling. Existing autoregressive\narchitectures generate end-effector waypoints sequentially as word tokens in\nlanguage modeling, which are limited to low-frequency control tasks. Unlike\nlanguage, robot actions are heterogeneous and often include continuous values\n-- such as joint positions, 2D pixel coordinates, and end-effector poses --\nwhich are not easily suited for language-based modeling. Based on this insight,\nwe introduce a straightforward enhancement: we extend causal transformers'\nsingle-token prediction to support predicting a variable number of tokens in a\nsingle step through our Chunking Causal Transformer (CCT). This enhancement\nenables robust performance across diverse tasks of various control frequencies,\ngreater efficiency by having fewer autoregression steps, and lead to a hybrid\naction sequence design by mixing different types of actions and using a\ndifferent chunk size for each action type. Based on CCT, we propose the\nAutoregressive Policy (ARP) architecture, which solves manipulation tasks by\ngenerating hybrid action sequences. We evaluate ARP across diverse robotic\nmanipulation environments, including Push-T, ALOHA, and RLBench, and show that\nARP, as a universal architecture, matches or outperforms the\nenvironment-specific state-of-the-art in all tested benchmarks, while being\nmore efficient in computation and parameter sizes. Videos of our real robot\ndemonstrations, all source code and the pretrained models of ARP can be found\nat http://github.com/mlzxy/arp.\n","authors":["Xinyu Zhang","Yuhan Liu","Haonan Chang","Liam Schramm","Abdeslam Boularias"],"pdf_url":"https://arxiv.org/pdf/2410.03132v5.pdf","comment":"(RA-L 2025) Add a new figure to explain why chunking autoregression\n  works. Put back the previous in-depth discussion for arxiv release"},{"id":"http://arxiv.org/abs/2503.20020v1","updated":"2025-03-25T19:02:56Z","published":"2025-03-25T19:02:56Z","title":"Gemini Robotics: Bringing AI into the Physical World","summary":"  Recent advancements in large multimodal models have led to the emergence of\nremarkable generalist capabilities in digital domains, yet their translation to\nphysical agents such as robots remains a significant challenge. This report\nintroduces a new family of AI models purposefully designed for robotics and\nbuilt upon the foundation of Gemini 2.0. We present Gemini Robotics, an\nadvanced Vision-Language-Action (VLA) generalist model capable of directly\ncontrolling robots. Gemini Robotics executes smooth and reactive movements to\ntackle a wide range of complex manipulation tasks while also being robust to\nvariations in object types and positions, handling unseen environments as well\nas following diverse, open vocabulary instructions. We show that with\nadditional fine-tuning, Gemini Robotics can be specialized to new capabilities\nincluding solving long-horizon, highly dexterous tasks, learning new\nshort-horizon tasks from as few as 100 demonstrations and adapting to\ncompletely novel robot embodiments. This is made possible because Gemini\nRobotics builds on top of the Gemini Robotics-ER model, the second model we\nintroduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends\nGemini's multimodal reasoning capabilities into the physical world, with\nenhanced spatial and temporal understanding. This enables capabilities relevant\nto robotics including object detection, pointing, trajectory and grasp\nprediction, as well as multi-view correspondence and 3D bounding box\npredictions. We show how this novel combination can support a variety of\nrobotics applications. We also discuss and address important safety\nconsiderations related to this new class of robotics foundation models. The\nGemini Robotics family marks a substantial step towards developing\ngeneral-purpose robots that realizes AI's potential in the physical world.\n","authors":[" Gemini Robotics Team","Saminda Abeyruwan","Joshua Ainslie","Jean-Baptiste Alayrac","Montserrat Gonzalez Arenas","Travis Armstrong","Ashwin Balakrishna","Robert Baruch","Maria Bauza","Michiel Blokzijl","Steven Bohez","Konstantinos Bousmalis","Anthony Brohan","Thomas Buschmann","Arunkumar Byravan","Serkan Cabi","Ken Caluwaerts","Federico Casarini","Oscar Chang","Jose Enrique Chen","Xi Chen","Hao-Tien Lewis Chiang","Krzysztof Choromanski","David D'Ambrosio","Sudeep Dasari","Todor Davchev","Coline Devin","Norman Di Palo","Tianli Ding","Adil Dostmohamed","Danny Driess","Yilun Du","Debidatta Dwibedi","Michael Elabd","Claudio Fantacci","Cody Fong","Erik Frey","Chuyuan Fu","Marissa Giustina","Keerthana Gopalakrishnan","Laura Graesser","Leonard Hasenclever","Nicolas Heess","Brandon Hernaez","Alexander Herzog","R. Alex Hofer","Jan Humplik","Atil Iscen","Mithun George Jacob","Deepali Jain","Ryan Julian","Dmitry Kalashnikov","M. Emre Karagozler","Stefani Karp","Chase Kew","Jerad Kirkland","Sean Kirmani","Yuheng Kuang","Thomas Lampe","Antoine Laurens","Isabel Leal","Alex X. Lee","Tsang-Wei Edward Lee","Jacky Liang","Yixin Lin","Sharath Maddineni","Anirudha Majumdar","Assaf Hurwitz Michaely","Robert Moreno","Michael Neunert","Francesco Nori","Carolina Parada","Emilio Parisotto","Peter Pastor","Acorn Pooley","Kanishka Rao","Krista Reymann","Dorsa Sadigh","Stefano Saliceti","Pannag Sanketi","Pierre Sermanet","Dhruv Shah","Mohit Sharma","Kathryn Shea","Charles Shu","Vikas Sindhwani","Sumeet Singh","Radu Soricut","Jost Tobias Springenberg","Rachel Sterneck","Razvan Surdulescu","Jie Tan","Jonathan Tompson","Vincent Vanhoucke","Jake Varley","Grace Vesom","Giulia Vezzani","Oriol Vinyals","Ayzaan Wahid","Stefan Welker","Paul Wohlhart","Fei Xia","Ted Xiao","Annie Xie","Jinyu Xie","Peng Xu","Sichun Xu","Ying Xu","Zhuo Xu","Yuxiang Yang","Rui Yao","Sergey Yaroshenko","Wenhao Yu","Wentao Yuan","Jingwei Zhang","Tingnan Zhang","Allan Zhou","Yuxiang Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.20020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20011v1","updated":"2025-03-25T18:55:00Z","published":"2025-03-25T18:55:00Z","title":"Hyperdimensional Uncertainty Quantification for Multimodal Uncertainty\n  Fusion in Autonomous Vehicles Perception","summary":"  Uncertainty Quantification (UQ) is crucial for ensuring the reliability of\nmachine learning models deployed in real-world autonomous systems. However,\nexisting approaches typically quantify task-level output prediction uncertainty\nwithout considering epistemic uncertainty at the multimodal feature fusion\nlevel, leading to sub-optimal outcomes. Additionally, popular uncertainty\nquantification methods, e.g., Bayesian approximations, remain challenging to\ndeploy in practice due to high computational costs in training and inference.\nIn this paper, we propose HyperDUM, a novel deterministic uncertainty method\n(DUM) that efficiently quantifies feature-level epistemic uncertainty by\nleveraging hyperdimensional computing. Our method captures the channel and\nspatial uncertainties through channel and patch -wise projection and bundling\ntechniques respectively. Multimodal sensor features are then adaptively\nweighted to mitigate uncertainty propagation and improve feature fusion. Our\nevaluations show that HyperDUM on average outperforms the state-of-the-art\n(SOTA) algorithms by up to 2.01%/1.27% in 3D Object Detection and up to 1.29%\nimprovement over baselines in semantic segmentation tasks under various types\nof uncertainties. Notably, HyperDUM requires 2.36x less Floating Point\nOperations and up to 38.30x less parameters than SOTA methods, providing an\nefficient solution for real-world autonomous systems.\n","authors":["Luke Chen","Junyao Wang","Trier Mortlock","Pramod Khargonekar","Mohammad Abdullah Al Faruque"],"pdf_url":"https://arxiv.org/pdf/2503.20011v1.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2503.19984v1","updated":"2025-03-25T18:13:28Z","published":"2025-03-25T18:13:28Z","title":"Hybrid Magnetically and Electrically Powered Metallo-Dielectric Janus\n  Microrobots: Enhanced Motion Control and Operation Beyond Planar Limits","summary":"  This study introduces the integration of hybrid magnetic and electric\nactuation mechanisms to achieve advanced motion capabilities for Janus particle\n(JP) microrobots. We demonstrate enhanced in-plane motion control through\nversatile control strategies and present the concepts of interplanar\ntransitions and 2.5-dimensional (2.5D) trajectories, enabled by magnetic\nlevitation and electrostatic trapping. These innovations expand the mobility of\nJPs into 3D space, allowing dynamic operation beyond the limitations of\ntraditional surface-bound motion. Key functionalities include obstacle\ncrossing, transitions to elevated surfaces, and discrete surface patterning\nenabling highly localized interventions. Using this set of tools, we also\nshowcase the controlled out-of-plane transport of both synthetic and biological\ncargo. Together, these advancements lay the groundwork for novel\nmicrorobot-related applications in microfluidic systems and biomedical\nresearch.\n","authors":["Ido Rachbuch","Sinwook Park","Yuval Katz","Touvia Miloh","Gilad Yossifon"],"pdf_url":"https://arxiv.org/pdf/2503.19984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07819v2","updated":"2025-03-25T18:08:49Z","published":"2025-03-10T20:01:56Z","title":"POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality","summary":"  In this paper, we present a novel algorithm for quantifying uncertainty and\ninformation gained within 3D Gaussian Splatting (3D-GS) through P-Optimality.\nWhile 3D-GS has proven to be a useful world model with high-quality\nrasterizations, it does not natively quantify uncertainty or information,\nposing a challenge for real-world applications such as 3D-GS SLAM. We propose\nto quantify information gain in 3D-GS by reformulating the problem through the\nlens of optimal experimental design, which is a classical solution widely used\nin literature. By restructuring information quantification of 3D-GS through\noptimal experimental design, we arrive at multiple solutions, of which\nT-Optimality and D-Optimality perform the best quantitatively and qualitatively\nas measured on two popular datasets. Additionally, we propose a block diagonal\ncovariance approximation which provides a measure of correlation at the expense\nof a greater computation cost.\n","authors":["Joey Wilson","Marcelino Almeida","Sachit Mahajan","Martin Labrie","Maani Ghaffari","Omid Ghasemalizadeh","Min Sun","Cheng-Hao Kuo","Arnab Sen"],"pdf_url":"https://arxiv.org/pdf/2503.07819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19916v1","updated":"2025-03-25T17:59:59Z","published":"2025-03-25T17:59:59Z","title":"EventFly: Event Camera Perception from Ground to the Sky","summary":"  Cross-platform adaptation in event-based dense perception is crucial for\ndeploying event cameras across diverse settings, such as vehicles, drones, and\nquadrupeds, each with unique motion dynamics, viewpoints, and class\ndistributions. In this work, we introduce EventFly, a framework for robust\ncross-platform adaptation in event camera perception. Our approach comprises\nthree key components: i) Event Activation Prior (EAP), which identifies\nhigh-activation regions in the target domain to minimize prediction entropy,\nfostering confident, domain-adaptive predictions; ii) EventBlend, a data-mixing\nstrategy that integrates source and target event voxel grids based on\nEAP-driven similarity and density maps, enhancing feature alignment; and iii)\nEventMatch, a dual-discriminator technique that aligns features from source,\ntarget, and blended domains for better domain-invariant learning. To\nholistically assess cross-platform adaptation abilities, we introduce EXPo, a\nlarge-scale benchmark with diverse samples across vehicle, drone, and quadruped\nplatforms. Extensive experiments validate our effectiveness, demonstrating\nsubstantial gains over popular adaptation methods. We hope this work can pave\nthe way for more adaptive, high-performing event perception across diverse and\ncomplex environments.\n","authors":["Lingdong Kong","Dongyue Lu","Xiang Xu","Lai Xing Ng","Wei Tsang Ooi","Benoit R. Cottereau"],"pdf_url":"https://arxiv.org/pdf/2503.19916v1.pdf","comment":"CVPR 2025; 30 pages, 8 figures, 16 tables; Project Page at\n  https://event-fly.github.io/"},{"id":"http://arxiv.org/abs/2503.19912v1","updated":"2025-03-25T17:59:57Z","published":"2025-03-25T17:59:57Z","title":"SuperFlow++: Enhanced Spatiotemporal Consistency for Cross-Modal Data\n  Pretraining","summary":"  LiDAR representation learning has emerged as a promising approach to reducing\nreliance on costly and labor-intensive human annotations. While existing\nmethods primarily focus on spatial alignment between LiDAR and camera sensors,\nthey often overlook the temporal dynamics critical for capturing motion and\nscene continuity in driving scenarios. To address this limitation, we propose\nSuperFlow++, a novel framework that integrates spatiotemporal cues in both\npretraining and downstream tasks using consecutive LiDAR-camera pairs.\nSuperFlow++ introduces four key components: (1) a view consistency alignment\nmodule to unify semantic information across camera views, (2) a dense-to-sparse\nconsistency regularization mechanism to enhance feature robustness across\nvarying point cloud densities, (3) a flow-based contrastive learning approach\nthat models temporal relationships for improved scene understanding, and (4) a\ntemporal voting strategy that propagates semantic information across LiDAR\nscans to improve prediction consistency. Extensive evaluations on 11\nheterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms\nstate-of-the-art methods across diverse tasks and driving conditions.\nFurthermore, by scaling both 2D and 3D backbones during pretraining, we uncover\nemergent properties that provide deeper insights into developing scalable 3D\nfoundation models. With strong generalizability and computational efficiency,\nSuperFlow++ establishes a new benchmark for data-efficient LiDAR-based\nperception in autonomous driving. The code is publicly available at\nhttps://github.com/Xiangxu-0103/SuperFlow\n","authors":["Xiang Xu","Lingdong Kong","Hui Shuai","Wenwei Zhang","Liang Pan","Kai Chen","Ziwei Liu","Qingshan Liu"],"pdf_url":"https://arxiv.org/pdf/2503.19912v1.pdf","comment":"Preprint; 15 pages, 6 figures, 10 tables; Code at\n  https://github.com/Xiangxu-0103/SuperFlow"},{"id":"http://arxiv.org/abs/2503.19893v1","updated":"2025-03-25T17:53:53Z","published":"2025-03-25T17:53:53Z","title":"Visuo-Tactile Object Pose Estimation for a Multi-Finger Robot Hand with\n  Low-Resolution In-Hand Tactile Sensing","summary":"  Accurate 3D pose estimation of grasped objects is an important prerequisite\nfor robots to perform assembly or in-hand manipulation tasks, but object\nocclusion by the robot's own hand greatly increases the difficulty of this\nperceptual task. Here, we propose that combining visual information and\nproprioception with binary, low-resolution tactile contact measurements from\nacross the interior surface of an articulated robotic hand can mitigate this\nissue. The visuo-tactile object-pose-estimation problem is formulated\nprobabilistically in a factor graph. The pose of the object is optimized to\nalign with the three kinds of measurements using a robust cost function to\nreduce the influence of visual or tactile outlier readings. The advantages of\nthe proposed approach are first demonstrated in simulation: a custom 15-DoF\nrobot hand with one binary tactile sensor per link grasps 17 YCB objects while\nobserved by an RGB-D camera. This low-resolution in-hand tactile sensing\nsignificantly improves object-pose estimates under high occlusion and also high\nvisual noise. We also show these benefits through grasping tests with a\npreliminary real version of our tactile hand, obtaining reasonable\nvisuo-tactile estimates of object pose at approximately 13.3 Hz on average.\n","authors":["Lukas Mack","Felix Grüninger","Benjamin A. Richardson","Regine Lendway","Katherine J. Kuchenbecker","Joerg Stueckler"],"pdf_url":"https://arxiv.org/pdf/2503.19893v1.pdf","comment":"Accepted for publication at the IEEE International Conference on\n  Robotics and Automation (ICRA), 2025"},{"id":"http://arxiv.org/abs/2503.19889v1","updated":"2025-03-25T17:53:25Z","published":"2025-03-25T17:53:25Z","title":"A Multi-Agent Framework Integrating Large Language Models and Generative\n  AI for Accelerated Metamaterial Design","summary":"  Metamaterials, renowned for their exceptional mechanical, electromagnetic,\nand thermal properties, hold transformative potential across diverse\napplications, yet their design remains constrained by labor-intensive\ntrial-and-error methods and limited data interoperability. Here, we introduce\nCrossMatAgent--a novel multi-agent framework that synergistically integrates\nlarge language models with state-of-the-art generative AI to revolutionize\nmetamaterial design. By orchestrating a hierarchical team of agents--each\nspecializing in tasks such as pattern analysis, architectural synthesis, prompt\nengineering, and supervisory feedback--our system leverages the multimodal\nreasoning of GPT-4o alongside the generative precision of DALL-E 3 and a\nfine-tuned Stable Diffusion XL model. This integrated approach automates data\naugmentation, enhances design fidelity, and produces simulation- and 3D\nprinting-ready metamaterial patterns. Comprehensive evaluations, including\nCLIP-based alignment, SHAP interpretability analyses, and mechanical\nsimulations under varied load conditions, demonstrate the framework's ability\nto generate diverse, reproducible, and application-ready designs. CrossMatAgent\nthus establishes a scalable, AI-driven paradigm that bridges the gap between\nconceptual innovation and practical realization, paving the way for accelerated\nmetamaterial development.\n","authors":["Jie Tian","Martin Taylor Sobczak","Dhanush Patil","Jixin Hou","Lin Pang","Arunachalam Ramanathan","Libin Yang","Xianyan Chen","Yuval Golan","Hongyue Sun","Kenan Song","Xianqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2503.19889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02341v2","updated":"2025-03-25T15:55:33Z","published":"2025-01-04T17:32:12Z","title":"UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude\n  Mobility","summary":"  Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has\nintroduced transformative advancements across various domains, like\ntransportation, logistics, and agriculture. Leveraging flexible perspectives\nand rapid maneuverability, UAVs extend traditional systems' perception and\naction capabilities, garnering widespread attention from academia and industry.\nHowever, current UAV operations primarily depend on human control, with only\nlimited autonomy in simple scenarios, and lack the intelligence and\nadaptability needed for more complex environments and tasks. The emergence of\nlarge language models (LLMs) demonstrates remarkable problem-solving and\ngeneralization capabilities, offering a promising pathway for advancing UAV\nintelligence. This paper explores the integration of LLMs and UAVs, beginning\nwith an overview of UAV systems' fundamental components and functionalities,\nfollowed by an overview of the state-of-the-art in LLM technology.\nSubsequently, it systematically highlights the multimodal data resources\navailable for UAVs, which provide critical support for training and evaluation.\nFurthermore, it categorizes and analyzes key tasks and application scenarios\nwhere UAVs and LLMs converge. Finally, a reference roadmap towards agentic UAVs\nis proposed, aiming to enable UAVs to achieve agentic intelligence through\nautonomous perception, memory, reasoning, and tool utilization. Related\nresources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs.\n","authors":["Yonglin Tian","Fei Lin","Yiduo Li","Tengchao Zhang","Qiyao Zhang","Xuan Fu","Jun Huang","Xingyuan Dai","Yutong Wang","Chunwei Tian","Bai Li","Yisheng Lv","Levente Kovács","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2501.02341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19764v1","updated":"2025-03-25T15:28:50Z","published":"2025-03-25T15:28:50Z","title":"OpenLex3D: A New Evaluation Benchmark for Open-Vocabulary 3D Scene\n  Representations","summary":"  3D scene understanding has been transformed by open-vocabulary language\nmodels that enable interaction via natural language. However, the evaluation of\nthese representations is limited to closed-set semantics that do not capture\nthe richness of language. This work presents OpenLex3D, a dedicated benchmark\nto evaluate 3D open-vocabulary scene representations. OpenLex3D provides\nentirely new label annotations for 23 scenes from Replica, ScanNet++, and HM3D,\nwhich capture real-world linguistic variability by introducing synonymical\nobject categories and additional nuanced descriptions. By introducing an\nopen-set 3D semantic segmentation task and an object retrieval task, we provide\ninsights on feature precision, segmentation, and downstream capabilities. We\nevaluate various existing 3D open-vocabulary methods on OpenLex3D, showcasing\nfailure cases, and avenues for improvement. The benchmark is publicly available\nat: https://openlex3d.github.io/.\n","authors":["Christina Kassab","Sacha Morin","Martin Büchner","Matías Mattamala","Kumaraditya Gupta","Abhinav Valada","Liam Paull","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2503.19764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19757v1","updated":"2025-03-25T15:19:56Z","published":"2025-03-25T15:19:56Z","title":"Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy","summary":"  While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https://robodita.github.io.\n","authors":["Zhi Hou","Tianyi Zhang","Yuwen Xiong","Haonan Duan","Hengjun Pu","Ronglei Tong","Chengyang Zhao","Xizhou Zhu","Yu Qiao","Jifeng Dai","Yuntao Chen"],"pdf_url":"https://arxiv.org/pdf/2503.19757v1.pdf","comment":"Preprint; https://robodita.github.io;"},{"id":"http://arxiv.org/abs/2411.06294v2","updated":"2025-03-25T15:13:32Z","published":"2024-11-09T21:37:04Z","title":"Hierarchical Performance-Based Design Optimization Framework for Soft\n  Grippers","summary":"  This paper presents a hierarchical, performance-based framework for the\ndesign optimization of multi-fingered soft grippers. To address the need for\nsystematically defined performance indices, the framework structures the\noptimization process into three integrated layers: Task Space, Motion Space,\nand Design Space. In the Task Space, performance indices are defined as core\nobjectives, while the Motion Space interprets these into specific movement\nprimitives. Finally, the Design Space applies parametric and topological\noptimization techniques to refine the geometry and material distribution of the\nsystem, achieving a balanced design across key performance metrics. The\nframework's layered structure enhances SG design, ensuring balanced performance\nand scalability for complex tasks and contributing to broader advancements in\nsoft robotics.\n","authors":["Hamed Rahimi Nohooji","Holger Voos"],"pdf_url":"https://arxiv.org/pdf/2411.06294v2.pdf","comment":"7 pages, 3 figures, 1 Algorithm"},{"id":"http://arxiv.org/abs/2503.19713v1","updated":"2025-03-25T14:39:04Z","published":"2025-03-25T14:39:04Z","title":"Semi-SD: Semi-Supervised Metric Depth Estimation via Surrounding Cameras\n  for Autonomous Driving","summary":"  In this paper, we introduce Semi-SD, a novel metric depth estimation\nframework tailored for surrounding cameras equipment in autonomous driving. In\nthis work, the input data consists of adjacent surrounding frames and camera\nparameters. We propose a unified spatial-temporal-semantic fusion module to\nconstruct the visual fused features. Cross-attention components for surrounding\ncameras and adjacent frames are utilized to focus on metric scale information\nrefinement and temporal feature matching. Building on this, we propose a pose\nestimation framework using surrounding cameras, their corresponding estimated\ndepths, and extrinsic parameters, which effectively address the scale ambiguity\nin multi-camera setups. Moreover, semantic world model and monocular depth\nestimation world model are integrated to supervised the depth estimation, which\nimprove the quality of depth estimation. We evaluate our algorithm on DDAD and\nnuScenes datasets, and the results demonstrate that our method achieves\nstate-of-the-art performance in terms of surrounding camera based depth\nestimation quality. The source code will be available on\nhttps://github.com/xieyuser/Semi-SD.\n","authors":["Yusen Xie","Zhengmin Huang","Shaojie Shen","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2503.19713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19692v1","updated":"2025-03-25T14:18:18Z","published":"2025-03-25T14:18:18Z","title":"Leveraging Cognitive States for Adaptive Scaffolding of Understanding in\n  Explanatory Tasks in HRI","summary":"  Understanding how scaffolding strategies influence human understanding in\nhuman-robot interaction is important for developing effective assistive\nsystems. This empirical study investigates linguistic scaffolding strategies\nbased on negation as an important means that de-biases the user from potential\nerrors but increases processing costs and hesitations as a means to ameliorate\nprocessing costs. In an adaptive strategy, the user state with respect to the\ncurrent state of understanding and processing capacity was estimated via a\nscoring scheme based on task performance, prior scaffolding strategy, and\ncurrent eye gaze behavior. In the study, the adaptive strategy of providing\nnegations and hesitations was compared with a non-adaptive strategy of\nproviding only affirmations. The adaptive scaffolding strategy was generated\nusing the computational model SHIFT. Our findings indicate that using adaptive\nscaffolding strategies with SHIFT tends to (1) increased processing costs, as\nreflected in longer reaction times, but (2) improved task understanding,\nevidenced by a lower error rate of almost 23%. We assessed the efficiency of\nSHIFT's selected scaffolding strategies across different cognitive states,\nfinding that in three out of five states, the error rate was lower compared to\nthe baseline condition. We discuss how these results align with the assumptions\nof the SHIFT model and highlight areas for refinement. Moreover, we demonstrate\nhow scaffolding strategies, such as negation and hesitation, contribute to more\neffective human-robot explanatory dialogues.\n","authors":["André Groß","Birte Richter","Bjarne Thomzik","Britta Wrede"],"pdf_url":"https://arxiv.org/pdf/2503.19692v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.19690v1","updated":"2025-03-25T14:17:15Z","published":"2025-03-25T14:17:15Z","title":"Risk-Aware Reinforcement Learning for Autonomous Driving: Improving\n  Safety When Driving through Intersection","summary":"  Applying reinforcement learning to autonomous driving has garnered widespread\nattention. However, classical reinforcement learning methods optimize policies\nby maximizing expected rewards but lack sufficient safety considerations, often\nputting agents in hazardous situations. This paper proposes a risk-aware\nreinforcement learning approach for autonomous driving to improve the safety\nperformance when crossing the intersection. Safe critics are constructed to\nevaluate driving risk and work in conjunction with the reward critic to update\nthe actor. Based on this, a Lagrangian relaxation method and cyclic gradient\niteration are combined to project actions into a feasible safe region.\nFurthermore, a Multi-hop and Multi-layer perception (MLP) mixed Attention\nMechanism (MMAM) is incorporated into the actor-critic network, enabling the\npolicy to adapt to dynamic traffic and overcome permutation sensitivity\nchallenges. This allows the policy to focus more effectively on surrounding\npotential risks while enhancing the identification of passing opportunities.\nSimulation tests are conducted on different tasks at unsignalized\nintersections. The results show that the proposed approach effectively reduces\ncollision rates and improves crossing efficiency in comparison to baseline\nalgorithms. Additionally, our ablation experiments demonstrate the benefits of\nincorporating risk-awareness and MMAM into RL.\n","authors":["Bo Leng","Ran Yu","Wei Han","Lu Xiong","Zhuoren Li","Hailong Huang"],"pdf_url":"https://arxiv.org/pdf/2503.19690v1.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2411.18335v2","updated":"2025-03-25T13:57:14Z","published":"2024-11-27T13:34:41Z","title":"Helvipad: A Real-World Dataset for Omnidirectional Stereo Depth\n  Estimation","summary":"  Despite progress in stereo depth estimation, omnidirectional imaging remains\nunderexplored, mainly due to the lack of appropriate data. We introduce\nHelvipad, a real-world dataset for omnidirectional stereo depth estimation,\nfeaturing 40K video frames from video sequences across diverse environments,\nincluding crowded indoor and outdoor scenes with various lighting conditions.\nCollected using two 360{\\deg} cameras in a top-bottom setup and a LiDAR sensor,\nthe dataset includes accurate depth and disparity labels by projecting 3D point\nclouds onto equirectangular images. Additionally, we provide an augmented\ntraining set with an increased label density by using depth completion. We\nbenchmark leading stereo depth estimation models for both standard and\nomnidirectional images. The results show that while recent stereo methods\nperform decently, a challenge persists in accurately estimating depth in\nomnidirectional imaging. To address this, we introduce necessary adaptations to\nstereo models, leading to improved performance.\n","authors":["Mehdi Zayene","Jannik Endres","Albias Havolli","Charles Corbière","Salim Cherkaoui","Alexandre Kontouli","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2411.18335v2.pdf","comment":"Accepted to CVPR 2025. Project page:\n  https://vita-epfl.github.io/Helvipad"},{"id":"http://arxiv.org/abs/2410.07584v2","updated":"2025-03-25T13:23:21Z","published":"2024-10-10T03:33:57Z","title":"Imitation Learning with Limited Actions via Diffusion Planners and Deep\n  Koopman Controllers","summary":"  Recent advances in diffusion-based robot policies have demonstrated\nsignificant potential in imitating multi-modal behaviors. However, these\napproaches typically require large quantities of demonstration data paired with\ncorresponding robot action labels, creating a substantial data collection\nburden. In this work, we propose a plan-then-control framework aimed at\nimproving the action-data efficiency of inverse dynamics controllers by\nleveraging observational demonstration data. Specifically, we adopt a Deep\nKoopman Operator framework to model the dynamical system and utilize\nobservation-only trajectories to learn a latent action representation. This\nlatent representation can then be effectively mapped to real high-dimensional\ncontinuous actions using a linear action decoder, requiring minimal\naction-labeled data. Through experiments on simulated robot manipulation tasks\nand a real robot experiment with multi-modal expert demonstrations, we\ndemonstrate that our approach significantly enhances action-data efficiency and\nachieves high task success rates with limited action data.\n","authors":["Jianxin Bi","Kelvin Lim","Kaiqi Chen","Yifei Huang","Harold Soh"],"pdf_url":"https://arxiv.org/pdf/2410.07584v2.pdf","comment":"Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2025"},{"id":"http://arxiv.org/abs/2503.19613v1","updated":"2025-03-25T12:54:25Z","published":"2025-03-25T12:54:25Z","title":"Energy-aware Joint Orchestration of 5G and Robots: Experimental Testbed\n  and Field Validation","summary":"  5G mobile networks introduce a new dimension for connecting and operating\nmobile robots in outdoor environments, leveraging cloud-native and offloading\nfeatures of 5G networks to enable fully flexible and collaborative cloud robot\noperations. However, the limited battery life of robots remains a significant\nobstacle to their effective adoption in real-world exploration scenarios. This\npaper explores, via field experiments, the potential energy-saving gains of\nOROS, a joint orchestration of 5G and Robot Operating System (ROS) that\ncoordinates multiple 5G-connected robots both in terms of navigation and\nsensing, as well as optimizes their cloud-native service resource utilization\nwhile minimizing total resource and energy consumption on the robots based on\nreal-time feedback. We designed, implemented and evaluated our proposed OROS in\nan experimental testbed composed of commercial off-the-shelf robots and a local\n5G infrastructure deployed on a campus. The experimental results demonstrated\nthat OROS significantly outperforms state-of-the-art approaches in terms of\nenergy savings by offloading demanding computational tasks to the 5G edge\ninfrastructure and dynamic energy management of on-board sensors (e.g.,\nswitching them off when they are not needed). This strategy achieves\napproximately 15% energy savings on the robots, thereby extending battery life,\nwhich in turn allows for longer operating times and better resource\nutilization.\n","authors":["Milan Groshev","Lanfranco Zanzi","Carmen Delgado","Xi Li","Antonio de la Oliva","Xavier Costa-Perez"],"pdf_url":"https://arxiv.org/pdf/2503.19613v1.pdf","comment":"14 pages, 15 figures, journal"},{"id":"http://arxiv.org/abs/2503.19556v1","updated":"2025-03-25T11:23:31Z","published":"2025-03-25T11:23:31Z","title":"ZodiAq: An Isotropic Flagella-Inspired Soft Underwater Drone for Safe\n  Marine Exploration","summary":"  The inherent challenges of robotic underwater exploration, such as\nhydrodynamic effects, the complexity of dynamic coupling, and the necessity for\nsensitive interaction with marine life, call for the adoption of soft robotic\napproaches in marine exploration. To address this, we present a novel\nprototype, ZodiAq, a soft underwater drone inspired by prokaryotic bacterial\nflagella. ZodiAq's unique dodecahedral structure, equipped with 12\nflagella-like arms, ensures design redundancy and compliance, ideal for\nnavigating complex underwater terrains. The prototype features a central unit\nbased on a Raspberry Pi, connected to a sensory system for inertial, depth, and\nvision detection, and an acoustic modem for communication. Combined with the\nimplemented control law, it renders ZodiAq an intelligent system. This paper\ndetails the design and fabrication process of ZodiAq, highlighting design\nchoices and prototype capabilities. Based on the strain-based modeling of\nCosserat rods, we have developed a digital twin of the prototype within a\nsimulation toolbox to ease analysis and control. To optimize its operation in\ndynamic aquatic conditions, a simplified model-based controller has been\ndeveloped and implemented, facilitating intelligent and adaptive movement in\nthe hydrodynamic environment. Extensive experimental demonstrations highlight\nthe drone's potential, showcasing its design redundancy, embodied intelligence,\ncrawling gait, and practical applications in diverse underwater settings. This\nresearch contributes significantly to the field of underwater soft robotics,\noffering a promising new avenue for safe, efficient, and environmentally\nconscious underwater exploration.\n","authors":["Anup Teejo Mathew","Daniel Feliu-Talegon","Yusuf Abdullahi Adamu","Ikhlas Ben Hmida","Costanza Armanini","Cesare Stefanini","Lakmal Seneviratne","Federico Renda"],"pdf_url":"https://arxiv.org/pdf/2503.19556v1.pdf","comment":"43 pages, including disclaimer page, pre-peer-review version of the\n  manuscript, and supplementary material"},{"id":"http://arxiv.org/abs/2412.06359v2","updated":"2025-03-25T10:43:50Z","published":"2024-12-09T10:23:03Z","title":"On-Device Self-Supervised Learning of Low-Latency Monocular Depth from\n  Only Events","summary":"  Event cameras provide low-latency perception for only milliwatts of power.\nThis makes them highly suitable for resource-restricted, agile robots such as\nsmall flying drones. Self-supervised learning based on contrast maximization\nholds great potential for event-based robot vision, as it foregoes the need for\nhigh-frequency ground truth and allows for online learning in the robot's\noperational environment. However, online, on-board learning raises the major\nchallenge of achieving sufficient computational efficiency for real-time\nlearning, while maintaining competitive visual perception performance. In this\nwork, we improve the time and memory efficiency of the contrast maximization\npipeline, making on-device learning of low-latency monocular depth possible. We\ndemonstrate that online learning on board a small drone yields more accurate\ndepth estimates and more successful obstacle avoidance behavior compared to\nonly pre-training. Benchmarking experiments show that the proposed pipeline is\nnot only efficient, but also achieves state-of-the-art depth estimation\nperformance among self-supervised approaches. Our work taps into the unused\npotential of online, on-device robot learning, promising smaller reality gaps\nand better performance.\n","authors":["Jesse Hagenaars","Yilun Wu","Federico Paredes-Vallés","Stein Stroobants","Guido de Croon"],"pdf_url":"https://arxiv.org/pdf/2412.06359v2.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2412.05066v2","updated":"2025-03-25T10:41:48Z","published":"2024-12-06T14:23:56Z","title":"BimArt: A Unified Approach for the Synthesis of 3D Bimanual Interaction\n  with Articulated Objects","summary":"  We present BimArt, a novel generative approach for synthesizing 3D bimanual\nhand interactions with articulated objects. Unlike prior works, we do not rely\non a reference grasp, a coarse hand trajectory, or separate modes for grasping\nand articulating. To achieve this, we first generate distance-based contact\nmaps conditioned on the object trajectory with an articulation-aware feature\nrepresentation, revealing rich bimanual patterns for manipulation. The learned\ncontact prior is then used to guide our hand motion generator, producing\ndiverse and realistic bimanual motions for object movement and articulation.\nOur work offers key insights into feature representation and contact prior for\narticulated objects, demonstrating their effectiveness in taming the complex,\nhigh-dimensional space of bimanual hand-object interactions. Through\ncomprehensive quantitative experiments, we demonstrate a clear step towards\nsimplified and high-quality hand-object animations that surpass the state of\nthe art in motion quality and diversity. Project page:\nhttps://vcai.mpi-inf.mpg.de/projects/bimart/.\n","authors":["Wanyue Zhang","Rishabh Dabral","Vladislav Golyanik","Vasileios Choutas","Eduardo Alvarado","Thabo Beeler","Marc Habermann","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2412.05066v2.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2503.19516v1","updated":"2025-03-25T10:11:06Z","published":"2025-03-25T10:11:06Z","title":"DataPlatter: Boosting Robotic Manipulation Generalization with Minimal\n  Costly Data","summary":"  The growing adoption of Vision-Language-Action (VLA) models in embodied AI\nintensifies the demand for diverse manipulation demonstrations. However, high\ncosts associated with data collection often result in insufficient data\ncoverage across all scenarios, which limits the performance of the models. It\nis observed that the spatial reasoning phase (SRP) in large workspace dominates\nthe failure cases. Fortunately, this data can be collected with low cost,\nunderscoring the potential of leveraging inexpensive data to improve model\nperformance. In this paper, we introduce the DataPlatter method, a framework\nthat decouples training trajectories into distinct task stages and leverages\nabundant easily collectible SRP data to enhance VLA model's generalization.\nThrough analysis we demonstrate that sub-task-specific training with additional\nSRP data with proper proportion can act as a performance catalyst for robot\nmanipulation, maximizing the utilization of costly physical interaction phase\n(PIP) data. Experiments show that through introducing large proportion of\ncost-effective SRP trajectories into a limited set of PIP data, we can achieve\na maximum improvement of 41\\% on success rate in zero-shot scenes, while with\nthe ability to transfer manipulation skill to novel targets.\n","authors":["Liming Zheng","Feng Yan","Fanfan Liu","Chengjian Feng","Yufeng Zhong","Yiyang Huang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2503.19516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19510v1","updated":"2025-03-25T10:01:57Z","published":"2025-03-25T10:01:57Z","title":"RoboFlamingo-Plus: Fusion of Depth and RGB Perception with\n  Vision-Language Models for Enhanced Robotic Manipulation","summary":"  As robotic technologies advancing towards more complex multimodal\ninteractions and manipulation tasks, the integration of advanced\nVision-Language Models (VLMs) has become a key driver in the field. Despite\nprogress with current methods, challenges persist in fusing depth and RGB\ninformation within 3D environments and executing tasks guided by linguistic\ninstructions. In response to these challenges, we have enhanced the existing\nRoboFlamingo framework by introducing RoboFlamingo-Plus, which incorporates\ndepth data into VLMs to significantly improve robotic manipulation performance.\nOur research achieves a nuanced fusion of RGB and depth information by\nintegrating a pre-trained Vision Transformer (ViT) with a resampling technique,\nclosely aligning this combined data with linguistic cues for superior\nmultimodal understanding. The novelty of RoboFlamingo-Plus lies in its\nadaptation of inputs for depth data processing, leveraging a pre-trained\nresampler for depth feature extraction, and employing cross-attention\nmechanisms for optimal feature integration. These improvements allow\nRoboFlamingo-Plus to not only deeply understand 3D environments but also easily\nperform complex, language-guided tasks in challenging settings. Experimental\nresults show that RoboFlamingo-Plus boosts robotic manipulation by 10-20% over\ncurrent methods, marking a significant advancement. Codes and model weights are\npublic at RoboFlamingo-Plus.\n","authors":["Sheng Wang"],"pdf_url":"https://arxiv.org/pdf/2503.19510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19506v1","updated":"2025-03-25T09:57:21Z","published":"2025-03-25T09:57:21Z","title":"MM-LINS: a Multi-Map LiDAR-Inertial System for Over-Degenerate\n  Environments","summary":"  SLAM plays a crucial role in automation tasks, such as warehouse logistics,\nhealthcare robotics, and restaurant delivery. These scenes come with various\nchallenges, including navigating around crowds of people, dealing with flying\nplastic bags that can temporarily blind sensors, and addressing reduced LiDAR\ndensity caused by cooking smoke. Such scenarios can result in over-degeneracy,\ncausing the map to drift. To address this issue, this paper presents a\nmulti-map LiDAR-inertial system (MM-LINS) for the first time. The front-end\nemploys an iterated error state Kalman filter for state estimation and\nintroduces a reliable evaluation strategy for degeneracy detection. If\nover-degeneracy is detected, the active map will be stored into sleeping maps.\nSubsequently, the system continuously attempts to construct new maps using a\ndynamic initialization method to ensure successful initialization upon leaving\nthe over-degeneracy. Regarding the back-end, the Scan Context descriptor is\nutilized to detect inter-map similarity. Upon successful recognition of a\nsleeping map that shares a common region with the active map, the overlapping\ntrajectory region is utilized to constrain the positional transformation near\nthe edge of the prior map. In response to this, a constraint-enhanced map\nfusion strategy is proposed to achieve high-precision positional and mapping\nresults. Experiments have been conducted separately on both public datasets\nthat exhibited over-degenerate conditions and in real-world environments. These\ntests demonstrated the effectiveness of MM-LINS in over-degeneracy environment.\nOur codes are open-sourced on Github.\n","authors":["Yongxin Ma","Jie Xu","Shenghai Yuan","Tian Zhi","Wenlu Yu","Jun Zhou","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2503.19506v1.pdf","comment":"Accepted by IEEE Transactions on Intelligent Vehicles"},{"id":"http://arxiv.org/abs/2412.00171v3","updated":"2025-03-25T09:43:25Z","published":"2024-11-29T17:36:03Z","title":"RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot\n  Task Planning and Execution in Open-World","summary":"  Existing robot policies predominantly adopt the task-centric approach,\nrequiring end-to-end task data collection. This results in limited\ngeneralization to new tasks and difficulties in pinpointing errors within\nlong-horizon, multi-stage tasks. To address this, we propose RoboMatrix, a\nskill-centric hierarchical framework designed for scalable robot task planning\nand execution in open-world environments. RoboMatrix extracts general\nmeta-skills from diverse complex tasks, enabling the completion of unseen tasks\nthrough skill composition. Its architecture consists of a high-level scheduling\nlayer that utilizes large language models (LLMs) for task decomposition, an\nintermediate skill layer housing meta-skill models, and a low-level hardware\nlayer for robot control. A key innovation of our work is the introduction of\nthe first unified vision-language-action (VLA) model capable of seamlessly\nintegrating both movement and manipulation within one model. This is achieved\nby combining vision and language prompts to generate discrete actions.\nExperimental results demonstrate that RoboMatrix achieves a 50% higher success\nrate than task-centric baselines when applied to unseen objects, scenes, and\ntasks. To advance open-world robotics research, we will open-source code,\nhardware designs, model weights, and datasets at\nhttps://github.com/WayneMao/RoboMatrix.\n","authors":["Weixin Mao","Weiheng Zhong","Zhou Jiang","Dong Fang","Zhongyue Zhang","Zihan Lan","Haosheng Li","Fan Jia","Tiancai Wang","Haoqiang Fan","Osamu Yoshie"],"pdf_url":"https://arxiv.org/pdf/2412.00171v3.pdf","comment":"17 pages, 16 figures"},{"id":"http://arxiv.org/abs/2503.19941v1","updated":"2025-03-25T09:21:10Z","published":"2025-03-25T09:21:10Z","title":"Body Discovery of Embodied AI","summary":"  In the pursuit of realizing artificial general intelligence (AGI), the\nimportance of embodied artificial intelligence (AI) becomes increasingly\napparent. Following this trend, research integrating robots with AGI has become\nprominent. As various kinds of embodiments have been designed, adaptability to\ndiverse embodiments will become important to AGI. We introduce a new challenge,\ntermed \"Body Discovery of Embodied AI\", focusing on tasks of recognizing\nembodiments and summarizing neural signal functionality. The challenge\nencompasses the precise definition of an AI body and the intricate task of\nidentifying embodiments in dynamic environments, where conventional approaches\noften prove inadequate. To address these challenges, we apply causal inference\nmethod and evaluate it by developing a simulator tailored for testing\nalgorithms with virtual environments. Finally, we validate the efficacy of our\nalgorithms through empirical testing, demonstrating their robust performance in\nvarious scenarios based on virtual environments.\n","authors":["Zhe Sun","Pengfei Tian","Xiaozhu Hu","Xiaoyu Zhao","Huiying Li","Zhenliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.19941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03146v2","updated":"2025-03-25T08:52:12Z","published":"2024-12-04T09:13:03Z","title":"MCVO: A Generic Visual Odometry for Arbitrarily Arranged Multi-Cameras","summary":"  Making multi-camera visual SLAM systems easier to set up and more robust to\nthe environment is attractive for vision robots. Existing monocular and\nbinocular vision SLAM systems have narrow sensing Field-of-View (FoV),\nresulting in degenerated accuracy and limited robustness in textureless\nenvironments. Thus multi-camera SLAM systems are gaining attention because they\ncan provide redundancy with much wider FoV. However, the usual arbitrary\nplacement and orientation of multiple cameras make the pose scale estimation\nand system updating challenging. To address these problems, we propose a robust\nvisual odometry system for rigidly-bundled arbitrarily-arranged multi-cameras,\nnamely MCVO, which can achieve metric-scale state estimation with high\nflexibility in the cameras' arrangement. Specifically, we first design a\nlearning-based feature tracking framework to shift the pressure of CPU\nprocessing of multiple video streams to GPU. Then we initialize the odometry\nsystem with the metric-scale poses under the rigid constraints between moving\ncameras. Finally, we fuse the features of the multi-cameras in the back-end to\nachieve robust pose estimation and online scale optimization. Additionally,\nmulti-camera features help improve the loop detection for pose graph\noptimization. Experiments on KITTI-360 and MultiCamData datasets validate its\nrobustness over arbitrarily arranged cameras. Compared with other stereo and\nmulti-camera visual SLAM systems, our method obtains higher pose accuracy with\nbetter generalization ability. Our codes and online demos are available at\nhttps://github.com/JunhaoWang615/MCVO\n","authors":["Huai Yu","Junhao Wang","Yao He","Wen Yang","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2412.03146v2.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.19457v1","updated":"2025-03-25T08:46:50Z","published":"2025-03-25T08:46:50Z","title":"G-DexGrasp: Generalizable Dexterous Grasping Synthesis Via Part-Aware\n  Prior Retrieval and Prior-Assisted Generation","summary":"  Recent advances in dexterous grasping synthesis have demonstrated significant\nprogress in producing reasonable and plausible grasps for many task purposes.\nBut it remains challenging to generalize to unseen object categories and\ndiverse task instructions. In this paper, we propose G-DexGrasp, a\nretrieval-augmented generation approach that can produce high-quality dexterous\nhand configurations for unseen object categories and language-based task\ninstructions. The key is to retrieve generalizable grasping priors, including\nthe fine-grained contact part and the affordance-related distribution of\nrelevant grasping instances, for the following synthesis pipeline.\nSpecifically, the fine-grained contact part and affordance act as generalizable\nguidance to infer reasonable grasping configurations for unseen objects with a\ngenerative model, while the relevant grasping distribution plays as\nregularization to guarantee the plausibility of synthesized grasps during the\nsubsequent refinement optimization. Our comparison experiments validate the\neffectiveness of our key designs for generalization and demonstrate the\nremarkable performance against the existing approaches. Project page:\nhttps://g-dexgrasp.github.io/\n","authors":["Juntao Jian","Xiuping Liu","Zixuan Chen","Manyi Li","Jian Liu","Ruizhen Hu"],"pdf_url":"https://arxiv.org/pdf/2503.19457v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.19397v1","updated":"2025-03-25T07:09:31Z","published":"2025-03-25T07:09:31Z","title":"Quality-focused Active Adversarial Policy for Safe Grasping in\n  Human-Robot Interaction","summary":"  Vision-guided robot grasping methods based on Deep Neural Networks (DNNs)\nhave achieved remarkable success in handling unknown objects, attributable to\ntheir powerful generalizability. However, these methods with this\ngeneralizability tend to recognize the human hand and its adjacent objects as\ngraspable targets, compromising safety during Human-Robot Interaction (HRI). In\nthis work, we propose the Quality-focused Active Adversarial Policy (QFAAP) to\nsolve this problem. Specifically, the first part is the Adversarial Quality\nPatch (AQP), wherein we design the adversarial quality patch loss and leverage\nthe grasp dataset to optimize a patch with high quality scores. Next, we\nconstruct the Projected Quality Gradient Descent (PQGD) and integrate it with\nthe AQP, which contains only the hand region within each real-time frame,\nendowing the AQP with fast adaptability to the human hand shape. Through AQP\nand PQGD, the hand can be actively adversarial with the surrounding objects,\nlowering their quality scores. Therefore, further setting the quality score of\nthe hand to zero will reduce the grasping priority of both the hand and its\nadjacent objects, enabling the robot to grasp other objects away from the hand\nwithout emergency stops. We conduct extensive experiments on the benchmark\ndatasets and a cobot, showing the effectiveness of QFAAP. Our code and demo\nvideos are available here: https://github.com/clee-jaist/QFAAP.\n","authors":["Chenghao Li","Razvan Beuran","Nak Young Chong"],"pdf_url":"https://arxiv.org/pdf/2503.19397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21257v2","updated":"2025-03-25T05:46:03Z","published":"2025-02-28T17:30:39Z","title":"RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract\n  to Concrete","summary":"  Recent advancements in Multimodal Large Language Models (MLLMs) have shown\nremarkable capabilities across various multimodal contexts. However, their\napplication in robotic scenarios, particularly for long-horizon manipulation\ntasks, reveals significant limitations. These limitations arise from the\ncurrent MLLMs lacking three essential robotic brain capabilities: Planning\nCapability, which involves decomposing complex manipulation instructions into\nmanageable sub-tasks; Affordance Perception, the ability to recognize and\ninterpret the affordances of interactive objects; and Trajectory Prediction,\nthe foresight to anticipate the complete manipulation trajectory necessary for\nsuccessful execution. To enhance the robotic brain's core capabilities from\nabstract to concrete, we introduce ShareRobot, a high-quality heterogeneous\ndataset that labels multi-dimensional information such as task planning, object\naffordance, and end-effector trajectory. ShareRobot's diversity and accuracy\nhave been meticulously refined by three human annotators. Building on this\ndataset, we developed RoboBrain, an MLLM-based model that combines robotic and\ngeneral multi-modal data, utilizes a multi-stage training strategy, and\nincorporates long videos and high-resolution images to improve its robotic\nmanipulation capabilities. Extensive experiments demonstrate that RoboBrain\nachieves state-of-the-art performance across various robotic tasks,\nhighlighting its potential to advance robotic brain capabilities.\n","authors":["Yuheng Ji","Huajie Tan","Jiayu Shi","Xiaoshuai Hao","Yuan Zhang","Hengyuan Zhang","Pengwei Wang","Mengdi Zhao","Yao Mu","Pengju An","Xinda Xue","Qinghang Su","Huaihai Lyu","Xiaolong Zheng","Jiaming Liu","Zhongyuan Wang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.21257v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05507v2","updated":"2025-03-25T05:43:28Z","published":"2024-12-07T02:40:55Z","title":"AutoURDF: Unsupervised Robot Modeling from Point Cloud Frames Using\n  Cluster Registration","summary":"  Robot description models are essential for simulation and control, yet their\ncreation often requires significant manual effort. To streamline this modeling\nprocess, we introduce AutoURDF, an unsupervised approach for constructing\ndescription files for unseen robots from point cloud frames. Our method\nleverages a cluster-based point cloud registration model that tracks the 6-DoF\ntransformations of point clusters. Through analyzing cluster movements, we\nhierarchically address the following challenges: (1) moving part segmentation,\n(2) body topology inference, and (3) joint parameter estimation. The complete\npipeline produces robot description files that are fully compatible with\nexisting simulators. We validate our method across a variety of robots, using\nboth synthetic and real-world scan data. Results indicate that our approach\noutperforms previous methods in registration and body topology estimation\naccuracy, offering a scalable solution for automated robot modeling.\n","authors":["Jiong Lin","Lechen Zhang","Kwansoo Lee","Jialong Ning","Judah Goldfeder","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2412.05507v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2412.20104v3","updated":"2025-03-25T04:15:15Z","published":"2024-12-28T10:12:12Z","title":"SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object\n  Interaction Synthesis","summary":"  Synthesizing realistic human-object interaction motions is a critical problem\nin VR/AR and human animation. Unlike the commonly studied scenarios involving a\nsingle human or hand interacting with one object, we address a more generic\nmulti-body setting with arbitrary numbers of humans, hands, and objects. This\ncomplexity introduces significant challenges in synchronizing motions due to\nthe high correlations and mutual influences among bodies. To address these\nchallenges, we introduce SyncDiff, a novel method for multi-body interaction\nsynthesis using a synchronized motion diffusion strategy. SyncDiff employs a\nsingle diffusion model to capture the joint distribution of multi-body motions.\nTo enhance motion fidelity, we propose a frequency-domain motion decomposition\nscheme. Additionally, we introduce a new set of alignment scores to emphasize\nthe synchronization of different body motions. SyncDiff jointly optimizes both\ndata sample likelihood and alignment likelihood through an explicit\nsynchronization strategy. Extensive experiments across four datasets with\nvarious multi-body configurations demonstrate the superiority of SyncDiff over\nexisting state-of-the-art motion synthesis methods.\n","authors":["Wenkun He","Yun Liu","Ruitao Liu","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2412.20104v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19330v1","updated":"2025-03-25T03:45:21Z","published":"2025-03-25T03:45:21Z","title":"MATT-GS: Masked Attention-based 3DGS for Robot Perception and Object\n  Detection","summary":"  This paper presents a novel masked attention-based 3D Gaussian Splatting\n(3DGS) approach to enhance robotic perception and object detection in\nindustrial and smart factory environments. U2-Net is employed for background\nremoval to isolate target objects from raw images, thereby minimizing clutter\nand ensuring that the model processes only relevant data. Additionally, a Sobel\nfilter-based attention mechanism is integrated into the 3DGS framework to\nenhance fine details - capturing critical features such as screws, wires, and\nintricate textures essential for high-precision tasks. We validate our approach\nusing quantitative metrics, including L1 loss, SSIM, PSNR, comparing the\nperformance of the background-removed and attention-incorporated 3DGS model\nagainst the ground truth images and the original 3DGS training baseline. The\nresults demonstrate significant improves in visual fidelity and detail\npreservation, highlighting the effectiveness of our method in enhancing robotic\nvision for object recognition and manipulation in complex industrial settings.\n","authors":["Jee Won Lee","Hansol Lim","SooYeun Yang","Jongseong Brad Choi"],"pdf_url":"https://arxiv.org/pdf/2503.19330v1.pdf","comment":"This work has been submitted to the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS) for possible publication"},{"id":"http://arxiv.org/abs/2503.19317v1","updated":"2025-03-25T03:25:57Z","published":"2025-03-25T03:25:57Z","title":"Towards Uncertainty Unification: A Case Study for Preference Learning","summary":"  Learning human preferences is essential for human-robot interaction, as it\nenables robots to adapt their behaviors to align with human expectations and\ngoals. However, the inherent uncertainties in both human behavior and robotic\nsystems make preference learning a challenging task. While probabilistic\nrobotics algorithms offer uncertainty quantification, the integration of human\npreference uncertainty remains underexplored. To bridge this gap, we introduce\nuncertainty unification and propose a novel framework, uncertainty-unified\npreference learning (UUPL), which enhances Gaussian Process (GP)-based\npreference learning by unifying human and robot uncertainties. Specifically,\nUUPL includes a human preference uncertainty model that improves GP posterior\nmean estimation, and an uncertainty-weighted Gaussian Mixture Model (GMM) that\nenhances GP predictive variance accuracy. Additionally, we design a\nuser-specific calibration process to align uncertainty representations across\nusers, ensuring consistency and reliability in the model performance.\nComprehensive experiments and user studies demonstrate that UUPL achieves\nstate-of-the-art performance in both prediction accuracy and user rating. An\nablation study further validates the effectiveness of human uncertainty model\nand uncertainty-weighted GMM of UUPL.\n","authors":["Shaoting Peng","Haonan Chen","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2503.19317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19302v1","updated":"2025-03-25T03:05:00Z","published":"2025-03-25T03:05:00Z","title":"Observation Adaptation via Annealed Importance Resampling for Partially\n  Observable Markov Decision Processes","summary":"  Partially observable Markov decision processes (POMDPs) are a general\nmathematical model for sequential decision-making in stochastic environments\nunder state uncertainty. POMDPs are often solved \\textit{online}, which enables\nthe algorithm to adapt to new information in real time. Online solvers\ntypically use bootstrap particle filters based on importance resampling for\nupdating the belief distribution. Since directly sampling from the ideal state\ndistribution given the latest observation and previous state is infeasible,\nparticle filters approximate the posterior belief distribution by propagating\nstates and adjusting weights through prediction and resampling steps. However,\nin practice, the importance resampling technique often leads to particle\ndegeneracy and sample impoverishment when the state transition model poorly\naligns with the posterior belief distribution, especially when the received\nobservation is highly informative. We propose an approach that constructs a\nsequence of bridge distributions between the state-transition and optimal\ndistributions through iterative Monte Carlo steps, better accommodating noisy\nobservations in online POMDP solvers. Our algorithm demonstrates significantly\nsuperior performance compared to state-of-the-art methods when evaluated across\nmultiple challenging POMDP domains.\n","authors":["Yunuo Zhang","Baiting Luo","Ayan Mukhopadhyay","Abhishek Dubey"],"pdf_url":"https://arxiv.org/pdf/2503.19302v1.pdf","comment":"Accepted as Oral Presentation to ICAPS 2025"},{"id":"http://arxiv.org/abs/2503.19288v1","updated":"2025-03-25T02:37:28Z","published":"2025-03-25T02:37:28Z","title":"A Novel Underwater Vehicle With Orientation Adjustable Thrusters: Design\n  and Adaptive Tracking Control","summary":"  Autonomous underwater vehicles (AUVs) are essential for marine exploration\nand research. However, conventional designs often struggle with limited\nmaneuverability in complex, dynamic underwater environments. This paper\nintroduces an innovative orientation-adjustable thruster AUV (OATAUV), equipped\nwith a redundant vector thruster configuration that enables full\nsix-degree-of-freedom (6-DOF) motion and composite maneuvers. To overcome\nchallenges associated with uncertain model parameters and environmental\ndisturbances, a novel feedforward adaptive model predictive controller (FFAMPC)\nis proposed to ensure robust trajectory tracking, which integrates real-time\nstate feedback with adaptive parameter updates. Extensive experiments,\nincluding closed-loop tracking and composite motion tests in a laboratory pool,\nvalidate the enhanced performance of the OAT-AUV. The results demonstrate that\nthe OAT-AUV's redundant vector thruster configuration enables 23.8% cost\nreduction relative to common vehicles, while the FF-AMPC controller achieves\n68.6% trajectory tracking improvement compared to PID controllers. Uniquely,\nthe system executes composite helical/spiral trajectories unattainable by\nsimilar vehicles.\n","authors":["Yifei Wang","Shihan Kong","Zhanhua Xin","Kaiwei Zhu","Dongyue Li","Junzhi Yu"],"pdf_url":"https://arxiv.org/pdf/2503.19288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19281v1","updated":"2025-03-25T02:23:47Z","published":"2025-03-25T02:23:47Z","title":"CubeRobot: Grounding Language in Rubik's Cube Manipulation via\n  Vision-Language Model","summary":"  Proving Rubik's Cube theorems at the high level represents a notable\nmilestone in human-level spatial imagination and logic thinking and reasoning.\nTraditional Rubik's Cube robots, relying on complex vision systems and fixed\nalgorithms, often struggle to adapt to complex and dynamic scenarios. To\novercome this limitation, we introduce CubeRobot, a novel vision-language model\n(VLM) tailored for solving 3x3 Rubik's Cubes, empowering embodied agents with\nmultimodal understanding and execution capabilities. We used the CubeCoT image\ndataset, which contains multiple-level tasks (43 subtasks in total) that humans\nare unable to handle, encompassing various cube states. We incorporate a\ndual-loop VisionCoT architecture and Memory Stream, a paradigm for extracting\ntask-related features from VLM-generated planning queries, thus enabling\nCubeRobot to independent planning, decision-making, reflection and separate\nmanagement of high- and low-level Rubik's Cube tasks. Furthermore, in low-level\nRubik's Cube restoration tasks, CubeRobot achieved a high accuracy rate of\n100%, similar to 100% in medium-level tasks, and achieved an accuracy rate of\n80% in high-level tasks.\n","authors":["Feiyang Wang","Xiaomin Yu","Wangyu Wu"],"pdf_url":"https://arxiv.org/pdf/2503.19281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19225v1","updated":"2025-03-25T00:12:48Z","published":"2025-03-25T00:12:48Z","title":"CoinFT: A Coin-Sized, Capacitive 6-Axis Force Torque Sensor for Robotic\n  Applications","summary":"  We introduce CoinFT, a capacitive 6-axis force/torque (F/T) sensor that is\ncompact, light, low-cost, and robust with an average mean-squared error of\n0.11N for force and 0.84mNm for moment when the input ranges from 0~10N and\n0~4N in normal and shear directions, respectively. CoinFT is a stack of two\nrigid PCBs with comb-shaped electrodes connected by an array of silicone rubber\npillars. The microcontroller interrogates the electrodes in different subsets\nin order to enhance sensitivity for measuring 6-axis F/T. The combination of\ndesirable features of CoinFT enables various contact-rich robot interactions at\na scale, across different embodiment domains including drones, robot\nend-effectors, and wearable haptic devices. We demonstrate the utility of\nCoinFT on drones by performing an attitude-based force control to perform tasks\nthat require careful contact force modulation. The design, fabrication, and\nfirmware of CoinFT are open-sourced at\nhttps://hojung-choi.github.io/coinft.github.io/.\n","authors":["Hojung Choi","Jun En Low","Tae Myung Huh","Gabriela A. Uribe","Seongheon Hong","Kenneth A. W. Hoffman","Julia Di","Tony G. Chen","Andrew A. Stanley","Mark R. Cutkosky"],"pdf_url":"https://arxiv.org/pdf/2503.19225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20820v1","updated":"2025-03-25T21:52:55Z","published":"2025-03-25T21:52:55Z","title":"Benchmarking Multi-Object Grasping","summary":"  In this work, we describe a multi-object grasping benchmark to evaluate the\ngrasping and manipulation capabilities of robotic systems in both pile and\nsurface scenarios. The benchmark introduces three robot multi-object grasping\nbenchmarking protocols designed to challenge different aspects of robotic\nmanipulation. These protocols are: 1) the Only-Pick-Once protocol, which\nassesses the robot's ability to efficiently pick multiple objects in a single\nattempt; 2) the Accurate pick-trnsferring protocol, which evaluates the robot's\ncapacity to selectively grasp and transport a specific number of objects from a\ncluttered environment; and 3) the Pick-transferring-all protocol, which\nchallenges the robot to clear an entire scene by sequentially grasping and\ntransferring all available objects. These protocols are intended to be adopted\nby the broader robotics research community, providing a standardized method to\nassess and compare robotic systems' performance in multi-object grasping tasks.\nWe establish baselines for these protocols using standard planning and\nperception algorithms on a Barrett hand, Robotiq parallel jar gripper, and the\nPisa/IIT Softhand-2, which is a soft underactuated robotic hand. We discuss the\nresults in relation to human performance in similar tasks we well.\n","authors":["Tianze Chen","Ricardo Frumento","Giulia Pagnanelli","Gianmarco Cei","Villa Keth","Shahadding Gafarov","Jian Gong","Zihe Ye","Marco Baracca","Salvatore D'Avella","Matteo Bianchi","Yu Sun"],"pdf_url":"https://arxiv.org/pdf/2503.20820v1.pdf","comment":"This paper contains 11 pages and 5 figures. This paper is under\n  review of a robotics journal"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2503.18945v2","updated":"2025-03-25T15:31:25Z","published":"2025-03-24T17:59:51Z","title":"Aether: Geometric-Aware Unified World Modeling","summary":"  The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance is\ncomparable with or even better than that of domain-specific models.\nAdditionally, Aether employs camera trajectories as geometry-informed action\nspaces, enabling effective action-conditioned prediction and visual planning.\nWe hope our work inspires the community to explore new frontiers in\nphysically-reasonable world modeling and its applications.\n","authors":[" Aether Team","Haoyi Zhu","Yifan Wang","Jianjun Zhou","Wenzheng Chang","Yang Zhou","Zizun Li","Junyi Chen","Chunhua Shen","Jiangmiao Pang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2503.18945v2.pdf","comment":"Project Page: https://aether-world.github.io/"},{"id":"http://arxiv.org/abs/2503.18860v2","updated":"2025-03-25T10:59:23Z","published":"2025-03-24T16:35:41Z","title":"HunyuanPortrait: Implicit Condition Control for Enhanced Portrait\n  Animation","summary":"  We introduce HunyuanPortrait, a diffusion-based condition control method that\nemploys implicit representations for highly controllable and lifelike portrait\nanimation. Given a single portrait image as an appearance reference and video\nclips as driving templates, HunyuanPortrait can animate the character in the\nreference image by the facial expression and head pose of the driving videos.\nIn our framework, we utilize pre-trained encoders to achieve the decoupling of\nportrait motion information and identity in videos. To do so, implicit\nrepresentation is adopted to encode motion information and is employed as\ncontrol signals in the animation phase. By leveraging the power of stable video\ndiffusion as the main building block, we carefully design adapter layers to\ninject control signals into the denoising unet through attention mechanisms.\nThese bring spatial richness of details and temporal consistency.\nHunyuanPortrait also exhibits strong generalization performance, which can\neffectively disentangle appearance and motion under different image styles. Our\nframework outperforms existing methods, demonstrating superior temporal\nconsistency and controllability. Our project is available at\nhttps://kkakkkka.github.io/HunyuanPortrait.\n","authors":["Zunnan Xu","Zhentao Yu","Zixiang Zhou","Jun Zhou","Xiaoyu Jin","Fa-Ting Hong","Xiaozhong Ji","Junwei Zhu","Chengfei Cai","Shiyu Tang","Qin Lin","Xiu Li","Qinglin Lu"],"pdf_url":"https://arxiv.org/pdf/2503.18860v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18854v2","updated":"2025-03-25T13:50:20Z","published":"2025-03-24T16:32:17Z","title":"MC-LLaVA: Multi-Concept Personalized Vision-Language Model","summary":"  Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA}.\n","authors":["Ruichuan An","Sihan Yang","Ming Lu","Renrui Zhang","Kai Zeng","Yulin Luo","Jiajun Cao","Hao Liang","Ying Chen","Qi She","Shanghang Zhang","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.18854v2.pdf","comment":"I sincerely apologize for any inconvenience caused. We actually\n  uploaded this paper to arXiv in November 2024, as arXiv:2411.11706. During\n  this update, we did not consider the replacement operation of arXiv, which\n  led to duplicate submissions. We have made modifications at the original\n  address arXiv:2411.11706"},{"id":"http://arxiv.org/abs/2503.18840v2","updated":"2025-03-25T10:52:26Z","published":"2025-03-24T16:13:04Z","title":"Learning to segment anatomy and lesions from disparately labeled sources\n  in brain MRI","summary":"  Segmenting healthy tissue structures alongside lesions in brain Magnetic\nResonance Images (MRI) remains a challenge for today's algorithms due to\nlesion-caused disruption of the anatomy and lack of jointly labeled training\ndatasets, where both healthy tissues and lesions are labeled on the same\nimages. In this paper, we propose a method that is robust to lesion-caused\ndisruptions and can be trained from disparately labeled training sets, i.e.,\nwithout requiring jointly labeled samples, to automatically segment both. In\ncontrast to prior work, we decouple healthy tissue and lesion segmentation in\ntwo paths to leverage multi-sequence acquisitions and merge information with an\nattention mechanism. During inference, an image-specific adaptation reduces\nadverse influences of lesion regions on healthy tissue predictions. During\ntraining, the adaptation is taken into account through meta-learning and\nco-training is used to learn from disparately labeled training images. Our\nmodel shows an improved performance on several anatomical structures and\nlesions on a publicly available brain glioblastoma dataset compared to the\nstate-of-the-art segmentation methods.\n","authors":["Meva Himmetoglu","Ilja Ciernik","Ender Konukoglu"],"pdf_url":"https://arxiv.org/pdf/2503.18840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04832v5","updated":"2025-03-25T09:08:09Z","published":"2025-03-05T10:59:32Z","title":"Lightweight Embedded FPGA Deployment of Learned Image Compression with\n  Knowledge Distillation and Hybrid Quantization","summary":"  Learnable Image Compression (LIC) has shown the potential to outperform\nstandardized video codecs in RD efficiency, prompting the research for\nhardware-friendly implementations. Most existing LIC hardware implementations\nprioritize latency to RD-efficiency and through an extensive exploration of the\nhardware design space. We present a novel design paradigm where the burden of\ntuning the design for a specific hardware platform is shifted towards model\ndimensioning and without compromising on RD-efficiency. First, we design a\nframework for distilling a leaner student LIC model from a reference teacher:\nby tuning a single model hyperparameters, we can meet the constraints of\ndifferent hardware platforms without a complex hardware design exploration.\nSecond, we propose a hardware-friendly implementation of the Generalized\nDivisive Normalization - GDN activation that preserves RD efficiency even post\nparameter quantization. Third, we design a pipelined FPGA configuration which\ntakes full advantage of available FPGA resources by leveraging parallel\nprocessing and optimizing resource allocation. Our experiments with a state of\nthe art LIC model show that we outperform all existing FPGA implementations\nwhile performing very close to the original model.\n","authors":["Alaa Mazouz","Sumanta Chaudhuri","Marco Cagnanzzo","Mihai Mitrea","Enzo Tartaglione","Attilio Fiandrotti"],"pdf_url":"https://arxiv.org/pdf/2503.04832v5.pdf","comment":"1. Submitted to IEEE Transactions on Circuits and Systems for Video\n  Technology in March 2025. 2. Corrected numerous mistakes from previous\n  versions in results, citations and metrics numbers in figures"},{"id":"http://arxiv.org/abs/2503.18783v2","updated":"2025-03-25T03:09:17Z","published":"2025-03-24T15:32:06Z","title":"Frequency Dynamic Convolution for Dense Image Prediction","summary":"  While Dynamic Convolution (DY-Conv) has shown promising performance by\nenabling adaptive weight selection through multiple parallel weights combined\nwith an attention mechanism, the frequency response of these weights tends to\nexhibit high similarity, resulting in high parameter costs but limited\nadaptability. In this work, we introduce Frequency Dynamic Convolution\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\nparameter budget in the Fourier domain. FDConv divides this budget into\nfrequency-based groups with disjoint Fourier indices, enabling the construction\nof frequency-diverse weights without increasing the parameter cost. To further\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\nfilter at the spatial level, while FBM decomposes weights into distinct\nfrequency bands in the frequency domain and modulates them dynamically based on\nlocal content. Extensive experiments on object detection, segmentation, and\nclassification validate the effectiveness of FDConv. We demonstrate that when\napplied to ResNet-50, FDConv achieves superior performance with a modest\nincrease of +3.6M parameters, outperforming previous methods that require\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\nMoreover, FDConv seamlessly integrates into a variety of architectures,\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\nsolution for modern vision tasks. The code is made publicly available at\nhttps://github.com/Linwei-Chen/FDConv.\n","authors":["Linwei Chen","Lin Gu","Liang Li","Chenggang Yan","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2503.18783v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.16942v3","updated":"2025-03-25T08:12:22Z","published":"2025-03-21T08:40:35Z","title":"Re-HOLD: Video Hand Object Interaction Reenactment via adaptive\n  Layout-instructed Diffusion Model","summary":"  Current digital human studies focusing on lip-syncing and body movement are\nno longer sufficient to meet the growing industrial demand, while human video\ngeneration techniques that support interacting with real-world environments\n(e.g., objects) have not been well investigated. Despite human hand synthesis\nalready being an intricate problem, generating objects in contact with hands\nand their interactions presents an even more challenging task, especially when\nthe objects exhibit obvious variations in size and shape. To tackle these\nissues, we present a novel video Reenactment framework focusing on Human-Object\nInteraction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD).\nOur key insight is to employ specialized layout representation for hands and\nobjects, respectively. Such representations enable effective disentanglement of\nhand modeling and object adaptation to diverse motion sequences. To further\nimprove the generation quality of HOI, we design an interactive textural\nenhancement module for both hands and objects by introducing two independent\nmemory banks. We also propose a layout adjustment strategy for the cross-object\nreenactment scenario to adaptively adjust unreasonable layouts caused by\ndiverse object sizes during inference. Comprehensive qualitative and\nquantitative evaluations demonstrate that our proposed framework significantly\noutperforms existing methods. Project page: https://fyycs.github.io/Re-HOLD.\n","authors":["Yingying Fan","Quanwei Yang","Kaisiyuan Wang","Hang Zhou","Yingying Li","Haocheng Feng","Errui Ding","Yu Wu","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16942v3.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2410.08107v4","updated":"2025-03-25T02:40:31Z","published":"2024-10-10T16:54:23Z","title":"IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera","summary":"  Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for\nnovel view synthesis have achieved remarkable progress with frame-based camera\n(e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel\ntype of bio-inspired visual sensor, i.e. event camera, has demonstrated\nadvantages in high temporal resolution, high dynamic range, low power\nconsumption and low latency. Due to its unique asynchronous and irregular data\ncapturing process, limited work has been proposed to apply neural\nrepresentation or 3D Gaussian splatting for an event camera. In this work, we\npresent IncEventGS, an incremental 3D Gaussian Splatting reconstruction\nalgorithm with a single event camera. To recover the 3D scene representation\nincrementally, we exploit the tracking and mapping paradigm of conventional\nSLAM pipelines for IncEventGS. Given the incoming event stream, the tracker\nfirstly estimates an initial camera motion based on prior reconstructed 3D-GS\nscene representation. The mapper then jointly refines both the 3D scene\nrepresentation and camera motion based on the previously estimated motion\ntrajectory from the tracker. The experimental results demonstrate that\nIncEventGS delivers superior performance compared to prior NeRF-based methods\nand other related baselines, even we do not have the ground-truth camera poses.\nFurthermore, our method can also deliver better performance compared to\nstate-of-the-art event visual odometry methods in terms of camera motion\nestimation. Code is publicly available at:\nhttps://github.com/wu-cvgl/IncEventGS.\n","authors":["Jian Huang","Chengrui Dong","Xuanhua Chen","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.08107v4.pdf","comment":"Code Page: https://github.com/wu-cvgl/IncEventGS"},{"id":"http://arxiv.org/abs/2503.18673v2","updated":"2025-03-25T06:18:47Z","published":"2025-03-24T13:46:21Z","title":"Any6D: Model-free 6D Pose Estimation of Novel Objects","summary":"  We introduce Any6D, a model-free framework for 6D object pose estimation that\nrequires only a single RGB-D anchor image to estimate both the 6D pose and size\nof unknown objects in novel scenes. Unlike existing methods that rely on\ntextured 3D models or multiple viewpoints, Any6D leverages a joint object\nalignment process to enhance 2D-3D alignment and metric scale estimation for\nimproved pose accuracy. Our approach integrates a render-and-compare strategy\nto generate and refine pose hypotheses, enabling robust performance in\nscenarios with occlusions, non-overlapping views, diverse lighting conditions,\nand large cross-environment variations. We evaluate our method on five\nchallenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,\ndemonstrating its effectiveness in significantly outperforming state-of-the-art\nmethods for novel object pose estimation. Project page:\nhttps://taeyeop.com/any6d\n","authors":["Taeyeop Lee","Bowen Wen","Minjun Kang","Gyuree Kang","In So Kweon","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2503.18673v2.pdf","comment":"CVPR 2025, Project Page: https://taeyeop.com/any6d"},{"id":"http://arxiv.org/abs/2503.18672v2","updated":"2025-03-25T10:00:27Z","published":"2025-03-24T13:44:12Z","title":"Feature Calibration enhanced Parameter Synthesis for CLIP-based\n  Class-incremental Learning","summary":"  Class-incremental Learning (CIL) enables models to continuously learn new\nclass knowledge while memorizing previous classes, facilitating their\nadaptation and evolution in dynamic environments. Traditional CIL methods are\nmainly based on visual features, which limits their ability to handle complex\nscenarios. In contrast, Vision-Language Models (VLMs) show promising potential\nto promote CIL by integrating pretrained knowledge with textual features.\nHowever, previous methods make it difficult to overcome catastrophic forgetting\nwhile preserving the generalization capabilities of VLMs. To tackle these\nchallenges, we propose Feature Calibration enhanced Parameter Synthesis (FCPS)\nin this paper. Specifically, our FCPS employs a specific parameter adjustment\nmechanism to iteratively refine the proportion of original visual features\nparticipating in the final class determination, ensuring the model's\nfoundational generalization capabilities. Meanwhile, parameter integration\nacross different tasks achieves a balance between learning new class knowledge\nand retaining old knowledge. Experimental results on popular benchmarks (e.g.,\nCIFAR100 and ImageNet100) validate the superiority of the proposed method.\n","authors":["Juncen Guo","Xiaoguang Zhu","Lianlong Sun","Liangyu Teng","Di Li","Yang Liu","Liang Song"],"pdf_url":"https://arxiv.org/pdf/2503.18672v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04680v3","updated":"2025-03-25T03:00:00Z","published":"2024-12-06T00:38:36Z","title":"Superpixel Tokenization for Vision Transformers: Preserving Semantic\n  Integrity in Visual Tokens","summary":"  Transformers, a groundbreaking architecture proposed for Natural Language\nProcessing (NLP), have also achieved remarkable success in Computer Vision. A\ncornerstone of their success lies in the attention mechanism, which models\nrelationships among tokens. While the tokenization process in NLP inherently\nensures that a single token does not contain multiple semantics, the\ntokenization of Vision Transformer (ViT) utilizes tokens from uniformly\npartitioned square image patches, which may result in an arbitrary mixing of\nvisual concepts in a token. In this work, we propose to substitute the\ngrid-based tokenization in ViT with superpixel tokenization, which employs\nsuperpixels to generate a token that encapsulates a sole visual concept.\nUnfortunately, the diverse shapes, sizes, and locations of superpixels make\nintegrating superpixels into ViT tokenization rather challenging. Our\ntokenization pipeline, comprised of pre-aggregate extraction and\nsuperpixel-aware aggregation, overcomes the challenges that arise in superpixel\ntokenization. Extensive experiments demonstrate that our approach, which\nexhibits strong compatibility with existing frameworks, enhances the accuracy\nand robustness of ViT on various downstream tasks.\n","authors":["Jaihyun Lew","Soohyuk Jang","Jaehoon Lee","Seungryong Yoo","Eunji Kim","Saehyung Lee","Jisoo Mok","Siwon Kim","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2412.04680v3.pdf","comment":"Project page: https://github.com/jangsoohyuk/SuiT"},{"id":"http://arxiv.org/abs/2503.12836v3","updated":"2025-03-25T05:07:43Z","published":"2025-03-17T05:32:15Z","title":"CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D\nreconstruction and novel view synthesis, leading to its widespread commercial\nuse. Consequently, copyright protection via watermarking has become critical.\nHowever, because 3DGS relies on millions of Gaussians, which require gigabytes\nof storage, efficient transfer and storage require compression. Existing 3DGS\nwatermarking methods are vulnerable to quantization-based compression, often\nresulting in the loss of the embedded watermark. To address this challenge, we\npropose a novel watermarking method that ensures watermark robustness after\nmodel compression while maintaining high rendering quality. In detail, we\nincorporate a quantization distortion layer that simulates compression during\ntraining, preserving the watermark under quantization-based compression. Also,\nwe propose a learnable watermark embedding feature that embeds the watermark\ninto the anchor feature, ensuring structural consistency and seamless\nintegration into the 3D scene. Furthermore, we present a frequency-aware anchor\ngrowing mechanism to enhance image quality in high-frequency regions by\neffectively identifying Guassians within these regions. Experimental results\nconfirm that our method preserves the watermark and maintains superior image\nquality under high compression, validating it as a promising approach for a\nsecure 3DGS model.\n","authors":["Sumin In","Youngdong Jang","Utae Jeong","MinHyuk Jang","Hyeongcheol Park","Eunbyung Park","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2503.12836v3.pdf","comment":"23 pages, 17 figures"},{"id":"http://arxiv.org/abs/2503.18559v2","updated":"2025-03-25T02:43:16Z","published":"2025-03-24T11:13:33Z","title":"AMD-Hummingbird: Towards an Efficient Text-to-Video Model","summary":"  Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.\n","authors":["Takashi Isobe","He Cui","Dong Zhou","Mengmeng Ge","Dong Li","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2503.18559v2.pdf","comment":"Homepage:\n  https://www.amd.com/en/developer/resources/technical-articles/amd-hummingbird-0-9b-text-to-video-diffusion-model-with-4-step-inferencing.html|\n  GitHub: https://github.com/AMD-AIG-AIMA/AMD-Hummingbird-T2V"},{"id":"http://arxiv.org/abs/2503.18527v2","updated":"2025-03-25T09:44:41Z","published":"2025-03-24T10:34:07Z","title":"AIM2PC: Aerial Image to 3D Building Point Cloud Reconstruction","summary":"  Three-dimensional urban reconstruction of buildings from single-view images\nhas attracted significant attention over the past two decades. However, recent\nmethods primarily focus on rooftops from aerial images, often overlooking\nessential geometrical details. Additionally, there is a notable lack of\ndatasets containing complete 3D point clouds for entire buildings, along with\nchallenges in obtaining reliable camera pose information for aerial images.\nThis paper addresses these challenges by presenting a novel methodology, AIM2PC\n, which utilizes our generated dataset that includes complete 3D point clouds\nand determined camera poses. Our approach takes features from a single aerial\nimage as input and concatenates them with essential additional conditions, such\nas binary masks and Sobel edge maps, to enable more edge-aware reconstruction.\nBy incorporating a point cloud diffusion model based on Centered denoising\nDiffusion Probabilistic Models (CDPM), we project these concatenated features\nonto the partially denoised point cloud using our camera poses at each\ndiffusion step. The proposed method is able to reconstruct the complete 3D\nbuilding point cloud, including wall information and demonstrates superior\nperformance compared to existing baseline techniques. To allow further\ncomparisons with our methodology the dataset has been made available at\nhttps://github.com/Soulaimene/AIM2PCDataset\n","authors":["Soulaimene Turki","Daniel Panangian","Houda Chaabouni-Chouayakh","Ksenia Bittner"],"pdf_url":"https://arxiv.org/pdf/2503.18527v2.pdf","comment":"Accepted to ISPRS Geospatial Week 2025"},{"id":"http://arxiv.org/abs/2503.18513v2","updated":"2025-03-25T08:13:59Z","published":"2025-03-24T10:07:46Z","title":"LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene","summary":"  Humans perceive and comprehend their surroundings through information\nspanning multiple frequencies. In immersive scenes, people naturally scan their\nenvironment to grasp its overall structure while examining fine details of\nobjects that capture their attention. However, current NeRF frameworks\nprimarily focus on modeling either high-frequency local views or the broad\nstructure of scenes with low-frequency information, which is limited to\nbalancing both. We introduce FA-NeRF, a novel frequency-aware framework for\nview synthesis that simultaneously captures the overall scene structure and\nhigh-definition details within a single NeRF model. To achieve this, we propose\na 3D frequency quantification method that analyzes the scene's frequency\ndistribution, enabling frequency-aware rendering. Our framework incorporates a\nfrequency grid for fast convergence and querying, a frequency-aware feature\nre-weighting strategy to balance features across different frequency contents.\nExtensive experiments show that our method significantly outperforms existing\napproaches in modeling entire scenes while preserving fine details. Project\npage: https://coscatter.github.io/LookCloser/\n","authors":["Xiaoyu Zhang","Weihong Pan","Chong Bao","Xiyu Zhang","Xiaojun Xiang","Hanqing Jiang","Hujun Bao"],"pdf_url":"https://arxiv.org/pdf/2503.18513v2.pdf","comment":"CVPR 2025. Project page: https://coscatter.github.io/LookCloser"}]},"2025-03-26T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2503.03734v3","updated":"2025-03-26T17:55:06Z","published":"2025-03-05T18:44:48Z","title":"OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature\n  Extraction","summary":"  Vision-Language-Action (VLA) models aim to predict robotic actions based on\nvisual observations and language instructions. Existing approaches require\nfine-tuning pre-trained visionlanguage models (VLMs) as visual and language\nfeatures are independently fed into downstream policies, degrading the\npre-trained semantic alignments. We propose OTTER, a novel VLA architecture\nthat leverages these existing alignments through explicit, text-aware visual\nfeature extraction. Instead of processing all visual features, OTTER\nselectively extracts and passes only task-relevant visual features that are\nsemantically aligned with the language instruction to the policy transformer.\nThis allows OTTER to keep the pre-trained vision-language encoders frozen.\nThereby, OTTER preserves and utilizes the rich semantic understanding learned\nfrom large-scale pre-training, enabling strong zero-shot generalization\ncapabilities. In simulation and real-world experiments, OTTER significantly\noutperforms existing VLA models, demonstrating strong zeroshot generalization\nto novel objects and environments. Video, code, checkpoints, and dataset:\nhttps://ottervla.github.io/.\n","authors":["Huang Huang","Fangchen Liu","Letian Fu","Tingfan Wu","Mustafa Mukadam","Jitendra Malik","Ken Goldberg","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2503.03734v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20754v1","updated":"2025-03-26T17:40:31Z","published":"2025-03-26T17:40:31Z","title":"Flying Vines: Design, Modeling, and Control of a Soft Aerial Robotic Arm","summary":"  Aerial robotic arms aim to enable inspection and environment interaction in\notherwise hard-to-reach areas from the air. However, many aerial manipulators\nfeature bulky or heavy robot manipulators mounted to large, high-payload aerial\nvehicles. Instead, we propose an aerial robotic arm with low mass and a small\nstowed configuration called a \"flying vine\". The flying vine consists of a\nsmall, maneuverable quadrotor equipped with a soft, growing, inflated beam as\nthe arm. This soft robot arm is underactuated, and positioning of the end\neffector is achieved by controlling the coupled quadrotor-vine dynamics. In\nthis work, we present the flying vine design and a modeling and control\nframework for tracking desired end effector trajectories. The dynamic model\nleverages data-driven modeling methods and introduces bilinear interpolation to\naccount for time-varying dynamic parameters. We use trajectory optimization to\nplan quadrotor controls that produce desired end effector motions. Experimental\nresults on a physical prototype demonstrate that our framework enables the\nflying vine to perform high-speed end effector tracking, laying a foundation\nfor performing dynamic maneuvers with soft aerial manipulators.\n","authors":["Rianna Jitosho","Crystal E. Winston","Shengan Yang","Jinxin Li","Maxwell Ahlquist","Nicholas John Woehrle","C. Karen Liu","Allison M. Okamura"],"pdf_url":"https://arxiv.org/pdf/2503.20754v1.pdf","comment":"Submitted to RA-L"},{"id":"http://arxiv.org/abs/2503.20723v1","updated":"2025-03-26T17:06:53Z","published":"2025-03-26T17:06:53Z","title":"Multi-Robot Coordination Under Physical Limitations","summary":"  Multi-robot coordination is fundamental to various applications, including\nautonomous exploration, search and rescue, and cooperative transportation. This\npaper presents an optimal consensus framework for multi-robot systems (MRSs)\nthat ensures efficient rendezvous while minimizing energy consumption and\naddressing actuator constraints. A critical challenge in real-world deployments\nis actuator limitations, particularly wheel velocity saturation, which can\nsignificantly degrade control performance. To address this issue, we\nincorporate Pontryagin Minimum Principle (PMP) into the control design,\nfacilitating constrained optimization while ensuring system stability and\nfeasibility. The resulting optimal control policy effectively balances\ncoordination efficiency and energy consumption, even in the presence of\nactuation constraints. The proposed framework is validated through extensive\nnumerical simulations and real-world experiments conducted using a team of\nRobotarium mobile robots. The experimental results confirm that our control\nstrategies achieve reliable and efficient coordinated rendezvous while\naddressing real-world challenges such as communication delays, sensor noise,\nand packet loss.\n","authors":["Tohid Kargar Tasooji","Sakineh Khodadadi"],"pdf_url":"https://arxiv.org/pdf/2503.20723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18562v5","updated":"2025-03-26T16:53:12Z","published":"2024-11-27T18:03:26Z","title":"DexHandDiff: Interaction-aware Diffusion Planning for Adaptive Dexterous\n  Manipulation","summary":"  Dexterous manipulation with contact-rich interactions is crucial for advanced\nrobotics. While recent diffusion-based planning approaches show promise for\nsimple manipulation tasks, they often produce unrealistic ghost states (e.g.,\nthe object automatically moves without hand contact) or lack adaptability when\nhandling complex sequential interactions. In this work, we introduce\nDexHandDiff, an interaction-aware diffusion planning framework for adaptive\ndexterous manipulation. DexHandDiff models joint state-action dynamics through\na dual-phase diffusion process which consists of pre-interaction contact\nalignment and post-contact goal-directed control, enabling goal-adaptive\ngeneralizable dexterous manipulation. Additionally, we incorporate dynamics\nmodel-based dual guidance and leverage large language models for automated\nguidance function generation, enhancing generalizability for physical\ninteractions and facilitating diverse goal adaptation through language cues.\nExperiments on physical interaction tasks such as door opening, pen and block\nre-orientation, object relocation, and hammer striking demonstrate\nDexHandDiff's effectiveness on goals outside training distributions, achieving\nover twice the average success rate (59.2% vs. 29.5%) compared to existing\nmethods. Our framework achieves an average of 70.7% success rate on goal\nadaptive dexterous tasks, highlighting its robustness and flexibility in\ncontact-rich manipulation.\n","authors":["Zhixuan Liang","Yao Mu","Yixiao Wang","Tianxing Chen","Wenqi Shao","Wei Zhan","Masayoshi Tomizuka","Ping Luo","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2411.18562v5.pdf","comment":"Accepted by CVPR 2025. Camera ready version. Previous DexDiffuser.\n  Project page: https://dexdiffuser.github.io/"},{"id":"http://arxiv.org/abs/2503.20714v1","updated":"2025-03-26T16:51:49Z","published":"2025-03-26T16:51:49Z","title":"Beyond Visuals: Investigating Force Feedback in Extended Reality for\n  Robot Data Collection","summary":"  This work explores how force feedback affects various aspects of robot data\ncollection within the Extended Reality (XR) setting. Force feedback has been\nproved to enhance the user experience in Extended Reality (XR) by providing\ncontact-rich information. However, its impact on robot data collection has not\nreceived much attention in the robotics community. This paper addresses this\nshortcoming by conducting an extensive user study on the effects of force\nfeedback during data collection in XR. We extended two XR-based robot control\ninterfaces, Kinesthetic Teaching and Motion Controllers, with haptic feedback\nfeatures. The user study is conducted using manipulation tasks ranging from\nsimple pick-place to complex peg assemble, requiring precise operations. The\nevaluations show that force feedback enhances task performance and user\nexperience, particularly in tasks requiring high-precision manipulation. These\nimprovements vary depending on the robot control interface and task complexity.\nThis paper provides new insights into how different factors influence the\nimpact of force feedback.\n","authors":["Xueyin Li","Xinkai Jiang","Philipp Dahlinger","Gerhard Neumann","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2503.20714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20693v1","updated":"2025-03-26T16:24:08Z","published":"2025-03-26T16:24:08Z","title":"Toward Dynamic Control of Tendon-Driven Continuum Robots using Clarke\n  Transform","summary":"  In this paper, we propose a dynamic model and control framework for\ntendon-driven continuum robots with multiple segments and an arbitrary number\nof tendons per segment. Our approach leverages the Clarke transform, the\nEuler-Lagrange formalism, and the piecewise constant curvature assumption to\nformulate a dynamic model on a two-dimensional manifold embedded in the joint\nspace that inherently satisfies tendon constraints. We present linear\ncontrollers that operate directly on this manifold, along with practical\nmethods for preventing negative tendon forces without compromising control\nfidelity. We validate these approaches in simulation and on a physical\nprototype with one segment and five tendons, demonstrating accurate dynamic\nbehavior and robust trajectory tracking under real-time conditions.\n","authors":["Christian Muhmann","Reinhard M. Grassmann","Max Bartholdt","Jessica Burgner-Kahrs"],"pdf_url":"https://arxiv.org/pdf/2503.20693v1.pdf","comment":"8 pages and 8 figures"},{"id":"http://arxiv.org/abs/2503.20646v1","updated":"2025-03-26T15:40:10Z","published":"2025-03-26T15:40:10Z","title":"Immersive and Wearable Thermal Rendering for Augmented Reality","summary":"  In augmented reality (AR), where digital content is overlaid onto the real\nworld, realistic thermal feedback has been shown to enhance immersion. Yet\ncurrent thermal feedback devices, heavily influenced by the needs of virtual\nreality, often hinder physical interactions and are ineffective for immersion\nin AR. To bridge this gap, we have identified three design considerations\nrelevant for AR thermal feedback: indirect feedback to maintain dexterity,\nthermal passthrough to preserve real-world temperature perception, and\nspatiotemporal rendering for dynamic sensations. We then created a unique and\ninnovative thermal feedback device that satisfies these criteria. Human subject\nexperiments assessing perceptual sensitivity, object temperature matching,\nspatial pattern recognition, and moving thermal stimuli demonstrated the impact\nof our design, enabling realistic temperature discrimination, virtual object\nperception, and enhanced immersion. These findings demonstrate that carefully\ndesigned thermal feedback systems can bridge the sensory gap between physical\nand virtual interactions, enhancing AR realism and usability.\n","authors":["Alexandra Watkins","Ritam Ghosh","Evan Chow","Nilanjan Sarkar"],"pdf_url":"https://arxiv.org/pdf/2503.20646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20642v1","updated":"2025-03-26T15:34:42Z","published":"2025-03-26T15:34:42Z","title":"Representation Improvement in Latent Space for Search-Based Testing of\n  Autonomous Robotic Systems","summary":"  Testing autonomous robotic systems, such as self-driving cars and unmanned\naerial vehicles, is challenging due to their interaction with highly\nunpredictable environments. A common practice is to first conduct\nsimulation-based testing, which, despite reducing real-world risks, remains\ntime-consuming and resource-intensive due to the vast space of possible test\nscenarios. A number of search-based approaches were proposed to generate test\nscenarios more efficiently. A key aspect of any search-based test generation\napproach is the choice of representation used during the search process.\nHowever, existing methods for improving test scenario representation remain\nlimited. We propose RILaST (Representation Improvement in Latent Space for\nSearch-Based Testing) approach, which enhances test representation by mapping\nit to the latent space of a variational autoencoder. We evaluate RILaST on two\nuse cases, including autonomous drone and autonomous lane-keeping assist\nsystem. The obtained results show that RILaST allows finding between 3 to 4.6\ntimes more failures than baseline approaches, achieving a high level of test\ndiversity.\n","authors":["Dmytro Humeniuk","Foutse Khomh"],"pdf_url":"https://arxiv.org/pdf/2503.20642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20631v1","updated":"2025-03-26T15:24:58Z","published":"2025-03-26T15:24:58Z","title":"Robust Flower Cluster Matching Using The Unscented Transform","summary":"  Monitoring flowers over time is essential for precision robotic pollination\nin agriculture. To accomplish this, a continuous spatial-temporal observation\nof plant growth can be done using stationary RGB-D cameras. However, image\nregistration becomes a serious challenge due to changes in the visual\nappearance of the plant caused by the pollination process and occlusions from\ngrowth and camera angles. Plants flower in a manner that produces distinct\nclusters on branches. This paper presents a method for matching flower clusters\nusing descriptors generated from RGB-D data and considers allowing for spatial\nuncertainty within the cluster. The proposed approach leverages the Unscented\nTransform to efficiently estimate plant descriptor uncertainty tolerances,\nenabling a robust image-registration process despite temporal changes. The\nUnscented Transform is used to handle the nonlinear transformations by\npropagating the uncertainty of flower positions to determine the variations in\nthe descriptor domain. A Monte Carlo simulation is used to validate the\nUnscented Transform results, confirming our method's effectiveness for flower\ncluster matching. Therefore, it can facilitate improved robotics pollination in\ndynamic environments.\n","authors":["Andy Chu","Rashik Shrestha","Yu Gu","Jason N. Gross"],"pdf_url":"https://arxiv.org/pdf/2503.20631v1.pdf","comment":"*CASE2025 Under Review*"},{"id":"http://arxiv.org/abs/2409.18253v2","updated":"2025-03-26T14:02:12Z","published":"2024-09-26T19:54:24Z","title":"UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation","summary":"  Terrain awareness is an essential milestone to enable truly autonomous\noff-road navigation. Accurately predicting terrain characteristics allows\noptimizing a vehicle's path against potential hazards. Recent methods use deep\nneural networks to predict traversability-related terrain properties in a\nself-supervised manner, relying on proprioception as a training signal.\nHowever, onboard cameras are inherently limited by their point-of-view relative\nto the ground, suffering from occlusions and vanishing pixel density with\ndistance. This paper introduces a novel approach for self-supervised terrain\ncharacterization using an aerial perspective from a hovering drone. We capture\nterrain-aligned images while sampling the environment with a ground vehicle,\neffectively training a simple predictor for vibrations, bumpiness, and energy\nconsumption. Our dataset includes 2.8 km of off-road data collected in forest\nenvironment, comprising 13 484 ground-based images and 12 935 aerial images.\nOur findings show that drone imagery improves terrain property prediction by\n21.37 % on the whole dataset and 37.35 % in high vegetation, compared to ground\nrobot images. We conduct ablation studies to identify the main causes of these\nperformance improvements. We also demonstrate the real-world applicability of\nour approach by scouting an unseen area with a drone, planning and executing an\noptimized path on the ground.\n","authors":["Jean-Michel Fortin","Olivier Gamache","William Fecteau","Effie Daum","William Larrivée-Hardy","François Pomerleau","Philippe Giguère"],"pdf_url":"https://arxiv.org/pdf/2409.18253v2.pdf","comment":"7 pages, 5 figures, submitted to ICRA 2025"},{"id":"http://arxiv.org/abs/2503.20544v1","updated":"2025-03-26T13:40:08Z","published":"2025-03-26T13:40:08Z","title":"Safety integrity framework for automated driving","summary":"  This paper describes the comprehensive safety framework that underpinned the\ndevelopment, release process, and regulatory approval of BMW's first SAE Level\n3 Automated Driving System. The framework combines established qualitative and\nquantitative methods from the fields of Systems Engineering, Engineering Risk\nAnalysis, Bayesian Data Analysis, Design of Experiments, and Statistical\nLearning in a novel manner. The approach systematically minimizes the risks\nassociated with hardware and software faults, performance limitations, and\ninsufficient specifications to an acceptable level that achieves a Positive\nRisk Balance. At the core of the framework is the systematic identification and\nquantification of uncertainties associated with hazard scenarios and the\nredundantly designed system based on designed experiments, field data, and\nexpert knowledge. The residual risk of the system is then estimated through\nStochastic Simulation and evaluated by Sensitivity Analysis. By integrating\nthese advanced analytical techniques into the V-Model, the framework fulfills,\nunifies, and complements existing automotive safety standards. It therefore\nprovides a comprehensive, rigorous, and transparent safety assurance process\nfor the development and deployment of Automated Driving Systems.\n","authors":["Moritz Werling","Rainer Faller","Wolfgang Betz","Daniel Straub"],"pdf_url":"https://arxiv.org/pdf/2503.20544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07544v2","updated":"2025-03-26T13:39:58Z","published":"2024-12-10T14:28:18Z","title":"Contractive Dynamical Imitation Policies for Efficient Out-of-Sample\n  Recovery","summary":"  Imitation learning is a data-driven approach to learning policies from expert\nbehavior, but it is prone to unreliable outcomes in out-of-sample (OOS)\nregions. While previous research relying on stable dynamical systems guarantees\nconvergence to a desired state, it often overlooks transient behavior. We\npropose a framework for learning policies modeled by contractive dynamical\nsystems, ensuring that all policy rollouts converge regardless of\nperturbations, and in turn, enable efficient OOS recovery. By leveraging\nrecurrent equilibrium networks and coupling layers, the policy structure\nguarantees contractivity for any parameter choice, which facilitates\nunconstrained optimization. We also provide theoretical upper bounds for\nworst-case and expected loss to rigorously establish the reliability of our\nmethod in deployment. Empirically, we demonstrate substantial OOS performance\nimprovements for simulated robotic manipulation and navigation tasks.\n","authors":["Amin Abyaneh","Mahrokh G. Boroujeni","Hsiu-Chin Lin","Giancarlo Ferrari-Trecate"],"pdf_url":"https://arxiv.org/pdf/2412.07544v2.pdf","comment":"International Conference on Learning Representations"},{"id":"http://arxiv.org/abs/2503.20530v1","updated":"2025-03-26T13:21:46Z","published":"2025-03-26T13:21:46Z","title":"Combining Machine Learning and Sampling-Based Search for Multi-Goal\n  Motion Planning with Dynamics","summary":"  This paper considers multi-goal motion planning in unstructured,\nobstacle-rich environments where a robot is required to reach multiple regions\nwhile avoiding collisions. The planned motions must also satisfy the\ndifferential constraints imposed by the robot dynamics. To find solutions\nefficiently, this paper leverages machine learning, Traveling Salesman Problem\n(TSP), and sampling-based motion planning. The approach expands a motion tree\nby adding collision-free and dynamically-feasible trajectories as branches. A\nTSP solver is used to compute a tour for each node to determine the order in\nwhich to reach the remaining goals by utilizing a cost matrix. An important\naspect of the approach is that it leverages machine learning to construct the\ncost matrix by combining runtime and distance predictions to single-goal\nmotion-planning problems. During the motion-tree expansion, priority is given\nto nodes associated with low-cost tours. Experiments with a vehicle model\noperating in obstacle-rich environments demonstrate the computational\nefficiency and scalability of the approach.\n","authors":["Yuanjie Lu","Erion Plaku"],"pdf_url":"https://arxiv.org/pdf/2503.20530v1.pdf","comment":"10 pages, 2025 International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)"},{"id":"http://arxiv.org/abs/2503.20523v1","updated":"2025-03-26T13:11:35Z","published":"2025-03-26T13:11:35Z","title":"GAIA-2: A Controllable Multi-View Generative World Model for Autonomous\n  Driving","summary":"  Generative models offer a scalable and flexible paradigm for simulating\ncomplex environments, yet current approaches fall short in addressing the\ndomain-specific requirements of autonomous driving - such as multi-agent\ninteractions, fine-grained control, and multi-camera consistency. We introduce\nGAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies\nthese capabilities within a single generative framework. GAIA-2 supports\ncontrollable video generation conditioned on a rich set of structured inputs:\nego-vehicle dynamics, agent configurations, environmental factors, and road\nsemantics. It generates high-resolution, spatiotemporally consistent\nmulti-camera videos across geographically diverse driving environments (UK, US,\nGermany). The model integrates both structured conditioning and external latent\nembeddings (e.g., from a proprietary driving model) to facilitate flexible and\nsemantically grounded scene synthesis. Through this integration, GAIA-2 enables\nscalable simulation of both common and rare driving scenarios, advancing the\nuse of generative world models as a core tool in the development of autonomous\nsystems. Videos are available at https://wayve.ai/thinking/gaia-2.\n","authors":["Lloyd Russell","Anthony Hu","Lorenzo Bertoni","George Fedoseev","Jamie Shotton","Elahe Arani","Gianluca Corrado"],"pdf_url":"https://arxiv.org/pdf/2503.20523v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2503.20521v1","updated":"2025-03-26T13:08:07Z","published":"2025-03-26T13:08:07Z","title":"Decremental Dynamics Planning for Robot Navigation","summary":"  Most, if not all, robot navigation systems employ a decomposed planning\nframework that includes global and local planning. To trade-off onboard\ncomputation and plan quality, current systems have to limit all robot dynamics\nconsiderations only within the local planner, while leveraging an extremely\nsimplified robot representation (e.g., a point-mass holonomic model without\ndynamics) in the global level. However, such an artificial decomposition based\non either full or zero consideration of robot dynamics can lead to gaps between\nthe two levels, e.g., a global path based on a holonomic point-mass model may\nnot be realizable by a non-holonomic robot, especially in highly constrained\nobstacle environments. Motivated by such a limitation, we propose a novel\nparadigm, Decremental Dynamics Planning that integrates dynamic constraints\ninto the entire planning process, with a focus on high-fidelity dynamics\nmodeling at the beginning and a gradual fidelity reduction as the planning\nprogresses. To validate the effectiveness of this paradigm, we augment three\ndifferent planners with DDP and show overall improved planning performance. We\nalso develop a new DDP-based navigation system, which achieves first place in\nthe simulation phase of the 2025 BARN Challenge. Both simulated and physical\nexperiments validate DDP's hypothesized benefits.\n","authors":["Yuanjie Lu","Tong Xu","Linji Wang","Nick Hawes","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.20521v1.pdf","comment":"7 pages. 2025 International Conference on Intelligent Robots and\n  Systems (IROS 2025)"},{"id":"http://arxiv.org/abs/2503.20518v1","updated":"2025-03-26T13:00:05Z","published":"2025-03-26T13:00:05Z","title":"Exploring the Effect of Robotic Embodiment and Empathetic Tone of LLMs\n  on Empathy Elicitation","summary":"  This study investigates the elicitation of empathy toward a third party\nthrough interaction with social agents. Participants engaged with either a\nphysical robot or a voice-enabled chatbot, both driven by a large language\nmodel (LLM) programmed to exhibit either an empathetic tone or remain neutral.\nThe interaction is focused on a fictional character, Katie Banks, who is in a\nchallenging situation and in need of financial donations. The willingness to\nhelp Katie, measured by the number of hours participants were willing to\nvolunteer, along with their perceptions of the agent, were assessed for 60\nparticipants. Results indicate that neither robotic embodiment nor empathetic\ntone significantly influenced participants' willingness to volunteer. While the\nLLM effectively simulated human empathy, fostering genuine empathetic responses\nin participants proved challenging.\n","authors":["Liza Darwesh","Jaspreet Singh","Marin Marian","Eduard Alexa","Koen Hindriks","Kim Baraka"],"pdf_url":"https://arxiv.org/pdf/2503.20518v1.pdf","comment":"*Liza Darwesh, Jaspreet Singh, Marin Marian, and Eduard Alexa\n  contributed equally to this work.*"},{"id":"http://arxiv.org/abs/2503.20425v1","updated":"2025-03-26T10:59:08Z","published":"2025-03-26T10:59:08Z","title":"Perspective-Shifted Neuro-Symbolic World Models: A Framework for\n  Socially-Aware Robot Navigation","summary":"  Navigating in environments alongside humans requires agents to reason under\nuncertainty and account for the beliefs and intentions of those around them.\nUnder a sequential decision-making framework, egocentric navigation can\nnaturally be represented as a Markov Decision Process (MDP). However, social\nnavigation additionally requires reasoning about the hidden beliefs of others,\ninherently leading to a Partially Observable Markov Decision Process (POMDP),\nwhere agents lack direct access to others' mental states. Inspired by Theory of\nMind and Epistemic Planning, we propose (1) a neuro-symbolic model-based\nreinforcement learning architecture for social navigation, addressing the\nchallenge of belief tracking in partially observable environments; and (2) a\nperspective-shift operator for belief estimation, leveraging recent work on\nInfluence-based Abstractions (IBA) in structured multi-agent settings.\n","authors":["Kevin Alcedo","Pedro U. Lima","Rachid Alami"],"pdf_url":"https://arxiv.org/pdf/2503.20425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16127v2","updated":"2025-03-26T10:07:40Z","published":"2025-03-20T13:19:54Z","title":"The Morphology-Control Trade-Off: Insights into Soft Robotic Efficiency","summary":"  Soft robotics holds transformative potential for enabling adaptive and\nadaptable systems in dynamic environments. However, the interplay between\nmorphological and control complexities and their collective impact on task\nperformance remains poorly understood. Therefore, in this study, we investigate\nthese trade-offs across tasks of differing difficulty levels using four\nwell-used morphological complexity metrics and control complexity measured by\nFLOPs. We investigate how these factors jointly influence task performance by\nutilizing the evolutionary robot experiments. Results show that optimal\nperformance depends on the alignment between morphology and control: simpler\nmorphologies and lightweight controllers suffice for easier tasks, while harder\ntasks demand higher complexities in both dimensions. In addition, a clear\ntrade-off between morphological and control complexities that achieve the same\ntask performance can be observed. Moreover, we also propose a sensitivity\nanalysis to expose the task-specific contributions of individual morphological\nmetrics. Our study establishes a framework for investigating the relationships\nbetween morphology, control, and task performance, advancing the development of\ntask-specific robotic designs that balance computational efficiency with\nadaptability. This study contributes to the practical application of soft\nrobotics in real-world scenarios by providing actionable insights.\n","authors":["Yue Xie","Kai-fung Chu","Xing Wang","Fumiya Iida"],"pdf_url":"https://arxiv.org/pdf/2503.16127v2.pdf","comment":"The paper is planed to be submitted to a journal"},{"id":"http://arxiv.org/abs/2503.20384v1","updated":"2025-03-26T10:05:38Z","published":"2025-03-26T10:05:38Z","title":"MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via\n  Mixture-of-Layers for Efficient Robot Manipulation","summary":"  Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs.\n","authors":["Rongyu Zhang","Menghang Dong","Yuan Zhang","Liang Heng","Xiaowei Chi","Gaole Dai","Li Du","Dan Wang","Yuan Du","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01586v2","updated":"2025-03-26T09:00:08Z","published":"2024-06-03T17:59:23Z","title":"ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic\n  Manipulation","summary":"  Diffusion models have been verified to be effective in generating complex\ndistributions from natural images to motion trajectories. Recent\ndiffusion-based methods show impressive performance in 3D robotic manipulation\ntasks, whereas they suffer from severe runtime inefficiency due to multiple\ndenoising steps, especially with high-dimensional observations. To this end, we\npropose a real-time robotic manipulation model named ManiCM that imposes the\nconsistency constraint to the diffusion process, so that the model can generate\nrobot actions in only one-step inference. Specifically, we formulate a\nconsistent diffusion process in the robot action space conditioned on the point\ncloud input, where the original action is required to be directly denoised from\nany point along the ODE trajectory. To model this process, we design a\nconsistency distillation technique to predict the action sample directly\ninstead of predicting the noise within the vision community for fast\nconvergence in the low-dimensional action manifold. We evaluate ManiCM on 31\nrobotic manipulation tasks from Adroit and Metaworld, and the results\ndemonstrate that our approach accelerates the state-of-the-art method by 10\ntimes in average inference speed while maintaining competitive average success\nrate.\n","authors":["Guanxing Lu","Zifeng Gao","Tianxing Chen","Wenxun Dai","Ziwei Wang","Wenbo Ding","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2406.01586v2.pdf","comment":"https://manicm-fast.github.io/"},{"id":"http://arxiv.org/abs/2503.17125v3","updated":"2025-03-26T08:55:34Z","published":"2025-03-21T13:20:39Z","title":"LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in\n  Reinforcement Learning","summary":"  Deep Reinforcement Learning (DRL) has demonstrated strong performance in\nrobotic control but remains susceptible to out-of-distribution (OOD) states,\noften resulting in unreliable actions and task failure. While previous methods\nhave focused on minimizing or preventing OOD occurrences, they largely neglect\nrecovery once an agent encounters such states. Although the latest research has\nattempted to address this by guiding agents back to in-distribution states,\ntheir reliance on uncertainty estimation hinders scalability in complex\nenvironments. To overcome this limitation, we introduce Language Models for\nOut-of-Distribution Recovery (LaMOuR), which enables recovery learning without\nrelying on uncertainty estimation. LaMOuR generates dense reward codes that\nguide the agent back to a state where it can successfully perform its original\ntask, leveraging the capabilities of LVLMs in image description, logical\nreasoning, and code generation. Experimental results show that LaMOuR\nsubstantially enhances recovery efficiency across diverse locomotion tasks and\neven generalizes effectively to complex environments, including humanoid\nlocomotion and mobile manipulation, where existing methods struggle. The code\nand supplementary materials are available at https://lamour-rl.github.io/.\n","authors":["Chan Kim","Seung-Woo Seo","Seong-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2503.17125v3.pdf","comment":"14 pages, 17 figures"},{"id":"http://arxiv.org/abs/2503.20324v1","updated":"2025-03-26T08:47:43Z","published":"2025-03-26T08:47:43Z","title":"CTS-CBS: A New Approach for Multi-Agent Collaborative Task Sequencing\n  and Path Finding","summary":"  This paper addresses a generalization problem of Multi-Agent Pathfinding\n(MAPF), called Collaborative Task Sequencing - Multi-Agent Pathfinding\n(CTS-MAPF), where agents must plan collision-free paths and visit a series of\nintermediate task locations in a specific order before reaching their final\ndestinations. To address this problem, we propose a new approach, Collaborative\nTask Sequencing - Conflict-Based Search (CTS-CBS), which conducts a two-level\nsearch. In the high level, it generates a search forest, where each tree\ncorresponds to a joint task sequence derived from the jTSP solution. In the low\nlevel, CTS-CBS performs constrained single-agent path planning to generate\npaths for each agent while adhering to high-level constraints. We also provide\nheoretical guarantees of its completeness and optimality (or sub-optimality\nwith a bounded parameter). To evaluate the performance of CTS-CBS, we create\ntwo datasets, CTS-MAPF and MG-MAPF, and conduct comprehensive experiments. The\nresults show that CTS-CBS adaptations for MG-MAPF outperform baseline\nalgorithms in terms of success rate (up to 20 times larger) and runtime (up to\n100 times faster), with less than a 10% sacrifice in solution quality.\nFurthermore, CTS-CBS offers flexibility by allowing users to adjust the\nsub-optimality bound omega to balance between solution quality and efficiency.\nFinally, practical robot tests demonstrate the algorithm's applicability in\nreal-world scenarios.\n","authors":["Junkai Jiang","Ruochen Li","Yibin Yang","Yihe Chen","Yuning Wang","Shaobing Xu","Jianqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.20324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02042v2","updated":"2025-03-26T08:46:46Z","published":"2025-03-03T20:36:15Z","title":"Optimizing Robot Programming: Mixed Reality Gripper Control","summary":"  Conventional robot programming methods are complex and time-consuming for\nusers. In recent years, alternative approaches such as mixed reality have been\nexplored to address these challenges and optimize robot programming. While the\nfindings of the mixed reality robot programming methods are convincing, most\nexisting methods rely on gesture interaction for robot programming. Since\ncontroller-based interactions have proven to be more reliable, this paper\nexamines three controller-based programming methods within a mixed reality\nscenario: 1) Classical Jogging, where the user positions the robot's end\neffector using the controller's thumbsticks, 2) Direct Control, where the\ncontroller's position and orientation directly corresponds to the end\neffector's, and 3) Gripper Control, where the controller is enhanced with a\n3D-printed gripper attachment to grasp and release objects. A within-subjects\nstudy (n = 30) was conducted to compare these methods. The findings indicate\nthat the Gripper Control condition outperforms the others in terms of task\ncompletion time, user experience, mental demand, and task performance, while\nalso being the preferred method. Therefore, it demonstrates promising potential\nas an effective and efficient approach for future robot programming. Video\navailable at https://youtu.be/83kWr8zUFIQ.\n","authors":["Maximilian Rettinger","Leander Hacker","Philipp Wolters","Gerhard Rigoll"],"pdf_url":"https://arxiv.org/pdf/2503.02042v2.pdf","comment":"Accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2503.12546v2","updated":"2025-03-26T08:19:53Z","published":"2025-03-16T15:27:22Z","title":"Polytope Volume Monitoring Problem: Formulation and Solution via\n  Parametric Linear Program Based Control Barrier Function","summary":"  Motivated by the latest research on feasible space monitoring of multiple\ncontrol barrier functions (CBFs) as well as polytopic collision avoidance, this\npaper studies the Polytope Volume Monitoring (PVM) problem, whose goal is to\ndesign a control law for inputs of nonlinear systems to prevent the volume of\nsome state-dependent polytope from decreasing to zero. Recent studies have\nexplored the idea of applying Chebyshev ball method in optimization theory to\nsolve the case study of PVM; however, the underlying difficulties caused by\nnonsmoothness have not been addressed. This paper continues the study on this\ntopic, where our main contribution is to establish the relationship between\nnonsmooth CBF and parametric optimization theory through directional\nderivatives for the first time, so as to solve PVM problems more conveniently.\nIn detail, inspired by Chebyshev ball approach, a parametric linear program\n(PLP) based nonsmooth barrier function candidate is established for PVM, and\nthen, sufficient conditions for it to be a nonsmooth CBF are proposed, based on\nwhich a quadratic program (QP) based safety filter with guaranteed feasibility\nis proposed to address PVM problems. Finally, a numerical simulation example is\ngiven to show the efficiency of the proposed safety filter.\n","authors":["Shizhen Wu","Jinyang Dong","Xu Fang","Ning Sun","Yongchun Fang"],"pdf_url":"https://arxiv.org/pdf/2503.12546v2.pdf","comment":"A simplified version is submitted to CDC2025"},{"id":"http://arxiv.org/abs/2404.15190v2","updated":"2025-03-26T07:42:56Z","published":"2024-04-21T08:10:20Z","title":"Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied\n  Instruction Following","summary":"  Embodied Instruction Following (EIF) is the task of executing natural\nlanguage instructions by navigating and interacting with objects in interactive\nenvironments. A key challenge in EIF is compositional task planning, typically\naddressed through supervised learning or few-shot in-context learning with\nlabeled data. To this end, we introduce the Socratic Planner, a self-QA-based\nzero-shot planning method that infers an appropriate plan without any further\ntraining. The Socratic Planner first facilitates self-questioning and answering\nby the Large Language Model (LLM), which in turn helps generate a sequence of\nsubgoals. While executing the subgoals, an embodied agent may encounter\nunexpected situations, such as unforeseen obstacles. The Socratic Planner then\nadjusts plans based on dense visual feedback through a visually-grounded\nre-planning mechanism. Experiments demonstrate the effectiveness of the\nSocratic Planner, outperforming current state-of-the-art planning models on the\nALFRED benchmark across all metrics, particularly excelling in long-horizon\ntasks that demand complex inference. We further demonstrate its real-world\napplicability through deployment on a physical robot for long-horizon tasks.\n","authors":["Suyeon Shin","Sujin jeon","Junghyun Kim","Gi-Cheon Kang","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.15190v2.pdf","comment":"8 pages, 6 figures, published to ICRA 2025"},{"id":"http://arxiv.org/abs/2411.16537v3","updated":"2025-03-26T07:30:26Z","published":"2024-11-25T16:21:34Z","title":"RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language\n  Models for Robotics","summary":"  Spatial understanding is a crucial capability that enables robots to perceive\ntheir surroundings, reason about their environment, and interact with it\nmeaningfully. In modern robotics, these capabilities are increasingly provided\nby vision-language models. However, these models face significant challenges in\nspatial reasoning tasks, as their training data are based on general-purpose\nimage datasets that often lack sophisticated spatial understanding. For\nexample, datasets frequently do not capture reference frame comprehension, yet\neffective spatial reasoning requires understanding whether to reason from ego-,\nworld-, or object-centric perspectives. To address this issue, we introduce\nRoboSpatial, a large-scale dataset for spatial understanding in robotics. It\nconsists of real indoor and tabletop scenes, captured as 3D scans and\negocentric images, and annotated with rich spatial information relevant to\nrobotics. The dataset includes 1M images, 5k 3D scans, and 3M annotated spatial\nrelationships, and the pairing of 2D egocentric images with 3D scans makes it\nboth 2D- and 3D- ready. Our experiments show that models trained with\nRoboSpatial outperform baselines on downstream tasks such as spatial affordance\nprediction, spatial relationship prediction, and robot manipulation.\n","authors":["Chan Hee Song","Valts Blukis","Jonathan Tremblay","Stephen Tyree","Yu Su","Stan Birchfield"],"pdf_url":"https://arxiv.org/pdf/2411.16537v3.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2411.16425v2","updated":"2025-03-26T07:26:43Z","published":"2024-11-25T14:27:55Z","title":"TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for\n  Zero-shot Object Navigation","summary":"  The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find\na previously unseen object by navigating in unfamiliar environments. Such a\ngoal-oriented exploration heavily relies on the ability to perceive,\nunderstand, and reason based on the spatial information of the environment.\nHowever, current LLM-based approaches convert visual observations to language\ndescriptions and reason in the linguistic space, leading to the loss of spatial\ninformation. In this paper, we introduce TopV-Nav, an MLLM-based method that\ndirectly reasons on the top-view map with sufficient spatial information. To\nfully unlock the MLLM's spatial reasoning potential in top-view perspective, we\npropose the Adaptive Visual Prompt Generation (AVPG) method to adaptively\nconstruct semantically-rich top-view map. It enables the agent to directly\nutilize spatial information contained in the top-view map to conduct thorough\nreasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to\ndynamically zoom top-view map at preferred scales, enhancing local fine-grained\nreasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to\npredict and to utilize target locations, facilitating global and human-like\nexploration. Experiments on MP3D and HM3D datasets demonstrate the superiority\nof our TopV-Nav.\n","authors":["Linqing Zhong","Chen Gao","Zihan Ding","Yue Liao","Huimin Ma","Shifeng Zhang","Xu Zhou","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16425v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2503.20280v1","updated":"2025-03-26T07:09:40Z","published":"2025-03-26T07:09:40Z","title":"Turning Circle-based Control Barrier Function for Efficient Collision\n  Avoidance of Nonholonomic Vehicles","summary":"  This paper presents a new control barrier function (CBF) designed to improve\nthe efficiency of collision avoidance for nonholonomic vehicles. Traditional\nCBFs typically rely on the shortest Euclidean distance to obstacles,\noverlooking the limited heading change ability of nonholonomic vehicles. This\noften leads to abrupt maneuvers and excessive speed reductions, which is not\ndesirable and reduces the efficiency of collision avoidance. Our approach\naddresses these limitations by incorporating the distance to the turning\ncircle, considering the vehicle's limited maneuverability imposed by its\nnonholonomic constraints. The proposed CBF is integrated with model predictive\ncontrol (MPC) to generate more efficient trajectories compared to existing\nmethods that rely solely on Euclidean distance-based CBFs. The effectiveness of\nthe proposed method is validated through numerical simulations on unicycle\nvehicles and experiments with underactuated surface vehicles.\n","authors":["Changyu Lee","Kiyong Park","Jinwhan Kim"],"pdf_url":"https://arxiv.org/pdf/2503.20280v1.pdf","comment":"This work has been submitted to an IEEE journal for possible\n  publication"},{"id":"http://arxiv.org/abs/2408.08160v3","updated":"2025-03-26T06:56:09Z","published":"2024-08-15T13:49:14Z","title":"General-purpose Clothes Manipulation with Semantic Keypoints","summary":"  Clothes manipulation is a critical capability for household robots; yet,\nexisting methods are often confined to specific tasks, such as folding or\nflattening, due to the complex high-dimensional geometry of deformable fabric.\nThis paper presents CLothes mAnipulation with Semantic keyPoints (CLASP) for\ngeneral-purpose clothes manipulation, which enables the robot to perform\ndiverse manipulation tasks over different types of clothes. The key idea of\nCLASP is semantic keypoints -- e.g., \"right shoulder\", \"left sleeve\", etc. -- a\nsparse spatial-semantic representation that is salient for both perception and\naction. Semantic keypoints of clothes can be effectively extracted from depth\nimages and are sufficient to represent a broad range of clothes manipulation\npolicies. CLASP leverages semantic keypoints to bridge LLM-powered task\nplanning and low-level action execution in a two-level hierarchy. Extensive\nsimulation experiments show that CLASP outperforms baseline methods across\ndiverse clothes types in both seen and unseen tasks. Further, experiments with\na Kinova dual-arm system on four distinct tasks -- folding, flattening,\nhanging, and placing -- confirm CLASP's performance on a real robot.\n","authors":["Yuhong Deng","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2408.08160v3.pdf","comment":"accepted by IEEE International Conference on Robotics and Automation\n  (ICRA 2025)"},{"id":"http://arxiv.org/abs/2406.05613v3","updated":"2025-03-26T06:44:06Z","published":"2024-06-09T02:12:06Z","title":"Distributed Motion Control of Multiple Mobile Manipulators for Reducing\n  Interaction Wrench in Object Manipulation","summary":"  In real-world cooperative manipulation of objects, multiple mobile\nmanipulator systems may suffer from disturbances and asynchrony, leading to\nexcessive interaction wrenches and potentially causing object damage or\nemergency stops. Existing methods often rely on torque control and dynamic\nmodels, which are uncommon in many industrial robots and settings.\nAdditionally, dynamic models often neglect joint friction forces and are not\naccurate. These methods are challenging to implement and validate in physical\nsystems. To address the problems, this paper presents a novel distributed\nmotion control approach aimed at reducing these unnecessary interaction\nwrenches. The control law is only based on local information and joint velocity\ncontrol to enhance practical applicability. The communication delays within the\ndistributed architecture are considered. The stability of the control law is\nrigorously proven by the Lyapunov theorem. In the simulations, the\neffectiveness is shown, and the impact of communication graph connectivity and\ncommunication delays has been studied. A comparison with other methods shows\nthe advantages of the proposed control law in terms of convergence speed and\nrobustness. Finally, the control law has been validated in physical\nexperiments. It does not require dynamic modeling or torque control, and thus\nis more user-friendly for physical robots.\n","authors":["Wenhang Liu","Meng Ren","Kun Song","Gaoming Chen","Michael Yu Wang","Zhenhua Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.05613v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19528v2","updated":"2025-03-26T06:33:48Z","published":"2024-09-29T03:07:34Z","title":"FoAM: Foresight-Augmented Multi-Task Imitation Policy for Robotic\n  Manipulation","summary":"  Multi-task imitation learning (MTIL) has shown significant potential in\nrobotic manipulation by enabling agents to perform various tasks using a single\npolicy. This simplifies the policy deployment and enhances the agent's\nadaptability across different scenarios. However, key challenges remain, such\nas maintaining action reliability (e.g., avoiding abnormal action sequences\nthat deviate from nominal task trajectories) and generalizing to unseen tasks\nwith a few expert demonstrations. To address these challenges, we introduce the\nForesight-Augmented Manipulation Policy (FoAM), a novel MTIL policy that\npioneers the use of multi-modal goal condition as input and introduces a\nforesight augmentation in addition to the general action reconstruction. FoAM\nenables the agent to reason about the visual consequences (states) of its\nactions and learn more expressive embedding that captures nuanced task\nvariations. Extensive experiments on over 100 tasks in simulation and\nreal-world settings demonstrate that FoAM significantly enhances MTIL policy\nperformance, outperforming state-of-the-art baselines by up to 41% in success\nrate. Meanwhile, we released our simulation suites, including a total of 10\nscenarios and over 80 challenging tasks designed for manipulation policy\ntraining and evaluation. See the project homepage projFoAM.github.io for\nproject details.\n","authors":["Litao Liu","Wentao Wang","Yifan Han","Zhuoli Xie","Pengfei Yi","Junyan Li","Yi Qin","Wenzhao Lian"],"pdf_url":"https://arxiv.org/pdf/2409.19528v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2302.10463v2","updated":"2025-03-26T05:54:55Z","published":"2023-02-21T06:11:08Z","title":"Vision-based Multi-future Trajectory Prediction: A Survey","summary":"  Vision-based trajectory prediction is an important task that supports safe\nand intelligent behaviours in autonomous systems. Many advanced approaches have\nbeen proposed over the years with improved spatial and temporal feature\nextraction. However, human behaviour is naturally diverse and uncertain. Given\nthe past trajectory and surrounding environment information, an agent can have\nmultiple plausible trajectories in the future. To tackle this problem, an\nessential task named multi-future trajectory prediction (MTP) has recently been\nstudied. This task aims to generate a diverse, acceptable and explainable\ndistribution of future predictions for each agent. In this paper, we present\nthe first survey for MTP with our unique taxonomies and a comprehensive\nanalysis of frameworks, datasets and evaluation metrics. We also compare models\non existing MTP datasets and conduct experiments on the ForkingPath dataset.\nFinally, we discuss multiple future directions that can help researchers\ndevelop novel multi-future trajectory prediction systems and other diverse\nlearning tasks similar to MTP.\n","authors":["Renhao Huang","Hao Xue","Maurice Pagnucco","Flora Salim","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2302.10463v2.pdf","comment":"Accepted by TNNLS 2025"},{"id":"http://arxiv.org/abs/2409.08681v2","updated":"2025-03-26T05:31:23Z","published":"2024-09-13T09:50:04Z","title":"SLIM: Scalable and Lightweight LiDAR Mapping in Urban Environments","summary":"  LiDAR point cloud maps are extensively utilized on roads for robot navigation\ndue to their high consistency. However, dense point clouds face challenges of\nhigh memory consumption and reduced maintainability for long-term operations.\nIn this study, we introduce SLIM, a scalable and lightweight mapping system for\nlong-term LiDAR mapping in urban environments. The system begins by\nparameterizing structural point clouds into lines and planes. These lightweight\nand structural representations meet the requirements of map merging, pose graph\noptimization, and bundle adjustment, ensuring incremental management and local\nconsistency. For long-term operations, a map-centric nonlinear factor recovery\nmethod is designed to sparsify poses while preserving mapping accuracy. We\nvalidate the SLIM system with multi-session real-world LiDAR data from\nclassical LiDAR mapping datasets, including KITTI, NCLT, HeLiPR and M2DGR. The\nexperiments demonstrate its capabilities in mapping accuracy, lightweightness,\nand scalability. Map re-use is also verified through map-based robot\nlocalization. Finally, with multi-session LiDAR data, the SLIM system provides\na globally consistent map with low memory consumption (~130 KB/km on KITTI).\n","authors":["Zehuan Yu","Zhijian Qiao","Wenyi Liu","Huan Yin","Shaojie Shen"],"pdf_url":"https://arxiv.org/pdf/2409.08681v2.pdf","comment":"Accepted for publication in IEEE Transactions on Robotics. Video:\n  https://youtu.be/8HQnYMf_BWI Code:\n  https://github.com/HKUST-Aerial-Robotics/SLIM"},{"id":"http://arxiv.org/abs/2503.20241v1","updated":"2025-03-26T05:15:26Z","published":"2025-03-26T05:15:26Z","title":"LGR: LLM-Guided Ranking of Frontiers for Object Goal Navigation","summary":"  Object Goal Navigation (OGN) is a fundamental task for robots and AI, with\nkey applications such as mobile robot image databases (MRID). In particular,\nmapless OGN is essential in scenarios involving unknown or dynamic\nenvironments. This study aims to enhance recent modular mapless OGN systems by\nleveraging the commonsense reasoning capabilities of large language models\n(LLMs). Specifically, we address the challenge of determining the visiting\norder in frontier-based exploration by framing it as a frontier ranking\nproblem. Our approach is grounded in recent findings that, while LLMs cannot\ndetermine the absolute value of a frontier, they excel at evaluating the\nrelative value between multiple frontiers viewed within a single image using\nthe view image as context. We dynamically manage the frontier list by adding\nand removing elements, using an LLM as a ranking model. The ranking results are\nrepresented as reciprocal rank vectors, which are ideal for multi-view,\nmulti-query information fusion. We validate the effectiveness of our method\nthrough evaluations in Habitat-Sim.\n","authors":["Mitsuaki Uno","Kanji Tanaka","Daiki Iwata","Yudai Noda","Shoya Miyazaki","Kouki Terashima"],"pdf_url":"https://arxiv.org/pdf/2503.20241v1.pdf","comment":"10 pages, 11 figures, technical report"},{"id":"http://arxiv.org/abs/2503.20237v1","updated":"2025-03-26T05:05:16Z","published":"2025-03-26T05:05:16Z","title":"A Virtual Fencing Framework for Safe and Efficient Collaborative\n  Robotics","summary":"  Collaborative robots (cobots) increasingly operate alongside humans,\ndemanding robust real-time safeguarding. Current safety standards (e.g., ISO\n10218, ANSI/RIA 15.06, ISO/TS 15066) require risk assessments but offer limited\nguidance for real-time responses. We propose a virtual fencing approach that\ndetects and predicts human motion, ensuring safe cobot operation. Safety and\nperformance tradeoffs are modeled as an optimization problem and solved via\nsequential quadratic programming. Experimental validation shows that our method\nminimizes operational pauses while maintaining safety, providing a modular\nsolution for human-robot collaboration.\n","authors":["Vineela Reddy Pippera Badguna","Aliasghar Arab","Durga Avinash Kodavalla"],"pdf_url":"https://arxiv.org/pdf/2503.20237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00312v2","updated":"2025-03-26T04:52:23Z","published":"2024-06-01T06:10:42Z","title":"NuRF: Nudging the Particle Filter in Radiance Fields for Robot Visual\n  Localization","summary":"  Can we localize a robot on a map only using monocular vision? This study\npresents NuRF, an adaptive and nudged particle filter framework in radiance\nfields for 6-DoF robot visual localization. NuRF leverages recent advancements\nin radiance fields and visual place recognition. Conventional visual place\nrecognition meets the challenges of data sparsity and artifact-induced\ninaccuracies. By utilizing radiance field-generated novel views, NuRF enhances\nvisual localization performance and combines coarse global localization with\nthe fine-grained pose tracking of a particle filter, ensuring continuous and\nprecise localization. Experimentally, our method converges 7 times faster than\nexisting Monte Carlo-based methods and achieves localization accuracy within 1\nmeter, offering an efficient and resilient solution for indoor visual\nlocalization.\n","authors":["Wugang Meng","Tianfu Wu","Huan Yin","Fumin Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.00312v2.pdf","comment":"Accepted for Publication in IEEE Transactions on Cognitive and\n  Developmental Systems"},{"id":"http://arxiv.org/abs/2503.20211v1","updated":"2025-03-26T04:12:54Z","published":"2025-03-26T04:12:54Z","title":"Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning\n  with Motion and Structure Priors","summary":"  Self-supervised depth estimation from monocular cameras in diverse outdoor\nconditions, such as daytime, rain, and nighttime, is challenging due to the\ndifficulty of learning universal representations and the severe lack of labeled\nreal-world adverse data. Previous methods either rely on synthetic inputs and\npseudo-depth labels or directly apply daytime strategies to adverse conditions,\nresulting in suboptimal results. In this paper, we present the first\nsynthetic-to-real robust depth estimation framework, incorporating motion and\nstructure priors to capture real-world knowledge effectively. In the synthetic\nadaptation, we transfer motion-structure knowledge inside cost volumes for\nbetter robust representation, using a frozen daytime model to train a depth\nestimator in synthetic adverse conditions. In the innovative real adaptation,\nwhich targets to fix synthetic-real gaps, models trained earlier identify the\nweather-insensitive regions with a designed consistency-reweighting strategy to\nemphasize valid pseudo-labels. We introduce a new regularization by gathering\nexplicit depth distributions to constrain the model when facing real-world\ndata. Experiments show that our method outperforms the state-of-the-art across\ndiverse conditions in multi-frame and single-frame evaluations. We achieve\nimprovements of 7.5% and 4.3% in AbsRel and RMSE on average for nuScenes and\nRobotcar datasets (daytime, nighttime, rain). In zero-shot evaluation of\nDrivingStereo (rain, fog), our method generalizes better than the previous\nones.\n","authors":["Weilong Yan","Ming Li","Haipeng Li","Shuwei Shao","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2503.20211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20208v1","updated":"2025-03-26T04:05:50Z","published":"2025-03-26T04:05:50Z","title":"Learning Adaptive Dexterous Grasping from Single Demonstrations","summary":"  How can robots learn dexterous grasping skills efficiently and apply them\nadaptively based on user instructions? This work tackles two key challenges:\nefficient skill acquisition from limited human demonstrations and\ncontext-driven skill selection. We introduce AdaDexGrasp, a framework that\nlearns a library of grasping skills from a single human demonstration per skill\nand selects the most suitable one using a vision-language model (VLM). To\nimprove sample efficiency, we propose a trajectory following reward that guides\nreinforcement learning (RL) toward states close to a human demonstration while\nallowing flexibility in exploration. To learn beyond the single demonstration,\nwe employ curriculum learning, progressively increasing object pose variations\nto enhance robustness. At deployment, a VLM retrieves the appropriate skill\nbased on user instructions, bridging low-level learned skills with high-level\nintent. We evaluate AdaDexGrasp in both simulation and real-world settings,\nshowing that our approach significantly improves RL efficiency and enables\nlearning human-like grasp strategies across varied object configurations.\nFinally, we demonstrate zero-shot transfer of our learned policies to a\nreal-world PSYONIC Ability Hand, with a 90% success rate across objects,\nsignificantly outperforming the baseline.\n","authors":["Liangzhi Shi","Yulin Liu","Lingqi Zeng","Bo Ai","Zhengdong Hong","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2503.20208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20207v1","updated":"2025-03-26T04:03:51Z","published":"2025-03-26T04:03:51Z","title":"Reasoning and Learning a Perceptual Metric for Self-Training of\n  Reflective Objects in Bin-Picking with a Low-cost Camera","summary":"  Bin-picking of metal objects using low-cost RGB-D cameras often suffers from\nsparse depth information and reflective surface textures, leading to errors and\nthe need for manual labeling. To reduce human intervention, we propose a\ntwo-stage framework consisting of a metric learning stage and a self-training\nstage. Specifically, to automatically process data captured by a low-cost\ncamera (LC), we introduce a Multi-object Pose Reasoning (MoPR) algorithm that\noptimizes pose hypotheses under depth, collision, and boundary constraints. To\nfurther refine pose candidates, we adopt a Symmetry-aware Lie-group based\nBayesian Gaussian Mixture Model (SaL-BGMM), integrated with the\nExpectation-Maximization (EM) algorithm, for symmetry-aware filtering.\nAdditionally, we propose a Weighted Ranking Information Noise Contrastive\nEstimation (WR-InfoNCE) loss to enable the LC to learn a perceptual metric from\nreconstructed data, supporting self-training on untrained or even unseen\nobjects. Experimental results show that our approach outperforms several\nstate-of-the-art methods on both the ROBI dataset and our newly introduced\nSelf-ROBI dataset.\n","authors":["Peiyuan Ni","Chee Meng Chew","Marcelo H. Ang Jr.","Gregory S. Chirikjian"],"pdf_url":"https://arxiv.org/pdf/2503.20207v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.20202v1","updated":"2025-03-26T03:55:41Z","published":"2025-03-26T03:55:41Z","title":"SARGes: Semantically Aligned Reliable Gesture Generation via Intent\n  Chain","summary":"  Co-speech gesture generation enhances human-computer interaction realism\nthrough speech-synchronized gesture synthesis. However, generating semantically\nmeaningful gestures remains a challenging problem. We propose SARGes, a novel\nframework that leverages large language models (LLMs) to parse speech content\nand generate reliable semantic gesture labels, which subsequently guide the\nsynthesis of meaningful co-speech gestures.First, we constructed a\ncomprehensive co-speech gesture ethogram and developed an LLM-based intent\nchain reasoning mechanism that systematically parses and decomposes gesture\nsemantics into structured inference steps following ethogram criteria,\neffectively guiding LLMs to generate context-aware gesture labels.\nSubsequently, we constructed an intent chain-annotated text-to-gesture label\ndataset and trained a lightweight gesture label generation model, which then\nguides the generation of credible and semantically coherent co-speech gestures.\nExperimental results demonstrate that SARGes achieves highly\nsemantically-aligned gesture labeling (50.2% accuracy) with efficient\nsingle-pass inference (0.4 seconds). The proposed method provides an\ninterpretable intent reasoning pathway for semantic gesture synthesis.\n","authors":["Nan Gao","Yihua Bao","Dongdong Weng","Jiayi Zhao","Jia Li","Yan Zhou","Pengfei Wan","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20176v1","updated":"2025-03-26T03:04:42Z","published":"2025-03-26T03:04:42Z","title":"Offline Reinforcement Learning with Discrete Diffusion Skills","summary":"  Skills have been introduced to offline reinforcement learning (RL) as\ntemporal abstractions to tackle complex, long-horizon tasks, promoting\nconsistent behavior and enabling meaningful exploration. While skills in\noffline RL are predominantly modeled within a continuous latent space, the\npotential of discrete skill spaces remains largely underexplored. In this\npaper, we propose a compact discrete skill space for offline RL tasks supported\nby state-of-the-art transformer-based encoder and diffusion-based decoder.\nCoupled with a high-level policy trained via offline RL techniques, our method\nestablishes a hierarchical RL framework where the trained diffusion decoder\nplays a pivotal role. Empirical evaluations show that the proposed algorithm,\nDiscrete Diffusion Skill (DDS), is a powerful offline RL method. DDS performs\ncompetitively on Locomotion and Kitchen tasks and excels on long-horizon tasks,\nachieving at least a 12 percent improvement on AntMaze-v2 benchmarks compared\nto existing offline RL approaches. Furthermore, DDS offers improved\ninterpretability, training stability, and online exploration compared to\nprevious skill-based methods.\n","authors":["RuiXi Qiao","Jie Cheng","Xingyuan Dai","Yonglin Tian","Yisheng Lv"],"pdf_url":"https://arxiv.org/pdf/2503.20176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16627v2","updated":"2025-03-26T02:40:00Z","published":"2024-11-25T18:03:50Z","title":"Inference-Time Policy Steering through Human Interactions","summary":"  Generative policies trained with human demonstrations can autonomously\naccomplish multimodal, long-horizon tasks. However, during inference, humans\nare often removed from the policy execution loop, limiting the ability to guide\na pre-trained policy towards a specific sub-goal or trajectory shape among\nmultiple predictions. Naive human intervention may inadvertently exacerbate\ndistribution shift, leading to constraint violations or execution failures. To\nbetter align policy output with human intent without inducing\nout-of-distribution errors, we propose an Inference-Time Policy Steering (ITPS)\nframework that leverages human interactions to bias the generative sampling\nprocess, rather than fine-tuning the policy on interaction data. We evaluate\nITPS across three simulated and real-world benchmarks, testing three forms of\nhuman interaction and associated alignment distance metrics. Among six sampling\nstrategies, our proposed stochastic sampling with diffusion policy achieves the\nbest trade-off between alignment and distribution shift. Videos are available\nat https://yanweiw.github.io/itps/.\n","authors":["Yanwei Wang","Lirui Wang","Yilun Du","Balakumar Sundaralingam","Xuning Yang","Yu-Wei Chao","Claudia Perez-D'Arpino","Dieter Fox","Julie Shah"],"pdf_url":"https://arxiv.org/pdf/2411.16627v2.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2411.13908v2","updated":"2025-03-26T02:02:52Z","published":"2024-11-21T07:46:58Z","title":"Hybrid Physics-ML Modeling for Marine Vehicle Maneuvering Motions in the\n  Presence of Environmental Disturbances","summary":"  A hybrid physics-machine learning modeling framework is proposed for the\nsurface vehicles' maneuvering motions to address the modeling capability and\nstability in the presence of environmental disturbances. From a deep learning\nperspective, the framework is based on a variant version of residual networks\nwith additional feature extraction. Initially, an imperfect physical model is\nderived and identified to capture the fundamental hydrodynamic characteristics\nof marine vehicles. This model is then integrated with a feedforward network\nthrough a residual block. Additionally, feature extraction from trigonometric\ntransformations is employed in the machine learning component to account for\nthe periodic influence of currents and waves. The proposed method is evaluated\nusing real navigational data from the 'JH7500' unmanned surface vehicle. The\nresults demonstrate the robust generalizability and accurate long-term\nprediction capabilities of the nonlinear dynamic model in specific\nenvironmental conditions. This approach has the potential to be extended and\napplied to develop a comprehensive high-fidelity simulator.\n","authors":["Zihao Wang","Jian Cheng","Liang Xu","Lizhu Hao","Yan Peng"],"pdf_url":"https://arxiv.org/pdf/2411.13908v2.pdf","comment":"The content of the manuscript will undergo significant revisions"},{"id":"http://arxiv.org/abs/2503.20134v1","updated":"2025-03-26T00:57:04Z","published":"2025-03-26T00:57:04Z","title":"DRPA-MPPI: Dynamic Repulsive Potential Augmented MPPI for Reactive\n  Navigation in Unstructured Environments","summary":"  Reactive mobile robot navigation in unstructured environments is challenging\nwhen robots encounter unexpected obstacles that invalidate previously planned\ntrajectories. Model predictive path integral control (MPPI) enables reactive\nplanning, but still suffers from limited prediction horizons that lead to local\nminima traps near obstacles. Current solutions rely on heuristic cost design or\nscenario-specific pre-training, which often limits their adaptability to new\nenvironments. We introduce dynamic repulsive potential augmented MPPI\n(DRPA-MPPI), which dynamically detects potential entrapments on the predicted\ntrajectories. Upon detecting local minima, DRPA-MPPI automatically switches\nbetween standard goal-oriented optimization and a modified cost function that\ngenerates repulsive forces away from local minima. Comprehensive testing in\nsimulated obstacle-rich environments confirms DRPA-MPPI's superior navigation\nperformance and safety compared to conventional methods with less computational\nburden.\n","authors":["Takahiro Fuke","Masafumi Endo","Kohei Honda","Genya Ishigami"],"pdf_url":"https://arxiv.org/pdf/2503.20134v1.pdf","comment":"9 pages, 4 figures, Submitted to the 2025 IEEE International\n  Conference on Automation Science and Engineering (CASE)"},{"id":"http://arxiv.org/abs/2503.20127v1","updated":"2025-03-26T00:33:38Z","published":"2025-03-26T00:33:38Z","title":"Bandwidth Allocation for Cloud-Augmented Autonomous Driving","summary":"  Autonomous vehicle (AV) control systems increasingly rely on ML models for\ntasks such as perception and planning. Current practice is to run these models\non the car's local hardware due to real-time latency constraints and\nreliability concerns, which limits model size and thus accuracy. Prior work has\nobserved that we could augment current systems by running larger models in the\ncloud, relying on faster cloud runtimes to offset the cellular network latency.\nHowever, prior work does not account for an important practical constraint:\nlimited cellular bandwidth. We show that, for typical bandwidth levels,\nproposed techniques for cloud-augmented AV models take too long to transfer\ndata, thus mostly falling back to the on-car models and resulting in no\naccuracy improvement.\n  In this work, we show that realizing cloud-augmented AV models requires\nintelligent use of this scarce bandwidth, i.e. carefully allocating bandwidth\nacross tasks and providing multiple data compression and model options. We\nformulate this as a resource allocation problem to maximize car utility, and\npresent our system \\sysname which achieves an increase in average model\naccuracy by up to 15 percentage points on driving scenarios from the Waymo Open\nDataset.\n","authors":["Peter Schafhalter","Alexander Krentsel","Joseph E. Gonzalez","Sylvia Ratnasamy","Scott Shenker","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2503.20127v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2503.21044v1","updated":"2025-03-26T23:24:22Z","published":"2025-03-26T23:24:22Z","title":"Exploring Interference between Concurrent Skin Stretches","summary":"  Proprioception is essential for coordinating human movements and enhancing\nthe performance of assistive robotic devices. Skin stretch feedback, which\nclosely aligns with natural proprioception mechanisms, presents a promising\nmethod for conveying proprioceptive information. To better understand the\nimpact of interference on skin stretch perception, we conducted a user study\nwith 30 participants that evaluated the effect of two simultaneous skin\nstretches on user perception. We observed that when participants experience\nsimultaneous skin stretch stimuli, a masking effect occurs which deteriorates\nperception performance in the collocated skin stretch configurations. However,\nthe perceived workload stays the same. These findings show that interference\ncan affect the perception of skin stretch such that multi-channel skin stretch\nfeedback designs should avoid locating modules in close proximity.\n","authors":["Ching Hei Cheng","Jonathan Eden","Denny Oetomo","Ying Tan"],"pdf_url":"https://arxiv.org/pdf/2503.21044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09833v2","updated":"2025-03-26T22:41:51Z","published":"2024-08-19T09:29:00Z","title":"Automated Vehicle Driver Monitoring Dataset from Real-World Scenarios","summary":"  From SAE Level 3 of automation onwards, drivers are allowed to engage in\nactivities that are not directly related to driving during their travel.\nHowever, in level 3, a misunderstanding of the capabilities of the system might\nlead drivers to engage in secondary tasks, which could impair their ability to\nreact to challenging traffic situations.\n  Anticipating driver activity allows for early detection of risky behaviors,\nto prevent accidents. To be able to predict the driver activity, a Deep\nLearning network needs to be trained on a dataset. However, the use of datasets\nbased on simulation for training and the migration to real-world data for\nprediction has proven to be suboptimal. Hence, this paper presents a real-world\ndriver activity dataset, openly accessible on IEEE Dataport, which encompasses\nvarious activities that occur in autonomous driving scenarios under various\nillumination and weather conditions. Results from the training process showed\nthat the dataset provides an excellent benchmark for implementing models for\ndriver activity recognition.\n","authors":["Mohamed Sabry","Walter Morales-Alvarez","Cristina Olaverri-Monreal"],"pdf_url":"https://arxiv.org/pdf/2408.09833v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2405.16439v3","updated":"2025-03-26T21:19:58Z","published":"2024-05-26T05:48:21Z","title":"Multi-Agent Inverse Reinforcement Learning in Real World Unstructured\n  Pedestrian Crowds","summary":"  Social robot navigation in crowded public spaces such as university campuses,\nrestaurants, grocery stores, and hospitals, is an increasingly important area\nof research. One of the core strategies for achieving this goal is to\nunderstand humans' intent--underlying psychological factors that govern their\nmotion--by learning their reward functions, typically via inverse reinforcement\nlearning (IRL). Despite significant progress in IRL, learning reward functions\nof multiple agents simultaneously in dense unstructured pedestrian crowds has\nremained intractable due to the nature of the tightly coupled social\ninteractions that occur in these scenarios \\textit{e.g.} passing,\nintersections, swerving, weaving, etc. In this paper, we present a new\nmulti-agent maximum entropy inverse reinforcement learning algorithm for real\nworld unstructured pedestrian crowds. Key to our approach is a simple, but\neffective, mathematical trick which we name the so-called\ntractability-rationality trade-off trick that achieves tractability at the cost\nof a slight reduction in accuracy. We compare our approach to the classical\nsingle-agent MaxEnt IRL as well as state-of-the-art trajectory prediction\nmethods on several datasets including the ETH, UCY, SCAND, JRDB, and a new\ndataset, called Speedway, collected at a busy intersection on a University\ncampus focusing on dense, complex agent interactions. Our key findings show\nthat, on the dense Speedway dataset, our approach ranks 1st among top 7\nbaselines with >2X improvement over single-agent IRL, and is competitive with\nstate-of-the-art large transformer-based encoder-decoder models on sparser\ndatasets such as ETH/UCY (ranks 3rd among top 7 baselines).\n","authors":["Rohan Chandra","Haresh Karnan","Negar Mehr","Peter Stone","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2405.16439v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01894v2","updated":"2025-03-26T20:42:44Z","published":"2025-02-04T00:00:06Z","title":"SimBEV: A Synthetic Multi-Task Multi-Sensor Driving Data Generation Tool\n  and Dataset","summary":"  Bird's-eye view (BEV) perception has garnered significant attention in\nautonomous driving in recent years, in part because BEV representation\nfacilitates multi-modal sensor fusion. BEV representation enables a variety of\nperception tasks including BEV segmentation, a concise view of the environment\nuseful for planning a vehicle's trajectory. However, this representation is not\nfully supported by existing datasets, and creation of new datasets for this\npurpose can be a time-consuming endeavor. To address this challenge, we\nintroduce SimBEV. SimBEV is a randomized synthetic data generation tool that is\nextensively configurable and scalable, supports a wide array of sensors,\nincorporates information from multiple sources to capture accurate BEV ground\ntruth, and enables a variety of perception tasks including BEV segmentation and\n3D object detection. SimBEV is used to create the SimBEV dataset, a large\ncollection of annotated perception data from diverse driving scenarios. SimBEV\nand the SimBEV dataset are open and available to the public.\n","authors":["Goodarz Mehr","Azim Eskandarian"],"pdf_url":"https://arxiv.org/pdf/2502.01894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20957v1","updated":"2025-03-26T19:45:18Z","published":"2025-03-26T19:45:18Z","title":"Pellet-based 3D Printing of Soft Thermoplastic Elastomeric Membranes for\n  Soft Robotic Applications","summary":"  Additive Manufacturing (AM) is a promising solution for handling the\ncomplexity of fabricating soft robots. However, the AM of hyperelastic\nmaterials is still challenging with limited material types. Within this work,\npellet-based 3D printing of very soft thermoplastic elastomers (TPEs) was\nexplored. Our results show that TPEs can have similar engineering stress and\nmaximum strain as Ecoflex OO-10. These TPEs were used to 3D-print airtight thin\nmembranes (0.2-1.2 mm), which could inflate up to a stretch of 1320\\%.\nCombining the membrane's large expansion and softness with the 3D printing of\nhollow structures simplified the design of a bending actuator that can bend 180\ndegrees and reach a blocked force of 238 times its weight. In addition, by 3D\nprinting TPE pellets and rigid filaments, the soft membrane could grasp objects\nby enveloping an object or as a sensorized sucker, which relied on the TPE's\nsoftness to conform to the object or act as a seal. In addition, the membrane\nof the sucker was utilized as a tactile sensor to detect an object before\nadhesion. These results suggest the feasibility of 3D printing soft robots by\nusing soft TPEs and membranes as an interesting class of materials and\nsensorized actuators, respectively.\n","authors":["Nick Willemstein","Herman van der Kooij","Ali Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2503.20957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20916v1","updated":"2025-03-26T18:43:49Z","published":"2025-03-26T18:43:49Z","title":"A Study of Perceived Safety for Soft Robotics in Caregiving Tasks","summary":"  In this project, we focus on human-robot interaction in caregiving scenarios\nlike bathing, where physical contact is inevitable and necessary for proper\ntask execution because force must be applied to the skin. Using finite element\nanalysis, we designed a 3D-printed gripper combining positive and negative\npressure for secure yet compliant handling. Preliminary tests showed it exerted\na lower, more uniform pressure profile than a standard rigid gripper. In a user\nstudy, participants' trust in robots significantly increased after they\nexperienced a brief bathing demonstration performed by a robotic arm equipped\nwith the soft gripper. These results suggest that soft robotics can enhance\nperceived safety and acceptance in intimate caregiving scenarios.\n","authors":["Cosima du Pasquier","Jennifer Grannen","Chuer Pan","Serin L. Huber","Aliyah Smith","Monroe Kennedy","Shuran Song","Dorsa Sadigh","Allison M. Okamura"],"pdf_url":"https://arxiv.org/pdf/2503.20916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20853v1","updated":"2025-03-26T17:59:51Z","published":"2025-03-26T17:59:51Z","title":"Unified Multimodal Discrete Diffusion","summary":"  Multimodal generative models that can understand and generate across multiple\nmodalities are dominated by autoregressive (AR) approaches, which process\ntokens sequentially from left to right, or top to bottom. These models jointly\nhandle images, text, video, and audio for various tasks such as image\ncaptioning, question answering, and image generation. In this work, we explore\ndiscrete diffusion models as a unified generative formulation in the joint text\nand image domain, building upon their recent success in text generation.\nDiscrete diffusion models offer several advantages over AR models, including\nimproved control over quality versus diversity of generated samples, the\nability to perform joint multimodal inpainting (across both text and image\ndomains), and greater controllability in generation through guidance.\nLeveraging these benefits, we present the first Unified Multimodal Discrete\nDiffusion (UniDisc) model which is capable of jointly understanding and\ngenerating text and images for a variety of downstream tasks. We compare\nUniDisc to multimodal AR models, performing a scaling analysis and\ndemonstrating that UniDisc outperforms them in terms of both performance and\ninference-time compute, enhanced controllability, editability, inpainting, and\nflexible trade-off between inference time and generation quality. Code and\nadditional visualizations are available at https://unidisc.github.io.\n","authors":["Alexander Swerdlow","Mihir Prabhudesai","Siddharth Gandhi","Deepak Pathak","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2503.20853v1.pdf","comment":"Project Website: https://unidisc.github.io"},{"id":"http://arxiv.org/abs/2503.20844v1","updated":"2025-03-26T15:08:58Z","published":"2025-03-26T15:08:58Z","title":"Robust Deep Reinforcement Learning in Robotics via Adaptive\n  Gradient-Masked Adversarial Attacks","summary":"  Deep reinforcement learning (DRL) has emerged as a promising approach for\nrobotic control, but its realworld deployment remains challenging due to its\nvulnerability to environmental perturbations. Existing white-box adversarial\nattack methods, adapted from supervised learning, fail to effectively target\nDRL agents as they overlook temporal dynamics and indiscriminately perturb all\nstate dimensions, limiting their impact on long-term rewards. To address these\nchallenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR)\nAttack, a white-box attack method that combines DRL with a gradient-based soft\nmasking mechanism to dynamically identify critical state dimensions and\noptimize adversarial policies. AGMR selectively allocates perturbations to the\nmost impactful state features and incorporates a dynamic adjustment mechanism\nto balance exploration and exploitation during training. Extensive experiments\ndemonstrate that AGMR outperforms state-of-the-art adversarial attack methods\nin degrading the performance of the victim agent and enhances the victim\nagent's robustness through adversarial defense mechanisms.\n","authors":["Zongyuan Zhang","Tianyang Duan","Zheng Lin","Dong Huang","Zihan Fang","Zekai Sun","Ling Xiong","Hongbin Liang","Heming Cui","Yong Cui","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2503.20844v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.20842v1","updated":"2025-03-26T13:56:30Z","published":"2025-03-26T13:56:30Z","title":"Anti Robot Speciesism","summary":"  Humanoid robots are a form of embodied artificial intelligence (AI) that\nlooks and acts more and more like humans. Powered by generative AI and advances\nin robotics, humanoid robots can speak and interact with humans rather\nnaturally but are still easily recognizable as robots. But how will we treat\nhumanoids when they seem indistinguishable from humans in appearance and mind?\nWe find a tendency (called \"anti-robot\" speciesism) to deny such robots\nhumanlike capabilities, driven by motivations to accord members of the human\nspecies preferential treatment. Six experiments show that robots are denied\nhumanlike attributes, simply because they are not biological beings and because\nhumans want to avoid feelings of cognitive dissonance when utilizing such\nrobots for unsavory tasks. Thus, people do not rationally attribute\ncapabilities to perfectly humanlike robots but deny them capabilities as it\nsuits them.\n","authors":["Julian De Freitas","Noah Castelo","Bernd Schmitt","Miklos Sarvary"],"pdf_url":"https://arxiv.org/pdf/2503.20842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20841v1","updated":"2025-03-26T13:27:17Z","published":"2025-03-26T13:27:17Z","title":"In vitro 2 In vivo : Bidirectional and High-Precision Generation of In\n  Vitro and In Vivo Neuronal Spike Data","summary":"  Neurons encode information in a binary manner and process complex signals.\nHowever, predicting or generating diverse neural activity patterns remains\nchallenging. In vitro and in vivo studies provide distinct advantages, yet no\nrobust computational framework seamlessly integrates both data types. We\naddress this by applying the Transformer model, widely used in large-scale\nlanguage models, to neural data. To handle binary data, we introduced Dice\nloss, enabling accurate cross-domain neural activity generation. Structural\nanalysis revealed how Dice loss enhances learning and identified key brain\nregions facilitating high-precision data generation. Our findings support the\n3Rs principle in animal research, particularly Replacement, and establish a\nmathematical framework bridging animal experiments and human clinical studies.\nThis work advances data-driven neuroscience and neural activity modeling,\npaving the way for more ethical and effective experimental methodologies.\n","authors":["Masanori Shimono"],"pdf_url":"https://arxiv.org/pdf/2503.20841v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.20839v1","updated":"2025-03-26T12:49:26Z","published":"2025-03-26T12:49:26Z","title":"TAR: Teacher-Aligned Representations via Contrastive Learning for\n  Quadrupedal Locomotion","summary":"  Quadrupedal locomotion via Reinforcement Learning (RL) is commonly addressed\nusing the teacher-student paradigm, where a privileged teacher guides a\nproprioceptive student policy. However, key challenges such as representation\nmisalignment between the privileged teacher and the proprioceptive-only\nstudent, covariate shift due to behavioral cloning, and lack of deployable\nadaptation lead to poor generalization in real-world scenarios. We propose\nTeacher-Aligned Representations via Contrastive Learning (TAR), a framework\nthat leverages privileged information with self-supervised contrastive learning\nto bridge this gap. By aligning representations to a privileged teacher in\nsimulation via contrastive objectives, our student policy learns structured\nlatent spaces and exhibits robust generalization to Out-of-Distribution (OOD)\nscenarios, surpassing the fully privileged \"Teacher\". Results showed\naccelerated training by 2x compared to state-of-the-art baselines to achieve\npeak performance. OOD scenarios showed better generalization by 40 percent on\naverage compared to existing methods. Additionally, TAR transitions seamlessly\ninto learning during deployment without requiring privileged states, setting a\nnew benchmark in sample-efficient, adaptive locomotion and enabling continual\nfine-tuning in real-world scenarios. Open-source code and videos are available\nat https://ammousa.github.io/TARLoco/.\n","authors":["Amr Mousa","Neil Karavis","Michele Caprio","Wei Pan","Richard Allmendinger"],"pdf_url":"https://arxiv.org/pdf/2503.20839v1.pdf","comment":"This work has been submitted to the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2025 for review"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2503.20785v1","updated":"2025-03-26T17:59:44Z","published":"2025-03-26T17:59:44Z","title":"Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency","summary":"  We present Free4D, a novel tuning-free framework for 4D scene generation from\na single image. Existing methods either focus on object-level generation,\nmaking scene-level generation infeasible, or rely on large-scale multi-view\nvideo datasets for expensive training, with limited generalization ability due\nto the scarcity of 4D scene data. In contrast, our key insight is to distill\npre-trained foundation models for consistent 4D scene representation, which\noffers promising advantages such as efficiency and generalizability. 1) To\nachieve this, we first animate the input image using image-to-video diffusion\nmodels followed by 4D geometric structure initialization. 2) To turn this\ncoarse structure into spatial-temporal consistent multiview videos, we design\nan adaptive guidance mechanism with a point-guided denoising strategy for\nspatial consistency and a novel latent replacement strategy for temporal\ncoherence. 3) To lift these generated observations into consistent 4D\nrepresentation, we propose a modulation-based refinement to mitigate\ninconsistencies while fully leveraging the generated information. The resulting\n4D representation enables real-time, controllable rendering, marking a\nsignificant advancement in single-image-based 4D scene generation.\n","authors":["Tianqi Liu","Zihao Huang","Zhaoxi Chen","Guangcong Wang","Shoukang Hu","Liao Shen","Huiqiang Sun","Zhiguo Cao","Wei Li","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2503.20785v1.pdf","comment":"Project Page: https://free4d.github.io/ , Code:\n  https://github.com/TQTQliu/Free4D"},{"id":"http://arxiv.org/abs/2503.20784v1","updated":"2025-03-26T17:59:31Z","published":"2025-03-26T17:59:31Z","title":"FB-4D: Spatial-Temporal Coherent Dynamic 3D Content Generation with\n  Feature Banks","summary":"  With the rapid advancements in diffusion models and 3D generation techniques,\ndynamic 3D content generation has become a crucial research area. However,\nachieving high-fidelity 4D (dynamic 3D) generation with strong spatial-temporal\nconsistency remains a challenging task. Inspired by recent findings that\npretrained diffusion features capture rich correspondences, we propose FB-4D, a\nnovel 4D generation framework that integrates a Feature Bank mechanism to\nenhance both spatial and temporal consistency in generated frames. In FB-4D, we\nstore features extracted from previous frames and fuse them into the process of\ngenerating subsequent frames, ensuring consistent characteristics across both\ntime and multiple views. To ensure a compact representation, the Feature Bank\nis updated by a proposed dynamic merging mechanism. Leveraging this Feature\nBank, we demonstrate for the first time that generating additional reference\nsequences through multiple autoregressive iterations can continuously improve\ngeneration performance. Experimental results show that FB-4D significantly\noutperforms existing methods in terms of rendering quality, spatial-temporal\nconsistency, and robustness. It surpasses all multi-view generation tuning-free\napproaches by a large margin and achieves performance on par with\ntraining-based methods.\n","authors":["Jinwei Li","Huan-ang Gao","Wenyi Li","Haohan Chi","Chenyu Liu","Chenxi Du","Yiqian Liu","Mingju Gao","Guiyu Zhang","Zongzheng Zhang","Li Yi","Yao Yao","Jingwei Zhao","Hongyang Li","Yikai Wang","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.20784v1.pdf","comment":"Project page:https://fb-4d.c7w.tech/"},{"id":"http://arxiv.org/abs/2501.16550v2","updated":"2025-03-26T17:59:18Z","published":"2025-01-27T22:48:36Z","title":"PhysAnimator: Physics-Guided Generative Cartoon Animation","summary":"  Creating hand-drawn animation sequences is labor-intensive and demands\nprofessional expertise. We introduce PhysAnimator, a novel approach for\ngenerating physically plausible meanwhile anime-stylized animation from static\nanime illustrations. Our method seamlessly integrates physics-based simulations\nwith data-driven generative models to produce dynamic and visually compelling\nanimations. To capture the fluidity and exaggeration characteristic of anime,\nwe perform image-space deformable body simulations on extracted mesh\ngeometries. We enhance artistic control by introducing customizable energy\nstrokes and incorporating rigging point support, enabling the creation of\ntailored animation effects such as wind interactions. Finally, we extract and\nwarp sketches from the simulation sequence, generating a texture-agnostic\nrepresentation, and employ a sketch-guided video diffusion model to synthesize\nhigh-quality animation frames. The resulting animations exhibit temporal\nconsistency and visual plausibility, demonstrating the effectiveness of our\nmethod in creating dynamic anime-style animations. See our project page for\nmore demos: https://xpandora.github.io/PhysAnimator/\n","authors":["Tianyi Xie","Yiwei Zhao","Ying Jiang","Chenfanfu Jiang"],"pdf_url":"https://arxiv.org/pdf/2501.16550v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.20782v1","updated":"2025-03-26T17:59:04Z","published":"2025-03-26T17:59:04Z","title":"Zero-Shot Audio-Visual Editing via Cross-Modal Delta Denoising","summary":"  In this paper, we introduce zero-shot audio-video editing, a novel task that\nrequires transforming original audio-visual content to align with a specified\ntextual prompt without additional model training. To evaluate this task, we\ncurate a benchmark dataset, AvED-Bench, designed explicitly for zero-shot\naudio-video editing. AvED-Bench includes 110 videos, each with a 10-second\nduration, spanning 11 categories from VGGSound. It offers diverse prompts and\nscenarios that require precise alignment between auditory and visual elements,\nenabling robust evaluation. We identify limitations in existing zero-shot audio\nand video editing methods, particularly in synchronization and coherence\nbetween modalities, which often result in inconsistent outcomes. To address\nthese challenges, we propose AvED, a zero-shot cross-modal delta denoising\nframework that leverages audio-video interactions to achieve synchronized and\ncoherent edits. AvED demonstrates superior results on both AvED-Bench and the\nrecent OAVE dataset to validate its generalization capabilities. Results are\navailable at https://genjib.github.io/project_page/AVED/index.html\n","authors":["Yan-Bo Lin","Kevin Lin","Zhengyuan Yang","Linjie Li","Jianfeng Wang","Chung-Ching Lin","Xiaofei Wang","Gedas Bertasius","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2503.20782v1.pdf","comment":"Project page: https://genjib.github.io/project_page/AVED/index.html"},{"id":"http://arxiv.org/abs/2503.20781v1","updated":"2025-03-26T17:59:02Z","published":"2025-03-26T17:59:02Z","title":"BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation","summary":"  We present BASKET, a large-scale basketball video dataset for fine-grained\nskill estimation. BASKET contains 4,477 hours of video capturing 32,232\nbasketball players from all over the world. Compared to prior skill estimation\ndatasets, our dataset includes a massive number of skilled participants with\nunprecedented diversity in terms of gender, age, skill level, geographical\nlocation, etc. BASKET includes 20 fine-grained basketball skills, challenging\nmodern video recognition models to capture the intricate nuances of player\nskill through in-depth video analysis. Given a long highlight video (8-10\nminutes) of a particular player, the model needs to predict the skill level\n(e.g., excellent, good, average, fair, poor) for each of the 20 basketball\nskills. Our empirical analysis reveals that the current state-of-the-art video\nmodels struggle with this task, significantly lagging behind the human\nbaseline. We believe that BASKET could be a useful resource for developing new\nvideo models with advanced long-range, fine-grained recognition capabilities.\nIn addition, we hope that our dataset will be useful for domain-specific\napplications such as fair basketball scouting, personalized player development,\nand many others. Dataset and code are available at\nhttps://github.com/yulupan00/BASKET.\n","authors":["Yulu Pan","Ce Zhang","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2503.20781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20776v1","updated":"2025-03-26T17:56:16Z","published":"2025-03-26T17:56:16Z","title":"Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields","summary":"  Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction.\n","authors":["Shijie Zhou","Hui Ren","Yijia Weng","Shuwang Zhang","Zhen Wang","Dejia Xu","Zhiwen Fan","Suya You","Zhangyang Wang","Leonidas Guibas","Achuta Kadambi"],"pdf_url":"https://arxiv.org/pdf/2503.20776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03734v3","updated":"2025-03-26T17:55:06Z","published":"2025-03-05T18:44:48Z","title":"OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature\n  Extraction","summary":"  Vision-Language-Action (VLA) models aim to predict robotic actions based on\nvisual observations and language instructions. Existing approaches require\nfine-tuning pre-trained visionlanguage models (VLMs) as visual and language\nfeatures are independently fed into downstream policies, degrading the\npre-trained semantic alignments. We propose OTTER, a novel VLA architecture\nthat leverages these existing alignments through explicit, text-aware visual\nfeature extraction. Instead of processing all visual features, OTTER\nselectively extracts and passes only task-relevant visual features that are\nsemantically aligned with the language instruction to the policy transformer.\nThis allows OTTER to keep the pre-trained vision-language encoders frozen.\nThereby, OTTER preserves and utilizes the rich semantic understanding learned\nfrom large-scale pre-training, enabling strong zero-shot generalization\ncapabilities. In simulation and real-world experiments, OTTER significantly\noutperforms existing VLA models, demonstrating strong zeroshot generalization\nto novel objects and environments. Video, code, checkpoints, and dataset:\nhttps://ottervla.github.io/.\n","authors":["Huang Huang","Fangchen Liu","Letian Fu","Tingfan Wu","Mustafa Mukadam","Jitendra Malik","Ken Goldberg","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2503.03734v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20771v1","updated":"2025-03-26T17:53:53Z","published":"2025-03-26T17:53:53Z","title":"Disentangled Source-Free Personalization for Facial Expression\n  Recognition with Neutral Target Data","summary":"  Facial Expression Recognition (FER) from videos is a crucial task in various\napplication areas, such as human-computer interaction and health monitoring\n(e.g., pain, depression, fatigue, and stress). Beyond the challenges of\nrecognizing subtle emotional or health states, the effectiveness of deep FER\nmodels is often hindered by the considerable variability of expressions among\nsubjects. Source-free domain adaptation (SFDA) methods are employed to adapt a\npre-trained source model using only unlabeled target domain data, thereby\navoiding data privacy and storage issues. Typically, SFDA methods adapt to a\ntarget domain dataset corresponding to an entire population and assume it\nincludes data from all recognition classes. However, collecting such\ncomprehensive target data can be difficult or even impossible for FER in\nhealthcare applications. In many real-world scenarios, it may be feasible to\ncollect a short neutral control video (displaying only neutral expressions) for\ntarget subjects before deployment. These videos can be used to adapt a model to\nbetter handle the variability of expressions among subjects. This paper\nintroduces the Disentangled Source-Free Domain Adaptation (DSFDA) method to\naddress the SFDA challenge posed by missing target expression data. DSFDA\nleverages data from a neutral target control video for end-to-end generation\nand adaptation of target data with missing non-neutral data. Our method learns\nto disentangle features related to expressions and identity while generating\nthe missing non-neutral target data, thereby enhancing model accuracy.\nAdditionally, our self-supervision strategy improves model adaptation by\nreconstructing target images that maintain the same identity and source\nexpression.\n","authors":["Masoumeh Sharafi","Emma Ollivier","Muhammad Osama Zeeshan","Soufiane Belharbi","Marco Pedersoli","Alessandro Lameiras Koerich","Simon Bacon"," Eric~Granger"],"pdf_url":"https://arxiv.org/pdf/2503.20771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20756v1","updated":"2025-03-26T17:45:29Z","published":"2025-03-26T17:45:29Z","title":"ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems","summary":"  Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Chenxi Wang","Jizhan Fang","Xiang Chen","Bozhong Tian","Ziwen Xu","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20756v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.17122v2","updated":"2025-03-26T17:38:07Z","published":"2025-03-21T13:17:28Z","title":"R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User\n  Focused Roadside Perception","summary":"  In autonomous driving, the integration of roadside perception systems is\nessential for overcoming occlusion challenges and enhancing the safety of\nVulnerable Road Users (VRUs). While LiDAR and visual (RGB) sensors are commonly\nused, thermal imaging remains underrepresented in datasets, despite its\nacknowledged advantages for VRU detection in extreme lighting conditions. In\nthis paper, we present R-LiViT, the first dataset to combine LiDAR, RGB, and\nthermal imaging from a roadside perspective, with a strong focus on VRUs.\nR-LiViT captures three intersections during both day and night, ensuring a\ndiverse dataset. It includes 10,000 LiDAR frames and 2,400 temporally and\nspatially aligned RGB and thermal images across over 150 traffic scenarios,\nwith 6 and 8 annotated classes respectively, providing a comprehensive resource\nfor tasks such as object detection and tracking. The dataset and the code for\nreproducing our evaluation results are made publicly available.\n","authors":["Jonas Mirlach","Lei Wan","Andreas Wiedholz","Hannan Ejaz Keen","Andreas Eich"],"pdf_url":"https://arxiv.org/pdf/2503.17122v2.pdf","comment":"10 pages, 7 figures, submitted to ICCV2025"},{"id":"http://arxiv.org/abs/2503.20752v1","updated":"2025-03-26T17:38:06Z","published":"2025-03-26T17:38:06Z","title":"Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning","summary":"  Visual reasoning abilities play a crucial role in understanding complex\nmultimodal data, advancing both domain-specific applications and artificial\ngeneral intelligence (AGI). Existing methods improve VLM reasoning via\nChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated\ntraining data to enhance visual reasoning capabilities. However, this training\nparadigm may lead to overfitting and cognitive rigidity, restricting the\nmodel's ability to transfer visual reasoning skills across domains and limiting\nits real-world applicability. To address these limitations, we propose\nReason-RFT, a novel reinforcement fine-tuning framework that significantly\nenhances generalization capabilities in visual reasoning tasks. Reason-RFT\nintroduces a two-phase training framework for visual reasoning: (1) Supervised\nFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the\nreasoning potential of Vision-Language Models (VLMs), followed by (2) Group\nRelative Policy Optimization (GRPO)-based reinforcement learning that generates\nmultiple reasoning-response pairs, significantly enhancing generalization in\nvisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,\nwe reconstructed a comprehensive dataset spanning visual counting, structure\nperception, and spatial transformation.cExperimental results demonstrate\nReasoning-RFT's three key advantages: (1) Performance Enhancement: achieving\nstate-of-the-art results across multiple tasks, outperforming most mainstream\nopen-source and proprietary models; (2) Generalization Superiority:\nconsistently maintaining robust performance across diverse tasks and domains,\noutperforming alternative training paradigms; (3) Data Efficiency: excelling in\nfew-shot learning scenarios while surpassing full-dataset SFT baselines.\n","authors":["Huajie Tan","Yuheng Ji","Xiaoshuai Hao","Minglan Lin","Pengwei Wang","Zhongyuan Wang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20752v1.pdf","comment":"35 pages, 22 figures"},{"id":"http://arxiv.org/abs/2503.20748v1","updated":"2025-03-26T17:33:23Z","published":"2025-03-26T17:33:23Z","title":"UniSTD: Towards Unified Spatio-Temporal Learning across Diverse\n  Disciplines","summary":"  Traditional spatiotemporal models generally rely on task-specific\narchitectures, which limit their generalizability and scalability across\ndiverse tasks due to domain-specific design requirements. In this paper, we\nintroduce \\textbf{UniSTD}, a unified Transformer-based framework for\nspatiotemporal modeling, which is inspired by advances in recent foundation\nmodels with the two-stage pretraining-then-adaption paradigm. Specifically, our\nwork demonstrates that task-agnostic pretraining on 2D vision and vision-text\ndatasets can build a generalizable model foundation for spatiotemporal\nlearning, followed by specialized joint training on spatiotemporal datasets to\nenhance task-specific adaptability. To improve the learning capabilities across\ndomains, our framework employs a rank-adaptive mixture-of-expert adaptation by\nusing fractional interpolation to relax the discrete variables so that can be\noptimized in the continuous space. Additionally, we introduce a temporal module\nto incorporate temporal dynamics explicitly. We evaluate our approach on a\nlarge-scale dataset covering 10 tasks across 4 disciplines, demonstrating that\na unified spatiotemporal model can achieve scalable, cross-task learning and\nsupport up to 10 tasks simultaneously within one model while reducing training\ncosts in multi-domain applications. Code will be available at\nhttps://github.com/1hunters/UniSTD.\n","authors":["Chen Tang","Xinzhu Ma","Encheng Su","Xiufeng Song","Xiaohong Liu","Wei-Hong Li","Lei Bai","Wanli Ouyang","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2503.20748v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.20746v1","updated":"2025-03-26T17:31:04Z","published":"2025-03-26T17:31:04Z","title":"PhysGen3D: Crafting a Miniature Interactive World from a Single Image","summary":"  Envisioning physically plausible outcomes from a single image requires a deep\nunderstanding of the world's dynamics. To address this, we introduce PhysGen3D,\na novel framework that transforms a single image into an amodal,\ncamera-centric, interactive 3D scene. By combining advanced image-based\ngeometric and semantic understanding with physics-based simulation, PhysGen3D\ncreates an interactive 3D world from a static image, enabling us to \"imagine\"\nand simulate future scenarios based on user input. At its core, PhysGen3D\nestimates 3D shapes, poses, physical and lighting properties of objects,\nthereby capturing essential physical attributes that drive realistic object\ninteractions. This framework allows users to specify precise initial\nconditions, such as object speed or material properties, for enhanced control\nover generated video outcomes. We evaluate PhysGen3D's performance against\nclosed-source state-of-the-art (SOTA) image-to-video models, including Pika,\nKling, and Gen-3, showing PhysGen3D's capacity to generate videos with\nrealistic physics while offering greater flexibility and fine-grained control.\nOur results show that PhysGen3D achieves a unique balance of photorealism,\nphysical plausibility, and user-driven interactivity, opening new possibilities\nfor generating dynamic, physics-grounded video from an image.\n","authors":["Boyuan Chen","Hanxiao Jiang","Shaowei Liu","Saurabh Gupta","Yunzhu Li","Hao Zhao","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.20746v1.pdf","comment":"CVPR 2025, Project page: https://by-luckk.github.io/PhysGen3D"},{"id":"http://arxiv.org/abs/2503.20745v1","updated":"2025-03-26T17:30:41Z","published":"2025-03-26T17:30:41Z","title":"MATHGLANCE: Multimodal Large Language Models Do Not Know Where to Look\n  in Mathematical Diagrams","summary":"  Diagrams serve as a fundamental form of visual language, representing complex\nconcepts and their inter-relationships through structured symbols, shapes, and\nspatial arrangements. Unlike natural images, their inherently symbolic and\nabstract nature poses significant challenges for Multimodal Large Language\nModels (MLLMs). However, current benchmarks conflate perceptual and reasoning\ntasks, making it difficult to assess whether MLLMs genuinely understand\nmathematical diagrams beyond superficial pattern recognition. To address this\ngap, we introduce MATHGLANCE, a benchmark specifically designed to isolate and\nevaluate mathematical perception in MLLMs. MATHGLANCE comprises 1.2K images and\n1.6K carefully curated questions spanning four perception tasks: shape\nclassification, object counting, relationship identification, and object\ngrounding, covering diverse domains including plane geometry, solid geometry,\nand graphical representations. Our evaluation of MLLMs reveals that their\nability to understand diagrams is notably limited, particularly in fine-grained\ngrounding tasks. In response, we construct GeoPeP, a perception-oriented\ndataset of 200K structured geometry image-text pairs explicitly annotated with\ngeometric primitives and precise spatial relationships. Training MLLM on GeoPeP\nleads to significant gains in perceptual accuracy, which in turn substantially\nimproves mathematical reasoning. Our benchmark and dataset establish critical\nstandards for evaluating and advancing multimodal mathematical understanding,\nproviding valuable resources and insights to foster future MLLM research.\n","authors":["Yanpeng Sun","Shan Zhang","Wei Tang","Aotian Chen","Piotr Koniusz","Kai Zou","Yuan Xue","Anton van den Hengel"],"pdf_url":"https://arxiv.org/pdf/2503.20745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20744v1","updated":"2025-03-26T17:29:08Z","published":"2025-03-26T17:29:08Z","title":"High Quality Diffusion Distillation on a Single GPU with Relative and\n  Absolute Position Matching","summary":"  We introduce relative and absolute position matching (RAPM), a diffusion\ndistillation method resulting in high quality generation that can be trained\nefficiently on a single GPU. Recent diffusion distillation research has\nachieved excellent results for high-resolution text-to-image generation with\nmethods such as phased consistency models (PCM) and improved distribution\nmatching distillation (DMD2). However, these methods generally require many\nGPUs (e.g.~8-64) and significant batchsizes (e.g.~128-2048) during training,\nresulting in memory and compute requirements that are beyond the resources of\nsome researchers. RAPM provides effective single-GPU diffusion distillation\ntraining with a batchsize of 1. The new method attempts to mimic the sampling\ntrajectories of the teacher model by matching the relative and absolute\npositions. The design of relative positions is inspired by PCM. Two\ndiscriminators are introduced accordingly in RAPM, one for matching relative\npositions and the other for absolute positions. Experimental results on\nStableDiffusion (SD) V1.5 and SDXL indicate that RAPM with 4 timesteps produces\ncomparable FID scores as the best method with 1 timestep under very limited\ncomputational resources.\n","authors":["Guoqiang Zhang","Kenta Niwa","J. P. Lewis","Cedric Mesnage","W. Bastiaan Kleijn"],"pdf_url":"https://arxiv.org/pdf/2503.20744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20739v1","updated":"2025-03-26T17:22:06Z","published":"2025-03-26T17:22:06Z","title":"Emotion Detection and Music Recommendation System","summary":"  As artificial intelligence becomes more and more ingrained in daily life, we\npresent a novel system that uses deep learning for music recommendation and\nemotion-based detection. Through the use of facial recognition and the DeepFace\nframework, our method analyses human emotions in real-time and then plays music\nthat reflects the mood it has discovered. The system uses a webcam to take\npictures, analyses the most common facial expression, and then pulls a playlist\nfrom local storage that corresponds to the mood it has detected. An engaging\nand customised experience is ensured by allowing users to manually change the\nsong selection via a dropdown menu or navigation buttons. By continuously\nlooping over the playlist, the technology guarantees continuity. The objective\nof our system is to improve emotional well-being through music therapy by\noffering a responsive and automated music-selection experience.\n","authors":["Swetha Kambham","Hubert Jhonson","Sai Prathap Reddy Kambham"],"pdf_url":"https://arxiv.org/pdf/2503.20739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20734v1","updated":"2025-03-26T17:15:43Z","published":"2025-03-26T17:15:43Z","title":"SChanger: Change Detection from a Semantic Change and Spatial\n  Consistency Perspective","summary":"  Change detection is a key task in Earth observation applications. Recently,\ndeep learning methods have demonstrated strong performance and widespread\napplication. However, change detection faces data scarcity due to the\nlabor-intensive process of accurately aligning remote sensing images of the\nsame area, which limits the performance of deep learning algorithms. To address\nthe data scarcity issue, we develop a fine-tuning strategy called the Semantic\nChange Network (SCN). We initially pre-train the model on single-temporal\nsupervised tasks to acquire prior knowledge of instance feature extraction. The\nmodel then employs a shared-weight Siamese architecture and extended Temporal\nFusion Module (TFM) to preserve this prior knowledge and is fine-tuned on\nchange detection tasks. The learned semantics for identifying all instances is\nchanged to focus on identifying only the changes. Meanwhile, we observe that\nthe locations of changes between the two images are spatially identical, a\nconcept we refer to as spatial consistency. We introduce this inductive bias\nthrough an attention map that is generated by large-kernel convolutions and\napplied to the features from both time points. This enhances the modeling of\nmulti-scale changes and helps capture underlying relationships in change\ndetection semantics. We develop a binary change detection model utilizing these\ntwo strategies. The model is validated against state-of-the-art methods on six\ndatasets, surpassing all benchmark methods and achieving F1 scores of 92.87%,\n86.43%, 68.95%, 97.62%, 84.58%, and 93.20% on the LEVIR-CD, LEVIR-CD+,\nS2Looking, CDD, SYSU-CD, and WHU-CD datasets, respectively.\n","authors":["Ziyu Zhou","Keyan Hu","Yutian Fang","Xiaoping Rui"],"pdf_url":"https://arxiv.org/pdf/2503.20734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20724v1","updated":"2025-03-26T17:07:24Z","published":"2025-03-26T17:07:24Z","title":"Dynamic Motion Blending for Versatile Motion Editing","summary":"  Text-guided motion editing enables high-level semantic control and iterative\nmodifications beyond traditional keyframe animation. Existing methods rely on\nlimited pre-collected training triplets, which severely hinders their\nversatility in diverse editing scenarios. We introduce MotionCutMix, an online\ndata augmentation technique that dynamically generates training triplets by\nblending body part motions based on input text. While MotionCutMix effectively\nexpands the training distribution, the compositional nature introduces\nincreased randomness and potential body part incoordination. To model such a\nrich distribution, we present MotionReFit, an auto-regressive diffusion model\nwith a motion coordinator. The auto-regressive architecture facilitates\nlearning by decomposing long sequences, while the motion coordinator mitigates\nthe artifacts of motion composition. Our method handles both spatial and\ntemporal motion edits directly from high-level human instructions, without\nrelying on additional specifications or Large Language Models. Through\nextensive experiments, we show that MotionReFit achieves state-of-the-art\nperformance in text-guided motion editing.\n","authors":["Nan Jiang","Hongjie Li","Ziye Yuan","Zimo He","Yixin Chen","Tengyu Liu","Yixin Zhu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2503.20724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20722v1","updated":"2025-03-26T17:03:46Z","published":"2025-03-26T17:03:46Z","title":"A weakly-supervised deep learning model for fast localisation and\n  delineation of the skeleton, internal organs, and spinal canal on Whole-Body\n  Diffusion-Weighted MRI (WB-DWI)","summary":"  Background: Apparent Diffusion Coefficient (ADC) values and Total Diffusion\nVolume (TDV) from Whole-body diffusion-weighted MRI (WB-DWI) are recognized\ncancer imaging biomarkers. However, manual disease delineation for ADC and TDV\nmeasurements is unfeasible in clinical practice, demanding automation. As a\nfirst step, we propose an algorithm to generate fast and reproducible\nprobability maps of the skeleton, adjacent internal organs (liver, spleen,\nurinary bladder, and kidneys), and spinal canal. Methods: We developed an\nautomated deep-learning pipeline based on a 3D patch-based Residual U-Net\narchitecture that localizes and delineates these anatomical structures on\nWB-DWI. The algorithm was trained using \"soft-labels\" (non-binary\nsegmentations) derived from a computationally intensive atlas-based approach.\nFor training and validation, we employed a multi-center WB-DWI dataset\ncomprising 532 scans from patients with Advanced Prostate Cancer (APC) or\nMultiple Myeloma (MM), with testing on 45 patients. Results: Our\nweakly-supervised deep learning model achieved an average dice\nscore/precision/recall of 0.66/0.6/0.73 for skeletal delineations,\n0.8/0.79/0.81 for internal organs, and 0.85/0.79/0.94 for spinal canal, with\nsurface distances consistently below 3 mm. Relative median ADC and\nlog-transformed volume differences between automated and manual expert-defined\nfull-body delineations were below 10% and 4%, respectively. The computational\ntime for generating probability maps was 12x faster than the atlas-based\nregistration algorithm (25 s vs. 5 min). An experienced radiologist rated the\nmodel's accuracy \"good\" or \"excellent\" on test datasets. Conclusion: Our model\noffers fast and reproducible probability maps for localizing and delineating\nbody regions on WB-DWI, enabling ADC and TDV quantification, potentially\nsupporting clinicians in disease staging and treatment response assessment.\n","authors":["A. Candito","A. Dragan","R. Holbrey","A. Ribeiro","R. Donners","C. Messiou","N. Tunariu","D. -M. Koh","M. D. Blackledge","The Institute of Cancer Research"," London","United Kingdom","The Royal Marsden NHS Foundation Trust"," London","United Kingdom","University Hospital Basel"," Basel"," Switzerland"],"pdf_url":"https://arxiv.org/pdf/2503.20722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18562v5","updated":"2025-03-26T16:53:12Z","published":"2024-11-27T18:03:26Z","title":"DexHandDiff: Interaction-aware Diffusion Planning for Adaptive Dexterous\n  Manipulation","summary":"  Dexterous manipulation with contact-rich interactions is crucial for advanced\nrobotics. While recent diffusion-based planning approaches show promise for\nsimple manipulation tasks, they often produce unrealistic ghost states (e.g.,\nthe object automatically moves without hand contact) or lack adaptability when\nhandling complex sequential interactions. In this work, we introduce\nDexHandDiff, an interaction-aware diffusion planning framework for adaptive\ndexterous manipulation. DexHandDiff models joint state-action dynamics through\na dual-phase diffusion process which consists of pre-interaction contact\nalignment and post-contact goal-directed control, enabling goal-adaptive\ngeneralizable dexterous manipulation. Additionally, we incorporate dynamics\nmodel-based dual guidance and leverage large language models for automated\nguidance function generation, enhancing generalizability for physical\ninteractions and facilitating diverse goal adaptation through language cues.\nExperiments on physical interaction tasks such as door opening, pen and block\nre-orientation, object relocation, and hammer striking demonstrate\nDexHandDiff's effectiveness on goals outside training distributions, achieving\nover twice the average success rate (59.2% vs. 29.5%) compared to existing\nmethods. Our framework achieves an average of 70.7% success rate on goal\nadaptive dexterous tasks, highlighting its robustness and flexibility in\ncontact-rich manipulation.\n","authors":["Zhixuan Liang","Yao Mu","Yixiao Wang","Tianxing Chen","Wenqi Shao","Wei Zhan","Masayoshi Tomizuka","Ping Luo","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2411.18562v5.pdf","comment":"Accepted by CVPR 2025. Camera ready version. Previous DexDiffuser.\n  Project page: https://dexdiffuser.github.io/"},{"id":"http://arxiv.org/abs/2503.20711v1","updated":"2025-03-26T16:47:14Z","published":"2025-03-26T16:47:14Z","title":"Demand Estimation with Text and Image Data","summary":"  We propose a demand estimation method that leverages unstructured text and\nimage data to infer substitution patterns. Using pre-trained deep learning\nmodels, we extract embeddings from product images and textual descriptions and\nincorporate them into a random coefficients logit model. This approach enables\nresearchers to estimate demand even when they lack data on product attributes\nor when consumers value hard-to-quantify attributes, such as visual design or\nfunctional benefits. Using data from a choice experiment, we show that our\napproach outperforms standard attribute-based models in counterfactual\npredictions of consumers' second choices. We also apply it across 40 product\ncategories on Amazon.com and consistently find that text and image data help\nidentify close substitutes within each category.\n","authors":["Giovanni Compiani","Ilya Morozov","Stephan Seiler"],"pdf_url":"https://arxiv.org/pdf/2503.20711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10347v3","updated":"2025-03-26T16:44:38Z","published":"2024-05-16T02:00:44Z","title":"Networking Systems for Video Anomaly Detection: A Tutorial and Survey","summary":"  The increasing utilization of surveillance cameras in smart cities, coupled\nwith the surge of online video applications, has heightened concerns regarding\npublic security and privacy protection, which propelled automated Video Anomaly\nDetection (VAD) into a fundamental research task within the Artificial\nIntelligence (AI) community. With the advancements in deep learning and edge\ncomputing, VAD has made significant progress and advances synergized with\nemerging applications in smart cities and video internet, which has moved\nbeyond the conventional research scope of algorithm engineering to deployable\nNetworking Systems for VAD (NSVAD), a practical hotspot for intersection\nexploration in the AI, IoVT, and computing fields. In this article, we\ndelineate the foundational assumptions, learning frameworks, and applicable\nscenarios of various deep learning-driven VAD routes, offering an exhaustive\ntutorial for novices in NSVAD. In addition, this article elucidates core\nconcepts by reviewing recent advances and typical solutions and aggregating\navailable research resources accessible at https://github.com/fdjingliu/NSVAD.\nLastly, this article projects future development trends and discusses how the\nintegration of AI and computing technologies can address existing research\nchallenges and promote open opportunities, serving as an insightful guide for\nprospective researchers and engineers.\n","authors":["Jing Liu","Yang Liu","Jieyu Lin","Jielin Li","Liang Cao","Peng Sun","Bo Hu","Liang Song","Azzedine Boukerche","Victor C. M. Leung"],"pdf_url":"https://arxiv.org/pdf/2405.10347v3.pdf","comment":"Revised to ACM Computing Surveys, under review, for more information\n  and supplementary material, please see https://github.com/fdjingliu/NSVAD"},{"id":"http://arxiv.org/abs/2503.20698v1","updated":"2025-03-26T16:28:04Z","published":"2025-03-26T16:28:04Z","title":"MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion","summary":"  Videos inherently contain multiple modalities, including visual events, text\noverlays, sounds, and speech, all of which are important for retrieval.\nHowever, state-of-the-art multimodal language models like VAST and LanguageBind\nare built on vision-language models (VLMs), and thus overly prioritize visual\nsignals. Retrieval benchmarks further reinforce this bias by focusing on visual\nqueries and neglecting other modalities. We create a search system MMMORRF that\nextracts text and features from both visual and audio modalities and integrates\nthem with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is\nboth effective and efficient, demonstrating practicality in searching videos\nbased on users' information needs instead of visual descriptive queries. We\nevaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed\nfor more targeted information needs, and find that it improves nDCG@20 by 81%\nover leading multimodal encoders and 37% over single-modality retrieval,\ndemonstrating the value of integrating diverse modalities.\n","authors":["Saron Samuel","Dan DeGenaro","Jimena Guallar-Blasco","Kate Sanders","Oluwaseun Eisape","Arun Reddy","Alexander Martin","Andrew Yates","Eugene Yang","Cameron Carpenter","David Etter","Efsun Kayi","Matthew Wiesner","Kenton Murray","Reno Kriz"],"pdf_url":"https://arxiv.org/pdf/2503.20698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06218v2","updated":"2025-03-26T16:23:33Z","published":"2024-06-10T12:33:47Z","title":"Data Augmentation in Earth Observation: A Diffusion Model Approach","summary":"  High-quality Earth Observation (EO) imagery is essential for accurate\nanalysis and informed decision making across sectors. However, data scarcity\ncaused by atmospheric conditions, seasonal variations, and limited geographical\ncoverage hinders the effective application of Artificial Intelligence (AI) in\nEO. Traditional data augmentation techniques, which rely on basic parameterized\nimage transformations, often fail to introduce sufficient diversity across key\nsemantic axes. These axes include natural changes such as snow and floods,\nhuman impacts like urbanization and roads, and disasters such as wildfires and\nstorms, which limits the accuracy of AI models in EO applications. To address\nthis, we propose a four-stage data augmentation approach that integrates\ndiffusion models to enhance semantic diversity. Our method employs meta-prompts\nfor instruction generation, vision-language models for rich captioning,\nEO-specific diffusion model fine-tuning, and iterative data augmentation.\nExtensive experiments using four augmentation techniques demonstrate that our\napproach consistently outperforms established methods, generating semantically\ndiverse EO images and improving AI model performance.\n","authors":["Tiago Sousa","Benoît Ries","Nicolas Guelfi"],"pdf_url":"https://arxiv.org/pdf/2406.06218v2.pdf","comment":"25 pages, 12 figures"},{"id":"http://arxiv.org/abs/2405.14239v2","updated":"2025-03-26T16:23:16Z","published":"2024-05-23T07:18:08Z","title":"Harmony: A Joint Self-Supervised and Weakly-Supervised Framework for\n  Learning General Purpose Visual Representations","summary":"  Vision-language contrastive learning frameworks like CLIP enable learning\nrepresentations from natural language supervision, and provide strong zero-shot\nclassification capabilities. However, due to the nature of the supervisory\nsignal in these paradigms, they lack the ability to learn localized features,\nleading to degraded performance on dense prediction tasks like segmentation and\ndetection. On the other hand, self-supervised learning methods have shown the\nability to learn granular representations, complementing the high-level\nfeatures in vision-language training. In this work, we present Harmony, a\nframework that combines vision-language training with discriminative and\ngenerative self-supervision to learn visual features that can be generalized\nacross different vision downstream tasks. Our framework is specifically\ndesigned to work on web-scraped data by not relying on negative examples and\naddressing the one-to-one correspondence issue using soft CLIP targets\ngenerated by an EMA model. We comprehensively evaluate Harmony across various\nvision downstream tasks and find that it significantly outperforms the baseline\nCLIP and the previously leading joint self and weakly-supervised methods,\nMaskCLIP and SLIP. Specifically, when comparing against these methods, Harmony\nshows superior performance in fine-tuning and zero-shot classification on\nImageNet-1k, semantic segmentation on ADE20K, and both object detection and\ninstance segmentation on MS-COCO, when pre-training a ViT-B on CC3M. We also\nshow that Harmony outperforms other self-supervised learning methods like iBOT\nand MAE across all tasks evaluated. Our code is publicly at\nhttps://github.com/MohammedSB/Harmony}{https://github.com/MohammedSB/Harmony\navailable.\n","authors":["Mohammed Baharoon","Jonathan Klein","Dominik L. Michels"],"pdf_url":"https://arxiv.org/pdf/2405.14239v2.pdf","comment":"22 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.20685v1","updated":"2025-03-26T16:20:02Z","published":"2025-03-26T16:20:02Z","title":"Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast\n  Ultrasound","summary":"  Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D\nautomated breast ultrasound (ABUS) is crucial for clinical diagnosis and\ntreatment planning. Therefore, developing an automated system for nodule\nsegmentation can enhance user independence and expedite clinical analysis.\nUnlike fully-supervised learning, weakly-supervised segmentation (WSS) can\nstreamline the laborious and intricate annotation process. However, current WSS\nmethods face challenges in achieving precise nodule segmentation, as many of\nthem depend on inaccurate activation maps or inefficient pseudo-mask generation\nalgorithms. In this study, we introduce a novel multi-agent reinforcement\nlearning-based WSS framework called Flip Learning, which relies solely on 2D/3D\nboxes for accurate segmentation. Specifically, multiple agents are employed to\nerase the target from the box to facilitate classification tag flipping, with\nthe erased region serving as the predicted segmentation mask. The key\ncontributions of this research are as follows: (1) Adoption of a\nsuperpixel/supervoxel-based approach to encode the standardized environment,\ncapturing boundary priors and expediting the learning process. (2) Introduction\nof three meticulously designed rewards, comprising a classification score\nreward and two intensity distribution rewards, to steer the agents' erasing\nprocess precisely, thereby avoiding both under- and over-segmentation. (3)\nImplementation of a progressive curriculum learning strategy to enable agents\nto interact with the environment in a progressively challenging manner, thereby\nenhancing learning efficiency. Extensively validated on the large in-house BUS\nand ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS\nmethods and foundation models, and achieves comparable performance as\nfully-supervised learning algorithms.\n","authors":["Yuhao Huang","Ao Chang","Haoran Dou","Xing Tao","Xinrui Zhou","Yan Cao","Ruobing Huang","Alejandro F Frangi","Lingyun Bao","Xin Yang","Dong Ni"],"pdf_url":"https://arxiv.org/pdf/2503.20685v1.pdf","comment":"Accepted by Medical Image Analysis. 24 pages, 13 figures, 18 tabels"},{"id":"http://arxiv.org/abs/2503.20682v1","updated":"2025-03-26T16:18:25Z","published":"2025-03-26T16:18:25Z","title":"GLRD: Global-Local Collaborative Reason and Debate with PSL for 3D\n  Open-Vocabulary Detection","summary":"  The task of LiDAR-based 3D Open-Vocabulary Detection (3D OVD) requires the\ndetector to learn to detect novel objects from point clouds without\noff-the-shelf training labels. Previous methods focus on the learning of\nobject-level representations and ignore the scene-level information, thus it is\nhard to distinguish objects with similar classes. In this work, we propose a\nGlobal-Local Collaborative Reason and Debate with PSL (GLRD) framework for the\n3D OVD task, considering both local object-level information and global\nscene-level information. Specifically, LLM is utilized to perform common sense\nreasoning based on object-level and scene-level information, where the\ndetection result is refined accordingly. To further boost the LLM's ability of\nprecise decisions, we also design a probabilistic soft logic solver (OV-PSL) to\nsearch for the optimal solution, and a debate scheme to confirm the class of\nconfusable objects. In addition, to alleviate the uneven distribution of\nclasses, a static balance scheme (SBC) and a dynamic balance scheme (DBC) are\ndesigned. In addition, to reduce the influence of noise in data and training,\nwe further propose Reflected Pseudo Labels Generation (RPLG) and\nBackground-Aware Object Localization (BAOL). Extensive experiments conducted on\nScanNet and SUN RGB-D demonstrate the superiority of GLRD, where absolute\nimprovements in mean average precision are $+2.82\\%$ on SUN RGB-D and $+3.72\\%$\non ScanNet in the partial open-vocabulary setting. In the full open-vocabulary\nsetting, the absolute improvements in mean average precision are $+4.03\\%$ on\nScanNet and $+14.11\\%$ on SUN RGB-D.\n","authors":["Xingyu Peng","Si Liu","Chen Gao","Yan Bai","Beipeng Mu","Xiaofei Wang","Huaxia Xia"],"pdf_url":"https://arxiv.org/pdf/2503.20682v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2503.20681v1","updated":"2025-03-26T16:17:22Z","published":"2025-03-26T16:17:22Z","title":"Benchmarking Machine Learning Methods for Distributed Acoustic Sensing","summary":"  Distributed acoustic sensing (DAS) technology represents an innovative\nfiber-optic-based sensing methodology that enables real-time acoustic signal\nmonitoring through the detection of minute perturbations along optical fibers.\nThis sensing approach offers compelling advantages, including extensive\nmeasurement ranges, exceptional spatial resolution, and an expansive dynamic\nmeasurement spectrum.\n  The integration of machine learning (ML) paradigms presents transformative\npotential for DAS technology, encompassing critical domains such as data\naugmentation, sophisticated preprocessing techniques, and advanced acoustic\nevent classification and recognition. By leveraging ML algorithms, DAS systems\ncan transition from traditional data processing methodologies to more automated\nand intelligent analytical frameworks.\n  The computational intelligence afforded by ML-enhanced DAS technologies\nfacilitates unprecedented monitoring capabilities across diverse critical\ninfrastructure sectors. Particularly noteworthy are the technology's\napplications in transportation infrastructure, energy management systems, and\nNatural disaster monitoring frameworks, where the precision of data acquisition\nand the reliability of intelligent decision-making mechanisms are paramount.\n  This research critically examines the comparative performance characteristics\nof classical machine learning methodologies and state-of-the-art deep learning\nmodels in the context of DAS data recognition and interpretation, offering\ncomprehensive insights into the evolving landscape of intelligent sensing\ntechnologies.\n","authors":["Shuaikai Shi","Qijun Zong"],"pdf_url":"https://arxiv.org/pdf/2503.20681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12843v3","updated":"2025-03-26T16:15:55Z","published":"2025-03-17T05:42:19Z","title":"Towards Scalable Foundation Model for Multi-modal and Hyperspectral\n  Geospatial Data","summary":"  Geospatial raster data, such as that collected by satellite-based imaging\nsystems at different times and spectral bands, hold immense potential for\nenabling a wide range of high-impact applications. This potential stems from\nthe rich information that is spatially and temporally contextualized across\nmultiple channels and sensing modalities. Recent work has adapted existing\nself-supervised learning approaches for such geospatial data. However, they\nfall short of scalable model architectures, leading to inflexibility and\ncomputational inefficiencies when faced with an increasing number of channels\nand modalities. To address these limitations, we introduce Low-rank Efficient\nSpatial-Spectral Vision Transformer with three key innovations: i) the LESS\nAttention Block that approximates high-dimensional spatial-spectral attention\nthrough Kronecker's product of the low-dimensional spatial and spectral\nattention components; ii) the Continuous Positional-Channel Embedding Layer\nthat preserves both the continuity and physical characteristics of each\nspatial-spectral patch; and iii) the Perception Field Mask that exploits local\nspatial dependencies by constraining attention to neighboring patches. To\nevaluate the proposed innovations, we construct GFM-Bench, which serves as a\ncomprehensive benchmark for such geospatial raster data. We pretrain LESS ViT\nusing a Hyperspectral Masked Autoencoder framework with integrated positional\nand channel masking strategies. Experimental results demonstrate that our\nproposed method achieves competitive performance against state-of-the-art\nmulti-modal geospatial foundation models while outperforming them on\ncross-satellite generalization tasks with higher computational efficiency. The\nflexibility and extensibility of our framework make it a promising direction\nfor future geospatial data analysis tasks that involve a wide range of\nmodalities and channels.\n","authors":["Haozhe Si","Yuxuan Wan","Minh Do","Deepak Vasisht","Han Zhao","Hendrik F. Hamann"],"pdf_url":"https://arxiv.org/pdf/2503.12843v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20680v1","updated":"2025-03-26T16:15:42Z","published":"2025-03-26T16:15:42Z","title":"Vision as LoRA","summary":"  We introduce Vision as LoRA (VoRA), a novel paradigm for transforming an LLM\ninto an MLLM. Unlike prevalent MLLM architectures that rely on external vision\nmodules for vision encoding, VoRA internalizes visual capabilities by\nintegrating vision-specific LoRA layers directly into the LLM. This design\nallows the added parameters to be seamlessly merged into the LLM during\ninference, eliminating structural complexity and minimizing computational\noverhead. Moreover, inheriting the LLM's ability of handling flexible context,\nVoRA can process inputs at arbitrary resolutions.\n  To further strengthen VoRA's visual capabilities, we introduce a block-wise\ndistillation method that transfers visual priors from a pre-trained ViT into\nthe LoRA layers, effectively accelerating training by injecting visual\nknowledge. Additionally, we apply bi-directional attention masks to better\ncapture the context information of an image. We successfully demonstrate that\nwith additional pre-training data, VoRA can perform comparably with\nconventional encode-based MLLMs. All training data, codes, and model weights\nwill be released at https://github.com/Hon-Wong/VoRA.\n","authors":["Han Wang","Yongjie Ye","Bingru Li","Yuxiang Nie","Jinghui Lu","Jingqun Tang","Yanjie Wang","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2503.20680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01814v2","updated":"2025-03-26T16:07:40Z","published":"2024-12-02T18:56:06Z","title":"COSMOS: Cross-Modality Self-Distillation for Vision Language\n  Pre-training","summary":"  Vision-Language Models (VLMs) trained with contrastive loss have achieved\nsignificant advancements in various vision and language tasks. However, the\nglobal nature of the contrastive loss makes VLMs focus predominantly on\nforeground objects, neglecting other crucial information in the image, which\nlimits their effectiveness in downstream tasks. To address these challenges, we\npropose COSMOS: CrOSs-MOdality Self-distillation for vision-language\npre-training that integrates a novel text-cropping strategy and cross-attention\nmodule into a self-supervised learning framework. We create global and local\nviews of images and texts (i.e., multi-modal augmentations), which are\nessential for self-distillation in VLMs. We further introduce a cross-attention\nmodule, enabling COSMOS to learn comprehensive cross-modal representations\noptimized via a cross-modality self-distillation loss. COSMOS consistently\noutperforms previous strong baselines on various zero-shot downstream tasks,\nincluding retrieval, classification, and semantic segmentation. Additionally,\nit surpasses CLIP-based models trained on larger datasets in visual perception\nand contextual understanding tasks. Code is available at\nhttps://github.com/ExplainableML/cosmos.\n","authors":["Sanghwan Kim","Rui Xiao","Mariana-Iuliana Georgescu","Stephan Alaniz","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2412.01814v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.20673v1","updated":"2025-03-26T16:05:01Z","published":"2025-03-26T16:05:01Z","title":"Mitigating Low-Level Visual Hallucinations Requires Self-Awareness:\n  Database, Model and Training Strategy","summary":"  The rapid development of multimodal large language models has resulted in\nremarkable advancements in visual perception and understanding, consolidating\nseveral tasks into a single visual question-answering framework. However, these\nmodels are prone to hallucinations, which limit their reliability as artificial\nintelligence systems. While this issue is extensively researched in natural\nlanguage processing and image captioning, there remains a lack of investigation\nof hallucinations in Low-level Visual Perception and Understanding (HLPU),\nespecially in the context of image quality assessment tasks. We consider that\nthese hallucinations arise from an absence of clear self-awareness within the\nmodels. To address this issue, we first introduce the HLPU instruction\ndatabase, the first instruction database specifically focused on hallucinations\nin low-level vision tasks. This database contains approximately 200K\nquestion-answer pairs and comprises four subsets, each covering different types\nof instructions. Subsequently, we propose the Self-Awareness Failure\nElimination (SAFEQA) model, which utilizes image features, salient region\nfeatures and quality features to improve the perception and comprehension\nabilities of the model in low-level vision tasks. Furthermore, we propose the\nEnhancing Self-Awareness Preference Optimization (ESA-PO) framework to increase\nthe model's awareness of knowledge boundaries, thereby mitigating the incidence\nof hallucination. Finally, we conduct comprehensive experiments on low-level\nvision tasks, with the results demonstrating that our proposed method\nsignificantly enhances self-awareness of the model in these tasks and reduces\nhallucinations. Notably, our proposed method improves both accuracy and\nself-awareness of the proposed model and outperforms close-source models in\nterms of various evaluation metrics.\n","authors":["Yinan Sun","Xiongkuo Min","Zicheng Zhang","Yixuan Gao","Yuqin Cao","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2503.20673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20672v1","updated":"2025-03-26T16:04:57Z","published":"2025-03-26T16:04:57Z","title":"BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation","summary":"  Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation.\n","authors":["Yuyang Peng","Shishi Xiao","Keming Wu","Qisheng Liao","Bohan Chen","Kevin Lin","Danqing Huang","Ji Li","Yuhui Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.20672v1.pdf","comment":"Accepted by CVPR 2025. Project Page: https://bizgen-msra.github.io"},{"id":"http://arxiv.org/abs/2109.06098v2","updated":"2025-03-26T15:57:28Z","published":"2021-09-13T16:19:25Z","title":"The mathematics of adversarial attacks in AI -- Why deep learning is\n  unstable despite the existence of stable neural networks","summary":"  The unprecedented success of deep learning (DL) makes it unchallenged when it\ncomes to classification problems. However, it is well established that the\ncurrent DL methodology produces universally unstable neural networks (NNs). The\ninstability problem has caused an enormous research effort -- with a vast\nliterature on so-called adversarial attacks -- yet there has been no solution\nto the problem. Our paper addresses why there has been no solution to the\nproblem, as we prove the following mathematical paradox: any training procedure\nbased on training neural networks for classification problems with a fixed\narchitecture will yield neural networks that are either inaccurate or unstable\n(if accurate) -- despite the provable existence of both accurate and stable\nneural networks for the same classification problems. The key is that the\nstable and accurate neural networks must have variable dimensions depending on\nthe input, in particular, variable dimensions is a necessary condition for\nstability.\n  Our result points towards the paradox that accurate and stable neural\nnetworks exist, however, modern algorithms do not compute them. This yields the\nquestion: if the existence of neural networks with desirable properties can be\nproven, can one also find algorithms that compute them? There are cases in\nmathematics where provable existence implies computability, but will this be\nthe case for neural networks? The contrary is true, as we demonstrate how\nneural networks can provably exist as approximate minimisers to standard\noptimisation problems with standard cost functions, however, no randomised\nalgorithm can compute them with probability better than 1/2.\n","authors":["Alexander Bastounis","Anders C Hansen","Verner Vlačić"],"pdf_url":"https://arxiv.org/pdf/2109.06098v2.pdf","comment":"31 pages, 1 figure. Revised to make minor changes to notation and\n  references"},{"id":"http://arxiv.org/abs/2503.20662v1","updated":"2025-03-26T15:56:48Z","published":"2025-03-26T15:56:48Z","title":"AutoRad-Lung: A Radiomic-Guided Prompting Autoregressive Vision-Language\n  Model for Lung Nodule Malignancy Prediction","summary":"  Lung cancer remains one of the leading causes of cancer-related mortality\nworldwide. A crucial challenge for early diagnosis is differentiating uncertain\ncases with similar visual characteristics and closely annotation scores. In\nclinical practice, radiologists rely on quantitative, hand-crafted Radiomic\nfeatures extracted from Computed Tomography (CT) images, while recent research\nhas primarily focused on deep learning solutions. More recently,\nVision-Language Models (VLMs), particularly Contrastive Language-Image\nPre-Training (CLIP)-based models, have gained attention for their ability to\nintegrate textual knowledge into lung cancer diagnosis. While CLIP-Lung models\nhave shown promising results, we identified the following potential\nlimitations: (a) dependence on radiologists' annotated attributes, which are\ninherently subjective and error-prone, (b) use of textual information only\nduring training, limiting direct applicability at inference, and (c)\nConvolutional-based vision encoder with randomly initialized weights, which\ndisregards prior knowledge. To address these limitations, we introduce\nAutoRad-Lung, which couples an autoregressively pre-trained VLM, with prompts\ngenerated from hand-crafted Radiomics. AutoRad-Lung uses the vision encoder of\nthe Large-Scale Autoregressive Image Model (AIMv2), pre-trained using a\nmulti-modal autoregressive objective. Given that lung tumors are typically\nsmall, irregularly shaped, and visually similar to healthy tissue, AutoRad-Lung\noffers significant advantages over its CLIP-based counterparts by capturing\npixel-level differences. Additionally, we introduce conditional context\noptimization, which dynamically generates context-specific prompts based on\ninput Radiomics, improving cross-modal alignment.\n","authors":["Sadaf Khademi","Mehran Shabanpour","Reza Taleei","Anastasia Oikonomou","Arash Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2503.20662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20663v1","updated":"2025-03-26T15:56:48Z","published":"2025-03-26T15:56:48Z","title":"ARMO: Autoregressive Rigging for Multi-Category Objects","summary":"  Recent advancements in large-scale generative models have significantly\nimproved the quality and diversity of 3D shape generation. However, most\nexisting methods focus primarily on generating static 3D models, overlooking\nthe potentially dynamic nature of certain shapes, such as humanoids, animals,\nand insects. To address this gap, we focus on rigging, a fundamental task in\nanimation that establishes skeletal structures and skinning for 3D models. In\nthis paper, we introduce OmniRig, the first large-scale rigging dataset,\ncomprising 79,499 meshes with detailed skeleton and skinning information.\nUnlike traditional benchmarks that rely on predefined standard poses (e.g.,\nA-pose, T-pose), our dataset embraces diverse shape categories, styles, and\nposes. Leveraging this rich dataset, we propose ARMO, a novel rigging framework\nthat utilizes an autoregressive model to predict both joint positions and\nconnectivity relationships in a unified manner. By treating the skeletal\nstructure as a complete graph and discretizing it into tokens, we encode the\njoints using an auto-encoder to obtain a latent embedding and an autoregressive\nmodel to predict the tokens. A mesh-conditioned latent diffusion model is used\nto predict the latent embedding for conditional skeleton generation. Our method\naddresses the limitations of regression-based approaches, which often suffer\nfrom error accumulation and suboptimal connectivity estimation. Through\nextensive experiments on the OmniRig dataset, our approach achieves\nstate-of-the-art performance in skeleton prediction, demonstrating improved\ngeneralization across diverse object categories. The code and dataset will be\nmade public for academic use upon acceptance.\n","authors":["Mingze Sun","Shiwei Mao","Keyi Chen","Yurun Chen","Shunlin Lu","Jingbo Wang","Junting Dong","Ruqi Huang"],"pdf_url":"https://arxiv.org/pdf/2503.20663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20654v1","updated":"2025-03-26T15:50:42Z","published":"2025-03-26T15:50:42Z","title":"AccidentSim: Generating Physically Realistic Vehicle Collision Videos\n  from Real-World Accident Reports","summary":"  Collecting real-world vehicle accident videos for autonomous driving research\nis challenging due to their rarity and complexity. While existing driving video\ngeneration methods may produce visually realistic videos, they often fail to\ndeliver physically realistic simulations because they lack the capability to\ngenerate accurate post-collision trajectories. In this paper, we introduce\nAccidentSim, a novel framework that generates physically realistic vehicle\ncollision videos by extracting and utilizing the physical clues and contextual\ninformation available in real-world vehicle accident reports. Specifically,\nAccidentSim leverages a reliable physical simulator to replicate post-collision\nvehicle trajectories from the physical and contextual information in the\naccident reports and to build a vehicle collision trajectory dataset. This\ndataset is then used to fine-tune a language model, enabling it to respond to\nuser prompts and predict physically consistent post-collision trajectories\nacross various driving scenarios based on user descriptions. Finally, we employ\nNeural Radiance Fields (NeRF) to render high-quality backgrounds, merging them\nwith the foreground vehicles that exhibit physically realistic trajectories to\ngenerate vehicle collision videos. Experimental results demonstrate that the\nvideos produced by AccidentSim excel in both visual and physical authenticity.\n","authors":["Xiangwen Zhang","Qian Zhang","Longfei Han","Qiang Qu","Xiaoming Chen"],"pdf_url":"https://arxiv.org/pdf/2503.20654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20653v1","updated":"2025-03-26T15:48:38Z","published":"2025-03-26T15:48:38Z","title":"UWarp: A Whole Slide Image Registration Pipeline to Characterize\n  Scanner-Induced Local Domain Shift","summary":"  Histopathology slide digitization introduces scanner-induced domain shift\nthat can significantly impact computational pathology models based on deep\nlearning methods. In the state-of-the-art, this shift is often characterized at\na broad scale (slide-level or dataset-level) but not patch-level, which limits\nour comprehension of the impact of localized tissue characteristics on the\naccuracy of the deep learning models. To address this challenge, we present a\ndomain shift analysis framework based on UWarp, a novel registration tool\ndesigned to accurately align histological slides scanned under varying\nconditions. UWarp employs a hierarchical registration approach, combining\nglobal affine transformations with fine-grained local corrections to achieve\nrobust tissue patch alignment. We evaluate UWarp using two private datasets,\nCypathLung and BosomShieldBreast, containing whole slide images scanned by\nmultiple devices. Our experiments demonstrate that UWarp outperforms existing\nopen-source registration methods, achieving a median target registration error\n(TRE) of less than 4 pixels (<1 micrometer at 40x magnification) while\nsignificantly reducing computational time. Additionally, we apply UWarp to\ncharacterize scanner-induced local domain shift in the predictions of\nBreast-NEOprAIdict, a deep learning model for breast cancer pathological\nresponse prediction. We find that prediction variability is strongly correlated\nwith tissue density on a given patch. Our findings highlight the importance of\nlocalized domain shift analysis and suggest that UWarp can serve as a valuable\ntool for improving model robustness and domain adaptation strategies in\ncomputational pathology.\n","authors":["Antoine Schieb","Bilal Hadjadji","Daniel Tshokola Mweze","Natalia Fernanda Valderrama","Valentin Derangère","Laurent Arnould","Sylvain Ladoire","Alain Lalande","Louis-Oscar Morel","Nathan Vinçon"],"pdf_url":"https://arxiv.org/pdf/2503.20653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20652v1","updated":"2025-03-26T15:47:50Z","published":"2025-03-26T15:47:50Z","title":"Imitating Radiological Scrolling: A Global-Local Attention Model for 3D\n  Chest CT Volumes Multi-Label Anomaly Classification","summary":"  The rapid increase in the number of Computed Tomography (CT) scan\nexaminations has created an urgent need for automated tools, such as organ\nsegmentation, anomaly classification, and report generation, to assist\nradiologists with their growing workload. Multi-label classification of\nThree-Dimensional (3D) CT scans is a challenging task due to the volumetric\nnature of the data and the variety of anomalies to be detected. Existing deep\nlearning methods based on Convolutional Neural Networks (CNNs) struggle to\ncapture long-range dependencies effectively, while Vision Transformers require\nextensive pre-training, posing challenges for practical use. Additionally,\nthese existing methods do not explicitly model the radiologist's navigational\nbehavior while scrolling through CT scan slices, which requires both global\ncontext understanding and local detail awareness. In this study, we present\nCT-Scroll, a novel global-local attention model specifically designed to\nemulate the scrolling behavior of radiologists during the analysis of 3D CT\nscans. Our approach is evaluated on two public datasets, demonstrating its\nefficacy through comprehensive experiments and an ablation study that\nhighlights the contribution of each model component.\n","authors":["Theo Di Piazza","Carole Lazarus","Olivier Nempont","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2503.20652v1.pdf","comment":"13 pages, 4 figures, under review for MIDL 2025"},{"id":"http://arxiv.org/abs/2411.11706v3","updated":"2025-03-26T15:44:01Z","published":"2024-11-18T16:33:52Z","title":"MC-LLaVA: Multi-Concept Personalized Vision-Language Model","summary":"  Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA.\n","authors":["Ruichuan An","Sihan Yang","Ming Lu","Renrui Zhang","Kai Zeng","Yulin Luo","Jiajun Cao","Hao Liang","Ying Chen","Qi She","Shanghang Zhang","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.11706v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20644v1","updated":"2025-03-26T15:37:17Z","published":"2025-03-26T15:37:17Z","title":"MMGen: Unified Multi-modal Image Generation and Understanding in One Go","summary":"  A unified diffusion framework for multi-modal generation and understanding\nhas the transformative potential to achieve seamless and controllable image\ndiffusion and other cross-modal tasks. In this paper, we introduce MMGen, a\nunified framework that integrates multiple generative tasks into a single\ndiffusion model. This includes: (1) multi-modal category-conditioned\ngeneration, where multi-modal outputs are generated simultaneously through a\nsingle inference process, given category information; (2) multi-modal visual\nunderstanding, which accurately predicts depth, surface normals, and\nsegmentation maps from RGB images; and (3) multi-modal conditioned generation,\nwhich produces corresponding RGB images based on specific modality conditions\nand other aligned modalities. Our approach develops a novel diffusion\ntransformer that flexibly supports multi-modal output, along with a simple\nmodality-decoupling strategy to unify various tasks. Extensive experiments and\napplications demonstrate the effectiveness and superiority of MMGen across\ndiverse tasks and conditions, highlighting its potential for applications that\nrequire simultaneous generation and understanding.\n","authors":["Jiepeng Wang","Zhaoqing Wang","Hao Pan","Yuan Liu","Dongdong Yu","Changhu Wang","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2503.20644v1.pdf","comment":"Our project page: https://jiepengwang.github.io/MMGen/"},{"id":"http://arxiv.org/abs/2503.20631v1","updated":"2025-03-26T15:24:58Z","published":"2025-03-26T15:24:58Z","title":"Robust Flower Cluster Matching Using The Unscented Transform","summary":"  Monitoring flowers over time is essential for precision robotic pollination\nin agriculture. To accomplish this, a continuous spatial-temporal observation\nof plant growth can be done using stationary RGB-D cameras. However, image\nregistration becomes a serious challenge due to changes in the visual\nappearance of the plant caused by the pollination process and occlusions from\ngrowth and camera angles. Plants flower in a manner that produces distinct\nclusters on branches. This paper presents a method for matching flower clusters\nusing descriptors generated from RGB-D data and considers allowing for spatial\nuncertainty within the cluster. The proposed approach leverages the Unscented\nTransform to efficiently estimate plant descriptor uncertainty tolerances,\nenabling a robust image-registration process despite temporal changes. The\nUnscented Transform is used to handle the nonlinear transformations by\npropagating the uncertainty of flower positions to determine the variations in\nthe descriptor domain. A Monte Carlo simulation is used to validate the\nUnscented Transform results, confirming our method's effectiveness for flower\ncluster matching. Therefore, it can facilitate improved robotics pollination in\ndynamic environments.\n","authors":["Andy Chu","Rashik Shrestha","Yu Gu","Jason N. Gross"],"pdf_url":"https://arxiv.org/pdf/2503.20631v1.pdf","comment":"*CASE2025 Under Review*"},{"id":"http://arxiv.org/abs/2412.03352v2","updated":"2025-03-26T15:19:56Z","published":"2024-12-04T14:35:06Z","title":"Intuitive Axial Augmentation Using Polar-Sine-Based Piecewise Distortion\n  for Medical Slice-Wise Segmentation","summary":"  Most data-driven models for medical image analysis rely on universal\naugmentations to improve accuracy. Experimental evidence has confirmed their\neffectiveness, but the unclear mechanism underlying them poses a barrier to the\nwidespread acceptance and trust in such methods within the medical community.\nWe revisit and acknowledge the unique characteristics of medical images apart\nfrom traditional digital images, and consequently, proposed a medical-specific\naugmentation algorithm that is more elastic and aligns well with radiology scan\nprocedure. The method performs piecewise affine with sinusoidal distorted ray\naccording to radius on polar coordinates, thus simulating uncertain postures of\nhuman lying flat on the scanning table. Our method could generate human\nvisceral distribution without affecting the fundamental relative position on\naxial plane. Two non-adaptive algorithms, namely Meta-based Scan Table Removal\nand Similarity-Guided Parameter Search, are introduced to bolster robustness of\nour augmentation method. In contrast to other methodologies, our method is\nhighlighted for its intuitive design and ease of understanding for medical\nprofessionals, thereby enhancing its applicability in clinical scenarios.\nExperiments show our method improves accuracy with two modality across multiple\nfamous segmentation frameworks without requiring more data samples. Our preview\ncode is available in: https://github.com/MGAMZ/PSBPD.\n","authors":["Yiqin Zhang","Qingkui Chen","Chen Huang","Zhengjie Zhang","Meiling Chen","Zhibing Fu"],"pdf_url":"https://arxiv.org/pdf/2412.03352v2.pdf","comment":"Published at Smart Health"},{"id":"http://arxiv.org/abs/2412.12919v2","updated":"2025-03-26T15:14:26Z","published":"2024-12-17T13:51:56Z","title":"4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel\n  Reconstruction from Sparse-View Dynamic DSA Images","summary":"  Reconstructing 3D vessel structures from sparse-view dynamic digital\nsubtraction angiography (DSA) images enables accurate medical assessment while\nreducing radiation exposure. Existing methods often produce suboptimal results\nor require excessive computation time. In this work, we propose 4D radiative\nGaussian splatting (4DRGS) to achieve high-quality reconstruction efficiently.\nIn detail, we represent the vessels with 4D radiative Gaussian kernels. Each\nkernel has time-invariant geometry parameters, including position, rotation,\nand scale, to model static vessel structures. The time-dependent central\nattenuation of each kernel is predicted from a compact neural network to\ncapture the temporal varying response of contrast agent flow. We splat these\nGaussian kernels to synthesize DSA images via X-ray rasterization and optimize\nthe model with real captured ones. The final 3D vessel volume is voxelized from\nthe well-trained kernels. Moreover, we introduce accumulated attenuation\npruning and bounded scaling activation to improve reconstruction quality.\nExtensive experiments on real-world patient data demonstrate that 4DRGS\nachieves impressive results in 5 minutes training, which is 32x faster than the\nstate-of-the-art method. This underscores the potential of 4DRGS for real-world\nclinics.\n","authors":["Zhentao Liu","Ruyi Zha","Huangxuan Zhao","Hongdong Li","Zhiming Cui"],"pdf_url":"https://arxiv.org/pdf/2412.12919v2.pdf","comment":"IPMI 2025 Oral; Zhentao Liu and Ruyi Zha made equal contributions"},{"id":"http://arxiv.org/abs/2411.19756v2","updated":"2025-03-26T15:13:24Z","published":"2024-11-29T15:00:38Z","title":"DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering","summary":"  Gaussian splatting enables fast novel view synthesis in static 3D\nenvironments. However, reconstructing real-world environments remains\nchallenging as distractors or occluders break the multi-view consistency\nassumption required for accurate 3D reconstruction. Most existing methods rely\non external semantic information from pre-trained models, introducing\nadditional computational overhead as pre-processing steps or during\noptimization. In this work, we propose a novel method, DeSplat, that directly\nseparates distractors and static scene elements purely based on volume\nrendering of Gaussian primitives. We initialize Gaussians within each camera\nview for reconstructing the view-specific distractors to separately model the\nstatic 3D scene and distractors in the alpha compositing stages. DeSplat yields\nan explicit scene separation of static elements and distractors, achieving\ncomparable results to prior distractor-free approaches without sacrificing\nrendering speed. We demonstrate DeSplat's effectiveness on three benchmark data\nsets for distractor-free novel view synthesis. See the project website at\nhttps://aaltoml.github.io/desplat/.\n","authors":["Yihao Wang","Marcus Klasson","Matias Turkulainen","Shuzhe Wang","Juho Kannala","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2411.19756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03283v2","updated":"2025-03-26T15:10:26Z","published":"2024-12-04T12:57:17Z","title":"Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models","summary":"  Integrating watermarking into the generation process of latent diffusion\nmodels (LDMs) simplifies detection and attribution of generated content.\nSemantic watermarks, such as Tree-Rings and Gaussian Shading, represent a novel\nclass of watermarking techniques that are easy to implement and highly robust\nagainst various perturbations. However, our work demonstrates a fundamental\nsecurity vulnerability of semantic watermarks. We show that attackers can\nleverage unrelated models, even with different latent spaces and architectures\n(UNet vs DiT), to perform powerful and realistic forgery attacks. Specifically,\nwe design two watermark forgery attacks. The first imprints a targeted\nwatermark into real images by manipulating the latent representation of an\narbitrary image in an unrelated LDM to get closer to the latent representation\nof a watermarked image. We also show that this technique can be used for\nwatermark removal. The second attack generates new images with the target\nwatermark by inverting a watermarked image and re-generating it with an\narbitrary prompt. Both attacks just need a single reference image with the\ntarget watermark. Overall, our findings question the applicability of semantic\nwatermarks by revealing that attackers can easily forge or remove these\nwatermarks under realistic conditions.\n","authors":["Andreas Müller","Denis Lukovnikov","Jonas Thietke","Asja Fischer","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2412.03283v2.pdf","comment":"28 pages, 22 figures, 8 tables, to be published in The IEEE/CVF\n  Conference on Computer Vision and Pattern Recognition 2025 (CVPR)"},{"id":"http://arxiv.org/abs/2503.16302v2","updated":"2025-03-26T15:08:12Z","published":"2025-03-20T16:23:44Z","title":"Unleashing Vecset Diffusion Model for Fast Shape Generation","summary":"  3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.\n","authors":["Zeqiang Lai","Yunfei Zhao","Zibo Zhao","Haolin Liu","Fuyun Wang","Huiwen Shi","Xianghui Yang","Qingxiang Lin","Jingwei Huang","Yuhong Liu","Jie Jiang","Chunchao Guo","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2503.16302v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2503.20612v1","updated":"2025-03-26T14:59:23Z","published":"2025-03-26T14:59:23Z","title":"IAP: Improving Continual Learning of Vision-Language Models via\n  Instance-Aware Prompting","summary":"  Recent pre-trained vision-language models (PT-VLMs) often face a Multi-Domain\nClass-Incremental Learning (MCIL) scenario in practice, where several classes\nand domains of multi-modal tasks are incrementally arrived. Without access to\npreviously learned tasks and unseen tasks, memory-constrained MCIL suffers from\nforward and backward forgetting. To alleviate the above challenges,\nparameter-efficient fine-tuning techniques (PEFT), such as prompt tuning, are\nemployed to adapt the PT-VLM to the diverse incrementally learned tasks. To\nachieve effective new task adaptation, existing methods only consider the\neffect of PEFT strategy selection, but neglect the influence of PEFT parameter\nsetting (e.g., prompting). In this paper, we tackle the challenge of optimizing\nprompt designs for diverse tasks in MCIL and propose an Instance-Aware\nPrompting (IAP) framework. Specifically, our Instance-Aware Gated Prompting\n(IA-GP) module enhances adaptation to new tasks while mitigating forgetting by\ndynamically assigning prompts across transformer layers at the instance level.\nOur Instance-Aware Class-Distribution-Driven Prompting (IA-CDDP) improves the\ntask adaptation process by determining an accurate task-label-related\nconfidence score for each instance. Experimental evaluations across 11\ndatasets, using three performance metrics, demonstrate the effectiveness of our\nproposed method. Code can be found at https://github.com/FerdinandZJU/IAP.\n","authors":["Hao Fu","Hanbin Zhao","Jiahua Dong","Chao Zhang","Hui Qian"],"pdf_url":"https://arxiv.org/pdf/2503.20612v1.pdf","comment":"Code can be found at https://github.com/FerdinandZJU/IAP"},{"id":"http://arxiv.org/abs/2410.04980v3","updated":"2025-03-26T14:45:59Z","published":"2024-10-07T12:21:49Z","title":"Comparison of marker-less 2D image-based methods for infant pose\n  estimation","summary":"  In this study we compare the performance of available generic- and\ninfant-pose estimators for a video-based automated general movement assessment\n(GMA), and the choice of viewing angle for optimal recordings, i.e.,\nconventional diagonal view used in GMA vs. top-down view. We used 4500\nannotated video-frames from 75 recordings of infant spontaneous motor functions\nfrom 4 to 26 weeks. To determine which pose estimation method and camera angle\nyield the best pose estimation accuracy on infants in a GMA related setting,\nthe distance to human annotations and the percentage of correct key-points\n(PCK) were computed and compared. The results show that the best performing\ngeneric model trained on adults, ViTPose, also performs best on infants. We see\nno improvement from using infant-pose estimators over the generic pose\nestimators on our infant dataset. However, when retraining a generic model on\nour data, there is a significant improvement in pose estimation accuracy. The\npose estimation accuracy obtained from the top-down view is significantly\nbetter than that obtained from the diagonal view, especially for the detection\nof the hip key-points. The results also indicate limited generalization\ncapabilities of infant-pose estimators to other infant datasets, which hints\nthat one should be careful when choosing infant pose estimators and using them\non infant datasets which they were not trained on. While the standard GMA\nmethod uses a diagonal view for assessment, pose estimation accuracy\nsignificantly improves using a top-down view. This suggests that a top-down\nview should be included in recording setups for automated GMA research.\n","authors":["Lennart Jahn","Sarah Flügge","Dajie Zhang","Luise Poustka","Sven Bölte","Florentin Wörgötter","Peter B Marschik","Tomas Kulvicius"],"pdf_url":"https://arxiv.org/pdf/2410.04980v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20595v1","updated":"2025-03-26T14:42:46Z","published":"2025-03-26T14:42:46Z","title":"Diffusion Counterfactuals for Image Regressors","summary":"  Counterfactual explanations have been successfully applied to create human\ninterpretable explanations for various black-box models. They are handy for\ntasks in the image domain, where the quality of the explanations benefits from\nrecent advances in generative models. Although counterfactual explanations have\nbeen widely applied to classification models, their application to regression\ntasks remains underexplored. We present two methods to create counterfactual\nexplanations for image regression tasks using diffusion-based generative models\nto address challenges in sparsity and quality: 1) one based on a Denoising\nDiffusion Probabilistic Model that operates directly in pixel-space and 2)\nanother based on a Diffusion Autoencoder operating in latent space. Both\nproduce realistic, semantic, and smooth counterfactuals on CelebA-HQ and a\nsynthetic data set, providing easily interpretable insights into the\ndecision-making process of the regression model and reveal spurious\ncorrelations. We find that for regression counterfactuals, changes in features\ndepend on the region of the predicted value. Large semantic changes are needed\nfor significant changes in predicted values, making it harder to find sparse\ncounterfactuals than with classifiers. Moreover, pixel space counterfactuals\nare more sparse while latent space counterfactuals are of higher quality and\nallow bigger semantic changes.\n","authors":["Trung Duc Ha","Sidney Bender"],"pdf_url":"https://arxiv.org/pdf/2503.20595v1.pdf","comment":"24 Pages, 5 Figures, Accepted at 3rd World Conference on eXplainable\n  Artificial Intelligence (xAI-2025), Code and reproduction instructions\n  available on GitHub, see\n  https://github.com/DevinTDHa/Diffusion-Counterfactuals-for-Image-Regressors"},{"id":"http://arxiv.org/abs/2503.18402v2","updated":"2025-03-26T14:34:29Z","published":"2025-03-24T07:17:27Z","title":"DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds","summary":"  3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian\nprimitives, where the rendering resolution and the primitive number, concluded\nas the optimization complexity, dominate the time cost in primitive\noptimization. In this paper, we propose DashGaussian, a scheduling scheme over\nthe optimization complexity of 3DGS that strips redundant complexity to\naccelerate 3DGS optimization. Specifically, we formulate 3DGS optimization as\nprogressively fitting 3DGS to higher levels of frequency components in the\ntraining views, and propose a dynamic rendering resolution scheme that largely\nreduces the optimization complexity based on this formulation. Besides, we\nargue that a specific rendering resolution should cooperate with a proper\nprimitive number for a better balance between computing redundancy and fitting\nquality, where we schedule the growth of the primitives to synchronize with the\nrendering resolution. Extensive experiments show that our method accelerates\nthe optimization of various 3DGS backbones by 45.7% on average while preserving\nthe rendering quality.\n","authors":["Youyu Chen","Junjun Jiang","Kui Jiang","Xiao Tang","Zhihao Li","Xianming Liu","Yinyu Nie"],"pdf_url":"https://arxiv.org/pdf/2503.18402v2.pdf","comment":"Accepted by CVPR2025. Project page: https://dashgaussian.github.io"},{"id":"http://arxiv.org/abs/2503.20571v1","updated":"2025-03-26T14:18:35Z","published":"2025-03-26T14:18:35Z","title":"Exploring Robustness of Cortical Morphometry in the presence of white\n  matter lesions, using Diffusion Models for Lesion Filling","summary":"  Cortical thickness measurements from magnetic resonance imaging, an important\nbiomarker in many neurodegenerative and neurological disorders, are derived by\nmany tools from an initial voxel-wise tissue segmentation. White matter (WM)\nhypointensities in T1-weighted imaging, such as those arising from multiple\nsclerosis or small vessel disease, are known to affect the output of brain\nsegmentation methods and therefore bias cortical thickness measurements. These\neffects are well-documented among traditional brain segmentation tools but have\nnot been studied extensively in tools based on deep-learning segmentations,\nwhich promise to be more robust. In this paper, we explore the potential of\ndeep learning to enhance the accuracy and efficiency of cortical thickness\nmeasurement in the presence of WM lesions, using a high-quality lesion filling\nalgorithm leveraging denoising diffusion networks.\n  A pseudo-3D U-Net architecture trained on the OASIS dataset to generate\nsynthetic healthy tissue, conditioned on binary lesion masks derived from the\nMSSEG dataset, allows realistic removal of white matter lesions in multiple\nsclerosis patients. By applying morphometry methods to patient images before\nand after lesion filling, we analysed robustness of global and regional\ncortical thickness measurements in the presence of white matter lesions.\nMethods based on a deep learning-based segmentation of the brain (Fastsurfer,\nDL+DiReCT, ANTsPyNet) exhibited greater robustness than those using classical\nsegmentation methods (Freesurfer, ANTs).\n","authors":["Vinzenz Uhr","Ivan Diaz","Christian Rummel","Richard McKinley"],"pdf_url":"https://arxiv.org/pdf/2503.20571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20563v1","updated":"2025-03-26T13:59:29Z","published":"2025-03-26T13:59:29Z","title":"TerraTorch: The Geospatial Foundation Models Toolkit","summary":"  TerraTorch is a fine-tuning and benchmarking toolkit for Geospatial\nFoundation Models built on PyTorch Lightning and tailored for satellite,\nweather, and climate data. It integrates domain-specific data modules,\npre-defined tasks, and a modular model factory that pairs any backbone with\ndiverse decoder heads. These components allow researchers and practitioners to\nfine-tune supported models in a no-code fashion by simply editing a training\nconfiguration. By consolidating best practices for model development and\nincorporating the automated hyperparameter optimization extension Iterate,\nTerraTorch reduces the expertise and time required to fine-tune or benchmark\nmodels on new Earth Observation use cases. Furthermore, TerraTorch directly\nintegrates with GEO-Bench, allowing for systematic and reproducible\nbenchmarking of Geospatial Foundation Models. TerraTorch is open sourced under\nApache 2.0, available at https://github.com/IBM/terratorch, and can be\ninstalled via pip install terratorch.\n","authors":["Carlos Gomes","Benedikt Blumenstiel","Joao Lucas de Sousa Almeida","Pedro Henrique de Oliveira","Paolo Fraccaro","Francesc Marti Escofet","Daniela Szwarcman","Naomi Simumba","Romeo Kienzler","Bianca Zadrozny"],"pdf_url":"https://arxiv.org/pdf/2503.20563v1.pdf","comment":"IGARSS 2025"},{"id":"http://arxiv.org/abs/2503.14945v2","updated":"2025-03-26T13:45:56Z","published":"2025-03-19T07:20:16Z","title":"Generating Multimodal Driving Scenes via Next-Scene Prediction","summary":"  Generative models in Autonomous Driving (AD) enable diverse scene creation,\nyet existing methods fall short by only capturing a limited range of\nmodalities, restricting the capability of generating controllable scenes for\ncomprehensive evaluation of AD systems. In this paper, we introduce a\nmultimodal generation framework that incorporates four major data modalities,\nincluding a novel addition of map modality. With tokenized modalities, our\nscene sequence generation framework autoregressively predicts each scene while\nmanaging computational demands through a two-stage approach. The Temporal\nAutoRegressive (TAR) component captures inter-frame dynamics for each modality\nwhile the Ordered AutoRegressive (OAR) component aligns modalities within each\nscene by sequentially predicting tokens in a fixed order. To maintain coherence\nbetween map and ego-action modalities, we introduce the Action-aware Map\nAlignment (AMA) module, which applies a transformation based on the ego-action\nto maintain coherence between these modalities. Our framework effectively\ngenerates complex, realistic driving scenes over extended sequences, ensuring\nmultimodal consistency and offering fine-grained control over scene elements.\nProject page: https://yanhaowu.github.io/UMGen/\n","authors":["Yanhao Wu","Haoyang Zhang","Tianwei Lin","Lichao Huang","Shujie Luo","Rui Wu","Congpei Qiu","Wei Ke","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14945v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.18227v3","updated":"2025-03-26T13:38:40Z","published":"2025-03-23T22:06:07Z","title":"PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation","summary":"  Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our code is released at https://github.com/logan-0623/PG-SAM.\n","authors":["Yiheng Zhong","Zihong Luo","Chengzhi Liu","Feilong Tang","Zelin Peng","Ming Hu","Yingzhen Hu","Jionglong Su","Zongyuan Ge","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2503.18227v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20540v1","updated":"2025-03-26T13:38:10Z","published":"2025-03-26T13:38:10Z","title":"Beyond Intermediate States: Explaining Visual Redundancy through\n  Language","summary":"  Multi-modal Large Langue Models (MLLMs) often process thousands of visual\ntokens, which consume a significant portion of the context window and impose a\nsubstantial computational burden. Prior work has empirically explored visual\ntoken pruning methods based on MLLMs' intermediate states (e.g., attention\nscores). However, they have limitations in precisely defining visual redundancy\ndue to their inability to capture the influence of visual tokens on MLLMs'\nvisual understanding (i.e., the predicted probabilities for textual token\ncandidates). To address this issue, we manipulate the visual input and\ninvestigate variations in the textual output from both token-centric and\ncontext-centric perspectives, achieving intuitive and comprehensive analysis.\nExperimental results reveal that visual tokens with low ViT-[cls] association\nand low text-to-image attention scores can contain recognizable information and\nsignificantly contribute to images' overall information. To develop a more\nreliable method for identifying and pruning redundant visual tokens, we\nintegrate these two perspectives and introduce a context-independent condition\nto identify redundant prototypes from training images, which probes the\nredundancy of each visual token during inference. Extensive experiments on\nsingle-image, multi-image and video comprehension tasks demonstrate the\neffectiveness of our method, notably achieving 90% to 110% of the performance\nwhile pruning 80% to 90% of visual tokens.\n","authors":["Dingchen Yang","Bowen Cao","Anran Zhang","Weibo Gu","Winston Hu","Guang Chen"],"pdf_url":"https://arxiv.org/pdf/2503.20540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20537v1","updated":"2025-03-26T13:35:43Z","published":"2025-03-26T13:35:43Z","title":"TD-BFR: Truncated Diffusion Model for Efficient Blind Face Restoration","summary":"  Diffusion-based methodologies have shown significant potential in blind face\nrestoration (BFR), leveraging their robust generative capabilities. However,\nthey are often criticized for two significant problems: 1) slow training and\ninference speed, and 2) inadequate recovery of fine-grained facial details. To\naddress these problems, we propose a novel Truncated Diffusion model for\nefficient Blind Face Restoration (TD-BFR), a three-stage paradigm tailored for\nthe progressive resolution of degraded images. Specifically, TD-BFR utilizes an\ninnovative truncated sampling method, starting from low-quality (LQ) images at\nlow resolution to enhance sampling speed, and then introduces an adaptive\ndegradation removal module to handle unknown degradations and connect the\ngeneration processes across different resolutions. Additionally, we further\nadapt the priors of pre-trained diffusion models to recover rich facial\ndetails. Our method efficiently restores high-quality images in a\ncoarse-to-fine manner and experimental results demonstrate that TD-BFR is, on\naverage, \\textbf{4.75$\\times$} faster than current state-of-the-art\ndiffusion-based BFR methods while maintaining competitive quality.\n","authors":["Ziying Zhang","Xiang Gao","Zhixin Wang","Qiang hu","Xiaoyun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20537v1.pdf","comment":"Accepted by ICME 2025"},{"id":"http://arxiv.org/abs/2312.11232v3","updated":"2025-03-26T13:34:53Z","published":"2023-12-18T14:30:54Z","title":"Scale-Equivariant Imaging: Self-Supervised Learning for Image\n  Super-Resolution and Deblurring","summary":"  Self-supervised methods have recently proved to be nearly as effective as\nsupervised ones in various imaging inverse problems, paving the way for\nlearning-based approaches in scientific and medical imaging applications where\nground truth data is hard or expensive to obtain. These methods critically rely\non invariance to translations and/or rotations of the image distribution to\nlearn from incomplete measurement data alone. However, existing approaches fail\nto obtain competitive performances in the problems of image super-resolution\nand deblurring, which play a key role in most imaging systems. In this work, we\nshow that invariance to roto-translations is insufficient to learn from\nmeasurements that only contain low-frequency information. Instead, we propose\nscale-equivariant imaging, a new self-supervised approach that leverages the\nfact that many image distributions are approximately scale-invariant, enabling\nthe recovery of high-frequency information lost in the measurement process. We\ndemonstrate throughout a series of experiments on real datasets that the\nproposed method outperforms other self-supervised approaches, and obtains\nperformances on par with fully supervised learning.\n","authors":["Jérémy Scanvic","Mike Davies","Patrice Abry","Julián Tachella"],"pdf_url":"https://arxiv.org/pdf/2312.11232v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07353v6","updated":"2025-03-26T13:32:28Z","published":"2023-12-12T15:21:57Z","title":"CLIP in Medical Imaging: A Survey","summary":"  Contrastive Language-Image Pre-training (CLIP), a simple yet effective\npre-training paradigm, successfully introduces text supervision to vision\nmodels. It has shown promising results across various tasks due to its\ngeneralizability and interpretability. The use of CLIP has recently gained\nincreasing interest in the medical imaging domain, serving as a pre-training\nparadigm for image-text alignment, or a critical component in diverse clinical\ntasks. With the aim of facilitating a deeper understanding of this promising\ndirection, this survey offers an in-depth exploration of the CLIP within the\ndomain of medical imaging, regarding both refined CLIP pre-training and\nCLIP-driven applications. In this paper, we (1) first start with a brief\nintroduction to the fundamentals of CLIP methodology; (2) then investigate the\nadaptation of CLIP pre-training in the medical imaging domain, focusing on how\nto optimize CLIP given characteristics of medical images and reports; (3)\nfurther explore practical utilization of CLIP pre-trained models in various\ntasks, including classification, dense prediction, and cross-modal tasks; and\n(4) finally discuss existing limitations of CLIP in the context of medical\nimaging, and propose forward-looking directions to address the demands of\nmedical imaging domain. Studies featuring technical and practical value are\nboth investigated. We expect this survey will provide researchers with a\nholistic understanding of the CLIP paradigm and its potential implications. The\nproject page of this survey can also be found on\nhttps://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging.\n","authors":["Zihao Zhao","Yuxiao Liu","Han Wu","Mei Wang","Yonghao Li","Sheng Wang","Lin Teng","Disheng Liu","Zhiming Cui","Qian Wang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2312.07353v6.pdf","comment":"Project page available at\n  https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging"},{"id":"http://arxiv.org/abs/2503.20523v1","updated":"2025-03-26T13:11:35Z","published":"2025-03-26T13:11:35Z","title":"GAIA-2: A Controllable Multi-View Generative World Model for Autonomous\n  Driving","summary":"  Generative models offer a scalable and flexible paradigm for simulating\ncomplex environments, yet current approaches fall short in addressing the\ndomain-specific requirements of autonomous driving - such as multi-agent\ninteractions, fine-grained control, and multi-camera consistency. We introduce\nGAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies\nthese capabilities within a single generative framework. GAIA-2 supports\ncontrollable video generation conditioned on a rich set of structured inputs:\nego-vehicle dynamics, agent configurations, environmental factors, and road\nsemantics. It generates high-resolution, spatiotemporally consistent\nmulti-camera videos across geographically diverse driving environments (UK, US,\nGermany). The model integrates both structured conditioning and external latent\nembeddings (e.g., from a proprietary driving model) to facilitate flexible and\nsemantically grounded scene synthesis. Through this integration, GAIA-2 enables\nscalable simulation of both common and rare driving scenarios, advancing the\nuse of generative world models as a core tool in the development of autonomous\nsystems. Videos are available at https://wayve.ai/thinking/gaia-2.\n","authors":["Lloyd Russell","Anthony Hu","Lorenzo Bertoni","George Fedoseev","Jamie Shotton","Elahe Arani","Gianluca Corrado"],"pdf_url":"https://arxiv.org/pdf/2503.20523v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2411.17130v2","updated":"2025-03-26T13:09:32Z","published":"2024-11-26T05:49:25Z","title":"TechCoach: Towards Technical-Point-Aware Descriptive Action Coaching","summary":"  To guide a learner in mastering action skills, it is crucial for a coach to\n1) reason through the learner's action execution and technical points\n(TechPoints), and 2) provide detailed, comprehensible feedback on what is done\nwell and what can be improved. However, existing score-based action assessment\nmethods are still far from reaching this practical scenario. To bridge this\ngap, we investigate a new task termed Descriptive Action Coaching (DescCoach)\nwhich requires the model to provide detailed commentary on what is done well\nand what can be improved beyond a simple quality score for action execution. To\nthis end, we first build a new dataset named EE4D-DescCoach. Through an\nautomatic annotation pipeline, our dataset goes beyond the existing action\nassessment datasets by providing detailed TechPoint-level commentary.\nFurthermore, we propose TechCoach, a new framework that explicitly incorporates\nTechPoint-level reasoning into the DescCoach process. The central to our method\nlies in the Context-aware TechPoint Reasoner, which enables TechCoach to learn\nTechPoint-related quality representation by querying visual context under the\nsupervision of TechPoint-level coaching commentary. By leveraging the visual\ncontext and the TechPoint-related quality representation, a unified\nTechPoint-aware Action Assessor is then employed to provide the overall\ncoaching commentary together with the quality score. Combining all of these, we\nestablish a new benchmark for DescCoach and evaluate the effectiveness of our\nmethod through extensive experiments. The data and code will be made publicly\navailable.\n","authors":["Yuan-Ming Li","An-Lan Wang","Kun-Yu Lin","Yu-Ming Tang","Ling-An Zeng","Jian-Fang Hu","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.17130v2.pdf","comment":"21 pages, 16 figures"},{"id":"http://arxiv.org/abs/2411.18968v2","updated":"2025-03-26T13:02:34Z","published":"2024-11-28T07:37:04Z","title":"Perception of Visual Content: Differences Between Humans and Foundation\n  Models","summary":"  Human-annotated content is often used to train machine learning (ML) models.\nHowever, recently, language and multi-modal foundational models have been used\nto replace and scale-up human annotator's efforts. This study compares\nhuman-generated and ML-generated annotations of images representing diverse\nsocio-economic contexts. We aim to understand differences in perception and\nidentify potential biases in content interpretation. Our dataset comprises\nimages of people from various geographical regions and income levels, covering\nvarious daily activities and home environments. We compare human and\nML-generated annotations semantically and evaluate their impact on predictive\nmodels. Our results show highest similarity between ML captions and human\nlabels from a low-level perspective, i.e., types of words that appear and\nsentence structures, but all three annotations are alike in how similar or\ndissimilar they perceive images across different regions. Additionally, ML\nCaptions resulted in best overall region classification performance, while ML\nObjects and ML Captions performed best overall for income regression. The\nvarying performance of annotation sets highlights the notion that all\nannotations are important, and that human-generated annotations are yet to be\nreplaceable.\n","authors":["Nardiena A. Pratama","Shaoyang Fan","Gianluca Demartini"],"pdf_url":"https://arxiv.org/pdf/2411.18968v2.pdf","comment":"12 pages, 5 figures, 5 tables; updated version for a\n  Revise-and-Resubmit at ICWSM 2025. This version includes a larger and more\n  diverse dataset, leading to updated results"},{"id":"http://arxiv.org/abs/2503.20519v1","updated":"2025-03-26T13:00:51Z","published":"2025-03-26T13:00:51Z","title":"MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D\n  Generation","summary":"  Recent advances in auto-regressive transformers have revolutionized\ngenerative modeling across different domains, from language processing to\nvisual generation, demonstrating remarkable capabilities. However, applying\nthese advances to 3D generation presents three key challenges: the unordered\nnature of 3D data conflicts with sequential next-token prediction paradigm,\nconventional vector quantization approaches incur substantial compression loss\nwhen applied to 3D meshes, and the lack of efficient scaling strategies for\nhigher resolution latent prediction. To address these challenges, we introduce\nMAR-3D, which integrates a pyramid variational autoencoder with a cascaded\nmasked auto-regressive transformer (Cascaded MAR) for progressive latent\nupscaling in the continuous space. Our architecture employs random masking\nduring training and auto-regressive denoising in random order during inference,\nnaturally accommodating the unordered property of 3D latent tokens.\nAdditionally, we propose a cascaded training strategy with condition\naugmentation that enables efficiently up-scale the latent token resolution with\nfast convergence. Extensive experiments demonstrate that MAR-3D not only\nachieves superior performance and generalization capabilities compared to\nexisting methods but also exhibits enhanced scaling capabilities compared to\njoint distribution modeling approaches (e.g., diffusion transformers).\n","authors":["Jinnan Chen","Lingting Zhu","Zeyu Hu","Shengju Qian","Yugang Chen","Xin Wang","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2503.20519v1.pdf","comment":"Aceepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.20516v1","updated":"2025-03-26T12:58:13Z","published":"2025-03-26T12:58:13Z","title":"Small Object Detection: A Comprehensive Survey on Challenges, Techniques\n  and Real-World Applications","summary":"  Small object detection (SOD) is a critical yet challenging task in computer\nvision, with applications like spanning surveillance, autonomous systems,\nmedical imaging, and remote sensing. Unlike larger objects, small objects\ncontain limited spatial and contextual information, making accurate detection\ndifficult. Challenges such as low resolution, occlusion, background\ninterference, and class imbalance further complicate the problem. This survey\nprovides a comprehensive review of recent advancements in SOD using deep\nlearning, focusing on articles published in Q1 journals during 2024-2025. We\nanalyzed challenges, state-of-the-art techniques, datasets, evaluation metrics,\nand real-world applications. Recent advancements in deep learning have\nintroduced innovative solutions, including multi-scale feature extraction,\nSuper-Resolution (SR) techniques, attention mechanisms, and transformer-based\narchitectures. Additionally, improvements in data augmentation, synthetic data\ngeneration, and transfer learning have addressed data scarcity and domain\nadaptation issues. Furthermore, emerging trends such as lightweight neural\nnetworks, knowledge distillation (KD), and self-supervised learning offer\npromising directions for improving detection efficiency, particularly in\nresource-constrained environments like Unmanned Aerial Vehicles (UAV)-based\nsurveillance and edge computing. We also review widely used datasets, along\nwith standard evaluation metrics such as mean Average Precision (mAP) and\nsize-specific AP scores. The survey highlights real-world applications,\nincluding traffic monitoring, maritime surveillance, industrial defect\ndetection, and precision agriculture. Finally, we discuss open research\nchallenges and future directions, emphasizing the need for robust domain\nadaptation techniques, better feature fusion strategies, and real-time\nperformance optimization.\n","authors":["Mahya Nikouei","Bita Baroutian","Shahabedin Nabavi","Fateme Taraghi","Atefe Aghaei","Ayoob Sajedi","Mohsen Ebrahimi Moghaddam"],"pdf_url":"https://arxiv.org/pdf/2503.20516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.09841v3","updated":"2025-03-26T12:51:33Z","published":"2021-10-19T10:54:01Z","title":"Cutting Voxel Projector a New Approach to Construct 3D Cone Beam CT\n  Operator","summary":"  We introduce a novel class of projectors for 3D cone beam tomographic\nreconstruction. Analytical formulas are derived to compute the relationship\nbetween the volume of a voxel projected onto a detector pixel and its\ncontribution to the line integral of attenuation recorded by that pixel. Based\non these formulas, we construct a near-exact projector and backprojector,\nparticularly suited for algebraic reconstruction techniques and hierarchical\nreconstruction approaches with nonuniform voxel grids. Unlike traditional\nprojectors, which assume a uniform grid with fixed voxel sizes, our method\nenables local refinement of voxels, allowing for adaptive grid resolution and\nimproved reconstruction quality in regions of interest. We have implemented\nthis cutting voxel projector along with a relaxed, speed-optimized version and\ncompared them to two established projectors: a ray-tracing projector based on\nSiddon's algorithm and a TT footprint projector. Our results demonstrate that\nthe cutting voxel projector achieves higher accuracy than the TT projector,\nespecially for large cone beam angles. Furthermore, the relaxed version of the\ncutting voxel projector offers a significant speed advantage, while maintaining\ncomparable accuracy. In contrast, Siddon's algorithm, tuned to achieve the same\naccuracy, is considerably slower than the cutting voxel projector. All\nalgorithms are implemented in a GPU optimized open-source framework for\nalgebraic reconstruction. GitHub repository of the project\nhttps://github.com/kulvait/KCT_cbct.\n","authors":["Vojtěch Kulvait","Julian Moosmann","Georg Rose"],"pdf_url":"https://arxiv.org/pdf/2110.09841v3.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.20504v1","updated":"2025-03-26T12:45:34Z","published":"2025-03-26T12:45:34Z","title":"Vision-Amplified Semantic Entropy for Hallucination Detection in Medical\n  Visual Question Answering","summary":"  Multimodal large language models (MLLMs) have demonstrated significant\npotential in medical Visual Question Answering (VQA). Yet, they remain prone to\nhallucinations-incorrect responses that contradict input images, posing\nsubstantial risks in clinical decision-making. Detecting these hallucinations\nis essential for establishing trust in MLLMs among clinicians and patients,\nthereby enabling their real-world adoption. Current hallucination detection\nmethods, especially semantic entropy (SE), have demonstrated promising\nhallucination detection capacity for LLMs. However, adapting SE to medical\nMLLMs by incorporating visual perturbations presents a dilemma. Weak\nperturbations preserve image content and ensure clinical validity, but may be\noverlooked by medical MLLMs, which tend to over rely on language priors. In\ncontrast, strong perturbations can distort essential diagnostic features,\ncompromising clinical interpretation. To address this issue, we propose Vision\nAmplified Semantic Entropy (VASE), which incorporates weak image\ntransformations and amplifies the impact of visual input, to improve\nhallucination detection in medical VQA. We first estimate the semantic\npredictive distribution under weak visual transformations to preserve clinical\nvalidity, and then amplify visual influence by contrasting this distribution\nwith that derived from a distorted image. The entropy of the resulting\ndistribution is estimated as VASE. Experiments on two medical open-ended VQA\ndatasets demonstrate that VASE consistently outperforms existing hallucination\ndetection methods.\n","authors":["Zehui Liao","Shishuai Hu","Ke Zou","Huazhu Fu","Liangli Zhen","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2503.20504v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.20502v1","updated":"2025-03-26T12:42:37Z","published":"2025-03-26T12:42:37Z","title":"MLLM-Selector: Necessity and Diversity-driven High-Value Data Selection\n  for Enhanced Visual Instruction Tuning","summary":"  Visual instruction tuning (VIT) has emerged as a crucial technique for\nenabling multi-modal large language models (MLLMs) to follow user instructions\nadeptly. Yet, a significant gap persists in understanding the attributes of\nhigh-quality instruction tuning data and frameworks for its automated\nselection. To address this, we introduce MLLM-Selector, an automated approach\nthat identifies valuable data for VIT by weighing necessity and diversity. Our\nprocess starts by randomly sampling a subset from the VIT data pool to\nfine-tune a pretrained model, thus creating a seed model with an initial\nability to follow instructions. Then, leveraging the seed model, we calculate\nnecessity scores for each sample in the VIT data pool to identify samples\npivotal for enhancing model performance. Our findings underscore the importance\nof mixing necessity and diversity in data choice, leading to the creation of\nMLLM-Selector, our methodology that fuses necessity scoring with strategic\nsampling for superior data refinement. Empirical results indicate that within\nidentical experimental conditions, MLLM-Selector surpasses LLaVA-1.5 in some\nbenchmarks with less than 1% of the data and consistently exceeds performance\nacross all validated benchmarks when using less than 50%.\n","authors":["Yiwei Ma","Guohai Xu","Xiaoshuai Sun","Jiayi Ji","Jie Lou","Debing Zhang","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2503.20502v1.pdf","comment":"Tech Report"},{"id":"http://arxiv.org/abs/2503.19753v2","updated":"2025-03-26T12:34:34Z","published":"2025-03-25T15:16:53Z","title":"A Survey on Event-driven 3D Reconstruction: Development under Different\n  Categories","summary":"  Event cameras have gained increasing attention for 3D reconstruction due to\ntheir high temporal resolution, low latency, and high dynamic range. They\ncapture per-pixel brightness changes asynchronously, allowing accurate\nreconstruction under fast motion and challenging lighting conditions. In this\nsurvey, we provide a comprehensive review of event-driven 3D reconstruction\nmethods, including stereo, monocular, and multimodal systems. We further\ncategorize recent developments based on geometric, learning-based, and hybrid\napproaches. Emerging trends, such as neural radiance fields and 3D Gaussian\nsplatting with event data, are also covered. The related works are structured\nchronologically to illustrate the innovations and progression within the field.\nTo support future research, we also highlight key research gaps and future\nresearch directions in dataset, experiment, evaluation, event representation,\netc.\n","authors":["Chuanzhi Xu","Haoxian Zhou","Haodong Chen","Vera Chung","Qiang Qu"],"pdf_url":"https://arxiv.org/pdf/2503.19753v2.pdf","comment":"6 pages, 1 figure, 6 tables, submitted to an anonymous conference\n  under double-blind review"},{"id":"http://arxiv.org/abs/2503.20492v1","updated":"2025-03-26T12:31:04Z","published":"2025-03-26T12:31:04Z","title":"Towards Efficient and General-Purpose Few-Shot Misclassification\n  Detection for Vision-Language Models","summary":"  Reliable prediction by classifiers is crucial for their deployment in high\nsecurity and dynamically changing situations. However, modern neural networks\noften exhibit overconfidence for misclassified predictions, highlighting the\nneed for confidence estimation to detect errors. Despite the achievements\nobtained by existing methods on small-scale datasets, they all require training\nfrom scratch and there are no efficient and effective misclassification\ndetection (MisD) methods, hindering practical application towards large-scale\nand ever-changing datasets. In this paper, we pave the way to exploit vision\nlanguage model (VLM) leveraging text information to establish an efficient and\ngeneral-purpose misclassification detection framework. By harnessing the power\nof VLM, we construct FSMisD, a Few-Shot prompt learning framework for MisD to\nrefrain from training from scratch and therefore improve tuning efficiency. To\nenhance misclassification detection ability, we use adaptive pseudo sample\ngeneration and a novel negative loss to mitigate the issue of overconfidence by\npushing category prompts away from pseudo features. We conduct comprehensive\nexperiments with prompt learning methods and validate the generalization\nability across various datasets with domain shift. Significant and consistent\nimprovement demonstrates the effectiveness, efficiency and generalizability of\nour approach.\n","authors":["Fanhu Zeng","Zhen Cheng","Fei Zhu","Xu-Yao Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20492v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2503.20491v1","updated":"2025-03-26T12:28:20Z","published":"2025-03-26T12:28:20Z","title":"VPO: Aligning Text-to-Video Generation Models with Prompt Optimization","summary":"  Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO.\n","authors":["Jiale Cheng","Ruiliang Lyu","Xiaotao Gu","Xiao Liu","Jiazheng Xu","Yida Lu","Jiayan Teng","Zhuoyi Yang","Yuxiao Dong","Jie Tang","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2503.20491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14754v2","updated":"2025-03-26T12:25:03Z","published":"2025-03-18T21:53:37Z","title":"Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection","summary":"  Street scene datasets, collected from Street View or dashboard cameras, offer\na promising means of detecting urban objects and incidents like street\nflooding. However, a major challenge in using these datasets is their lack of\nreliable labels: there are myriad types of incidents, many types occur rarely,\nand ground-truth measures of where incidents occur are lacking. Here, we\npropose BayFlood, a two-stage approach which circumvents this difficulty.\nFirst, we perform zero-shot classification of where incidents occur using a\npretrained vision-language model (VLM). Second, we fit a spatial Bayesian model\non the VLM classifications. The zero-shot approach avoids the need to annotate\nlarge training sets, and the Bayesian model provides frequent desiderata in\nurban settings - principled measures of uncertainty, smoothing across\nlocations, and incorporation of external data like stormwater accumulation\nzones. We comprehensively validate this two-stage approach, showing that VLMs\nprovide strong zero-shot signal for floods across multiple cities and time\nperiods, the Bayesian model improves out-of-sample prediction relative to\nbaseline methods, and our inferred flood risk correlates with known external\npredictors of risk. Having validated our approach, we show it can be used to\nimprove urban flood detection: our analysis reveals 113,738 people who are at\nhigh risk of flooding overlooked by current methods, identifies demographic\nbiases in existing methods, and suggests locations for new flood sensors. More\nbroadly, our results showcase how Bayesian modeling of zero-shot LM annotations\nrepresents a promising paradigm because it avoids the need to collect large\nlabeled datasets and leverages the power of foundation models while providing\nthe expressiveness and uncertainty quantification of Bayesian models.\n","authors":["Matt Franchi","Nikhil Garg","Wendy Ju","Emma Pierson"],"pdf_url":"https://arxiv.org/pdf/2503.14754v2.pdf","comment":"In review"},{"id":"http://arxiv.org/abs/2406.14526v2","updated":"2025-03-26T12:21:42Z","published":"2024-06-20T17:38:16Z","title":"Fantastic Copyrighted Beasts and How (Not) to Generate Them","summary":"  Recent studies show that image and video generation models can be prompted to\nreproduce copyrighted content from their training data, raising serious legal\nconcerns about copyright infringement. Copyrighted characters (e.g., Mario,\nBatman) present a significant challenge: at least one lawsuit has already\nawarded damages based on the generation of such characters. Consequently,\ncommercial services like DALL-E have started deploying interventions. However,\nlittle research has systematically examined these problems: (1) Can users\neasily prompt models to generate copyrighted characters, even if it is\nunintentional?; (2) How effective are the existing mitigation strategies? To\naddress these questions, we introduce a novel evaluation framework with metrics\nthat assess both the generated image's similarity to copyrighted characters and\nits consistency with user intent, grounded in a set of popular copyrighted\ncharacters from diverse studios and regions. We show that state-of-the-art\nimage and video generation models can still generate characters even if\ncharacters' names are not explicitly mentioned, sometimes with only two generic\nkeywords (e.g., prompting with \"videogame, plumber\" consistently generates\nNintendo's Mario character). We also introduce semi-automatic techniques to\nidentify such keywords or descriptions that trigger character generation. Using\nthis framework, we evaluate mitigation strategies, including prompt rewriting\nand new approaches we propose. Our findings reveal that common methods, such as\nDALL-E's prompt rewriting, are insufficient alone and require supplementary\nstrategies like negative prompting. Our work provides empirical grounding for\ndiscussions on copyright mitigation strategies and offers actionable insights\nfor model deployers implementing these safeguards.\n","authors":["Luxi He","Yangsibo Huang","Weijia Shi","Tinghao Xie","Haotian Liu","Yue Wang","Luke Zettlemoyer","Chiyuan Zhang","Danqi Chen","Peter Henderson"],"pdf_url":"https://arxiv.org/pdf/2406.14526v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00741v2","updated":"2025-03-26T12:16:53Z","published":"2025-01-01T06:07:03Z","title":"Towards End-to-End Neuromorphic Voxel-based 3D Object Reconstruction\n  Without Physical Priors","summary":"  Neuromorphic cameras, also known as event cameras, are asynchronous\nbrightness-change sensors that can capture extremely fast motion without\nsuffering from motion blur, making them particularly promising for 3D\nreconstruction in extreme environments. However, existing research on 3D\nreconstruction using monocular neuromorphic cameras is limited, and most of the\nmethods rely on estimating physical priors and employ complex multi-step\npipelines. In this work, we propose an end-to-end method for dense voxel 3D\nreconstruction using neuromorphic cameras that eliminates the need to estimate\nphysical priors. Our method incorporates a novel event representation to\nenhance edge features, enabling the proposed feature-enhancement model to learn\nmore effectively. Additionally, we introduced Optimal Binarization Threshold\nSelection Principle as a guideline for future related work, using the optimal\nreconstruction results achieved with threshold optimization as the benchmark.\nOur method achieves a 54.6% improvement in reconstruction accuracy compared to\nthe baseline method.\n","authors":["Chuanzhi Xu","Langyi Chen","Haodong Chen","Vera Chung","Qiang Qu"],"pdf_url":"https://arxiv.org/pdf/2501.00741v2.pdf","comment":"6 pages, 15 figures, 5 tables, accepted by IEEE International\n  Conference on Multimedia & Expo (ICME) 2025"},{"id":"http://arxiv.org/abs/2309.14949v2","updated":"2025-03-26T12:16:13Z","published":"2023-09-26T14:06:26Z","title":"Towards Real-World Test-Time Adaptation: Tri-Net Self-Training with\n  Balanced Normalization","summary":"  Test-Time Adaptation aims to adapt source domain model to testing data at\ninference stage with success demonstrated in adapting to unseen corruptions.\nHowever, these attempts may fail under more challenging real-world scenarios.\nExisting works mainly consider real-world test-time adaptation under non-i.i.d.\ndata stream and continual domain shift. In this work, we first complement the\nexisting real-world TTA protocol with a globally class imbalanced testing set.\nWe demonstrate that combining all settings together poses new challenges to\nexisting methods. We argue the failure of state-of-the-art methods is first\ncaused by indiscriminately adapting normalization layers to imbalanced testing\ndata. To remedy this shortcoming, we propose a balanced batchnorm layer to swap\nout the regular batchnorm at inference stage. The new batchnorm layer is\ncapable of adapting without biasing towards majority classes. We are further\ninspired by the success of self-training (ST) in learning from unlabeled data\nand adapt ST for test-time adaptation. However, ST alone is prone to over\nadaption which is responsible for the poor performance under continual domain\nshift. Hence, we propose to improve self-training under continual domain shift\nby regularizing model updates with an anchored loss. The final TTA model,\ntermed as TRIBE, is built upon a tri-net architecture with balanced batchnorm\nlayers. We evaluate TRIBE on four datasets representing real-world TTA\nsettings. TRIBE consistently achieves the state-of-the-art performance across\nmultiple evaluation protocols. The code is available at\nhttps://github.com/Gorilla-Lab-SCUT/TRIBE.\n","authors":["Yongyi Su","Xun Xu","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2309.14949v2.pdf","comment":"Accepted by AAAI 2024. 19 pages, 7 figures and 22 tables"},{"id":"http://arxiv.org/abs/2503.20484v1","updated":"2025-03-26T12:15:25Z","published":"2025-03-26T12:15:25Z","title":"Contrastive Learning Guided Latent Diffusion Model for Image-to-Image\n  Translation","summary":"  The diffusion model has demonstrated superior performance in synthesizing\ndiverse and high-quality images for text-guided image translation. However,\nthere remains room for improvement in both the formulation of text prompts and\nthe preservation of reference image content. First, variations in target text\nprompts can significantly influence the quality of the generated images, and it\nis often challenging for users to craft an optimal prompt that fully captures\nthe content of the input image. Second, while existing models can introduce\ndesired modifications to specific regions of the reference image, they\nfrequently induce unintended alterations in areas that should remain unchanged.\nTo address these challenges, we propose pix2pix-zeroCon, a zero-shot\ndiffusion-based method that eliminates the need for additional training by\nleveraging patch-wise contrastive loss. Specifically, we automatically\ndetermine the editing direction in the text embedding space based on the\nreference image and target prompts. Furthermore, to ensure precise content and\nstructural preservation in the edited image, we introduce cross-attention\nguiding loss and patch-wise contrastive loss between the generated and original\nimage embeddings within a pre-trained diffusion model. Notably, our approach\nrequires no additional training and operates directly on a pre-trained\ntext-to-image diffusion model. Extensive experiments demonstrate that our\nmethod surpasses existing models in image-to-image translation, achieving\nenhanced fidelity and controllability.\n","authors":["Qi Si","Bo Wang","Zhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20484v1.pdf","comment":"11 pages, 13 figures"},{"id":"http://arxiv.org/abs/2503.20483v1","updated":"2025-03-26T12:13:35Z","published":"2025-03-26T12:13:35Z","title":"Dissecting and Mitigating Diffusion Bias via Mechanistic\n  Interpretability","summary":"  Diffusion models have demonstrated impressive capabilities in synthesizing\ndiverse content. However, despite their high-quality outputs, these models\noften perpetuate social biases, including those related to gender and race.\nThese biases can potentially contribute to harmful real-world consequences,\nreinforcing stereotypes and exacerbating inequalities in various social\ncontexts. While existing research on diffusion bias mitigation has\npredominantly focused on guiding content generation, it often neglects the\nintrinsic mechanisms within diffusion models that causally drive biased\noutputs. In this paper, we investigate the internal processes of diffusion\nmodels, identifying specific decision-making mechanisms, termed bias features,\nembedded within the model architecture. By directly manipulating these\nfeatures, our method precisely isolates and adjusts the elements responsible\nfor bias generation, permitting granular control over the bias levels in the\ngenerated content. Through experiments on both unconditional and conditional\ndiffusion models across various social bias attributes, we demonstrate our\nmethod's efficacy in managing generation distribution while preserving image\nquality. We also dissect the discovered model mechanism, revealing different\nintrinsic features controlling fine-grained aspects of generation, boosting\nfurther research on mechanistic interpretability of diffusion models.\n","authors":["Yingdong Shi","Changming Li","Yifan Wang","Yongxiang Zhao","Anqi Pang","Sibei Yang","Jingyi Yu","Kan Ren"],"pdf_url":"https://arxiv.org/pdf/2503.20483v1.pdf","comment":"CVPR 2025; Project Page:\n  https://foundation-model-research.github.io/difflens"},{"id":"http://arxiv.org/abs/2503.19385v2","updated":"2025-03-26T12:12:38Z","published":"2025-03-25T06:30:45Z","title":"Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing","summary":"  We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.\n","authors":["Jaihoon Kim","Taehoon Yoon","Jisung Hwang","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2503.19385v2.pdf","comment":"Project page: https://flow-inference-time-scaling.github.io/"},{"id":"http://arxiv.org/abs/2503.20472v1","updated":"2025-03-26T11:53:03Z","published":"2025-03-26T11:53:03Z","title":"From Trial to Triumph: Advancing Long Video Understanding via Visual\n  Context Sample Scaling and Self-reward Alignment","summary":"  Multi-modal Large language models (MLLMs) show remarkable ability in video\nunderstanding. Nevertheless, understanding long videos remains challenging as\nthe models can only process a finite number of frames in a single inference,\npotentially omitting crucial visual information. To address the challenge, we\npropose generating multiple predictions through visual context sampling,\nfollowed by a scoring mechanism to select the final prediction. Specifically,\nwe devise a bin-wise sampling strategy that enables MLLMs to generate diverse\nanswers based on various combinations of keyframes, thereby enriching the\nvisual context. To determine the final prediction from the sampled answers, we\nemploy a self-reward by linearly combining three scores: (1) a frequency score\nindicating the prevalence of each option, (2) a marginal confidence score\nreflecting the inter-intra sample certainty of MLLM predictions, and (3) a\nreasoning score for different question types, including clue-guided answering\nfor global questions and temporal self-refocusing for local questions. The\nfrequency score ensures robustness through majority correctness, the\nconfidence-aligned score reflects prediction certainty, and the typed-reasoning\nscore addresses cases with sparse key visual information using tailored\nstrategies. Experiments show that this approach covers the correct answer for a\nhigh percentage of long video questions, on seven datasets show that our method\nimproves the performance of three MLLMs.\n","authors":["Yucheng Suo","Fan Ma","Linchao Zhu","Tianyi Wang","Fengyun Rao","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2503.20472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08923v3","updated":"2025-03-26T11:37:00Z","published":"2024-11-12T08:14:54Z","title":"Aligning Visual Contrastive learning models via Preference Optimization","summary":"  Contrastive learning models have demonstrated impressive abilities to capture\nsemantic similarities by aligning representations in the embedding space.\nHowever, their performance can be limited by the quality of the training data\nand its inherent biases. While Preference Optimization (PO) methods such as\nReinforcement Learning from Human Feedback (RLHF) and Direct Preference\nOptimization (DPO) have been applied to align generative models with human\npreferences, their use in contrastive learning has yet to be explored. This\npaper introduces a novel method for training contrastive learning models using\ndifferent PO methods to break down complex concepts. Our method systematically\naligns model behavior with desired preferences, enhancing performance on the\ntargeted task. In particular, we focus on enhancing model robustness against\ntypographic attacks and inductive biases, commonly seen in contrastive\nvision-language models like CLIP. Our experiments demonstrate that models\ntrained using PO outperform standard contrastive learning techniques while\nretaining their ability to handle adversarial challenges and maintain accuracy\non other downstream tasks. This makes our method well-suited for tasks\nrequiring fairness, robustness, and alignment with specific preferences. We\nevaluate our method for tackling typographic attacks on images and explore its\nability to disentangle gender concepts and mitigate gender bias, showcasing the\nversatility of our approach.\n","authors":["Amirabbas Afzali","Borna Khodabandeh","Ali Rasekh","Mahyar JafariNodeh","Sepehr kazemi","Simon Gottschalk"],"pdf_url":"https://arxiv.org/pdf/2411.08923v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20454v1","updated":"2025-03-26T11:33:18Z","published":"2025-03-26T11:33:18Z","title":"Lipschitz Constant Meets Condition Number: Learning Robust and Compact\n  Deep Neural Networks","summary":"  Recent research has revealed that high compression of Deep Neural Networks\n(DNNs), e.g., massive pruning of the weight matrix of a DNN, leads to a severe\ndrop in accuracy and susceptibility to adversarial attacks. Integration of\nnetwork pruning into an adversarial training framework has been proposed to\npromote adversarial robustness. It has been observed that a highly pruned\nweight matrix tends to be ill-conditioned, i.e., increasing the condition\nnumber of the weight matrix. This phenomenon aggravates the vulnerability of a\nDNN to input noise. Although a highly pruned weight matrix is considered to be\nable to lower the upper bound of the local Lipschitz constant to tolerate large\ndistortion, the ill-conditionedness of such a weight matrix results in a\nnon-robust DNN model. To overcome this challenge, this work develops novel\njoint constraints to adjust the weight distribution of networks, namely, the\nTransformed Sparse Constraint joint with Condition Number Constraint (TSCNC),\nwhich copes with smoothing distribution and differentiable constraint functions\nto reduce condition number and thus avoid the ill-conditionedness of weight\nmatrices. Furthermore, our theoretical analyses unveil the relevance between\nthe condition number and the local Lipschitz constant of the weight matrix,\nnamely, the sharply increasing condition number becomes the dominant factor\nthat restricts the robustness of over-sparsified models. Extensive experiments\nare conducted on several public datasets, and the results show that the\nproposed constraints significantly improve the robustness of a DNN with high\npruning rates.\n","authors":["Yangqi Feng","Shing-Ho J. Lin","Baoyuan Gao","Xian Wei"],"pdf_url":"https://arxiv.org/pdf/2503.20454v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.05874v3","updated":"2025-03-26T11:27:37Z","published":"2025-02-09T12:23:40Z","title":"MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor\n  Scene Generation","summary":"  Controllable 3D scene generation has extensive applications in virtual\nreality and interior design, where the generated scenes should exhibit high\nlevels of realism and controllability in terms of geometry. Scene graphs\nprovide a suitable data representation that facilitates these applications.\nHowever, current graph-based methods for scene generation are constrained to\ntext-based inputs and exhibit insufficient adaptability to flexible user\ninputs, hindering the ability to precisely control object geometry. To address\nthis issue, we propose MMGDreamer, a dual-branch diffusion model for scene\ngeneration that incorporates a novel Mixed-Modality Graph, visual enhancement\nmodule, and relation predictor. The mixed-modality graph allows object nodes to\nintegrate textual and visual modalities, with optional relationships between\nnodes. It enhances adaptability to flexible user inputs and enables meticulous\ncontrol over the geometry of objects in the generated scenes. The visual\nenhancement module enriches the visual fidelity of text-only nodes by\nconstructing visual representations using text embeddings. Furthermore, our\nrelation predictor leverages node representations to infer absent relationships\nbetween nodes, resulting in more coherent scene layouts. Extensive experimental\nresults demonstrate that MMGDreamer exhibits superior control of object\ngeometry, achieving state-of-the-art scene generation performance. Project\npage: https://yangzhifeio.github.io/project/MMGDreamer.\n","authors":["Zhifei Yang","Keyang Lu","Chao Zhang","Jiaxing Qi","Hanqi Jiang","Ruifei Ma","Shenglin Yin","Yifan Xu","Mingzhe Xing","Zhen Xiao","Jieyi Long","Guangyao Zhai"],"pdf_url":"https://arxiv.org/pdf/2502.05874v3.pdf","comment":"Accepted by AAAI 2025 Main Track"},{"id":"http://arxiv.org/abs/2503.20446v1","updated":"2025-03-26T11:22:17Z","published":"2025-03-26T11:22:17Z","title":"Attention Xception UNet (AXUNet): A Novel Combination of CNN and\n  Self-Attention for Brain Tumor Segmentation","summary":"  Accurate segmentation of glioma brain tumors is crucial for diagnosis and\ntreatment planning. Deep learning techniques offer promising solutions, but\noptimal model architectures remain under investigation. We used the BraTS 2021\ndataset, selecting T1 with contrast enhancement (T1CE), T2, and\nFluid-Attenuated Inversion Recovery (FLAIR) sequences for model development.\nThe proposed Attention Xception UNet (AXUNet) architecture integrates an\nXception backbone with dot-product self-attention modules, inspired by\nstate-of-the-art (SOTA) large language models such as Google Bard and OpenAI\nChatGPT, within a UNet-shaped model. We compared AXUNet with SOTA models.\nComparative evaluation on the test set demonstrated improved results over\nbaseline models. Inception-UNet and Xception-UNet achieved mean Dice scores of\n90.88 and 93.24, respectively. Attention ResUNet (AResUNet) attained a mean\nDice score of 92.80, with the highest score of 84.92 for enhancing tumor (ET)\namong all models. Attention Gate UNet (AGUNet) yielded a mean Dice score of\n90.38. AXUNet outperformed all models with a mean Dice score of 93.73. It\ndemonstrated superior Dice scores across whole tumor (WT) and tumor core (TC)\nregions, achieving 92.59 for WT, 86.81 for TC, and 84.89 for ET. The\nintegration of the Xception backbone and dot-product self-attention mechanisms\nin AXUNet showcases enhanced performance in capturing spatial and contextual\ninformation. The findings underscore the potential utility of AXUNet in\nfacilitating precise tumor delineation.\n","authors":["Farzan Moodi","Fereshteh Khodadadi Shoushtari","Gelareh Valizadeh","Dornaz Mazinani","Hanieh Mobarak Salari","Hamidreza Saligheh Rad"],"pdf_url":"https://arxiv.org/pdf/2503.20446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19683v2","updated":"2025-03-26T11:21:23Z","published":"2025-03-25T14:10:54Z","title":"Unlocking the Hidden Potential of CLIP in Generalizable Deepfake\n  Detection","summary":"  This paper tackles the challenge of detecting partially manipulated facial\ndeepfakes, which involve subtle alterations to specific facial features while\nretaining the overall context, posing a greater detection difficulty than fully\nsynthetic faces. We leverage the Contrastive Language-Image Pre-training (CLIP)\nmodel, specifically its ViT-L/14 visual encoder, to develop a generalizable\ndetection method that performs robustly across diverse datasets and unknown\nforgery techniques with minimal modifications to the original model. The\nproposed approach utilizes parameter-efficient fine-tuning (PEFT) techniques,\nsuch as LN-tuning, to adjust a small subset of the model's parameters,\npreserving CLIP's pre-trained knowledge and reducing overfitting. A tailored\npreprocessing pipeline optimizes the method for facial images, while\nregularization strategies, including L2 normalization and metric learning on a\nhyperspherical manifold, enhance generalization. Trained on the FaceForensics++\ndataset and evaluated in a cross-dataset fashion on Celeb-DF-v2, DFDC, FFIW,\nand others, the proposed method achieves competitive detection accuracy\ncomparable to or outperforming much more complex state-of-the-art techniques.\nThis work highlights the efficacy of CLIP's visual encoder in facial deepfake\ndetection and establishes a simple, powerful baseline for future research,\nadvancing the field of generalizable deepfake detection. The code is available\nat: https://github.com/yermandy/deepfake-detection\n","authors":["Andrii Yermakov","Jan Cech","Jiri Matas"],"pdf_url":"https://arxiv.org/pdf/2503.19683v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20436v1","updated":"2025-03-26T11:10:29Z","published":"2025-03-26T11:10:29Z","title":"Siformer: Feature-isolated Transformer for Efficient Skeleton-based Sign\n  Language Recognition","summary":"  Sign language recognition (SLR) refers to interpreting sign language glosses\nfrom given videos automatically. This research area presents a complex\nchallenge in computer vision because of the rapid and intricate movements\ninherent in sign languages, which encompass hand gestures, body postures, and\neven facial expressions. Recently, skeleton-based action recognition has\nattracted increasing attention due to its ability to handle variations in\nsubjects and backgrounds independently. However, current skeleton-based SLR\nmethods exhibit three limitations: 1) they often neglect the importance of\nrealistic hand poses, where most studies train SLR models on non-realistic\nskeletal representations; 2) they tend to assume complete data availability in\nboth training or inference phases, and capture intricate relationships among\ndifferent body parts collectively; 3) these methods treat all sign glosses\nuniformly, failing to account for differences in complexity levels regarding\nskeletal representations. To enhance the realism of hand skeletal\nrepresentations, we present a kinematic hand pose rectification method for\nenforcing constraints. Mitigating the impact of missing data, we propose a\nfeature-isolated mechanism to focus on capturing local spatial-temporal\ncontext. This method captures the context concurrently and independently from\nindividual features, thus enhancing the robustness of the SLR model.\nAdditionally, to adapt to varying complexity levels of sign glosses, we develop\nan input-adaptive inference approach to optimise computational efficiency and\naccuracy. Experimental results demonstrate the effectiveness of our approach,\nas evidenced by achieving a new state-of-the-art (SOTA) performance on WLASL100\nand LSA64. For WLASL100, we achieve a top-1 accuracy of 86.50\\%, marking a\nrelative improvement of 2.39% over the previous SOTA. For LSA64, we achieve a\ntop-1 accuracy of 99.84%.\n","authors":["Muxin Pu","Mei Kuan Lim","Chun Yong Chong"],"pdf_url":"https://arxiv.org/pdf/2503.20436v1.pdf","comment":"10 pages, ACM Multimedia"},{"id":"http://arxiv.org/abs/2503.12150v2","updated":"2025-03-26T11:08:20Z","published":"2025-03-15T14:13:23Z","title":"Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis","summary":"  This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.\n","authors":["Hongyu Sun","Qiuhong Ke","Ming Cheng","Yongcai Wang","Deying Li","Chenhui Gou","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2503.12150v2.pdf","comment":"Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables"},{"id":"http://arxiv.org/abs/2411.17945v2","updated":"2025-03-26T11:06:10Z","published":"2024-11-26T23:39:43Z","title":"MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D\n  Content Creation","summary":"  Generating high-fidelity 3D content from text prompts remains a significant\nchallenge in computer vision due to the limited size, diversity, and annotation\ndepth of the existing datasets. To address this, we introduce MARVEL-40M+, an\nextensive dataset with 40 million text annotations for over 8.9 million 3D\nassets aggregated from seven major 3D datasets. Our contribution is a novel\nmulti-stage annotation pipeline that integrates open-source pretrained\nmulti-view VLMs and LLMs to automatically produce multi-level descriptions,\nranging from detailed (150-200 words) to concise semantic tags (10-20 words).\nThis structure supports both fine-grained 3D reconstruction and rapid\nprototyping. Furthermore, we incorporate human metadata from source datasets\ninto our annotation pipeline to add domain-specific information in our\nannotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D,\na two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our\nannotations and use a pretrained image-to-3D network to generate 3D textured\nmeshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly\noutperforms existing datasets in annotation quality and linguistic diversity,\nachieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators. Project\npage is available at https://sankalpsinha-cmos.github.io/MARVEL/.\n","authors":["Sankalp Sinha","Mohammad Sadil Khan","Muhammad Usama","Shino Sam","Didier Stricker","Sk Aziz Ali","Muhammad Zeshan Afzal"],"pdf_url":"https://arxiv.org/pdf/2411.17945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20429v1","updated":"2025-03-26T11:01:10Z","published":"2025-03-26T11:01:10Z","title":"Latent Beam Diffusion Models for Decoding Image Sequences","summary":"  While diffusion models excel at generating high-quality images from text\nprompts, they struggle with visual consistency in image sequences. Existing\nmethods generate each image independently, leading to disjointed narratives - a\nchallenge further exacerbated in non-linear storytelling, where scenes must\nconnect beyond adjacent frames. We introduce a novel beam search strategy for\nlatent space exploration, enabling conditional generation of full image\nsequences with beam search decoding. Unlike prior approaches that use fixed\nlatent priors, our method dynamically searches for an optimal sequence of\nlatent representations, ensuring coherent visual transitions. To address beam\nsearch's quadratic complexity, we integrate a cross-attention mechanism that\nefficiently scores search paths and enables pruning, prioritizing alignment\nwith both textual prompts and visual context. Human evaluations confirm that\nour approach outperforms baseline methods, producing full sequences with\nsuperior coherence, visual continuity, and textual alignment. By bridging\nadvances in search optimization and latent space refinement, this work sets a\nnew standard for structured image sequence generation.\n","authors":["Guilherme Fernandes","Vasco Ramos","Regev Cohen","Idan Szpektor","João Magalhães"],"pdf_url":"https://arxiv.org/pdf/2503.20429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20428v1","updated":"2025-03-26T11:01:00Z","published":"2025-03-26T11:01:00Z","title":"Evaluating Facial Expression Recognition Datasets for Deep Learning: A\n  Benchmark Study with Novel Similarity Metrics","summary":"  This study investigates the key characteristics and suitability of widely\nused Facial Expression Recognition (FER) datasets for training deep learning\nmodels. In the field of affective computing, FER is essential for interpreting\nhuman emotions, yet the performance of FER systems is highly contingent on the\nquality and diversity of the underlying datasets. To address this issue, we\ncompiled and analyzed 24 FER datasets, including those targeting specific age\ngroups such as children, adults, and the elderly, and processed them through a\ncomprehensive normalization pipeline. In addition, we enriched the datasets\nwith automatic annotations for age and gender, enabling a more nuanced\nevaluation of their demographic properties. To further assess dataset efficacy,\nwe introduce three novel metricsLocal, Global, and Paired Similarity, which\nquantitatively measure dataset difficulty, generalization capability, and\ncross-dataset transferability. Benchmark experiments using state-of-the-art\nneural networks reveal that large-scale, automatically collected datasets\n(e.g., AffectNet, FER2013) tend to generalize better, despite issues with\nlabeling noise and demographic biases, whereas controlled datasets offer higher\nannotation quality but limited variability. Our findings provide actionable\nrecommendations for dataset selection and design, advancing the development of\nmore robust, fair, and effective FER systems.\n","authors":["F. Xavier Gaya-Morey","Cristina Manresa-Yee","Célia Martinie","Jose M. Buades-Rubio"],"pdf_url":"https://arxiv.org/pdf/2503.20428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20419v1","updated":"2025-03-26T10:50:02Z","published":"2025-03-26T10:50:02Z","title":"Cherry Yield Forecast: Harvest Prediction for Individual Sweet Cherry\n  Trees","summary":"  This paper is part of a publication series from the For5G project that has\nthe goal of creating digital twins of sweet cherry trees. At the beginning a\nbrief overview of the revious work in this project is provided. Afterwards the\nfocus shifts to a crucial problem in the fruit farming domain: the difficulty\nof making reliable yield predictions early in the season. Following three Satin\nsweet cherry trees along the year 2023 enabled the collection of accurate\nground truth data about the development of cherries from dormancy until\nharvest. The methodology used to collect this data is presented, along with its\nvaluation and visualization. The predictive power of counting objects at all\nrelevant vegetative stages of the fruit development cycle in cherry trees with\nregards to yield predictions is investigated. It is found that all investigated\nfruit states are suitable for yield predictions based on linear regression.\nConceptionally, there is a trade-off between earliness and external events with\nthe potential to invalidate the prediction. Considering this, two optimal\ntimepoints are suggested that are opening cluster stage before the start of the\nflowering and the early fruit stage right after the second fruit drop. However,\nboth timepoints are challenging to solve with automated procedures based on\nimage data. Counting developing cherries based on images is exceptionally\ndifficult due to the small fruit size and their tendency to be occluded by\nleaves. It was not possible to obtain satisfying results relying on a\nstate-of-the-art fruit-counting method. Counting the elements within a bursting\nbud is also challenging, even when using high resolution cameras. It is\nconcluded that accurate yield prediction for sweet cherry trees is possible\nwhen objects are manually counted and that automated features extraction with\nsimilar accuracy remains an open problem yet to be solved.\n","authors":["Andreas Gilson","Peter Pietrzyk","Chiara Paglia","Annika Killer","Fabian Keil","Lukas Meyer","Dominikus Kittemann","Patrick Noack","Oliver Scholz"],"pdf_url":"https://arxiv.org/pdf/2503.20419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20418v1","updated":"2025-03-26T10:49:44Z","published":"2025-03-26T10:49:44Z","title":"ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework\n  for Image-Based Virtual Try-On","summary":"  This paper introduces ITA-MDT, the Image-Timestep-Adaptive Masked Diffusion\nTransformer Framework for Image-Based Virtual Try-On (IVTON), designed to\novercome the limitations of previous approaches by leveraging the Masked\nDiffusion Transformer (MDT) for improved handling of both global garment\ncontext and fine-grained details. The IVTON task involves seamlessly\nsuperimposing a garment from one image onto a person in another, creating a\nrealistic depiction of the person wearing the specified garment. Unlike\nconventional diffusion-based virtual try-on models that depend on large\npre-trained U-Net architectures, ITA-MDT leverages a lightweight, scalable\ntransformer-based denoising diffusion model with a mask latent modeling scheme,\nachieving competitive results while reducing computational overhead. A key\ncomponent of ITA-MDT is the Image-Timestep Adaptive Feature Aggregator (ITAFA),\na dynamic feature aggregator that combines all of the features from the image\nencoder into a unified feature of the same size, guided by diffusion timestep\nand garment image complexity. This enables adaptive weighting of features,\nallowing the model to emphasize either global information or fine-grained\ndetails based on the requirements of the denoising stage. Additionally, the\nSalient Region Extractor (SRE) module is presented to identify complex region\nof the garment to provide high-resolution local information to the denoising\nmodel as an additional condition alongside the global information of the full\ngarment image. This targeted conditioning strategy enhances detail preservation\nof fine details in highly salient garment regions, optimizing computational\nresources by avoiding unnecessarily processing entire garment image.\nComparative evaluations confirms that ITA-MDT improves efficiency while\nmaintaining strong performance, reaching state-of-the-art results in several\nmetrics.\n","authors":["Ji Woo Hong","Tri Ton","Trung X. Pham","Gwanhyeong Koo","Sunjae Yoon","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2503.20418v1.pdf","comment":"CVPR 2025, Project Page: https://jiwoohong93.github.io/ita-mdt/"},{"id":"http://arxiv.org/abs/2410.10122v3","updated":"2025-03-26T10:48:17Z","published":"2024-10-14T03:22:26Z","title":"MuseTalk: Real-Time High-Fidelity Video Dubbing via Spatio-Temporal\n  Sampling","summary":"  Real-time video dubbing that preserves identity consistency while achieving\naccurate lip synchronization remains a critical challenge. Existing approaches\nface a trilemma: diffusion-based methods achieve high visual fidelity but\nsuffer from prohibitive computational costs, while GAN-based solutions\nsacrifice lip-sync accuracy or dental details for real-time performance. We\npresent MuseTalk, a novel two-stage training framework that resolves this\ntrade-off through latent space optimization and spatio-temporal data sampling\nstrategy. Our key innovations include: (1) During the Facial Abstract\nPretraining stage, we propose Informative Frame Sampling to temporally align\nreference-source pose pairs, eliminating redundant feature interference while\npreserving identity cues. (2) In the Lip-Sync Adversarial Finetuning stage, we\nemploy Dynamic Margin Sampling to spatially select the most suitable\nlip-movement-promoting regions, balancing audio-visual synchronization and\ndental clarity. (3) MuseTalk establishes an effective audio-visual feature\nfusion framework in the latent space, delivering 30 FPS output at 256*256\nresolution on an NVIDIA V100 GPU. Extensive experiments demonstrate that\nMuseTalk outperforms state-of-the-art methods in visual fidelity while\nachieving comparable lip-sync accuracy. %The codes and models will be made\npublicly available upon acceptance. The code is made available at\n\\href{https://github.com/TMElyralab/MuseTalk}{https://github.com/TMElyralab/MuseTalk}\n","authors":["Yue Zhang","Zhizhou Zhong","Minhao Liu","Zhaokang Chen","Bin Wu","Yubin Zeng","Chao Zhan","Yingjie He","Junxin Huang","Wenjiang Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.10122v3.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.18147v2","updated":"2025-03-26T10:42:11Z","published":"2025-03-23T17:24:32Z","title":"PHT-CAD: Efficient CAD Parametric Primitive Analysis with Progressive\n  Hierarchical Tuning","summary":"  Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing,\nyet 2D Parametric Primitive Analysis (PPA) remains underexplored due to two key\nchallenges: structural constraint reasoning and advanced semantic\nunderstanding. To tackle these challenges, we first propose an Efficient Hybrid\nParametrization (EHP) for better representing 2D engineering drawings. EHP\ncontains four types of atomic component i.e., point, line, circle, and arc).\nAdditionally, we propose PHT-CAD, a novel 2D PPA framework that harnesses the\nmodality alignment and reasoning capabilities of Vision-Language Models (VLMs)\nfor precise engineering drawing analysis. In PHT-CAD, we introduce four\ndedicated regression heads to predict corresponding atomic components. To train\nPHT-CAD, a three-stage training paradigm Progressive Hierarchical Tuning (PHT)\nis proposed to progressively enhance PHT-CAD's capability to perceive\nindividual primitives, infer structural constraints, and align annotation\nlayers with their corresponding geometric representations. Considering that\nexisting datasets lack complete annotation layers and real-world engineering\ndrawings, we introduce ParaCAD, the first large-scale benchmark that explicitly\nintegrates both the geometric and annotation layers. ParaCAD comprises over 10\nmillion annotated drawings for training and 3,000 real-world industrial\ndrawings with complex topological structures and physical constraints for test.\nExtensive experiments demonstrate the effectiveness of PHT-CAD and highlight\nthe practical significance of ParaCAD in advancing 2D PPA research.\n","authors":["Ke Niu","Yuwen Chen","Haiyang Yu","Zhuofan Chen","Xianghui Que","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2503.18147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04234v3","updated":"2025-03-26T10:41:29Z","published":"2024-12-05T15:10:13Z","title":"DEIM: DETR with Improved Matching for Fast Convergence","summary":"  We introduce DEIM, an innovative and efficient training framework designed to\naccelerate convergence in real-time object detection with Transformer-based\narchitectures (DETR). To mitigate the sparse supervision inherent in one-to-one\n(O2O) matching in DETR models, DEIM employs a Dense O2O matching strategy. This\napproach increases the number of positive samples per image by incorporating\nadditional targets, using standard data augmentation techniques. While Dense\nO2O matching speeds up convergence, it also introduces numerous low-quality\nmatches that could affect performance. To address this, we propose the\nMatchability-Aware Loss (MAL), a novel loss function that optimizes matches\nacross various quality levels, enhancing the effectiveness of Dense O2O.\nExtensive experiments on the COCO dataset validate the efficacy of DEIM. When\nintegrated with RT-DETR and D-FINE, it consistently boosts performance while\nreducing training time by 50%. Notably, paired with RT-DETRv2, DEIM achieves\n53.2% AP in a single day of training on an NVIDIA 4090 GPU. Additionally,\nDEIM-trained real-time models outperform leading real-time object detectors,\nwith DEIM-D-FINE-L and DEIM-D-FINE-X achieving 54.7% and 56.5% AP at 124 and 78\nFPS on an NVIDIA T4 GPU, respectively, without the need for additional data. We\nbelieve DEIM sets a new baseline for advancements in real-time object\ndetection. Our code and pre-trained models are available at\nhttps://github.com/ShihuaHuang95/DEIM.\n","authors":["Shihua Huang","Zhichao Lu","Xiaodong Cun","Yongjun Yu","Xiao Zhou","Xi Shen"],"pdf_url":"https://arxiv.org/pdf/2412.04234v3.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.20382v1","updated":"2025-03-26T10:03:46Z","published":"2025-03-26T10:03:46Z","title":"RSRWKV: A Linear-Complexity 2D Attention Mechanism for Efficient Remote\n  Sensing Vision Task","summary":"  High-resolution remote sensing analysis faces challenges in global context\nmodeling due to scene complexity and scale diversity. While CNNs excel at local\nfeature extraction via parameter sharing, their fixed receptive fields\nfundamentally restrict long-range dependency modeling. Vision Transformers\n(ViTs) effectively capture global semantic relationships through self-attention\nmechanisms but suffer from quadratic computational complexity relative to image\nresolution, creating critical efficiency bottlenecks for high-resolution\nimagery. The RWKV model's linear-complexity sequence modeling achieves\nbreakthroughs in NLP but exhibits anisotropic limitations in vision tasks due\nto its 1D scanning mechanism. To address these challenges, we propose RSRWKV,\nfeaturing a novel 2D-WKV scanning mechanism that bridges sequential processing\nand 2D spatial reasoning while maintaining linear complexity. This enables\nisotropic context aggregation across multiple directions. The MVC-Shift module\nenhances multi-scale receptive field coverage, while the ECA module strengthens\ncross-channel feature interaction and semantic saliency modeling. Experimental\nresults demonstrate RSRWKV's superior performance over CNN and Transformer\nbaselines in classification, detection, and segmentation tasks on NWPU\nRESISC45, VHR-10.v2, and GLH-Water datasets, offering a scalable solution for\nhigh-resolution remote sensing analysis.\n","authors":["Chunshan Li","Rong Wang","Xiaofei Yang","Dianhui Chu"],"pdf_url":"https://arxiv.org/pdf/2503.20382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20368v1","updated":"2025-03-26T09:44:40Z","published":"2025-03-26T09:44:40Z","title":"Pluggable Style Representation Learning for Multi-Style Transfer","summary":"  Due to the high diversity of image styles, the scalability to various styles\nplays a critical role in real-world applications. To accommodate a large amount\nof styles, previous multi-style transfer approaches rely on enlarging the model\nsize while arbitrary-style transfer methods utilize heavy backbones. However,\nthe additional computational cost introduced by more model parameters hinders\nthese methods to be deployed on resource-limited devices. To address this\nchallenge, in this paper, we develop a style transfer framework by decoupling\nthe style modeling and transferring. Specifically, for style modeling, we\npropose a style representation learning scheme to encode the style information\ninto a compact representation. Then, for style transferring, we develop a\nstyle-aware multi-style transfer network (SaMST) to adapt to diverse styles\nusing pluggable style representations. In this way, our framework is able to\naccommodate diverse image styles in the learned style representations without\nintroducing additional overhead during inference, thereby maintaining\nefficiency. Experiments show that our style representation can extract accurate\nstyle information. Moreover, qualitative and quantitative results demonstrate\nthat our method achieves state-of-the-art performance in terms of both accuracy\nand efficiency. The codes are available in\nhttps://github.com/The-Learning-And-Vision-Atelier-LAVA/SaMST.\n","authors":["Hongda Liu","Longguang Wang","Weijun Guan","Ye Zhang","Yulan Guo"],"pdf_url":"https://arxiv.org/pdf/2503.20368v1.pdf","comment":"18 pages, 13 figures, 2 tables"},{"id":"http://arxiv.org/abs/2503.20362v1","updated":"2025-03-26T09:39:58Z","published":"2025-03-26T09:39:58Z","title":"Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video\n  Understanding","summary":"  Large Vision-Language Models (LVLMs) demonstrate remarkable performance in\nshort-video tasks such as video question answering, but struggle in long-video\nunderstanding. The linear frame sampling strategy, conventionally used by\nLVLMs, fails to account for the non-linear distribution of key events in video\ndata, often introducing redundant or irrelevant information in longer contexts\nwhile risking the omission of critical events in shorter ones. To address this,\nwe propose SelfReS, a non-linear spatiotemporal self-reflective sampling method\nthat dynamically selects key video fragments based on user prompts. Unlike\nprior approaches, SelfReS leverages the inherently sparse attention maps of\nLVLMs to define reflection tokens, enabling relevance-aware token selection\nwithout requiring additional training or external modules. Experiments\ndemonstrate that SelfReS can be seamlessly integrated into strong base LVLMs,\nimproving long-video task accuracy and achieving up to 46% faster inference\nspeed within the same GPU memory budget.\n","authors":["Joao Pereira","Vasco Lopes","David Semedo","Joao Neves"],"pdf_url":"https://arxiv.org/pdf/2503.20362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20354v1","updated":"2025-03-26T09:27:09Z","published":"2025-03-26T09:27:09Z","title":"SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic\n  Activation Sparsity","summary":"  Despite the growing integration of deep models into mobile terminals, the\naccuracy of these models declines significantly due to various deployment\ninterferences. Test-time adaptation (TTA) has emerged to improve the\nperformance of deep models by adapting them to unlabeled target data online.\nYet, the significant memory cost, particularly in resource-constrained\nterminals, impedes the effective deployment of most backward-propagation-based\nTTA methods. To tackle memory constraints, we introduce SURGEON, a method that\nsubstantially reduces memory cost while preserving comparable accuracy\nimprovements during fully test-time adaptation (FTTA) without relying on\nspecific network architectures or modifications to the original training\nprocedure. Specifically, we propose a novel dynamic activation sparsity\nstrategy that directly prunes activations at layer-specific dynamic ratios\nduring adaptation, allowing for flexible control of learning ability and memory\ncost in a data-sensitive manner. Among this, two metrics, Gradient Importance\nand Layer Activation Memory, are considered to determine the layer-wise pruning\nratios, reflecting accuracy contribution and memory efficiency, respectively.\nExperimentally, our method surpasses the baselines by not only reducing memory\nusage but also achieving superior accuracy, delivering SOTA performance across\ndiverse datasets, architectures, and tasks.\n","authors":["Ke Ma","Jiaqi Tang","Bin Guo","Fan Dang","Sicong Liu","Zhui Zhu","Lei Wu","Cheng Fang","Ying-Cong Chen","Zhiwen Yu","Yunhao Liu"],"pdf_url":"https://arxiv.org/pdf/2503.20354v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.20349v1","updated":"2025-03-26T09:20:42Z","published":"2025-03-26T09:20:42Z","title":"Consistency Trajectory Matching for One-Step Generative Super-Resolution","summary":"  Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency.\n","authors":["Weiyi You","Mingyang Zhang","Leheng Zhang","Kexuan Shi","Xingyu Zhou","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2503.20349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20348v1","updated":"2025-03-26T09:20:30Z","published":"2025-03-26T09:20:30Z","title":"VideoGEM: Training-free Action Grounding in Videos","summary":"  Vision-language foundation models have shown impressive capabilities across\nvarious zero-shot tasks, including training-free localization and grounding,\nprimarily focusing on localizing objects in images. However, leveraging those\ncapabilities to localize actions and events in videos is challenging, as\nactions have less physical outline and are usually described by higher-level\nconcepts. In this work, we propose VideoGEM, the first training-free spatial\naction grounding method based on pretrained image- and video-language\nbackbones. Namely, we adapt the self-self attention formulation of GEM to\nspatial activity grounding. We observe that high-level semantic concepts, such\nas actions, usually emerge in the higher layers of the image- and\nvideo-language models. We, therefore, propose a layer weighting in the\nself-attention path to prioritize higher layers. Additionally, we introduce a\ndynamic weighting method to automatically tune layer weights to capture each\nlayer`s relevance to a specific prompt. Finally, we introduce a prompt\ndecomposition, processing action, verb, and object prompts separately,\nresulting in a better spatial localization of actions. We evaluate the proposed\napproach on three image- and video-language backbones, CLIP, OpenCLIP, and\nViCLIP, and on four video grounding datasets, V-HICO, DALY,\nYouCook-Interactions, and GroundingYouTube, showing that the proposed\ntraining-free approach is able to outperform current trained state-of-the-art\napproaches for spatial video grounding.\n","authors":["Felix Vogel","Walid Bousselham","Anna Kukleva","Nina Shvetsova","Hilde Kuehne"],"pdf_url":"https://arxiv.org/pdf/2503.20348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03239v3","updated":"2025-03-26T09:09:55Z","published":"2024-11-05T16:37:30Z","title":"Decoupling Fine Detail and Global Geometry for Compressed Depth Map\n  Super-Resolution","summary":"  Recovering high-quality depth maps from compressed sources has gained\nsignificant attention due to the limitations of consumer-grade depth cameras\nand the bandwidth restrictions during data transmission. However, current\nmethods still suffer from two challenges. First, bit-depth compression produces\na uniform depth representation in regions with subtle variations, hindering the\nrecovery of detailed information. Second, densely distributed random noise\nreduces the accuracy of estimating the global geometric structure of the scene.\nTo address these challenges, we propose a novel framework, termed\ngeometry-decoupled network (GDNet), for compressed depth map super-resolution\nthat decouples the high-quality depth map reconstruction process by handling\nglobal and detailed geometric features separately. To be specific, we propose\nthe fine geometry detail encoder (FGDE), which is designed to aggregate fine\ngeometry details in high-resolution low-level image features while\nsimultaneously enriching them with complementary information from\nlow-resolution context-level image features. In addition, we develop the global\ngeometry encoder (GGE) that aims at suppressing noise and extracting global\ngeometric information effectively via constructing compact feature\nrepresentation in a low-rank space. We conduct experiments on multiple\nbenchmark datasets, demonstrating that our GDNet significantly outperforms\ncurrent methods in terms of geometric consistency and detail recovery. In the\nECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st\nplace award. Our codes are available at: https://github.com/Ian0926/GDNet.\n","authors":["Huan Zheng","Wencheng Han","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2411.03239v3.pdf","comment":"Accepted by CVPR 2025 & The 1st place award for the ECCV 2024 AIM\n  Compressed Depth Upsampling Challenge"},{"id":"http://arxiv.org/abs/2412.01256v2","updated":"2025-03-26T09:08:24Z","published":"2024-12-02T08:25:09Z","title":"NLPrompt: Noise-Label Prompt Learning for Vision-Language Models","summary":"  The emergence of vision-language foundation models, such as CLIP, has\nrevolutionized image-text representation, enabling a broad range of\napplications via prompt learning. Despite its promise, real-world datasets\noften contain noisy labels that can degrade prompt learning performance. In\nthis paper, we demonstrate that using mean absolute error (MAE) loss in prompt\nlearning, named PromptMAE, significantly enhances robustness against noisy\nlabels while maintaining high accuracy. Though MAE is straightforward and\nrecognized for its robustness, it is rarely used in noisy-label learning due to\nits slow convergence and poor performance outside prompt learning scenarios. To\nelucidate the robustness of PromptMAE, we leverage feature learning theory to\nshow that MAE can suppress the influence of noisy samples, thereby improving\nthe signal-to-noise ratio and enhancing overall robustness. Additionally, we\nintroduce PromptOT, a prompt-based optimal transport data purification method\nto enhance the robustness further. PromptOT employs text features in\nvision-language models as prototypes to construct an optimal transportation\nmatrix. This matrix effectively partitions datasets into clean and noisy\nsubsets, allowing for the application of cross-entropy loss to the clean subset\nand MAE loss to the noisy subset. Our Noise-Label Prompt Learning method, named\nNLPrompt, offers a simple and efficient approach that leverages the expressive\nrepresentations and precise alignment capabilities of vision-language models\nfor robust prompt learning. We validate NLPrompt through extensive experiments\nacross various noise settings, demonstrating significant performance\nimprovements.\n","authors":["Bikang Pan","Qun Li","Xiaoying Tang","Wei Huang","Zhen Fang","Feng Liu","Jingya Wang","Jingyi Yu","Ye Shi"],"pdf_url":"https://arxiv.org/pdf/2412.01256v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04880v2","updated":"2025-03-26T09:07:32Z","published":"2024-12-06T09:23:31Z","title":"MozzaVID: Mozzarella Volumetric Image Dataset","summary":"  Influenced by the complexity of volumetric imaging, there is a shortage of\nestablished datasets useful for benchmarking volumetric deep-learning models.\nAs a consequence, new and existing models are not easily comparable, limiting\nthe development of architectures optimized specifically for volumetric data. To\ncounteract this trend, we introduce MozzaVID - a large, clean, and versatile\nvolumetric classification dataset. Our dataset contains X-ray computed\ntomography (CT) images of mozzarella microstructure and enables the\nclassification of 25 cheese types and 149 cheese samples. We provide data in\nthree different resolutions, resulting in three dataset instances containing\nfrom 591 to 37,824 images. While being general-purpose, the dataset also\nfacilitates investigating mozzarella structure properties. The structure of\nfood directly affects its functional properties and thus its consumption\nexperience. Understanding food structure helps tune the production and\nmimicking it enables sustainable alternatives to animal-derived food products.\nThe complex and disordered nature of food structures brings a unique challenge,\nwhere a choice of appropriate imaging method, scale, and sample size is not\ntrivial. With this dataset we aim to address these complexities, contributing\nto more robust structural analysis models. The dataset can be downloaded from:\nhttps://archive.compute.dtu.dk/files/public/projects/MozzaVID/.\n","authors":["Pawel Tomasz Pieta","Peter Winkel Rasmussen","Anders Bjorholm Dahl","Jeppe Revall Frisvad","Siavash Arjomand Bigdeli","Carsten Gundlach","Anders Nymark Christensen"],"pdf_url":"https://arxiv.org/pdf/2412.04880v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03907v3","updated":"2025-03-26T09:06:21Z","published":"2024-12-05T06:26:32Z","title":"ONER: Online Experience Replay for Incremental Anomaly Detection","summary":"  Incremental anomaly detection aims to sequentially identify defects in\nindustrial product lines but suffers from catastrophic forgetting, primarily\ndue to knowledge overwriting during parameter updates and feature conflicts\nbetween tasks. In this work, We propose ONER (ONline Experience Replay), an\nend-to-end framework that addresses these issues by synergistically integrating\ntwo types of experience: (1) decomposed prompts, which dynamically generate\nimage-conditioned prompts from reusable modules to retain prior knowledge thus\nprevent knowledge overwriting, and (2) semantic prototypes, which enforce\nseparability in latent feature spaces at pixel and image levels to mitigate\ncross-task feature conflicts. Extensive experiments demonstrate the superiority\nof ONER, achieving state-of-the-art performance with +4.4% Pixel AUROC and\n+28.3% Pixel AUPR improvements on the MVTec AD dataset over prior methods.\nRemarkably, ONER achieves this with only 0.019M parameters and 5 training\nepochs per task, confirming its efficiency and stability for real-world\nindustrial deployment.\n","authors":["Yizhou Jin","Jiahui Zhu","Guodong Wang","Shiwei Li","Jinjin Zhang","Xinyue Liu","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.03907v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18597v2","updated":"2025-03-26T09:05:41Z","published":"2024-12-24T18:51:19Z","title":"DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion\n  Transformer for Tuning-Free Multi-Prompt Longer Video Generation","summary":"  Sora-like video generation models have achieved remarkable progress with a\nMulti-Modal Diffusion Transformer MM-DiT architecture. However, the current\nvideo generation models predominantly focus on single-prompt, struggling to\ngenerate coherent scenes with multiple sequential prompts that better reflect\nreal-world dynamic scenarios. While some pioneering works have explored\nmulti-prompt video generation, they face significant challenges including\nstrict training data requirements, weak prompt following, and unnatural\ntransitions. To address these problems, we propose DiTCtrl, a training-free\nmulti-prompt video generation method under MM-DiT architectures for the first\ntime. Our key idea is to take the multi-prompt video generation task as\ntemporal video editing with smooth transitions. To achieve this goal, we first\nanalyze MM-DiT's attention mechanism, finding that the 3D full attention\nbehaves similarly to that of the cross/self-attention blocks in the UNet-like\ndiffusion models, enabling mask-guided precise semantic control across\ndifferent prompts with attention sharing for multi-prompt video generation.\nBased on our careful design, the video generated by DiTCtrl achieves smooth\ntransitions and consistent object motion given multiple sequential prompts\nwithout additional training. Besides, we also present MPVBench, a new benchmark\nspecially designed for multi-prompt video generation to evaluate the\nperformance of multi-prompt generation. Extensive experiments demonstrate that\nour method achieves state-of-the-art performance without additional training.\n","authors":["Minghong Cai","Xiaodong Cun","Xiaoyu Li","Wenze Liu","Zhaoyang Zhang","Yong Zhang","Ying Shan","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2412.18597v2.pdf","comment":"CVPR 2025; 21 pages, 23 figures, Project page:\n  https://onevfall.github.io/project_page/ditctrl ; GitHub repository:\n  https://github.com/TencentARC/DiTCtrl"},{"id":"http://arxiv.org/abs/2503.20337v1","updated":"2025-03-26T09:02:37Z","published":"2025-03-26T09:02:37Z","title":"Progressive Focused Transformer for Single Image Super-Resolution","summary":"  Transformer-based methods have achieved remarkable results in image\nsuper-resolution tasks because they can capture non-local dependencies in\nlow-quality input images. However, this feature-intensive modeling approach is\ncomputationally expensive because it calculates the similarities between\nnumerous features that are irrelevant to the query features when obtaining\nattention weights. These unnecessary similarity calculations not only degrade\nthe reconstruction performance but also introduce significant computational\noverhead. How to accurately identify the features that are important to the\ncurrent query features and avoid similarity calculations between irrelevant\nfeatures remains an urgent problem. To address this issue, we propose a novel\nand effective Progressive Focused Transformer (PFT) that links all isolated\nattention maps in the network through Progressive Focused Attention (PFA) to\nfocus attention on the most important tokens. PFA not only enables the network\nto capture more critical similar features, but also significantly reduces the\ncomputational cost of the overall network by filtering out irrelevant features\nbefore calculating similarities. Extensive experiments demonstrate the\neffectiveness of the proposed method, achieving state-of-the-art performance on\nvarious single image super-resolution benchmarks.\n","authors":["Wei Long","Xingyu Zhou","Leheng Zhang","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2503.20337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08741v3","updated":"2025-03-26T09:01:55Z","published":"2025-03-11T08:25:40Z","title":"Oasis: One Image is All You Need for Multimodal Instruction Data\n  Synthesis","summary":"  The success of multi-modal large language models (MLLMs) has been largely\nattributed to the large-scale training data. However, the training data of many\nMLLMs is unavailable due to privacy concerns. The expensive and labor-intensive\nprocess of collecting multi-modal data further exacerbates the problem. Is it\npossible to synthesize multi-modal training data automatically without\ncompromising diversity and quality? In this paper, we propose a new method,\nOasis, to synthesize high-quality multi-modal data with only images. Oasis\nbreaks through traditional methods by prompting only images to the MLLMs, thus\nextending the data diversity by a large margin. Our method features a delicate\nquality control method which ensures the data quality. We collected over 500k\ndata and conducted incremental experiments on LLaVA-NeXT. Extensive experiments\ndemonstrate that our method can significantly improve the performance of MLLMs.\nThe image-based synthesis also allows us to focus on the specific-domain\nability of MLLMs. Code and dataset are publicly available at\nhttps://github.com/Letian2003/MM_INF.\n","authors":["Letian Zhang","Quan Cui","Bingchen Zhao","Cheng Yang"],"pdf_url":"https://arxiv.org/pdf/2503.08741v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01136v2","updated":"2025-03-26T08:59:35Z","published":"2024-12-02T05:20:35Z","title":"Referring Video Object Segmentation via Language-aligned Track Selection","summary":"  Referring video object segmentation (RVOS) requires tracking and segmenting\nan object throughout a video according to a given natural language expression,\ndemanding both complex motion understanding and the alignment of visual\nrepresentations with language descriptions. Given these challenges, the\nrecently proposed Segment Anything Model 2 (SAM2) emerges as a potential\ncandidate due to its ability to generate coherent segmentation mask tracks\nacross video frames, and provide an inherent spatio-temporal objectness in its\nobject token representations. In this paper, we introduce SOLA (Selection by\nObject Language Alignment), a novel framework that leverages SAM2 object tokens\nas compact video-level object representations, which are aligned with language\nfeatures through a lightweight track selection module. To effectively\nfacilitate this alignment, we propose an IoU-based pseudo-labeling strategy,\nwhich bridges the modality gap between SAM2 representations with language\nfeatures. Extensive experiments show that SOLA achieves state-of-the-art\nperformance on the MeViS dataset and demonstrate that SOLA offers an effective\nsolution for RVOS. Our project page is available at:\nhttps://cvlab-kaist.github.io/SOLA.\n","authors":["Seongchan Kim","Woojeong Jin","Sangbeom Lim","Heeji Yoon","Hyunwook Choi","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2412.01136v2.pdf","comment":"Project page is available at https://cvlab-kaist.github.io/SOLA"},{"id":"http://arxiv.org/abs/2503.20328v1","updated":"2025-03-26T08:55:18Z","published":"2025-03-26T08:55:18Z","title":"Euclidean Distance to Convex Polyhedra and Application to Class\n  Representation in Spectral Images","summary":"  With the aim of estimating the abundance map from observations only, linear\nunmixing approaches are not always suitable to spectral images, especially when\nthe number of bands is too small or when the spectra of the observed data are\ntoo correlated. To address this issue in the general case, we present a novel\napproach which provides an adapted spatial density function based on any\narbitrary linear classifier. A robust mathematical formulation for computing\nthe Euclidean distance to polyhedral sets is presented, along with an efficient\nalgorithm that provides the exact minimum-norm point in a polyhedron. An\nempirical evaluation on the widely-used Samson hyperspectral dataset\ndemonstrates that the proposed method surpasses state-of-the-art approaches in\nreconstructing abundance maps. Furthermore, its application to spectral images\nof a Lithium-ion battery, incompatible with linear unmixing models, validates\nthe method's generality and effectiveness.\n","authors":["Antoine Bottenmuller","Florent Magaud","Arnaud Demortière","Etienne Decencière","Petr Dokladal"],"pdf_url":"https://arxiv.org/pdf/2503.20328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07746v4","updated":"2025-03-26T08:48:13Z","published":"2024-03-12T15:28:51Z","title":"Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified\n  3D Perception","summary":"  Low-cost, vision-centric 3D perception systems for autonomous driving have\nmade significant progress in recent years, narrowing the gap to expensive\nLiDAR-based methods. The primary challenge in becoming a fully reliable\nalternative lies in robust depth prediction capabilities, as camera-based\nsystems struggle with long detection ranges and adverse lighting and weather\nconditions. In this work, we introduce HyDRa, a novel camera-radar fusion\narchitecture for diverse 3D perception tasks. Building upon the principles of\ndense BEV (Bird's Eye View)-based architectures, HyDRa introduces a hybrid\nfusion approach to combine the strengths of complementary camera and radar\nfeatures in two distinct representation spaces. Our Height Association\nTransformer module leverages radar features already in the perspective view to\nproduce more robust and accurate depth predictions. In the BEV, we refine the\ninitial sparse representation by a Radar-weighted Depth Consistency. HyDRa\nachieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and\n58.4 AMOTA (+1.5) on the public nuScenes dataset. Moreover, our new\nsemantically rich and spatially accurate BEV features can be directly converted\ninto a powerful occupancy representation, beating all previous camera-based\nmethods on the Occ3D benchmark by an impressive 3.7 mIoU. Code and models are\navailable at https://github.com/phi-wol/hydra.\n","authors":["Philipp Wolters","Johannes Gilg","Torben Teepe","Fabian Herzog","Anouar Laouichi","Martin Hofmann","Gerhard Rigoll"],"pdf_url":"https://arxiv.org/pdf/2403.07746v4.pdf","comment":"Accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2503.20322v1","updated":"2025-03-26T08:44:11Z","published":"2025-03-26T08:44:11Z","title":"Dynamic Pyramid Network for Efficient Multimodal Large Language Model","summary":"  Multimodal large language models (MLLMs) have demonstrated impressive\nperformance in various vision-language (VL) tasks, but their expensive\ncomputations still limit the real-world application. To address this issue,\nrecent efforts aim to compress the visual features to save the computational\ncosts of MLLMs. However, direct visual compression methods, e.g. efficient\nprojectors, inevitably destroy the visual semantics in MLLM, especially in\ndifficult samples. To overcome this shortcoming, we propose a novel dynamic\npyramid network (DPN) for efficient MLLMs. Specifically, DPN formulates MLLM as\na hierarchical structure where visual features are gradually compressed with\nincreasing depth. In this case, even with a high compression ratio,\nfine-grained visual information can still be perceived in shallow layers. To\nmaximize the benefit of DPN, we further propose an innovative Dynamic Pooling\nExperts (DPE) that can dynamically choose the optimal visual compression rate\naccording to input features. With this design, harder samples will be assigned\nlarger computations, thus preserving the model performance. To validate our\napproach, we conduct extensive experiments on two popular MLLMs and ten\nbenchmarks. Experimental results show that DPN can save up to 56% average FLOPs\non LLaVA while further achieving +0.74% performance gains. Besides, the\ngeneralization ability of DPN is also validated on the existing high-resolution\nMLLM called LLaVA-HR. Our source codes are anonymously released at\nhttps://github.com/aihao2000/DPN-LLaVA.\n","authors":["Hao Ai","Kunyi Wang","Zezhou Wang","Hao Lu","Jin Tian","Yaxin Luo","Peng Xing","Jen-Yuan Huang","Huaxia Li","Gen luo"],"pdf_url":"https://arxiv.org/pdf/2503.20322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20321v1","updated":"2025-03-26T08:43:21Z","published":"2025-03-26T08:43:21Z","title":"Recovering Dynamic 3D Sketches from Videos","summary":"  Understanding 3D motion from videos presents inherent challenges due to the\ndiverse types of movement, ranging from rigid and deformable objects to\narticulated structures. To overcome this, we propose Liv3Stroke, a novel\napproach for abstracting objects in motion with deformable 3D strokes. The\ndetailed movements of an object may be represented by unstructured motion\nvectors or a set of motion primitives using a pre-defined articulation from a\ntemplate model. Just as a free-hand sketch can intuitively visualize scenes or\nintentions with a sparse set of lines, we utilize a set of parametric 3D curves\nto capture a set of spatially smooth motion elements for general objects with\nunknown structures. We first extract noisy, 3D point cloud motion guidance from\nvideo frames using semantic features, and our approach deforms a set of curves\nto abstract essential motion features as a set of explicit 3D representations.\nSuch abstraction enables an understanding of prominent components of motions\nwhile maintaining robustness to environmental factors. Our approach allows\ndirect analysis of 3D object movements from video, tackling the uncertainty\nthat typically occurs when translating real-world motion into recorded footage.\nThe project page is accessible via: https://jaeah.me/liv3stroke_web}\n","authors":["Jaeah Lee","Changwoon Choi","Young Min Kim","Jaesik Park"],"pdf_url":"https://arxiv.org/pdf/2503.20321v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.16973v2","updated":"2025-03-26T08:43:09Z","published":"2025-03-21T09:41:24Z","title":"ARFlow: Human Action-Reaction Flow Matching with Physical Guidance","summary":"  Human action-reaction synthesis, a fundamental challenge in modeling causal\nhuman interactions, plays a critical role in applications ranging from virtual\nreality to social robotics. While diffusion-based models have demonstrated\npromising performance, they exhibit two key limitations for interaction\nsynthesis: reliance on complex noise-to-reaction generators with intricate\nconditional mechanisms, and frequent physical violations in generated motions.\nTo address these issues, we propose Action-Reaction Flow Matching (ARFlow), a\nnovel framework that establishes direct action-to-reaction mappings,\neliminating the need for complex conditional mechanisms. Our approach\nintroduces two key innovations: an x1-prediction method that directly outputs\nhuman motions instead of velocity fields, enabling explicit constraint\nenforcement; and a training-free, gradient-based physical guidance mechanism\nthat effectively prevents body penetration artifacts during sampling. Extensive\nexperiments on NTU120 and Chi3D datasets demonstrate that ARFlow not only\noutperforms existing methods in terms of Fr\\'echet Inception Distance and\nmotion diversity but also significantly reduces body collisions, as measured by\nour new Intersection Volume and Intersection Frequency metrics.\n","authors":["Wentao Jiang","Jingya Wang","Haotao Lu","Kaiyang Ji","Baoxiong Jia","Siyuan Huang","Ye Shi"],"pdf_url":"https://arxiv.org/pdf/2503.16973v2.pdf","comment":"Project Page: https://arflow2025.github.io/"},{"id":"http://arxiv.org/abs/2503.20318v1","updated":"2025-03-26T08:36:25Z","published":"2025-03-26T08:36:25Z","title":"EditCLIP: Representation Learning for Image Editing","summary":"  We introduce EditCLIP, a novel representation-learning approach for image\nediting. Our method learns a unified representation of edits by jointly\nencoding an input image and its edited counterpart, effectively capturing their\ntransformation. To evaluate its effectiveness, we employ EditCLIP to solve two\ntasks: exemplar-based image editing and automated edit evaluation. In\nexemplar-based image editing, we replace text-based instructions in\nInstructPix2Pix with EditCLIP embeddings computed from a reference exemplar\nimage pair. Experiments demonstrate that our approach outperforms\nstate-of-the-art methods while being more efficient and versatile. For\nautomated evaluation, EditCLIP assesses image edits by measuring the similarity\nbetween the EditCLIP embedding of a given image pair and either a textual\nediting instruction or the EditCLIP embedding of another reference image pair.\nExperiments show that EditCLIP aligns more closely with human judgments than\nexisting CLIP-based metrics, providing a reliable measure of edit quality and\nstructural preservation.\n","authors":["Qian Wang","Aleksandar Cvejic","Abdelrahman Eldesokey","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2503.20318v1.pdf","comment":"Project page: https://qianwangx.github.io/EditCLIP/"},{"id":"http://arxiv.org/abs/2408.06072v3","updated":"2025-03-26T08:33:10Z","published":"2024-08-12T11:47:11Z","title":"CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer","summary":"  We present CogVideoX, a large-scale text-to-video generation model based on\ndiffusion transformer, which can generate 10-second continuous videos aligned\nwith text prompt, with a frame rate of 16 fps and resolution of 768 * 1360\npixels. Previous video generation models often had limited movement and short\ndurations, and is difficult to generate videos with coherent narratives based\non text. We propose several designs to address these issues. First, we propose\na 3D Variational Autoencoder (VAE) to compress videos along both spatial and\ntemporal dimensions, to improve both compression rate and video fidelity.\nSecond, to improve the text-video alignment, we propose an expert transformer\nwith the expert adaptive LayerNorm to facilitate the deep fusion between the\ntwo modalities. Third, by employing a progressive training and multi-resolution\nframe pack technique, CogVideoX is adept at producing coherent, long-duration,\ndifferent shape videos characterized by significant motions. In addition, we\ndevelop an effective text-video data processing pipeline that includes various\ndata preprocessing strategies and a video captioning method, greatly\ncontributing to the generation quality and semantic alignment. Results show\nthat CogVideoX demonstrates state-of-the-art performance across both multiple\nmachine metrics and human evaluations. The model weight of both 3D Causal VAE,\nVideo caption model and CogVideoX are publicly available at\nhttps://github.com/THUDM/CogVideo.\n","authors":["Zhuoyi Yang","Jiayan Teng","Wendi Zheng","Ming Ding","Shiyu Huang","Jiazheng Xu","Yuanming Yang","Wenyi Hong","Xiaohan Zhang","Guanyu Feng","Da Yin","Yuxuan Zhang","Weihan Wang","Yean Cheng","Bin Xu","Xiaotao Gu","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2408.06072v3.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2503.20316v1","updated":"2025-03-26T08:33:03Z","published":"2025-03-26T08:33:03Z","title":"AI-Driven MRI Spine Pathology Detection: A Comprehensive Deep Learning\n  Approach for Automated Diagnosis in Diverse Clinical Settings","summary":"  Study Design This study presents the development of an autonomous AI system\nfor MRI spine pathology detection, trained on a dataset of 2 million MRI spine\nscans sourced from diverse healthcare facilities across India. The AI system\nintegrates advanced architectures, including Vision Transformers, U-Net with\ncross-attention, MedSAM, and Cascade R-CNN, enabling comprehensive\nclassification, segmentation, and detection of 43 distinct spinal pathologies.\nThe dataset is balanced across age groups, genders, and scanner manufacturers\nto ensure robustness and adaptability. Subgroup analyses were conducted to\nvalidate the model's performance across different patient demographics, imaging\nconditions, and equipment types.\n  Performance The AI system achieved up to 97.9 percent multi-pathology\ndetection, demonstrating consistent performance across age, gender, and\nmanufacturer subgroups. The normal vs. abnormal classification achieved 98.0\npercent accuracy, and the system was deployed across 13 major healthcare\nenterprises in India, encompassing diagnostic centers, large hospitals, and\ngovernment facilities. During deployment, it processed approximately 100,000\nplus MRI spine scans, leading to reduced reporting times and increased\ndiagnostic efficiency by automating the identification of common spinal\nconditions.\n  Conclusion The AI system's high precision and recall validate its capability\nas a reliable tool for autonomous normal/abnormal classification, pathology\nsegmentation, and detection. Its scalability and adaptability address critical\ndiagnostic gaps, optimize radiology workflows, and improve patient care across\nvaried healthcare environments in India.\n","authors":["Bargava Subramanian","Naveen Kumarasami","Praveen Shastry","Raghotham Sripadraj","Kalyan Sivasailam","Anandakumar D","Abinaya Ramachandran","Sudhir MP","Gunakutti G","Kishore Prasath Venkatesh"],"pdf_url":"https://arxiv.org/pdf/2503.20316v1.pdf","comment":"20 pages , 3 figurea"},{"id":"http://arxiv.org/abs/2412.18951v2","updated":"2025-03-26T08:29:03Z","published":"2024-12-25T17:31:54Z","title":"TopoBDA: Towards Bezier Deformable Attention for Road Topology\n  Understanding","summary":"  Understanding road topology is crucial for autonomous driving. This paper\nintroduces TopoBDA (Topology with Bezier Deformable Attention), a novel\napproach that enhances road topology comprehension by leveraging Bezier\nDeformable Attention (BDA). TopoBDA processes multi-camera 360-degree imagery\nto generate Bird's Eye View (BEV) features, which are refined through a\ntransformer decoder employing BDA. BDA utilizes Bezier control points to drive\nthe deformable attention mechanism, improving the detection and representation\nof elongated and thin polyline structures, such as lane centerlines.\nAdditionally, TopoBDA integrates two auxiliary components: an instance mask\nformulation loss and a one-to-many set prediction loss strategy, to further\nrefine centerline detection and enhance road topology understanding.\nExperimental evaluations on the OpenLane-V2 dataset demonstrate that TopoBDA\noutperforms existing methods, achieving state-of-the-art results in centerline\ndetection and topology reasoning. TopoBDA also achieves the best results on the\nOpenLane-V1 dataset in 3D lane detection. Further experiments on integrating\nmulti-modal data -- such as LiDAR, radar, and SDMap -- show that multimodal\ninputs can further enhance performance in road topology understanding.\n","authors":["Muhammet Esat Kalfaoglu","Halil Ibrahim Ozturk","Ozsel Kilinc","Alptekin Temizel"],"pdf_url":"https://arxiv.org/pdf/2412.18951v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20315v1","updated":"2025-03-26T08:28:28Z","published":"2025-03-26T08:28:28Z","title":"SpikeDerain: Unveiling Clear Videos from Rainy Sequences Using Color\n  Spike Streams","summary":"  Restoring clear frames from rainy videos presents a significant challenge due\nto the rapid motion of rain streaks. Traditional frame-based visual sensors,\nwhich capture scene content synchronously, struggle to capture the fast-moving\ndetails of rain accurately. In recent years, neuromorphic sensors have\nintroduced a new paradigm for dynamic scene perception, offering microsecond\ntemporal resolution and high dynamic range. However, existing multimodal\nmethods that fuse event streams with RGB images face difficulties in handling\nthe complex spatiotemporal interference of raindrops in real scenes, primarily\ndue to hardware synchronization errors and computational redundancy. In this\npaper, we propose a Color Spike Stream Deraining Network (SpikeDerain), capable\nof reconstructing spike streams of dynamic scenes and accurately removing rain\nstreaks. To address the challenges of data scarcity in real continuous rainfall\nscenes, we design a physically interpretable rain streak synthesis model that\ngenerates parameterized continuous rain patterns based on arbitrary background\nimages. Experimental results demonstrate that the network, trained with this\nsynthetic data, remains highly robust even under extreme rainfall conditions.\nThese findings highlight the effectiveness and robustness of our method across\nvarying rainfall levels and datasets, setting new standards for video deraining\ntasks. The code will be released soon.\n","authors":["Hanwen Liang","Xian Zhong","Wenxuan Liu","Yajing Zheng","Wenxin Huang","Zhaofei Yu","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2503.20315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04847v3","updated":"2025-03-26T08:27:17Z","published":"2025-02-07T11:36:36Z","title":"HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion\n  Video Generation","summary":"  Human motion video generation has advanced significantly, while existing\nmethods still struggle with accurately rendering detailed body parts like hands\nand faces, especially in long sequences and intricate motions. Current\napproaches also rely on fixed resolution and struggle to maintain visual\nconsistency. To address these limitations, we propose HumanDiT, a pose-guided\nDiffusion Transformer (DiT)-based framework trained on a large and wild dataset\ncontaining 14,000 hours of high-quality video to produce high-fidelity videos\nwith fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,\nsupports numerous video resolutions and variable sequence lengths, facilitating\nlearning for long-sequence video generation; (ii) we introduce a prefix-latent\nreference strategy to maintain personalized characteristics across extended\nsequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to\ngenerate subsequent pose sequences, facilitating video continuation from static\nimages or existing videos. It also utilizes a Pose Adapter to enable pose\ntransfer with given sequences. Extensive experiments demonstrate its superior\nperformance in generating long-form, pose-accurate videos across diverse\nscenarios.\n","authors":["Qijun Gan","Yi Ren","Chen Zhang","Zhenhui Ye","Pan Xie","Xiang Yin","Zehuan Yuan","Bingyue Peng","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.04847v3.pdf","comment":"https://agnjason.github.io/HumanDiT-page/"},{"id":"http://arxiv.org/abs/2503.20314v1","updated":"2025-03-26T08:25:43Z","published":"2025-03-26T08:25:43Z","title":"Wan: Open and Advanced Large-Scale Video Generative Models","summary":"  This report presents Wan, a comprehensive and open suite of video foundation\nmodels designed to push the boundaries of video generation. Built upon the\nmainstream diffusion transformer paradigm, Wan achieves significant\nadvancements in generative capabilities through a series of innovations,\nincluding our novel VAE, scalable pre-training strategies, large-scale data\ncuration, and automated evaluation metrics. These contributions collectively\nenhance the model's performance and versatility. Specifically, Wan is\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\ntrained on a vast dataset comprising billions of images and videos,\ndemonstrates the scaling laws of video generation with respect to both data and\nmodel size. It consistently outperforms the existing open-source models as well\nas state-of-the-art commercial solutions across multiple internal and external\nbenchmarks, demonstrating a clear and significant performance superiority.\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\nparameters, for efficiency and effectiveness respectively. It also covers\nmultiple downstream applications, including image-to-video, instruction-guided\nvideo editing, and personal video generation, encompassing up to eight tasks.\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\nincluding source code and all models, with the goal of fostering the growth of\nthe video generation community. This openness seeks to significantly expand the\ncreative possibilities of video production in the industry and provide academia\nwith high-quality video foundation models. All the code and models are\navailable at https://github.com/Wan-Video/Wan2.1.\n","authors":[" WanTeam"," :","Ang Wang","Baole Ai","Bin Wen","Chaojie Mao","Chen-Wei Xie","Di Chen","Feiwu Yu","Haiming Zhao","Jianxiao Yang","Jianyuan Zeng","Jiayu Wang","Jingfeng Zhang","Jingren Zhou","Jinkai Wang","Jixuan Chen","Kai Zhu","Kang Zhao","Keyu Yan","Lianghua Huang","Mengyang Feng","Ningyi Zhang","Pandeng Li","Pingyu Wu","Ruihang Chu","Ruili Feng","Shiwei Zhang","Siyang Sun","Tao Fang","Tianxing Wang","Tianyi Gui","Tingyu Weng","Tong Shen","Wei Lin","Wei Wang","Wei Wang","Wenmeng Zhou","Wente Wang","Wenting Shen","Wenyuan Yu","Xianzhong Shi","Xiaoming Huang","Xin Xu","Yan Kou","Yangyu Lv","Yifei Li","Yijing Liu","Yiming Wang","Yingya Zhang","Yitong Huang","Yong Li","You Wu","Yu Liu","Yulin Pan","Yun Zheng","Yuntao Hong","Yupeng Shi","Yutong Feng","Zeyinzi Jiang","Zhen Han","Zhi-Fan Wu","Ziyu Liu"],"pdf_url":"https://arxiv.org/pdf/2503.20314v1.pdf","comment":"60 pages, 33 figures"},{"id":"http://arxiv.org/abs/2503.20310v1","updated":"2025-03-26T08:20:17Z","published":"2025-03-26T08:20:17Z","title":"Enabling Heterogeneous Adversarial Transferability via Feature\n  Permutation Attacks","summary":"  Adversarial attacks in black-box settings are highly practical, with\ntransfer-based attacks being the most effective at generating adversarial\nexamples (AEs) that transfer from surrogate models to unseen target models.\nHowever, their performance significantly degrades when transferring across\nheterogeneous architectures -- such as CNNs, MLPs, and Vision Transformers\n(ViTs) -- due to fundamental architectural differences. To address this, we\npropose Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method\nthat enhances adversarial transferability across diverse architectures. FPA\nintroduces a novel feature permutation (FP) operation, which rearranges pixel\nvalues in selected feature maps to simulate long-range dependencies,\neffectively making CNNs behave more like ViTs and MLPs. This enhances feature\ndiversity and improves transferability both across heterogeneous architectures\nand within homogeneous CNNs. Extensive evaluations on 14 state-of-the-art\narchitectures show that FPA achieves maximum absolute gains in attack success\nrates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming\nexisting black-box attacks. Additionally, FPA is highly generalizable and can\nseamlessly integrate with other transfer-based attacks to further boost their\nperformance. Our findings establish FPA as a robust, efficient, and\ncomputationally lightweight strategy for enhancing adversarial transferability\nacross heterogeneous architectures.\n","authors":["Tao Wu","Tie Luo"],"pdf_url":"https://arxiv.org/pdf/2503.20310v1.pdf","comment":"PAKDD 2025. Main Track"},{"id":"http://arxiv.org/abs/2503.20309v1","updated":"2025-03-26T08:19:02Z","published":"2025-03-26T08:19:02Z","title":"Instruction-Oriented Preference Alignment for Enhancing Multi-Modal\n  Comprehension Capability of MLLMs","summary":"  Preference alignment has emerged as an effective strategy to enhance the\nperformance of Multimodal Large Language Models (MLLMs) following supervised\nfine-tuning. While existing preference alignment methods predominantly target\nhallucination factors, they overlook the factors essential for multi-modal\ncomprehension capabilities, often narrowing their improvements on hallucination\nmitigation. To bridge this gap, we propose Instruction-oriented Preference\nAlignment (IPA), a scalable framework designed to automatically construct\nalignment preferences grounded in instruction fulfillment efficacy. Our method\ninvolves an automated preference construction coupled with a dedicated\nverification process that identifies instruction-oriented factors, avoiding\nsignificant variability in response representations. Additionally, IPA\nincorporates a progressive preference collection pipeline, further recalling\nchallenging samples through model self-evolution and reference-guided\nrefinement. Experiments conducted on Qwen2VL-7B demonstrate IPA's effectiveness\nacross multiple benchmarks, including hallucination evaluation, visual question\nanswering, and text understanding tasks, highlighting its capability to enhance\ngeneral comprehension.\n","authors":["Zitian Wang","Yue Liao","Kang Rong","Fengyun Rao","Yibo Yang","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2503.20309v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2503.20308v1","updated":"2025-03-26T08:18:57Z","published":"2025-03-26T08:18:57Z","title":"Perceptually Accurate 3D Talking Head Generation: New Definitions,\n  Speech-Mesh Representation, and Evaluation Metrics","summary":"  Recent advancements in speech-driven 3D talking head generation have made\nsignificant progress in lip synchronization. However, existing models still\nstruggle to capture the perceptual alignment between varying speech\ncharacteristics and corresponding lip movements. In this work, we claim that\nthree criteria -- Temporal Synchronization, Lip Readability, and Expressiveness\n-- are crucial for achieving perceptually accurate lip movements. Motivated by\nour hypothesis that a desirable representation space exists to meet these three\ncriteria, we introduce a speech-mesh synchronized representation that captures\nintricate correspondences between speech signals and 3D face meshes. We found\nthat our learned representation exhibits desirable characteristics, and we plug\nit into existing models as a perceptual loss to better align lip movements to\nthe given speech. In addition, we utilize this representation as a perceptual\nmetric and introduce two other physically grounded lip synchronization metrics\nto assess how well the generated 3D talking heads align with these three\ncriteria. Experiments show that training 3D talking head generation models with\nour perceptual loss significantly improve all three aspects of perceptually\naccurate lip synchronization. Codes and datasets are available at\nhttps://perceptual-3d-talking-head.github.io/.\n","authors":["Lee Chae-Yeon","Oh Hyun-Bin","Han EunGi","Kim Sung-Bin","Suekyeong Nam","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2503.20308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20306v1","updated":"2025-03-26T08:10:29Z","published":"2025-03-26T08:10:29Z","title":"3D Convolutional Neural Networks for Improved Detection of Intracranial\n  bleeding in CT Imaging","summary":"  Background: Intracranial bleeding (IB) is a life-threatening condition caused\nby traumatic brain injuries, including epidural, subdural, subarachnoid, and\nintraparenchymal hemorrhages. Rapid and accurate detection is crucial to\nprevent severe complications. Traditional imaging can be slow and prone to\nvariability, especially in high-pressure scenarios. Artificial Intelligence\n(AI) provides a solution by quickly analyzing medical images, identifying\nsubtle hemorrhages, and flagging urgent cases. By enhancing diagnostic speed\nand accuracy, AI improves workflows and patient care. This article explores\nAI's role in transforming IB detection in emergency settings.\n  Methods: A U-shaped 3D Convolutional Neural Network (CNN) automates IB\ndetection and classification in volumetric CT scans. Advanced preprocessing,\nincluding CLAHE and intensity normalization, enhances image quality. The\narchitecture preserves spatial and contextual details for precise segmentation.\nA dataset of 2,912 annotated CT scans was used for training and evaluation.\n  Results: The model achieved high performance across major bleed types, with\nprecision, recall, and accuracy exceeding 90 percent in most cases 96 percent\nprecision for epidural hemorrhages and 94 percent accuracy for subarachnoid\nhemorrhages. Its ability to classify and localize hemorrhages highlights its\nclinical reliability.\n  Conclusion: This U-shaped 3D CNN offers a scalable solution for automating IB\ndetection, reducing diagnostic delays, and improving emergency care outcomes.\nFuture work will expand dataset diversity, optimize real-time processing, and\nintegrate multimodal data for enhanced clinical applicability.\n","authors":["Bargava Subramanian","Naveen Kumarasami","Praveen Shastry","Kalyan Sivasailam","Anandakumar D","Elakkiya R","Harsha KG","Rithanya V","Harini T","Afshin Hussain","Kishore Prasath Venkatesh"],"pdf_url":"https://arxiv.org/pdf/2503.20306v1.pdf","comment":"12 pages,4 figures"},{"id":"http://arxiv.org/abs/2503.20301v1","updated":"2025-03-26T07:59:04Z","published":"2025-03-26T07:59:04Z","title":"Attribute-formed Class-specific Concept Space: Endowing Language\n  Bottleneck Model with Better Interpretability and Scalability","summary":"  Language Bottleneck Models (LBMs) are proposed to achieve interpretable image\nrecognition by classifying images based on textual concept bottlenecks.\nHowever, current LBMs simply list all concepts together as the bottleneck\nlayer, leading to the spurious cue inference problem and cannot generalized to\nunseen classes. To address these limitations, we propose the Attribute-formed\nLanguage Bottleneck Model (ALBM). ALBM organizes concepts in the\nattribute-formed class-specific space, where concepts are descriptions of\nspecific attributes for specific classes. In this way, ALBM can avoid the\nspurious cue inference problem by classifying solely based on the essential\nconcepts of each class. In addition, the cross-class unified attribute set also\nensures that the concept spaces of different classes have strong correlations,\nas a result, the learned concept classifier can be easily generalized to unseen\nclasses. Moreover, to further improve interpretability, we propose Visual\nAttribute Prompt Learning (VAPL) to extract visual features on fine-grained\nattributes. Furthermore, to avoid labor-intensive concept annotation, we\npropose the Description, Summary, and Supplement (DSS) strategy to\nautomatically generate high-quality concept sets with a complete and precise\nattribute. Extensive experiments on 9 widely used few-shot benchmarks\ndemonstrate the interpretability, transferability, and performance of our\napproach. The code and collected concept sets are available at\nhttps://github.com/tiggers23/ALBM.\n","authors":["Jianyang Zhang","Qianli Luo","Guowu Yang","Wenjing Yang","Weide Liu","Guosheng Lin","Fengmao Lv"],"pdf_url":"https://arxiv.org/pdf/2503.20301v1.pdf","comment":"This paper has been accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2404.15190v2","updated":"2025-03-26T07:42:56Z","published":"2024-04-21T08:10:20Z","title":"Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied\n  Instruction Following","summary":"  Embodied Instruction Following (EIF) is the task of executing natural\nlanguage instructions by navigating and interacting with objects in interactive\nenvironments. A key challenge in EIF is compositional task planning, typically\naddressed through supervised learning or few-shot in-context learning with\nlabeled data. To this end, we introduce the Socratic Planner, a self-QA-based\nzero-shot planning method that infers an appropriate plan without any further\ntraining. The Socratic Planner first facilitates self-questioning and answering\nby the Large Language Model (LLM), which in turn helps generate a sequence of\nsubgoals. While executing the subgoals, an embodied agent may encounter\nunexpected situations, such as unforeseen obstacles. The Socratic Planner then\nadjusts plans based on dense visual feedback through a visually-grounded\nre-planning mechanism. Experiments demonstrate the effectiveness of the\nSocratic Planner, outperforming current state-of-the-art planning models on the\nALFRED benchmark across all metrics, particularly excelling in long-horizon\ntasks that demand complex inference. We further demonstrate its real-world\napplicability through deployment on a physical robot for long-horizon tasks.\n","authors":["Suyeon Shin","Sujin jeon","Junghyun Kim","Gi-Cheon Kang","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.15190v2.pdf","comment":"8 pages, 6 figures, published to ICRA 2025"},{"id":"http://arxiv.org/abs/2503.20297v1","updated":"2025-03-26T07:37:53Z","published":"2025-03-26T07:37:53Z","title":"Traversing Distortion-Perception Tradeoff using a Single Score-Based\n  Generative Model","summary":"  The distortion-perception (DP) tradeoff reveals a fundamental conflict\nbetween distortion metrics (e.g., MSE and PSNR) and perceptual quality. Recent\nresearch has increasingly concentrated on evaluating denoising algorithms\nwithin the DP framework. However, existing algorithms either prioritize\nperceptual quality by sacrificing acceptable distortion, or focus on minimizing\nMSE for faithful restoration. When the goal shifts or noisy measurements vary,\nadapting to different points on the DP plane needs retraining or even\nre-designing the model. Inspired by recent advances in solving inverse problems\nusing score-based generative models, we explore the potential of flexibly and\noptimally traversing DP tradeoffs using a single pre-trained score-based model.\nSpecifically, we introduce a variance-scaled reverse diffusion process and\ntheoretically characterize the marginal distribution. We then prove that the\nproposed sample process is an optimal solution to the DP tradeoff for\nconditional Gaussian distribution. Experimental results on two-dimensional and\nimage datasets illustrate that a single score network can effectively and\nflexibly traverse the DP tradeoff for general denoising problems.\n","authors":["Yuhan Wang","Suzhi Bi","Ying-Jun Angela Zhang","Xiaojun Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.20297v1.pdf","comment":"Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2025"},{"id":"http://arxiv.org/abs/2503.19523v2","updated":"2025-03-26T07:37:33Z","published":"2025-03-25T10:23:26Z","title":"One Framework to Rule Them All: Unifying RL-Based and RL-Free Methods in\n  RLHF","summary":"  In this article, we primarily examine a variety of RL-based and RL-free\nmethods designed to address Reinforcement Learning from Human Feedback (RLHF)\nand Large Reasoning Models (LRMs). We begin with a concise overview of the\ntypical steps involved in RLHF and LRMs. Next, we reinterpret several RL-based\nand RL-free algorithms through the perspective of neural structured bandit\nprediction, providing a clear conceptual framework that uncovers a deeper\nconnection between these seemingly distinct approaches. Following this, we\nbriefly review some core principles of reinforcement learning, drawing\nattention to an often-overlooked aspect in existing RLHF studies. This leads to\na detailed derivation of the standard RLHF objective within a full RL context,\ndemonstrating its equivalence to neural structured bandit prediction. Finally,\nby reinvestigating the principles behind Proximal Policy Optimization (PPO), we\npinpoint areas needing adjustment, which culminates in the introduction of the\nGeneralized Reinforce Optimization (GRO) framework, seamlessly integrating\nRL-based and RL-free methods in RLHF. We look forward to the community's\nefforts to empirically validate GRO and invite constructive feedback.\n","authors":["Xin Cai"],"pdf_url":"https://arxiv.org/pdf/2503.19523v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08497v2","updated":"2025-03-26T07:35:10Z","published":"2025-03-11T14:48:01Z","title":"MMRL: Multi-Modal Representation Learning for Vision-Language Models","summary":"  Large-scale pre-trained Vision-Language Models (VLMs) have become essential\nfor transfer learning across diverse tasks. However, adapting these models with\nlimited few-shot data often leads to overfitting, diminishing their performance\non new tasks. To tackle this issue, we propose a novel Multi-Modal\nRepresentation Learning (MMRL) framework that introduces a shared, learnable,\nand modality-agnostic representation space. MMRL projects the space tokens to\ntext and image representation tokens, facilitating more effective multi-modal\ninteractions. Unlike previous approaches that solely optimize class token\nfeatures, MMRL integrates representation tokens at higher layers of the\nencoders--where dataset-specific features are more prominent--while preserving\ngeneralized knowledge in the lower layers. During training, both representation\nand class features are optimized, with trainable projection layer applied to\nthe representation tokens, whereas the class token projection layer remains\nfrozen to retain pre-trained knowledge. Furthermore, a regularization term is\nintroduced to align the class features and text features with the zero-shot\nfeatures from the frozen VLM, thereby safeguarding the model's generalization\ncapacity. For inference, a decoupling strategy is employed, wherein both\nrepresentation and class features are utilized for base classes, while only the\nclass features, which retain more generalized knowledge, are used for new\ntasks. Extensive experiments across 15 datasets demonstrate that MMRL\noutperforms state-of-the-art methods, achieving a balanced trade-off between\ntask-specific adaptation and generalization. Code is available at\nhttps://github.com/yunncheng/MMRL.\n","authors":["Yuncheng Guo","Xiaodong Gu"],"pdf_url":"https://arxiv.org/pdf/2503.08497v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.20294v1","updated":"2025-03-26T07:35:09Z","published":"2025-03-26T07:35:09Z","title":"Context-Aware Weakly Supervised Image Manipulation Localization with SAM\n  Refinement","summary":"  Malicious image manipulation poses societal risks, increasing the importance\nof effective image manipulation detection methods. Recent approaches in image\nmanipulation detection have largely been driven by fully supervised approaches,\nwhich require labor-intensive pixel-level annotations. Thus, it is essential to\nexplore weakly supervised image manipulation localization methods that only\nrequire image-level binary labels for training. However, existing weakly\nsupervised image manipulation methods overlook the importance of edge\ninformation for accurate localization, leading to suboptimal localization\nperformance. To address this, we propose a Context-Aware Boundary Localization\n(CABL) module to aggregate boundary features and learn context-inconsistency\nfor localizing manipulated areas. Furthermore, by leveraging Class Activation\nMapping (CAM) and Segment Anything Model (SAM), we introduce the CAM-Guided SAM\nRefinement (CGSR) module to generate more accurate manipulation localization\nmaps. By integrating two modules, we present a novel weakly supervised\nframework based on a dual-branch Transformer-CNN architecture. Our method\nachieves outstanding localization performance across multiple datasets.\n","authors":["Xinghao Wang","Changtao Miao","Dianmo Sheng","Tao Gong","Qi Chu","Bin Liu","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2503.20294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20291v1","updated":"2025-03-26T07:33:36Z","published":"2025-03-26T07:33:36Z","title":"CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at\n  Intermediate Resolution with Structure-Aware Multimodal U-Nets","summary":"  Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at\nintermediate resolution (4-8 {\\AA}) is crucial in protein structure\ndetermination. Recent advances in deep learning have led to the development of\nautomated approaches for enhancing experimental cryo-EM density maps. Yet,\nthese methods are not optimized for intermediate-resolution maps and rely on\nmap density features alone. To address this, we propose CryoSAMU, a novel\nmethod designed to enhance 3D cryo-EM density maps of protein structures using\nstructure-aware multimodal U-Nets and trained on curated\nintermediate-resolution density maps. We comprehensively evaluate CryoSAMU\nacross various metrics and demonstrate its competitive performance compared to\nstate-of-the-art methods. Notably, CryoSAMU achieves significantly faster\nprocessing speed, showing promise for future practical applications. Our code\nis available at https://github.com/chenwei-zhang/CryoSAMU.\n","authors":["Chenwei Zhang","Anne Condon","Khanh Dao Duc"],"pdf_url":"https://arxiv.org/pdf/2503.20291v1.pdf","comment":"18 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4\n  supplementary tables"},{"id":"http://arxiv.org/abs/2503.20289v1","updated":"2025-03-26T07:31:52Z","published":"2025-03-26T07:31:52Z","title":"RelTriple: Learning Plausible Indoor Layouts by Integrating Relationship\n  Triples into the Diffusion Process","summary":"  The generation of indoor furniture layouts has significant applications in\naugmented reality, smart homes, and architectural design. Successful furniture\narrangement requires proper physical relationships (e.g., collision avoidance)\nand spacing relationships between furniture and their functional zones to be\nrespected. However, manually defined relationships are almost always incomplete\nand can produce unrealistic layouts. This work instead extracts spacing\nrelationships automatically based on a hierarchical analysis and adopts the\nDelaunay Triangulation to produce important triple relationships. Compared to\npairwise relationship modeling, triple relationships account for interactions\nand space utilization among multiple objects. To this end, we introduce\nRelTriple, a novel approach that enhances furniture distribution by learning\nspacing relationships between objects and regions. We formulate triple\nrelationships as object-to-object (O2O) losses and object-to-region (O2R)\nlosses and integrate them directly into the training process of generative\ndiffusion. Our approach consistently improves over existing state-of-the-art\nmethods in visual results evaluation metrics on unconditional layout\ngeneration, floorplan-conditioned layout generation, and scene rearrangement,\nachieving at least 12% on the introduced spatial relationship metric and\nsuperior spatial coherence and practical usability.\n","authors":["Kaifan Sun","Bingchen Yang","Peter Wonka","Jun Xiao","Haiyong Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.20289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20287v1","updated":"2025-03-26T07:30:58Z","published":"2025-03-26T07:30:58Z","title":"InsViE-1M: Effective Instruction-based Video Editing with Elaborate\n  Dataset Construction","summary":"  Instruction-based video editing allows effective and interactive editing of\nvideos using only instructions without extra inputs such as masks or\nattributes. However, collecting high-quality training triplets (source video,\nedited video, instruction) is a challenging task. Existing datasets mostly\nconsist of low-resolution, short duration, and limited amount of source videos\nwith unsatisfactory editing quality, limiting the performance of trained\nediting models. In this work, we present a high-quality Instruction-based Video\nEditing dataset with 1M triplets, namely InsViE-1M. We first curate\nhigh-resolution and high-quality source videos and images, then design an\neffective editing-filtering pipeline to construct high-quality editing triplets\nfor model training. For a source video, we generate multiple edited samples of\nits first frame with different intensities of classifier-free guidance, which\nare automatically filtered by GPT-4o with carefully crafted guidelines. The\nedited first frame is propagated to subsequent frames to produce the edited\nvideo, followed by another round of filtering for frame quality and motion\nevaluation. We also generate and filter a variety of video editing triplets\nfrom high-quality images. With the InsViE-1M dataset, we propose a multi-stage\nlearning strategy to train our InsViE model, progressively enhancing its\ninstruction following and editing ability. Extensive experiments demonstrate\nthe advantages of our InsViE-1M dataset and the trained model over\nstate-of-the-art works. Codes are available at InsViE.\n","authors":["Yuhui Wu","Liyi Chen","Ruibin Li","Shihao Wang","Chenxi Xie","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16537v3","updated":"2025-03-26T07:30:26Z","published":"2024-11-25T16:21:34Z","title":"RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language\n  Models for Robotics","summary":"  Spatial understanding is a crucial capability that enables robots to perceive\ntheir surroundings, reason about their environment, and interact with it\nmeaningfully. In modern robotics, these capabilities are increasingly provided\nby vision-language models. However, these models face significant challenges in\nspatial reasoning tasks, as their training data are based on general-purpose\nimage datasets that often lack sophisticated spatial understanding. For\nexample, datasets frequently do not capture reference frame comprehension, yet\neffective spatial reasoning requires understanding whether to reason from ego-,\nworld-, or object-centric perspectives. To address this issue, we introduce\nRoboSpatial, a large-scale dataset for spatial understanding in robotics. It\nconsists of real indoor and tabletop scenes, captured as 3D scans and\negocentric images, and annotated with rich spatial information relevant to\nrobotics. The dataset includes 1M images, 5k 3D scans, and 3M annotated spatial\nrelationships, and the pairing of 2D egocentric images with 3D scans makes it\nboth 2D- and 3D- ready. Our experiments show that models trained with\nRoboSpatial outperform baselines on downstream tasks such as spatial affordance\nprediction, spatial relationship prediction, and robot manipulation.\n","authors":["Chan Hee Song","Valts Blukis","Jonathan Tremblay","Stephen Tyree","Yu Su","Stan Birchfield"],"pdf_url":"https://arxiv.org/pdf/2411.16537v3.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2411.16425v2","updated":"2025-03-26T07:26:43Z","published":"2024-11-25T14:27:55Z","title":"TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for\n  Zero-shot Object Navigation","summary":"  The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find\na previously unseen object by navigating in unfamiliar environments. Such a\ngoal-oriented exploration heavily relies on the ability to perceive,\nunderstand, and reason based on the spatial information of the environment.\nHowever, current LLM-based approaches convert visual observations to language\ndescriptions and reason in the linguistic space, leading to the loss of spatial\ninformation. In this paper, we introduce TopV-Nav, an MLLM-based method that\ndirectly reasons on the top-view map with sufficient spatial information. To\nfully unlock the MLLM's spatial reasoning potential in top-view perspective, we\npropose the Adaptive Visual Prompt Generation (AVPG) method to adaptively\nconstruct semantically-rich top-view map. It enables the agent to directly\nutilize spatial information contained in the top-view map to conduct thorough\nreasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to\ndynamically zoom top-view map at preferred scales, enhancing local fine-grained\nreasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to\npredict and to utilize target locations, facilitating global and human-like\nexploration. Experiments on MP3D and HM3D datasets demonstrate the superiority\nof our TopV-Nav.\n","authors":["Linqing Zhong","Chen Gao","Zihan Ding","Yue Liao","Huimin Ma","Shifeng Zhang","Xu Zhou","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16425v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2406.09166v3","updated":"2025-03-26T07:15:23Z","published":"2024-06-13T14:27:53Z","title":"Fine-Grained Domain Generalization with Feature Structuralization","summary":"  Fine-grained domain generalization (FGDG) is a more challenging task than\ntraditional DG tasks due to its small inter-class variations and relatively\nlarge intra-class disparities. When domain distribution changes, the\nvulnerability of subtle features leads to a severe deterioration in model\nperformance. Nevertheless, humans inherently demonstrate the capacity for\ngeneralizing to out-of-distribution data, leveraging structured\nmulti-granularity knowledge that emerges from discerning the commonality and\nspecificity within categories. Likewise, we propose a Feature Structuralized\nDomain Generalization (FSDG) model, wherein features experience\nstructuralization into common, specific, and confounding segments, harmoniously\naligned with their relevant semantic concepts, to elevate performance in FGDG.\nSpecifically, feature structuralization (FS) is accomplished through joint\noptimization of five constraints: a decorrelation function applied to\ndisentangled segments, three constraints ensuring common feature consistency\nand specific feature distinctiveness, and a prediction calibration term. By\nimposing these stipulations, FSDG is prompted to disentangle and align features\nbased on multi-granularity knowledge, facilitating robust subtle distinctions\namong categories. Extensive experimentation on three benchmarks consistently\nvalidates the superiority of FSDG over state-of-the-art counterparts, with an\naverage improvement of 6.2% in FGDG performance. Beyond that, the\nexplainability analysis on explicit concept matching intensity between the\nshared concepts among categories and the model channels, along with experiments\non various mainstream model architectures, substantiates the validity of FS.\n","authors":["Wenlong Yu","Dongyue Chen","Qilong Wang","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2406.09166v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20282v1","updated":"2025-03-26T07:15:08Z","published":"2025-03-26T07:15:08Z","title":"Faster Parameter-Efficient Tuning with Token Redundancy Reduction","summary":"  Parameter-efficient tuning (PET) aims to transfer pre-trained foundation\nmodels to downstream tasks by learning a small number of parameters. Compared\nto traditional fine-tuning, which updates the entire model, PET significantly\nreduces storage and transfer costs for each task regardless of exponentially\nincreasing pre-trained model capacity. However, most PET methods inherit the\ninference latency of their large backbone models and often introduce additional\ncomputational overhead due to additional modules (e.g. adapters), limiting\ntheir practicality for compute-intensive applications. In this paper, we\npropose Faster Parameter-Efficient Tuning (FPET), a novel approach that\nenhances inference speed and training efficiency while maintaining high storage\nefficiency. Specifically, we introduce a plug-and-play token redundancy\nreduction module delicately designed for PET. This module refines tokens from\nthe self-attention layer using an adapter to learn the accurate similarity\nbetween tokens and cuts off the tokens through a fully-differentiable token\nmerging strategy, which uses a straight-through estimator for optimal token\nreduction. Experimental results prove that our FPET achieves faster inference\nand higher memory efficiency than the pre-trained backbone while keeping\ncompetitive performance on par with state-of-the-art PET methods.\n","authors":["Kwonyoung Kim","Jungin Park","Jin Kim","Hyeongjun Kwon","Kwanghoon Sohn"],"pdf_url":"https://arxiv.org/pdf/2503.20282v1.pdf","comment":"CVPR 2025 Camera-ready"},{"id":"http://arxiv.org/abs/2502.20087v2","updated":"2025-03-26T07:10:07Z","published":"2025-02-27T13:45:15Z","title":"OverLoCK: An Overview-first-Look-Closely-next ConvNet with\n  Context-Mixing Dynamic Kernels","summary":"  Top-down attention plays a crucial role in the human vision system, wherein\nthe brain initially obtains a rough overview of a scene to discover salient\ncues (i.e., overview first), followed by a more careful finer-grained\nexamination (i.e., look closely next). However, modern ConvNets remain confined\nto a pyramid structure that successively downsamples the feature map for\nreceptive field expansion, neglecting this crucial biomimetic principle. We\npresent OverLoCK, the first pure ConvNet backbone architecture that explicitly\nincorporates a top-down attention mechanism. Unlike pyramid backbone networks,\nour design features a branched architecture with three synergistic\nsub-networks: 1) a Base-Net that encodes low/mid-level features; 2) a\nlightweight Overview-Net that generates dynamic top-down attention through\ncoarse global context modeling (i.e., overview first); and 3) a robust\nFocus-Net that performs finer-grained perception guided by top-down attention\n(i.e., look closely next). To fully unleash the power of top-down attention, we\nfurther propose a novel context-mixing dynamic convolution (ContMix) that\neffectively models long-range dependencies while preserving inherent local\ninductive biases even when the input resolution increases, addressing critical\nlimitations in existing convolutions. Our OverLoCK exhibits a notable\nperformance improvement over existing methods. For instance, OverLoCK-T\nachieves a Top-1 accuracy of 84.2%, significantly surpassing ConvNeXt-B while\nusing only around one-third of the FLOPs/parameters. On object detection, our\nOverLoCK-S clearly surpasses MogaNet-B by 1% in AP^b. On semantic segmentation,\nour OverLoCK-T remarkably improves UniRepLKNet-T by 1.7% in mIoU. Code is\npublicly available at https://rb.gy/wit4jh.\n","authors":["Meng Lou","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2502.20087v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.19739v2","updated":"2025-03-26T06:54:19Z","published":"2025-03-25T15:04:53Z","title":"FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via\n  Frequency-Decoupled Alignment and Degradation-Robust Fusion","summary":"  Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth. Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE\n","authors":["Pihai Sun","Junjun Jiang","Yuanqi Yao","Youyu Chen","Wenbo Zhao","Kui Jiang","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2503.19739v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.13726v3","updated":"2025-03-26T06:38:56Z","published":"2024-10-17T16:32:36Z","title":"DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework\n  for Talking Head Video Generation","summary":"  Talking head generation intends to produce vivid and realistic talking head\nvideos from a single portrait and speech audio clip. Although significant\nprogress has been made in diffusion-based talking head generation, almost all\nmethods rely on autoregressive strategies, which suffer from limited context\nutilization beyond the current generation step, error accumulation, and slower\ngeneration speed. To address these challenges, we present DAWN (Dynamic frame\nAvatar With Non-autoregressive diffusion), a framework that enables all-at-once\ngeneration of dynamic-length video sequences. Specifically, it consists of two\nmain components: (1) audio-driven holistic facial dynamics generation in the\nlatent motion space, and (2) audio-driven head pose and blink generation.\nExtensive experiments demonstrate that our method generates authentic and vivid\nvideos with precise lip motions, and natural pose/blink movements.\nAdditionally, with a high generation speed, DAWN possesses strong extrapolation\ncapabilities, ensuring the stable production of high-quality long videos. These\nresults highlight the considerable promise and potential impact of DAWN in the\nfield of talking head video generation. Furthermore, we hope that DAWN sparks\nfurther exploration of non-autoregressive approaches in diffusion models. Our\ncode will be publicly available at https://github.com/Hanbo-Cheng/DAWN-pytorch.\n","authors":["Hanbo Cheng","Limin Lin","Chenyu Liu","Pengcheng Xia","Pengfei Hu","Jiefeng Ma","Jun Du","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2410.13726v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20271v1","updated":"2025-03-26T06:38:31Z","published":"2025-03-26T06:38:31Z","title":"ViLBench: A Suite for Vision-Language Process Reward Modeling","summary":"  Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data.\n","authors":["Haoqin Tu","Weitao Feng","Hardy Chen","Hui Liu","Xianfeng Tang","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2503.20271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18290v3","updated":"2025-03-26T06:37:46Z","published":"2025-02-25T15:28:41Z","title":"Stealthy Backdoor Attack in Self-Supervised Learning Vision Encoders for\n  Large Vision Language Models","summary":"  Self-supervised learning (SSL) vision encoders learn high-quality image\nrepresentations and thus have become a vital part of developing vision modality\nof large vision language models (LVLMs). Due to the high cost of training such\nencoders, pre-trained encoders are widely shared and deployed into many LVLMs,\nwhich are security-critical or bear societal significance. Under this practical\nscenario, we reveal a new backdoor threat that significant visual\nhallucinations can be induced into these LVLMs by merely compromising vision\nencoders. Because of the sharing and reuse of these encoders, many downstream\nLVLMs may inherit backdoor behaviors from encoders, leading to widespread\nbackdoors. In this work, we propose BadVision, the first method to exploit this\nvulnerability in SSL vision encoders for LVLMs with novel trigger optimization\nand backdoor learning techniques. We evaluate BadVision on two types of SSL\nencoders and LVLMs across eight benchmarks. We show that BadVision effectively\ndrives the LVLMs to attacker-chosen hallucination with over 99% attack success\nrate, causing a 77.6% relative visual understanding error while maintaining the\nstealthiness. SoTA backdoor detection methods cannot detect our attack\neffectively.\n","authors":["Zhaoyi Liu","Huan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.18290v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20268v1","updated":"2025-03-26T06:33:32Z","published":"2025-03-26T06:33:32Z","title":"EGVD: Event-Guided Video Diffusion Model for Physically Realistic\n  Large-Motion Frame Interpolation","summary":"  Video frame interpolation (VFI) in scenarios with large motion remains\nchallenging due to motion ambiguity between frames. While event cameras can\ncapture high temporal resolution motion information, existing event-based VFI\nmethods struggle with limited training data and complex motion patterns. In\nthis paper, we introduce Event-Guided Video Diffusion Model (EGVD), a novel\nframework that leverages the powerful priors of pre-trained stable video\ndiffusion models alongside the precise temporal information from event cameras.\nOur approach features a Multi-modal Motion Condition Generator (MMCG) that\neffectively integrates RGB frames and event signals to guide the diffusion\nprocess, producing physically realistic intermediate frames. We employ a\nselective fine-tuning strategy that preserves spatial modeling capabilities\nwhile efficiently incorporating event-guided temporal information. We\nincorporate input-output normalization techniques inspired by recent advances\nin diffusion modeling to enhance training stability across varying noise\nlevels. To improve generalization, we construct a comprehensive dataset\ncombining both real and simulated event data across diverse scenarios.\nExtensive experiments on both real and simulated datasets demonstrate that EGVD\nsignificantly outperforms existing methods in handling large motion and\nchallenging lighting conditions, achieving substantial improvements in\nperceptual quality metrics (27.4% better LPIPS on Prophesee and 24.1% on BSRGB)\nwhile maintaining competitive fidelity measures. Code and datasets available\nat: https://github.com/OpenImagingLab/EGVD.\n","authors":["Ziran Zhang","Xiaohui Li","Yihao Liu","Yujin Wang","Yueting Chen","Tianfan Xue","Shi Guo"],"pdf_url":"https://arxiv.org/pdf/2503.20268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17168v2","updated":"2025-03-26T06:24:56Z","published":"2025-03-21T14:17:02Z","title":"Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based\n  Object Detection Systems for Autonomous Driving","summary":"  Light Detection and Ranging (LiDAR) is an essential sensor technology for\nautonomous driving as it can capture high-resolution 3D data. As 3D object\ndetection systems (OD) can interpret such point cloud data, they play a key\nrole in the driving decisions of autonomous vehicles. Consequently, such 3D OD\nmust be robust against all types of perturbations and must therefore be\nextensively tested. One approach is the use of adversarial examples, which are\nsmall, sometimes sophisticated perturbations in the input data that change,\ni.e., falsify, the prediction of the OD. These perturbations are carefully\ndesigned based on the weaknesses of the OD. The robustness of the OD cannot be\nquantified with adversarial examples in general, because if the OD is\nvulnerable to a given attack, it is unclear whether this is due to the\nrobustness of the OD or whether the attack algorithm produces particularly\nstrong adversarial examples. The contribution of this work is Hi-ALPS --\nHierarchical Adversarial-example-based LiDAR Perturbation Level System, where\nhigher robustness of the OD is required to withstand the perturbations as the\nperturbation levels increase. In doing so, the Hi-ALPS levels successively\nimplement a heuristic followed by established adversarial example approaches.\nIn a series of comprehensive experiments using Hi-ALPS, we quantify the\nrobustness of six state-of-the-art 3D OD under different types of\nperturbations. The results of the experiments show that none of the OD is\nrobust against all Hi-ALPS levels; an important factor for the ranking is that\nhuman observers can still correctly recognize the perturbed objects, as the\nrespective perturbations are small. To increase the robustness of the OD, we\ndiscuss the applicability of state-of-the-art countermeasures. In addition, we\nderive further suggestions for countermeasures based on our experimental\nresults.\n","authors":["Alexandra Arzberger","Ramin Tavakoli Kolagari"],"pdf_url":"https://arxiv.org/pdf/2503.17168v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19443v2","updated":"2025-03-26T06:20:44Z","published":"2025-03-25T08:31:43Z","title":"COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on\n  Boundary-Adaptive Gaussian Splitting","summary":"  Accurate object segmentation is crucial for high-quality scene understanding\nin the 3D vision domain. However, 3D segmentation based on 3D Gaussian\nSplatting (3DGS) struggles with accurately delineating object boundaries, as\nGaussian primitives often span across object edges due to their inherent volume\nand the lack of semantic guidance during training. In order to tackle these\nchallenges, we introduce Clear Object Boundaries for 3DGS Segmentation\n(COB-GS), which aims to improve segmentation accuracy by clearly delineating\nblurry boundaries of interwoven Gaussian primitives within the scene. Unlike\nexisting approaches that remove ambiguous Gaussians and sacrifice visual\nquality, COB-GS, as a 3DGS refinement method, jointly optimizes semantic and\nvisual information, allowing the two different levels to cooperate with each\nother effectively. Specifically, for the semantic guidance, we introduce a\nboundary-adaptive Gaussian splitting technique that leverages semantic gradient\nstatistics to identify and split ambiguous Gaussians, aligning them closely\nwith object boundaries. For the visual optimization, we rectify the degraded\nsuboptimal texture of the 3DGS scene, particularly along the refined boundary\nstructures. Experimental results show that COB-GS substantially improves\nsegmentation accuracy and robustness against inaccurate masks from pre-trained\nmodel, yielding clear boundaries while preserving high visual quality. Code is\navailable at https://github.com/ZestfulJX/COB-GS.\n","authors":["Jiaxin Zhang","Junjun Jiang","Youyu Chen","Kui Jiang","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2503.19443v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.19793v2","updated":"2025-03-26T06:11:10Z","published":"2025-03-25T16:01:37Z","title":"In the Blink of an Eye: Instant Game Map Editing using a Generative-AI\n  Smart Brush","summary":"  With video games steadily increasing in complexity, automated generation of\ngame content has found widespread interest. However, the task of 3D gaming map\nart creation remains underexplored to date due to its unique complexity and\ndomain-specific challenges. While recent works have addressed related topics\nsuch as retro-style level generation and procedural terrain creation, these\nworks primarily focus on simpler data distributions. To the best of our\nknowledge, we are the first to demonstrate the application of modern AI\ntechniques for high-resolution texture manipulation in complex, highly detailed\nAAA 3D game environments. We introduce a novel Smart Brush for map editing,\ndesigned to assist artists in seamlessly modifying selected areas of a game map\nwith minimal effort. By leveraging generative adversarial networks and\ndiffusion models we propose two variants of the brush that enable efficient and\ncontext-aware generation. Our hybrid workflow aims to enhance both artistic\nflexibility and production efficiency, enabling the refinement of environments\nwithout manually reworking every detail, thus helping to bridge the gap between\nautomation and creative control in game development. A comparative evaluation\nof our two methods with adapted versions of several state-of-the art models\nshows that our GAN-based brush produces the sharpest and most detailed outputs\nwhile preserving image context while the evaluated state-of-the-art models tend\ntowards blurrier results and exhibit difficulties in maintaining contextual\nconsistency.\n","authors":["Vitaly Gnatyuk","Valeriia Koriukina","Ilya Levoshevich","Pavel Nurminskiy","Guenter Wallner"],"pdf_url":"https://arxiv.org/pdf/2503.19793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18185v3","updated":"2025-03-26T06:10:48Z","published":"2025-02-25T13:26:06Z","title":"VesselSAM: Leveraging SAM for Aortic Vessel Segmentation with LoRA and\n  Atrous Attention","summary":"  Medical image segmentation is crucial for clinical diagnosis and treatment\nplanning, especially when dealing with complex anatomical structures such as\nvessels. However, accurately segmenting vessels remains challenging due to\ntheir small size, intricate edge structures, and susceptibility to artifacts\nand imaging noise. In this work, we propose VesselSAM, an enhanced version of\nthe Segment Anything Model (SAM), specifically tailored for aortic vessel\nsegmentation. VesselSAM incorporates AtrousLoRA, a novel module integrating\nAtrous Attention and Low-Rank Adaptation (LoRA), to enhance segmentation\nperformance. Atrous Attention enables the model to capture multi-scale\ncontextual information, preserving both fine-grained local details and broader\nglobal context. Additionally, LoRA facilitates efficient fine-tuning of the\nfrozen SAM image encoder, reducing the number of trainable parameters and\nthereby enhancing computational efficiency. We evaluate VesselSAM using two\nchallenging datasets: the Aortic Vessel Tree (AVT) dataset and the Type-B\nAortic Dissection (TBAD) dataset. VesselSAM achieves state-of-the-art\nperformance, attaining DSC scores of 93.50\\%, 93.25\\%, 93.02\\%, and 93.26\\%\nacross multi-center datasets. Our results demonstrate that VesselSAM delivers\nhigh segmentation accuracy while significantly reducing computational overhead\ncompared to existing large-scale models. This development paves the way for\nenhanced AI-based aortic vessel segmentation in clinical environments. The code\nand models will be released at https://github.com/Adnan-CAS/AtrousLora.\n","authors":["Adnan Iltaf","Rayan Merghani Ahmed","Zhenxi Zhang","Bin Li","Shoujun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.18185v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2412.08614v3","updated":"2025-03-26T06:10:20Z","published":"2024-12-11T18:37:42Z","title":"Benchmarking Large Vision-Language Models via Directed Scene Graph for\n  Comprehensive Image Captioning","summary":"  Generating detailed captions comprehending text-rich visual content in images\nhas received growing attention for Large Vision-Language Models (LVLMs).\nHowever, few studies have developed benchmarks specifically tailored for\ndetailed captions to measure their accuracy and comprehensiveness. In this\npaper, we introduce a detailed caption benchmark, termed as CompreCap, to\nevaluate the visual context from a directed scene graph view. Concretely, we\nfirst manually segment the image into semantically meaningful regions (i.e.,\nsemantic segmentation mask) according to common-object vocabulary, while also\ndistinguishing attributes of objects within all those regions. Then directional\nrelation labels of these objects are annotated to compose a directed scene\ngraph that can well encode rich compositional information of the image. Based\non our directed scene graph, we develop a pipeline to assess the generated\ndetailed captions from LVLMs on multiple levels, including the object-level\ncoverage, the accuracy of attribute descriptions, the score of key\nrelationships, etc. Experimental results on the CompreCap dataset confirm that\nour evaluation method aligns closely with human evaluation scores across LVLMs.\n","authors":["Fan Lu","Wei Wu","Kecheng Zheng","Shuailei Ma","Biao Gong","Jiawei Liu","Wei Zhai","Yang Cao","Yujun Shen","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2412.08614v3.pdf","comment":"Accepted by CVPR2025. Code and Dataset:\n  https://github.com/LuFan31/CompreCap"},{"id":"http://arxiv.org/abs/2302.10463v2","updated":"2025-03-26T05:54:55Z","published":"2023-02-21T06:11:08Z","title":"Vision-based Multi-future Trajectory Prediction: A Survey","summary":"  Vision-based trajectory prediction is an important task that supports safe\nand intelligent behaviours in autonomous systems. Many advanced approaches have\nbeen proposed over the years with improved spatial and temporal feature\nextraction. However, human behaviour is naturally diverse and uncertain. Given\nthe past trajectory and surrounding environment information, an agent can have\nmultiple plausible trajectories in the future. To tackle this problem, an\nessential task named multi-future trajectory prediction (MTP) has recently been\nstudied. This task aims to generate a diverse, acceptable and explainable\ndistribution of future predictions for each agent. In this paper, we present\nthe first survey for MTP with our unique taxonomies and a comprehensive\nanalysis of frameworks, datasets and evaluation metrics. We also compare models\non existing MTP datasets and conduct experiments on the ForkingPath dataset.\nFinally, we discuss multiple future directions that can help researchers\ndevelop novel multi-future trajectory prediction systems and other diverse\nlearning tasks similar to MTP.\n","authors":["Renhao Huang","Hao Xue","Maurice Pagnucco","Flora Salim","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2302.10463v2.pdf","comment":"Accepted by TNNLS 2025"},{"id":"http://arxiv.org/abs/2503.20258v1","updated":"2025-03-26T05:54:13Z","published":"2025-03-26T05:54:13Z","title":"Mamba-3D as Masked Autoencoders for Accurate and Data-Efficient Analysis\n  of Medical Ultrasound Videos","summary":"  Ultrasound videos are an important form of clinical imaging data, and deep\nlearning-based automated analysis can improve diagnostic accuracy and clinical\nefficiency. However, the scarcity of labeled data and the inherent challenges\nof video analysis have impeded the advancement of related methods. In this\nwork, we introduce E-ViM$^3$, a data-efficient Vision Mamba network that\npreserves the 3D structure of video data, enhancing long-range dependencies and\ninductive biases to better model space-time correlations. With our design of\nEnclosure Global Tokens (EGT), the model captures and aggregates global\nfeatures more effectively than competing methods. To further improve data\nefficiency, we employ masked video modeling for self-supervised pre-training,\nwith the proposed Spatial-Temporal Chained (STC) masking strategy designed to\nadapt to various video scenarios. Experiments demonstrate that E-ViM$^3$\nperforms as the state-of-the-art in two high-level semantic analysis tasks\nacross four datasets of varying sizes: EchoNet-Dynamic, CAMUS, MICCAI-BUV, and\nWHBUS. Furthermore, our model achieves competitive performance with limited\nlabels, highlighting its potential impact on real-world clinical applications.\n","authors":["Jiaheng Zhou","Yanfeng Zhou","Wei Fang","Yuxing Tang","Le Lu","Ge Yang"],"pdf_url":"https://arxiv.org/pdf/2503.20258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20252v1","updated":"2025-03-26T05:38:45Z","published":"2025-03-26T05:38:45Z","title":"LogicQA: Logical Anomaly Detection with Vision Language Model Generated\n  Questions","summary":"  Anomaly Detection (AD) focuses on detecting samples that differ from the\nstandard pattern, making it a vital tool in process control. Logical anomalies\nmay appear visually normal yet violate predefined constraints on object\npresence, arrangement, or quantity, depending on reasoning and explainability.\nWe introduce LogicQA, a framework that enhances AD by providing industrial\noperators with explanations for logical anomalies. LogicQA compiles\nautomatically generated questions into a checklist and collects responses to\nidentify violations of logical constraints. LogicQA is training-free,\nannotation-free, and operates in a few-shot setting. We achieve\nstate-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO\nAD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the\nexplanations of anomalies. Also, our approach has shown outstanding performance\non semiconductor SEM corporate data, further validating its effectiveness in\nindustrial applications.\n","authors":["Yejin Kwon","Daeun Moon","Youngje Oh","Hyunsoo Yoon"],"pdf_url":"https://arxiv.org/pdf/2503.20252v1.pdf","comment":null}]},"2025-03-27T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2503.20646v2","updated":"2025-03-27T17:39:31Z","published":"2025-03-26T15:40:10Z","title":"Immersive and Wearable Thermal Rendering for Augmented Reality","summary":"  In augmented reality (AR), where digital content is overlaid onto the real\nworld, realistic thermal feedback has been shown to enhance immersion. Yet\ncurrent thermal feedback devices, heavily influenced by the needs of virtual\nreality, often hinder physical interactions and are ineffective for immersion\nin AR. To bridge this gap, we have identified three design considerations\nrelevant for AR thermal feedback: indirect feedback to maintain dexterity,\nthermal passthrough to preserve real-world temperature perception, and\nspatiotemporal rendering for dynamic sensations. We then created a unique and\ninnovative thermal feedback device that satisfies these criteria. Human subject\nexperiments assessing perceptual sensitivity, object temperature matching,\nspatial pattern recognition, and moving thermal stimuli demonstrated the impact\nof our design, enabling realistic temperature discrimination, virtual object\nperception, and enhanced immersion. These findings demonstrate that carefully\ndesigned thermal feedback systems can bridge the sensory gap between physical\nand virtual interactions, enhancing AR realism and usability.\n","authors":["Alexandra Watkins","Ritam Ghosh","Evan Chow","Nilanjan Sarkar"],"pdf_url":"https://arxiv.org/pdf/2503.20646v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21727v1","updated":"2025-03-27T17:38:43Z","published":"2025-03-27T17:38:43Z","title":"Enhancing Underwater Navigation through Cross-Correlation-Aware Deep\n  INS/DVL Fusion","summary":"  The accurate navigation of autonomous underwater vehicles critically depends\non the precision of Doppler velocity log (DVL) velocity measurements. Recent\nadvancements in deep learning have demonstrated significant potential in\nimproving DVL outputs by leveraging spatiotemporal dependencies across multiple\nsensor modalities. However, integrating these estimates into model-based\nfilters, such as the extended Kalman filter, introduces statistical\ninconsistencies, most notably, cross-correlations between process and\nmeasurement noise. This paper addresses this challenge by proposing a\ncross-correlation-aware deep INS/DVL fusion framework. Building upon BeamsNet,\na convolutional neural network designed to estimate AUV velocity using DVL and\ninertial data, we integrate its output into a navigation filter that explicitly\naccounts for the cross-correlation induced between the noise sources. This\napproach improves filter consistency and better reflects the underlying sensor\nerror structure. Evaluated on two real-world underwater trajectories, the\nproposed method outperforms both least squares and cross-correlation-neglecting\napproaches in terms of state uncertainty. Notably, improvements exceed 10% in\nvelocity and misalignment angle confidence metrics. Beyond demonstrating\nempirical performance, this framework provides a theoretically principled\nmechanism for embedding deep learning outputs within stochastic filters.\n","authors":["Nadav Cohen","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2503.21727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05944v2","updated":"2025-03-27T15:55:04Z","published":"2024-03-09T15:50:25Z","title":"Model-Predictive Trajectory Generation for Aerial Search and Coverage","summary":"  This paper introduces a trajectory planning algorithm for search and coverage\nmissions with an Unmanned Aerial Vehicle (UAV) based on an uncertainty map that\nrepresents prior knowledge of the target region, modeled by a Gaussian Mixture\nModel (GMM). The trajectory planning problem is formulated as an Optimal\nControl Problem (OCP), which aims to maximize the uncertainty reduction within\na specified mission duration. However, this results in an intractable OCP whose\nobjective functional cannot be expressed in closed form. To address this, we\npropose a Model Predictive Control (MPC) algorithm based on a relaxed\nformulation of the objective function to approximate the optimal solutions.\nThis relaxation promotes efficient map exploration by penalizing overlaps in\nthe UAV's visibility regions along the trajectory. The algorithm can produce\nefficient and smooth trajectories, and it can be efficiently implemented using\nstandard Nonlinear Programming solvers, being suitable for real-time planning.\nUnlike traditional methods, which often rely on discretizing the mission space\nand using complex mixed-integer formulations, our approach is computationally\nefficient and easier to implement. The MPC algorithm is initially assessed in\nMATLAB, followed by Gazebo simulations and actual experimental tests conducted\nin an outdoor environment. The results demonstrate that the proposed strategy\ncan generate efficient and smooth trajectories for search and coverage\nmissions.\n","authors":["Hugo Matias","Daniel Silvestre"],"pdf_url":"https://arxiv.org/pdf/2403.05944v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09631v3","updated":"2025-03-27T15:12:29Z","published":"2024-06-13T23:42:34Z","title":"Towards Optimizing a Convex Cover of Collision-Free Space for Trajectory\n  Generation","summary":"  We propose an online iterative algorithm to optimize a convex cover to\nunder-approximate the free space for autonomous navigation to delineate Safe\nFlight Corridors (SFC). The convex cover consists of a set of polytopes such\nthat the union of the polytopes represents obstacle-free space, allowing us to\nfind trajectories for robots that lie within the convex cover. In order to find\nthe SFC that facilitates trajectory optimization, we iteratively find\noverlapping polytopes of maximum volumes that include specified waypoints\ninitialized by a geometric or kinematic planner. Constraints at waypoints\nappear in two alternating stages of a joint optimization problem, which is\nsolved by a novel heuristic-based iterative algorithm with partially\ndistributed variables. We validate the effectiveness of our proposed algorithm\nusing a range of parameterized environments and show its applications for\ntwo-stage motion planning.\n","authors":["Yuwei Wu","Igor Spasojevic","Pratik Chaudhari","Vijay Kumar"],"pdf_url":"https://arxiv.org/pdf/2406.09631v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21591v1","updated":"2025-03-27T15:08:03Z","published":"2025-03-27T15:08:03Z","title":"Dataset and Analysis of Long-Term Skill Acquisition in Robot-Assisted\n  Minimally Invasive Surgery","summary":"  Objective: We aim to investigate long-term robotic surgical skill acquisition\namong surgical residents and the effects of training intervals and fatigue on\nperformance. Methods: For six months, surgical residents participated in three\ntraining sessions once a month, surrounding a single 26-hour hospital shift. In\neach shift, they participated in training sessions scheduled before, during,\nand after the shift. In each training session, they performed three dry-lab\ntraining tasks: Ring Tower Transfer, Knot-Tying, and Suturing. We collected a\ncomprehensive dataset, including videos synchronized with kinematic data,\nactivity tracking, and scans of the suturing pads. Results: We collected a\ndataset of 972 trials performed by 18 residents of different surgical\nspecializations. Participants demonstrated consistent performance improvement\nacross all tasks. In addition, we found variations in between-shift learning\nand forgetting across metrics and tasks, and hints for possible effects of\nfatigue. Conclusion: The findings from our first analysis shed light on the\nlong-term learning processes of robotic surgical skills with extended intervals\nand varying levels of fatigue. Significance: This study lays the groundwork for\nfuture research aimed at optimizing training protocols and enhancing AI\napplications in surgery, ultimately contributing to improved patient outcomes.\nThe dataset will be made available upon acceptance of our journal submission.\n","authors":["Yarden Sharon","Alex Geftler","Hanna Kossowsky Lev","Ilana Nisky"],"pdf_url":"https://arxiv.org/pdf/2503.21591v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.21564v1","updated":"2025-03-27T14:47:43Z","published":"2025-03-27T14:47:43Z","title":"Cooking Task Planning using LLM and Verified by Graph Network","summary":"  Cooking tasks remain a challenging problem for robotics due to their\ncomplexity. Videos of people cooking are a valuable source of information for\nsuch task, but introduces a lot of variability in terms of how to translate\nthis data to a robotic environment. This research aims to streamline this\nprocess, focusing on the task plan generation step, by using a Large Language\nModel (LLM)-based Task and Motion Planning (TAMP) framework to autonomously\ngenerate cooking task plans from videos with subtitles, and execute them.\nConventional LLM-based task planning methods are not well-suited for\ninterpreting the cooking video data due to uncertainty in the videos, and the\nrisk of hallucination in its output. To address both of these problems, we\nexplore using LLMs in combination with Functional Object-Oriented Networks\n(FOON), to validate the plan and provide feedback in case of failure. This\ncombination can generate task sequences with manipulation motions that are\nlogically correct and executable by a robot. We compare the execution of the\ngenerated plans for 5 cooking recipes from our approach against the plans\ngenerated by a few-shot LLM-only approach for a dual-arm robot setup. It could\nsuccessfully execute 4 of the plans generated by our approach, whereas only 1\nof the plans generated by solely using the LLM could be executed.\n","authors":["Ryunosuke Takebayashi","Vitor Hideyo Isume","Takuya Kiyokawa","Weiwei Wan","Kensuke Harada"],"pdf_url":"https://arxiv.org/pdf/2503.21564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13255v3","updated":"2025-03-27T14:03:25Z","published":"2024-02-20T18:59:57Z","title":"How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey","summary":"  Over the past two decades, research in the field of Simultaneous Localization\nand Mapping (SLAM) has undergone a significant evolution, highlighting its\ncritical role in enabling autonomous exploration of unknown environments. This\nevolution ranges from hand-crafted methods, through the era of deep learning,\nto more recent developments focused on Neural Radiance Fields (NeRFs) and 3D\nGaussian Splatting (3DGS) representations. Recognizing the growing body of\nresearch and the absence of a comprehensive survey on the topic, this paper\naims to provide the first comprehensive overview of SLAM progress through the\nlens of the latest advancements in radiance fields. It sheds light on the\nbackground, evolutionary path, inherent strengths and limitations, and serves\nas a fundamental reference to highlight the dynamic progress and specific\nchallenges.\n","authors":["Fabio Tosi","Youmin Zhang","Ziren Gong","Erik Sandström","Stefano Mattoccia","Martin R. Oswald","Matteo Poggi"],"pdf_url":"https://arxiv.org/pdf/2402.13255v3.pdf","comment":"Updated to November 2024"},{"id":"http://arxiv.org/abs/2503.21491v1","updated":"2025-03-27T13:27:46Z","published":"2025-03-27T13:27:46Z","title":"Data-Driven Contact-Aware Control Method for Real-Time Deformable Tool\n  Manipulation: A Case Study in the Environmental Swabbing","summary":"  Deformable Object Manipulation (DOM) remains a critical challenge in robotics\ndue to the complexities of developing suitable model-based control strategies.\nDeformable Tool Manipulation (DTM) further complicates this task by introducing\nadditional uncertainties between the robot and its environment. While humans\neffortlessly manipulate deformable tools using touch and experience, robotic\nsystems struggle to maintain stability and precision. To address these\nchallenges, we present a novel State-Adaptive Koopman LQR (SA-KLQR) control\nframework for real-time deformable tool manipulation, demonstrated through a\ncase study in environmental swab sampling for food safety. This method\nleverages Koopman operator-based control to linearize nonlinear dynamics while\nadapting to state-dependent variations in tool deformation and contact forces.\nA tactile-based feedback system dynamically estimates and regulates the swab\ntool's angle, contact pressure, and surface coverage, ensuring compliance with\nfood safety standards. Additionally, a sensor-embedded contact pad monitors\nforce distribution to mitigate tool pivoting and deformation, improving\nstability during dynamic interactions. Experimental results validate the\nSA-KLQR approach, demonstrating accurate contact angle estimation, robust\ntrajectory tracking, and reliable force regulation. The proposed framework\nenhances precision, adaptability, and real-time control in deformable tool\nmanipulation, bridging the gap between data-driven learning and optimal control\nin robotic interaction tasks.\n","authors":["Siavash Mahmoudi","Amirreza Davar","Dongyi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.21491v1.pdf","comment":"Submitted for Journal Review"},{"id":"http://arxiv.org/abs/2503.18684v2","updated":"2025-03-27T13:19:46Z","published":"2025-03-24T13:55:47Z","title":"Efficient Continual Adaptation of Pretrained Robotic Policy with Online\n  Meta-Learned Adapters","summary":"  Continual adaptation is essential for general autonomous agents. For example,\na household robot pretrained with a repertoire of skills must still adapt to\nunseen tasks specific to each household. Motivated by this, building upon\nparameter-efficient fine-tuning in language models, prior works have explored\nlightweight adapters to adapt pretrained policies, which can preserve learned\nfeatures from the pretraining phase and demonstrate good adaptation\nperformances. However, these approaches treat task learning separately,\nlimiting knowledge transfer between tasks. In this paper, we propose Online\nMeta-Learned adapters (OMLA). Instead of applying adapters directly, OMLA can\nfacilitate knowledge transfer from previously learned tasks to current learning\ntasks through a novel meta-learning objective. Extensive experiments in both\nsimulated and real-world environments demonstrate that OMLA can lead to better\nadaptation performances compared to the baseline methods. The project link:\nhttps://ricky-zhu.github.io/OMLA/.\n","authors":["Ruiqi Zhu","Endong Sun","Guanhe Huang","Oya Celiktutan"],"pdf_url":"https://arxiv.org/pdf/2503.18684v2.pdf","comment":"Project link: https://ricky-zhu.github.io/OMLA/"},{"id":"http://arxiv.org/abs/2503.21425v1","updated":"2025-03-27T12:10:51Z","published":"2025-03-27T12:10:51Z","title":"STAMICS: Splat, Track And Map with Integrated Consistency and Semantics\n  for Dense RGB-D SLAM","summary":"  Simultaneous Localization and Mapping (SLAM) is a critical task in robotics,\nenabling systems to autonomously navigate and understand complex environments.\nCurrent SLAM approaches predominantly rely on geometric cues for mapping and\nlocalization, but they often fail to ensure semantic consistency, particularly\nin dynamic or densely populated scenes. To address this limitation, we\nintroduce STAMICS, a novel method that integrates semantic information with 3D\nGaussian representations to enhance both localization and mapping accuracy.\nSTAMICS consists of three key components: a 3D Gaussian-based scene\nrepresentation for high-fidelity reconstruction, a graph-based clustering\ntechnique that enforces temporal semantic consistency, and an open-vocabulary\nsystem that allows for the classification of unseen objects. Extensive\nexperiments show that STAMICS significantly improves camera pose estimation and\nmap quality, outperforming state-of-the-art methods while reducing\nreconstruction errors. Code will be public available.\n","authors":["Yongxu Wang","Xu Cao","Weiyun Yi","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2503.21425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09769v2","updated":"2025-03-27T12:09:05Z","published":"2024-08-19T07:58:10Z","title":"Integrating Naturalistic Insights in Objective Multi-Vehicle Safety\n  Framework","summary":"  As autonomous vehicle technology advances, the precise assessment of safety\nin complex traffic scenarios becomes crucial, especially in mixed-vehicle\nenvironments where human perception of safety must be taken into account. This\npaper presents a framework designed for assessing traffic safety in\nmulti-vehicle situations, facilitating the simultaneous utilization of diverse\nobjective safety metrics. Additionally, it allows the integration of subjective\nperception of safety by adjusting model parameters. The framework was applied\nto evaluate various model configurations in car-following scenarios on a\nhighway, utilizing naturalistic driving datasets. The evaluation of the model\nshowed an outstanding performance, particularly when integrating multiple\nobjective safety measures. Furthermore, the performance was significantly\nenhanced when considering all surrounding vehicles.\n","authors":["Enrico Del Re","Amirhesam Aghanouri","Cristina Olaverri-Monreal"],"pdf_url":"https://arxiv.org/pdf/2408.09769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21406v1","updated":"2025-03-27T11:50:29Z","published":"2025-03-27T11:50:29Z","title":"Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for\n  Skill Learning","summary":"  Imitation learning is a popular method for teaching robots new behaviors.\nHowever, most existing methods focus on teaching short, isolated skills rather\nthan long, multi-step tasks. To bridge this gap, imitation learning algorithms\nmust not only learn individual skills but also an abstract understanding of how\nto sequence these skills to perform extended tasks effectively. This paper\naddresses this challenge by proposing a neuro-symbolic imitation learning\nframework. Using task demonstrations, the system first learns a symbolic\nrepresentation that abstracts the low-level state-action space. The learned\nrepresentation decomposes a task into easier subtasks and allows the system to\nleverage symbolic planning to generate abstract plans. Subsequently, the system\nutilizes this task decomposition to learn a set of neural skills capable of\nrefining abstract plans into actionable robot commands. Experimental results in\nthree simulated robotic environments demonstrate that, compared to baselines,\nour neuro-symbolic approach increases data efficiency, improves generalization\ncapabilities, and facilitates interpretability.\n","authors":["Leon Keller","Daniel Tanneberg","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2503.21406v1.pdf","comment":"IEEE International Conference on Robotics and Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2503.21401v1","updated":"2025-03-27T11:47:20Z","published":"2025-03-27T11:47:20Z","title":"AcL: Action Learner for Fault-Tolerant Quadruped Locomotion Control","summary":"  Quadrupedal robots can learn versatile locomotion skills but remain\nvulnerable when one or more joints lose power. In contrast, dogs and cats can\nadopt limping gaits when injured, demonstrating their remarkable ability to\nadapt to physical conditions. Inspired by such adaptability, this paper\npresents Action Learner (AcL), a novel teacher-student reinforcement learning\nframework that enables quadrupeds to autonomously adapt their gait for stable\nwalking under multiple joint faults. Unlike conventional teacher-student\napproaches that enforce strict imitation, AcL leverages teacher policies to\ngenerate style rewards, guiding the student policy without requiring precise\nreplication. We train multiple teacher policies, each corresponding to a\ndifferent fault condition, and subsequently distill them into a single student\npolicy with an encoder-decoder architecture. While prior works primarily\naddress single-joint faults, AcL enables quadrupeds to walk with up to four\nfaulty joints across one or two legs, autonomously switching between different\nlimping gaits when faults occur. We validate AcL on a real Go2 quadruped robot\nunder single- and double-joint faults, demonstrating fault-tolerant, stable\nwalking, smooth gait transitions between normal and lamb gaits, and robustness\nagainst external disturbances.\n","authors":["Tianyu Xu","Yaoyu Cheng","Pinxi Shen","Lin Zhao"," Electrical","Computer Engineering","National University of Singapore"," Singapore","Mechanical Engineering","National University of Singapore"," Singapore"],"pdf_url":"https://arxiv.org/pdf/2503.21401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01791v3","updated":"2025-03-27T11:30:07Z","published":"2023-10-03T04:40:38Z","title":"Online POMDP Planning with Anytime Deterministic Guarantees","summary":"  Decision-making under uncertainty is a critical aspect of many practical\nautonomous systems due to incomplete information. Partially Observable Markov\nDecision Processes (POMDPs) offer a mathematically principled framework for\nformulating decision-making problems under such conditions. However, finding an\noptimal solution for a POMDP is generally intractable. In recent years, there\nhas been a significant progress of scaling approximate solvers from small to\nmoderately sized problems, using online tree search solvers. Often, such\napproximate solvers are limited to probabilistic or asymptotic guarantees\ntowards the optimal solution. In this paper, we derive a deterministic\nrelationship for discrete POMDPs between an approximated and the optimal\nsolution. We show that at any time, we can derive bounds that relate between\nthe existing solution and the optimal one. We show that our derivations provide\nan avenue for a new set of algorithms and can be attached to existing\nalgorithms that have a certain structure to provide them with deterministic\nguarantees with marginal computational overhead. In return, not only do we\ncertify the solution quality, but we demonstrate that making a decision based\non the deterministic guarantee may result in superior performance compared to\nthe original algorithm without the deterministic certification.\n","authors":["Moran Barenboim","Vadim Indelman"],"pdf_url":"https://arxiv.org/pdf/2310.01791v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19690v2","updated":"2025-03-27T11:02:31Z","published":"2025-03-25T14:17:15Z","title":"Risk-Aware Reinforcement Learning for Autonomous Driving: Improving\n  Safety When Driving through Intersection","summary":"  Applying reinforcement learning to autonomous driving has garnered widespread\nattention. However, classical reinforcement learning methods optimize policies\nby maximizing expected rewards but lack sufficient safety considerations, often\nputting agents in hazardous situations. This paper proposes a risk-aware\nreinforcement learning approach for autonomous driving to improve the safety\nperformance when crossing the intersection. Safe critics are constructed to\nevaluate driving risk and work in conjunction with the reward critic to update\nthe actor. Based on this, a Lagrangian relaxation method and cyclic gradient\niteration are combined to project actions into a feasible safe region.\nFurthermore, a Multi-hop and Multi-layer perception (MLP) mixed Attention\nMechanism (MMAM) is incorporated into the actor-critic network, enabling the\npolicy to adapt to dynamic traffic and overcome permutation sensitivity\nchallenges. This allows the policy to focus more effectively on surrounding\npotential risks while enhancing the identification of passing opportunities.\nSimulation tests are conducted on different tasks at unsignalized\nintersections. The results show that the proposed approach effectively reduces\ncollision rates and improves crossing efficiency in comparison to baseline\nalgorithms. Additionally, our ablation experiments demonstrate the benefits of\nincorporating risk-awareness and MMAM into RL.\n","authors":["Bo Leng","Ran Yu","Wei Han","Lu Xiong","Zhuoren Li","Hailong Huang"],"pdf_url":"https://arxiv.org/pdf/2503.19690v2.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.21350v1","updated":"2025-03-27T10:38:37Z","published":"2025-03-27T10:38:37Z","title":"A Data-Driven Method for INS/DVL Alignment","summary":"  Autonomous underwater vehicles (AUVs) are sophisticated robotic platforms\ncrucial for a wide range of applications. The accuracy of AUV navigation\nsystems is critical to their success. Inertial sensors and Doppler velocity\nlogs (DVL) fusion is a promising solution for long-range underwater navigation.\nHowever, the effectiveness of this fusion depends heavily on an accurate\nalignment between the inertial sensors and the DVL. While current alignment\nmethods show promise, there remains significant room for improvement in terms\nof accuracy, convergence time, and alignment trajectory efficiency. In this\nresearch we propose an end-to-end deep learning framework for the alignment\nprocess. By leveraging deep-learning capabilities, such as noise reduction and\ncapture of nonlinearities in the data, we show using simulative data, that our\nproposed approach enhances both alignment accuracy and reduces convergence time\nbeyond current model-based methods.\n","authors":["Guy Damari","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2503.21350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21338v1","updated":"2025-03-27T10:14:46Z","published":"2025-03-27T10:14:46Z","title":"UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based\n  on Uncertainty-Guided NeRF Augmentation","summary":"  Visual place recognition (VPR) is crucial for robots to identify previously\nvisited locations, playing an important role in autonomous navigation in both\nindoor and outdoor environments. However, most existing VPR datasets are\nlimited to single-viewpoint scenarios, leading to reduced recognition accuracy,\nparticularly in multi-directional driving or feature-sparse scenes. Moreover,\nobtaining additional data to mitigate these limitations is often expensive.\nThis paper introduces a novel training paradigm to improve the performance of\nexisting VPR networks by enhancing multi-view diversity within current datasets\nthrough uncertainty estimation and NeRF-based data augmentation. Specifically,\nwe initially train NeRF using the existing VPR dataset. Then, our devised\nself-supervised uncertainty estimation network identifies places with high\nuncertainty. The poses of these uncertain places are input into NeRF to\ngenerate new synthetic observations for further training of VPR networks.\nAdditionally, we propose an improved storage method for efficient organization\nof augmented and original training data. We conducted extensive experiments on\nthree datasets and tested three different VPR backbone networks. The results\ndemonstrate that our proposed training paradigm significantly improves VPR\nperformance by fully utilizing existing data, outperforming other training\napproaches. We further validated the effectiveness of our approach on\nself-recorded indoor and outdoor datasets, consistently demonstrating superior\nresults. Our dataset and code have been released at\n\\href{https://github.com/nubot-nudt/UGNA-VPR}{https://github.com/nubot-nudt/UGNA-VPR}.\n","authors":["Yehui Shen","Lei Zhang","Qingqiu Li","Xiongwei Zhao","Yue Wang","Huimin Lu","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2503.21338v1.pdf","comment":"Accepted to IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2410.21630v2","updated":"2025-03-27T10:01:08Z","published":"2024-10-29T00:22:02Z","title":"Constrained Nonlinear Kaczmarz Projection on Intersections of Manifolds\n  for Coordinated Multi-Robot Mobile Manipulation","summary":"  Cooperative manipulation tasks impose various structure-, task-, and\nrobot-specific constraints on mobile manipulators. However, current methods\nstruggle to model and solve these myriad constraints simultaneously. We propose\na twofold solution: first, we model constraints as a family of manifolds\namenable to simultaneous solving. Second, we introduce the constrained\nnonlinear Kaczmarz (cNKZ) projection technique to produce constraint-satisfying\nsolutions. Experiments show that cNKZ dramatically outperforms baseline\napproaches, which cannot find solutions at all. We integrate cNKZ with a\nsampling-based motion planning algorithm to generate complex, coordinated\nmotions for 3 to 6 mobile manipulators (18--36 DoF), with cNKZ solving up to 80\nnonlinear constraints simultaneously and achieving up to a 92% success rate in\ncluttered environments. We also demonstrate our approach on hardware using\nthree Turtlebot3 Waffle Pi robots with OpenMANIPULATOR-X arms.\n","authors":["Akshaya Agrawal","Parker Mayer","Zachary Kingston","Geoffrey A. Hollinger"],"pdf_url":"https://arxiv.org/pdf/2410.21630v2.pdf","comment":"Accepted for publication at IEEE International Conference on Robotics\n  and Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2503.12101v2","updated":"2025-03-27T09:28:37Z","published":"2025-03-15T12:12:11Z","title":"MUSE: A Real-Time Multi-Sensor State Estimator for Quadruped Robots","summary":"  This paper introduces an innovative state estimator, MUSE (MUlti-sensor State\nEstimator), designed to enhance state estimation's accuracy and real-time\nperformance in quadruped robot navigation. The proposed state estimator builds\nupon our previous work presented in [1]. It integrates data from a range of\nonboard sensors, including IMUs, encoders, cameras, and LiDARs, to deliver a\ncomprehensive and reliable estimation of the robot's pose and motion, even in\nslippery scenarios. We tested MUSE on a Unitree Aliengo robot, successfully\nclosing the locomotion control loop in difficult scenarios, including slippery\nand uneven terrain. Benchmarking against Pronto [2] and VILENS [3] showed 67.6%\nand 26.7% reductions in translational errors, respectively. Additionally, MUSE\noutperformed DLIO [4], a LiDAR-inertial odometry system in rotational errors\nand frequency, while the proprioceptive version of MUSE (P-MUSE) outperformed\nTSIF [5], with a 45.9% reduction in absolute trajectory error (ATE).\n","authors":["Ylenia Nisticò","João Carlos Virgolino Soares","Lorenzo Amatucci","Geoff Fink","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2503.12101v2.pdf","comment":"Accepted for publication in IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2503.21293v1","updated":"2025-03-27T09:22:27Z","published":"2025-03-27T09:22:27Z","title":"Lidar-only Odometry based on Multiple Scan-to-Scan Alignments over a\n  Moving Window","summary":"  Lidar-only odometry considers the pose estimation of a mobile robot based on\nthe accumulation of motion increments extracted from consecutive lidar scans.\nMany existing approaches to the problem use a scan-to-map registration, which\nneglects the accumulation of errors within the maintained map due to drift.\nOther methods use a refinement step that jointly optimizes the local map on a\nfeature basis. We propose a solution that avoids this by using multiple\nindependent scan-to-scan Iterative Closest Points (ICP) registrations to\nprevious scans in order to derive constraints for a pose graph. The\noptimization of the pose graph then not only yields an accurate estimate for\nthe latest pose, but also enables the refinement of previous scans in the\noptimization window. By avoiding the need to recompute the scan-to-scan\nalignments, the computational load is minimized. Extensive evaluation on the\npublic KITTI and MulRan datasets as well as on a custom automotive lidar\ndataset is carried out. Results show that the proposed approach achieves\nstate-of-the-art estimation accuracy, while alleviating the mentioned issues.\n","authors":["Aaron Kurda","Simon Steuernagel","Marcus Baum"],"pdf_url":"https://arxiv.org/pdf/2503.21293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21291v1","updated":"2025-03-27T09:20:35Z","published":"2025-03-27T09:20:35Z","title":"An analysis of higher-order kinematics formalisms for an innovative\n  surgical parallel robot","summary":"  The paper presents a novel modular hybrid parallel robot for pancreatic\nsurgery and its higher-order kinematics derived based on various formalisms.\nThe classical vector, homogeneous transformation matrices and dual quaternion\napproaches are studied for the kinematic functions using both classical\ndifferentiation and multidual algebra. The algorithms for inverse kinematics\nfor all three studied formalisms are presented for both differentiation and\nmultidual algebra approaches. Furthermore, these algorithms are compared based\non numerical stability, execution times and number and type of mathematical\nfunctions and operators contained in each algorithm. A statistical analysis\nshows that there is significant improvement in execution time for the\nalgorithms implemented using multidual algebra, while the numerical stability\nis appropriate for all algorithms derived based on differentiation and\nmultidual algebra. While the implementation of the kinematic algorithms using\nmultidual algebra shows positive results when benchmarked on a standard PC,\nfurther work is required to evaluate the multidual algorithms on\nhardware/software used for the modular parallel robot command and control.\n","authors":["Calin Vaida","Iosif Birlescu","Bogdan Gherman","Daniel Condurache","Damien Chablat","Doina Pisla"],"pdf_url":"https://arxiv.org/pdf/2503.21291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21288v1","updated":"2025-03-27T09:11:34Z","published":"2025-03-27T09:11:34Z","title":"Haptic bilateral teleoperation system for free-hand dental procedures","summary":"  Free-hand dental procedures are typically repetitive, time-consuming and\nrequire high precision and manual dexterity. Dental robots can play a key role\nin improving procedural accuracy and safety, enhancing patient comfort, and\nreducing operator workload. However, robotic solutions for free-hand procedures\nremain limited or completely lacking, and their acceptance is still low. To\naddress this gap, we develop a haptic bilateral teleoperation system (HBTS) for\nfree-hand dental procedures. The system includes a dedicated mechanical\nend-effector, compatible with standard clinical tools, and equipped with an\nendoscopic camera for improved visibility of the intervention site. By ensuring\nmotion and force correspondence between the operator's actions and the robot's\nmovements, monitored through visual feedback, we enhance the operator's sensory\nawareness and motor accuracy. Furthermore, recognizing the need to ensure\nprocedural safety, we limit interaction forces by scaling the motion references\nprovided to the admittance controller based solely on measured contact forces.\nThis ensures effective force limitation in all contact states without requiring\nprior knowledge of the environment. The proposed HBTS is validated in a dental\nscaling procedure using a dental phantom. The results show that the system\nimproves the naturalness, safety, and accuracy of teleoperation, highlighting\nits potential to enhance free-hand dental procedures.\n","authors":["Lorenzo Pagliara","Enrico Ferrentino","Andrea Chiacchio","Giovanni Russo"],"pdf_url":"https://arxiv.org/pdf/2503.21288v1.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2503.21281v1","updated":"2025-03-27T09:01:30Z","published":"2025-03-27T09:01:30Z","title":"Output-Feedback Boundary Control of Thermally and Flow-Induced\n  Vibrations in Slender Timoshenko Beams","summary":"  This work is motivated by the engineering challenge of suppressing vibrations\nin turbine blades of aero engines, which often operate under extreme thermal\nconditions and high-Mach aerodynamic environments that give rise to complex\nvibration phenomena, commonly referred to as thermally-induced and flow-induced\nvibrations. Using Hamilton's variational principle, the system is modeled as a\nrotating slender Timoshenko beam under thermal and aerodynamic loads, described\nby a mixed hyperbolic-parabolic PDE system where instabilities occur both\nwithin the PDE domain and at the uncontrolled boundary, and the two types of\nPDEs are cascaded in the domain. For such a system, we present the\nstate-feedback control design based on the PDE backstepping method. Recognizing\nthat the distributed temperature gradients and structural vibrations in the\nTimoshenko beam are typically unmeasurable in practice, we design a state\nobserver for the mixed hyperbolic-parabolic PDE system. Based on this observer,\nan output-feedback controller is then built to regulate the overall system\nusing only available boundary measurements. In the closed-loop system, the\nstate of the uncontrolled boundary, i.e., the furthest state from the control\ninput, is proved to be exponentially convergent to zero, and all signals are\nproved as uniformly ultimately bounded. The proposed control design is\nvalidated on an aero-engine flexible blade under extreme thermal and\naerodynamic conditions.\n","authors":["Chengyi Wang","Ji Wang"],"pdf_url":"https://arxiv.org/pdf/2503.21281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07538v2","updated":"2025-03-27T08:55:23Z","published":"2024-05-13T08:19:02Z","title":"Mirroring the Parking Target: An Optimal-Control-Based Parking Motion\n  Planner with Strengthened Parking Reliability and Faster Parking Completion","summary":"  Automated Parking Assist (APA) systems are now facing great challenges of low\nadoption in applications, due to users' concerns about parking capability,\nreliability, and completion efficiency. To upgrade the conventional APA\nplanners and enhance user's acceptance, this research proposes an\noptimal-control-based parking motion planner. Its highlight lies in its control\nlogic: planning trajectories by mirroring the parking target. This method\nenables: i) parking capability in narrow spaces; ii) better parking reliability\nby expanding Operation Design Domain (ODD); iii) faster completion of parking\nprocess; iv) enhanced computational efficiency; v) universal to all types of\nparking. A comprehensive evaluation is conducted. Results demonstrate the\nproposed planner does enhance parking success rate by 40.6%, improve parking\ncompletion efficiency by 18.0%, and expand ODD by 86.1%. It shows its\nsuperiority in difficult parking cases, such as the parallel parking scenario\nand narrow spaces. Moreover, the average computation time of the proposed\nplanner is 74 milliseconds. Results indicate that the proposed planner is ready\nfor real-time commercial applications.\n","authors":["Jia Hu","Yongwei Feng","Shuoyuan Li","Haoran Wang","Jaehyun So","Junnian Zheng"],"pdf_url":"https://arxiv.org/pdf/2405.07538v2.pdf","comment":"IEEE Transactions on Intelligent Transportation Systems (2024)"},{"id":"http://arxiv.org/abs/2405.07556v3","updated":"2025-03-27T08:51:23Z","published":"2024-05-13T08:36:06Z","title":"Safety-Aware Human-Lead Vehicle Platooning by Proactively Reacting to\n  Uncertain Human Behaving","summary":"  Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as a\npromising vehicle platooning technology in real-world implementation. By\nutilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reduces\nthe cost and enhances the reliability of perception and decision-making.\nHowever, state-of-the-art HL-CACC technology still has a great limitation on\ndriving safety due to the lack of considering the leading human driver's\nuncertain behavior. In this study, a HL-CACC controller is designed based on\nStochastic Model Predictive Control (SMPC). It is enabled to predict the\ndriving intention of the leading Connected Human-Driven Vehicle (CHV). The\nproposed controller has the following features: i) enhanced perceived safety in\noscillating traffic; ii) guaranteed safety against hard brakes; iii)\ncomputational efficiency for real-time implementation. The proposed controller\nis evaluated on a PreScan&Simulink simulation platform. Real vehicle trajectory\ndata is collected for the calibration of the simulation. Results reveal that\nthe proposed controller: i) improves perceived safety by 19.17% in oscillating\ntraffic; ii) enhances actual safety by 7.76% against hard brakes; iii) is\nconfirmed with string stability. The computation time is approximately 3.2\nmilliseconds when running on a laptop equipped with an Intel i5-13500H CPU.\nThis indicates the proposed controller is ready for real-time implementation.\n","authors":["Jia Hu","Shuhan Wang","Yiming Zhang","Haoran Wang","Zhilong Liu","Guangzhi Cao"],"pdf_url":"https://arxiv.org/pdf/2405.07556v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21257v1","updated":"2025-03-27T08:28:22Z","published":"2025-03-27T08:28:22Z","title":"OminiAdapt: Learning Cross-Task Invariance for Robust and\n  Environment-Aware Robotic Manipulation","summary":"  With the rapid development of embodied intelligence, leveraging large-scale\nhuman data for high-level imitation learning on humanoid robots has become a\nfocal point of interest in both academia and industry. However, applying\nhumanoid robots to precision operation domains remains challenging due to the\ncomplexities they face in perception and control processes, the long-standing\nphysical differences in morphology and actuation mechanisms between humanoid\nrobots and humans, and the lack of task-relevant features obtained from\negocentric vision. To address the issue of covariate shift in imitation\nlearning, this paper proposes an imitation learning algorithm tailored for\nhumanoid robots. By focusing on the primary task objectives, filtering out\nbackground information, and incorporating channel feature fusion with spatial\nattention mechanisms, the proposed algorithm suppresses environmental\ndisturbances and utilizes a dynamic weight update strategy to significantly\nimprove the success rate of humanoid robots in accomplishing target tasks.\nExperimental results demonstrate that the proposed method exhibits robustness\nand scalability across various typical task scenarios, providing new ideas and\napproaches for autonomous learning and control in humanoid robots. The project\nwill be open-sourced on GitHub.\n","authors":["Yongxu Wang","Weiyun Yi","Xinhao Kong","Wanting Li"],"pdf_url":"https://arxiv.org/pdf/2503.21257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21204v1","updated":"2025-03-27T06:45:55Z","published":"2025-03-27T06:45:55Z","title":"Dimensional optimization of single-DOF planar rigid link-flapping\n  mechanisms for high lift and low power","summary":"  Rigid link flapping mechanisms remain the most practical choice for flapping\nwing micro-aerial vehicles (MAVs) to carry useful payloads and onboard\nbatteries for free flight due to their long-term durability and reliability.\nHowever, to achieve high agility and maneuverability-like insects-MAVs with\nthese mechanisms require significant weight reduction. One approach involves\nusing single-DOF planar rigid linkages, which are rarely optimized\ndimensionally for high lift and low power so that smaller motors and batteries\ncould be used. We integrated a mechanism simulator based on a quasistatic\nnonlinear finite element method with an unsteady vortex lattice method-based\naerodynamic analysis tool within an optimization routine. We optimized three\ndifferent mechanism topologies from the literature. As a result, significant\npower savings were observed up to 42% in some cases, due to increased amplitude\nand higher lift coefficients resulting from optimized asymmetric sweeping\nvelocity profiles. We also conducted an uncertainty analysis that revealed the\nneed for high manufacturing tolerances to ensure reliable mechanism\nperformance. The presented unified computational tool also facilitates the\noptimal selection of MAV components based on the payload and flight time\nrequirements.\n","authors":["Shyam Sunder Nishad","Anupam Saxena"],"pdf_url":"https://arxiv.org/pdf/2503.21204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18769v2","updated":"2025-03-27T06:39:47Z","published":"2025-03-24T15:16:51Z","title":"AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning","summary":"  This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of language models for robotic manipulation in\n3D Cartesian space. AlphaSpace employs a hierarchical semantics-based\ntokenization strategy that encodes spatial information at both coarse and\nfine-grained levels. Our approach represents objects with their attributes,\npositions, and height information through structured tokens, enabling precise\nspatial reasoning without relying on traditional vision-based embeddings. This\napproach enables LLMs to accurately manipulate objects by positioning them at\nspecific (x, y, z) coordinates. Experimental results suggest that AlphaSpace\ndemonstrates promising potential for improving manipulation tasks, achieving a\ntotal accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude\n3.5 Sonnet. These results demonstrate the potential of structured spatial\nencoding for manipulation tasks and warrant further exploration.\n","authors":["Alan Dao","Dinh Bach Vu","Bui Quang Huy"],"pdf_url":"https://arxiv.org/pdf/2503.18769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21168v1","updated":"2025-03-27T05:37:16Z","published":"2025-03-27T05:37:16Z","title":"TAGA: A Tangent-Based Reactive Approach for Socially Compliant Robot\n  Navigation Around Human Groups","summary":"  Robot navigation in densely populated environments presents significant\nchallenges, particularly regarding the interplay between individual and group\ndynamics. Current navigation models predominantly address interactions with\nindividual pedestrians while failing to account for human groups that naturally\nform in real-world settings. Conversely, the limited models implementing\ngroup-aware navigation typically prioritize group dynamics at the expense of\nindividual interactions, both of which are essential for socially appropriate\nnavigation. This research extends an existing simulation framework to\nincorporate both individual pedestrians and human groups. We present Tangent\nAction for Group Avoidance (TAGA), a modular reactive mechanism that can be\nintegrated with existing navigation frameworks to enhance their group-awareness\ncapabilities. TAGA dynamically modifies robot trajectories using tangent\naction-based avoidance strategies while preserving the underlying model's\ncapacity to navigate around individuals. Additionally, we introduce Group\nCollision Rate (GCR), a novel metric to quantitatively assess how effectively\nrobots maintain group integrity during navigation. Through comprehensive\nsimulation-based benchmarking, we demonstrate that integrating TAGA with\nstate-of-the-art navigation models (ORCA, Social Force, DS-RNN, and AG-RL)\nreduces group intrusions by 45.7-78.6% while maintaining comparable success\nrates and navigation efficiency. Future work will focus on real-world\nimplementation and validation of this approach.\n","authors":["Utsha Kumar Roy","Sejuti Rahman"],"pdf_url":"https://arxiv.org/pdf/2503.21168v1.pdf","comment":"6 pages, 3 figures. Submitted as a conference paper in IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS), 2025"},{"id":"http://arxiv.org/abs/2503.17125v4","updated":"2025-03-27T05:17:19Z","published":"2025-03-21T13:20:39Z","title":"LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in\n  Reinforcement Learning","summary":"  Deep Reinforcement Learning (DRL) has demonstrated strong performance in\nrobotic control but remains susceptible to out-of-distribution (OOD) states,\noften resulting in unreliable actions and task failure. While previous methods\nhave focused on minimizing or preventing OOD occurrences, they largely neglect\nrecovery once an agent encounters such states. Although the latest research has\nattempted to address this by guiding agents back to in-distribution states,\ntheir reliance on uncertainty estimation hinders scalability in complex\nenvironments. To overcome this limitation, we introduce Language Models for\nOut-of-Distribution Recovery (LaMOuR), which enables recovery learning without\nrelying on uncertainty estimation. LaMOuR generates dense reward codes that\nguide the agent back to a state where it can successfully perform its original\ntask, leveraging the capabilities of LVLMs in image description, logical\nreasoning, and code generation. Experimental results show that LaMOuR\nsubstantially enhances recovery efficiency across diverse locomotion tasks and\neven generalizes effectively to complex environments, including humanoid\nlocomotion and mobile manipulation, where existing methods struggle. The code\nand supplementary materials are available at https://lamour-rl.github.io/.\n","authors":["Chan Kim","Seung-Woo Seo","Seong-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2503.17125v4.pdf","comment":"This paper is currently under security review and will be re-released\n  once the review is complete"},{"id":"http://arxiv.org/abs/2503.21141v1","updated":"2025-03-27T04:12:27Z","published":"2025-03-27T04:12:27Z","title":"Safe Human Robot Navigation in Warehouse Scenario","summary":"  The integration of autonomous mobile robots (AMRs) in industrial\nenvironments, particularly warehouses, has revolutionized logistics and\noperational efficiency. However, ensuring the safety of human workers in\ndynamic, shared spaces remains a critical challenge. This work proposes a novel\nmethodology that leverages control barrier functions (CBFs) to enhance safety\nin warehouse navigation. By integrating learning-based CBFs with the Open\nRobotics Middleware Framework (OpenRMF), the system achieves adaptive and\nsafety-enhanced controls in multi-robot, multi-agent scenarios. Experiments\nconducted using various robot platforms demonstrate the efficacy of the\nproposed approach in avoiding static and dynamic obstacles, including human\npedestrians. Our experiments evaluate different scenarios in which the number\nof robots, robot platforms, speed, and number of obstacles are varied, from\nwhich we achieve promising performance.\n","authors":["Seth Farrell","Chenghao Li","Hongzhan Yu","Ryo Yoshimitsu","Sicun Gao","Henrik I. Christensen"],"pdf_url":"https://arxiv.org/pdf/2503.21141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10356v2","updated":"2025-03-27T03:53:51Z","published":"2025-01-17T18:57:39Z","title":"DexForce: Extracting Force-informed Actions from Kinesthetic\n  Demonstrations for Dexterous Manipulation","summary":"  Imitation learning requires high-quality demonstrations consisting of\nsequences of state-action pairs. For contact-rich dexterous manipulation tasks\nthat require dexterity, the actions in these state-action pairs must produce\nthe right forces. Current widely-used methods for collecting dexterous\nmanipulation demonstrations are difficult to use for demonstrating contact-rich\ntasks due to unintuitive human-to-robot motion retargeting and the lack of\ndirect haptic feedback. Motivated by these concerns, we propose DexForce.\nDexForce leverages contact forces, measured during kinesthetic demonstrations,\nto compute force-informed actions for policy learning. We collect\ndemonstrations for six tasks and show that policies trained on our\nforce-informed actions achieve an average success rate of 76% across all tasks.\nIn contrast, policies trained directly on actions that do not account for\ncontact forces have near-zero success rates. We also conduct a study ablating\nthe inclusion of force data in policy observations. We find that while using\nforce data never hurts policy performance, it helps most for tasks that require\nadvanced levels of precision and coordination, like opening an AirPods case and\nunscrewing a nut.\n","authors":["Claire Chen","Zhongchun Yu","Hojung Choi","Mark Cutkosky","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2501.10356v2.pdf","comment":"Videos can be found here:\n  https://clairelc.github.io/dexforce.github.io/"},{"id":"http://arxiv.org/abs/2503.14734v2","updated":"2025-03-27T02:52:43Z","published":"2025-03-18T21:06:21Z","title":"GR00T N1: An Open Foundation Model for Generalist Humanoid Robots","summary":"  General-purpose robots need a versatile body and an intelligent mind. Recent\nadvancements in humanoid robots have shown great promise as a hardware platform\nfor building generalist autonomy in the human world. A robot foundation model,\ntrained on massive and diverse data sources, is essential for enabling the\nrobots to reason about novel situations, robustly handle real-world\nvariability, and rapidly learn new tasks. To this end, we introduce GR00T N1,\nan open foundation model for humanoid robots. GR00T N1 is a\nVision-Language-Action (VLA) model with a dual-system architecture. The\nvision-language module (System 2) interprets the environment through vision and\nlanguage instructions. The subsequent diffusion transformer module (System 1)\ngenerates fluid motor actions in real time. Both modules are tightly coupled\nand jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture\nof real-robot trajectories, human videos, and synthetically generated datasets.\nWe show that our generalist robot model GR00T N1 outperforms the\nstate-of-the-art imitation learning baselines on standard simulation benchmarks\nacross multiple robot embodiments. Furthermore, we deploy our model on the\nFourier GR-1 humanoid robot for language-conditioned bimanual manipulation\ntasks, achieving strong performance with high data efficiency.\n","authors":[" NVIDIA"," :","Johan Bjorck","Fernando Castañeda","Nikita Cherniadev","Xingye Da","Runyu Ding","Linxi \"Jim\" Fan","Yu Fang","Dieter Fox","Fengyuan Hu","Spencer Huang","Joel Jang","Zhenyu Jiang","Jan Kautz","Kaushil Kundalia","Lawrence Lao","Zhiqi Li","Zongyu Lin","Kevin Lin","Guilin Liu","Edith Llontop","Loic Magne","Ajay Mandlekar","Avnish Narayan","Soroush Nasiriany","Scott Reed","You Liang Tan","Guanzhi Wang","Zu Wang","Jing Wang","Qi Wang","Jiannan Xiang","Yuqi Xie","Yinzhen Xu","Zhenjia Xu","Seonghyeon Ye","Zhiding Yu","Ao Zhang","Hao Zhang","Yizhou Zhao","Ruijie Zheng","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.14734v2.pdf","comment":"Authors are listed alphabetically. Project leads are Linxi \"Jim\" Fan\n  and Yuke Zhu. For more information, see\n  https://developer.nvidia.com/isaac/gr00t"},{"id":"http://arxiv.org/abs/2412.06779v2","updated":"2025-03-27T02:34:48Z","published":"2024-12-09T18:58:43Z","title":"AnyBimanual: Transferring Unimanual Policy for General Bimanual\n  Manipulation","summary":"  Performing general language-conditioned bimanual manipulation tasks is of\ngreat importance for many applications ranging from household service to\nindustrial assembly. However, collecting bimanual manipulation data is\nexpensive due to the high-dimensional action space, which poses challenges for\nconventional methods to handle general bimanual manipulation tasks. In\ncontrast, unimanual policy has recently demonstrated impressive\ngeneralizability across a wide range of tasks because of scaled model\nparameters and training data, which can provide sharable manipulation knowledge\nfor bimanual systems. To this end, we propose a plug-and-play method named\nAnyBimanual, which transfers pre-trained unimanual policy to general bimanual\nmanipulation policy with few bimanual demonstrations. Specifically, we first\nintroduce a skill manager to dynamically schedule the skill representations\ndiscovered from pre-trained unimanual policy for bimanual manipulation tasks,\nwhich linearly combines skill primitives with task-oriented compensation to\nrepresent the bimanual manipulation instruction. To mitigate the observation\ndiscrepancy between unimanual and bimanual systems, we present a visual aligner\nto generate soft masks for visual embedding of the workspace, which aims to\nalign visual input of unimanual policy model for each arm with those during\npretraining stage. AnyBimanual shows superiority on 12 simulated tasks from\nRLBench2 with a sizable 12.67% improvement in success rate over previous\nmethods. Experiments on 9 real-world tasks further verify its practicality with\nan average success rate of 84.62%.\n","authors":["Guanxing Lu","Tengbo Yu","Haoyuan Deng","Season Si Chen","Yansong Tang","Ziwei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.06779v2.pdf","comment":"Project page: https://anybimanual.github.io/"},{"id":"http://arxiv.org/abs/2412.20104v4","updated":"2025-03-27T02:17:08Z","published":"2024-12-28T10:12:12Z","title":"SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object\n  Interaction Synthesis","summary":"  Synthesizing realistic human-object interaction motions is a critical problem\nin VR/AR and human animation. Unlike the commonly studied scenarios involving a\nsingle human or hand interacting with one object, we address a more generic\nmulti-body setting with arbitrary numbers of humans, hands, and objects. This\ncomplexity introduces significant challenges in synchronizing motions due to\nthe high correlations and mutual influences among bodies. To address these\nchallenges, we introduce SyncDiff, a novel method for multi-body interaction\nsynthesis using a synchronized motion diffusion strategy. SyncDiff employs a\nsingle diffusion model to capture the joint distribution of multi-body motions.\nTo enhance motion fidelity, we propose a frequency-domain motion decomposition\nscheme. Additionally, we introduce a new set of alignment scores to emphasize\nthe synchronization of different body motions. SyncDiff jointly optimizes both\ndata sample likelihood and alignment likelihood through an explicit\nsynchronization strategy. Extensive experiments across four datasets with\nvarious multi-body configurations demonstrate the superiority of SyncDiff over\nexisting state-of-the-art motion synthesis methods.\n","authors":["Wenkun He","Yun Liu","Ruitao Liu","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2412.20104v4.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.21065v1","updated":"2025-03-27T00:38:39Z","published":"2025-03-27T00:38:39Z","title":"Fuzzy-Logic-based model predictive control: A paradigm integrating\n  optimal and common-sense decision making","summary":"  This paper introduces a novel concept, fuzzy-logic-based model predictive\ncontrol (FLMPC), along with a multi-robot control approach for exploring\nunknown environments and locating targets. Traditional model predictive control\n(MPC) methods rely on Bayesian theory to represent environmental knowledge and\noptimize a stochastic cost function, often leading to high computational costs\nand lack of effectiveness in locating all the targets. Our approach instead\nleverages FLMPC and extends it to a bi-level parent-child architecture for\nenhanced coordination and extended decision making horizon. Extracting\nhigh-level information from probability distributions and local observations,\nFLMPC simplifies the optimization problem and significantly extends its\noperational horizon compared to other MPC methods. We conducted extensive\nsimulations in unknown 2-dimensional environments with randomly placed\nobstacles and humans. We compared the performance and computation time of FLMPC\nagainst MPC with a stochastic cost function, then evaluated the impact of\nintegrating the high-level parent FLMPC layer. The results indicate that our\napproaches significantly improve both performance and computation time,\nenhancing coordination of robots and reducing the impact of uncertainty in\nlarge-scale search and rescue environments.\n","authors":["Filip Surma","Anahita Jamshidnejad"],"pdf_url":"https://arxiv.org/pdf/2503.21065v1.pdf","comment":"50 Pages, 8 figures, 3 tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2503.21780v1","updated":"2025-03-27T17:59:58Z","published":"2025-03-27T17:59:58Z","title":"Semantic Library Adaptation: LoRA Retrieval and Fusion for\n  Open-Vocabulary Semantic Segmentation","summary":"  Open-vocabulary semantic segmentation models associate vision and text to\nlabel pixels from an undefined set of classes using textual queries, providing\nversatile performance on novel datasets. However, large shifts between training\nand test domains degrade their performance, requiring fine-tuning for effective\nreal-world applications. We introduce Semantic Library Adaptation (SemLA), a\nnovel framework for training-free, test-time domain adaptation. SemLA leverages\na library of LoRA-based adapters indexed with CLIP embeddings, dynamically\nmerging the most relevant adapters based on proximity to the target domain in\nthe embedding space. This approach constructs an ad-hoc model tailored to each\nspecific input without additional training. Our method scales efficiently,\nenhances explainability by tracking adapter contributions, and inherently\nprotects data privacy, making it ideal for sensitive applications.\nComprehensive experiments on a 20-domain benchmark built over 10 standard\ndatasets demonstrate SemLA's superior adaptability and performance across\ndiverse settings, establishing a new standard in domain adaptation for\nopen-vocabulary semantic segmentation.\n","authors":["Reza Qorbani","Gianluca Villani","Theodoros Panagiotakopoulos","Marc Botet Colomer","Linus Härenstam-Nielsen","Mattia Segu","Pier Luigi Dovesi","Jussi Karlgren","Daniel Cremers","Federico Tombari","Matteo Poggi"],"pdf_url":"https://arxiv.org/pdf/2503.21780v1.pdf","comment":"CVPR 2025. Project page: https://thegoodailab.org/semla Code:\n  https://github.com/rezaqorbani/SemLA"},{"id":"http://arxiv.org/abs/2503.21781v1","updated":"2025-03-27T17:59:58Z","published":"2025-03-27T17:59:58Z","title":"VideoMage: Multi-Subject and Motion Customization of Text-to-Video\n  Diffusion Models","summary":"  Customized text-to-video generation aims to produce high-quality videos that\nincorporate user-specified subject identities or motion patterns. However,\nexisting methods mainly focus on personalizing a single concept, either subject\nidentity or motion pattern, limiting their effectiveness for multiple subjects\nwith the desired motion patterns. To tackle this challenge, we propose a\nunified framework VideoMage for video customization over both multiple subjects\nand their interactive motions. VideoMage employs subject and motion LoRAs to\ncapture personalized content from user-provided images and videos, along with\nan appearance-agnostic motion learning approach to disentangle motion patterns\nfrom visual appearance. Furthermore, we develop a spatial-temporal composition\nscheme to guide interactions among subjects within the desired motion patterns.\nExtensive experiments demonstrate that VideoMage outperforms existing methods,\ngenerating coherent, user-controlled videos with consistent subject identities\nand interactions.\n","authors":["Chi-Pin Huang","Yen-Siang Wu","Hung-Kai Chung","Kai-Po Chang","Fu-En Yang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2503.21781v1.pdf","comment":"CVPR 2025. Project Page:\n  https://jasper0314-huang.github.io/videomage-customization"},{"id":"http://arxiv.org/abs/2503.21782v1","updated":"2025-03-27T17:59:58Z","published":"2025-03-27T17:59:58Z","title":"Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model","summary":"  Video understanding models often struggle with high computational\nrequirements, extensive parameter counts, and slow inference speed, making them\ninefficient for practical use. To tackle these challenges, we propose\nMobile-VideoGPT, an efficient multimodal framework designed to operate with\nfewer than a billion parameters. Unlike traditional video large multimodal\nmodels (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders,\nefficient projectors, and a small language model (SLM), enabling real-time\nthroughput. To further improve efficiency, we present an Attention-Based Frame\nScoring mechanism to select the key-frames, along with an efficient token\nprojector that prunes redundant visual tokens and preserves essential\ncontextual cues. We evaluate our model across well-established six video\nunderstanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest).\nOur results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per\nsecond while outperforming existing state-of-the-art 0.5B-parameter models by 6\npoints on average with 40% fewer parameters and more than 2x higher throughput.\nOur code and models are publicly available at:\nhttps://github.com/Amshaker/Mobile-VideoGPT.\n","authors":["Abdelrahman Shaker","Muhammad Maaz","Chenhui Gou","Hamid Rezatofighi","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2503.21782v1.pdf","comment":"Technical Report. Project Page:\n  https://amshaker.github.io/Mobile-VideoGPT"},{"id":"http://arxiv.org/abs/2503.21779v1","updated":"2025-03-27T17:59:57Z","published":"2025-03-27T17:59:57Z","title":"X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction","summary":"  Four-dimensional computed tomography (4D CT) reconstruction is crucial for\ncapturing dynamic anatomical changes but faces inherent limitations from\nconventional phase-binning workflows. Current methods discretize temporal\nresolution into fixed phases with respiratory gating devices, introducing\nmotion misalignment and restricting clinical practicality. In this paper, We\npropose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT\nreconstruction by integrating dynamic radiative Gaussian splatting with\nself-supervised respiratory motion learning. Our approach models anatomical\ndynamics through a spatiotemporal encoder-decoder architecture that predicts\ntime-varying Gaussian deformations, eliminating phase discretization. To remove\ndependency on external gating devices, we introduce a physiology-driven\nperiodic consistency loss that learns patient-specific breathing cycles\ndirectly from projections via differentiable optimization. Extensive\nexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR\ngain over traditional methods and 2.25 dB improvement against prior Gaussian\nsplatting techniques. By unifying continuous motion modeling with hardware-free\nperiod learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for\ndynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.\n","authors":["Weihao Yu","Yuanhao Cai","Ruyi Zha","Zhiwen Fan","Chenxin Li","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.21779v1.pdf","comment":"Project Page: https://x2-gaussian.github.io/"},{"id":"http://arxiv.org/abs/2412.09603v2","updated":"2025-03-27T17:59:55Z","published":"2024-12-12T18:59:25Z","title":"Do Multimodal Large Language Models See Like Humans?","summary":"  Multimodal Large Language Models (MLLMs) have achieved impressive results on\nvarious vision tasks, leveraging recent advancements in large language models.\nHowever, a critical question remains unaddressed: do MLLMs perceive visual\ninformation similarly to humans? Current benchmarks lack the ability to\nevaluate MLLMs from this perspective. To address this challenge, we introduce\nHVSBench, a large-scale benchmark designed to assess the alignment between\nMLLMs and the human visual system (HVS) on fundamental vision tasks that mirror\nhuman vision. HVSBench curated over 85K multimodal samples, spanning 13\ncategories and 5 fields in HVS, including Prominence, Subitizing, Prioritizing,\nFree-Viewing, and Searching. Extensive experiments demonstrate the\neffectiveness of our benchmark in providing a comprehensive evaluation of\nMLLMs. Specifically, we evaluate 13 MLLMs, revealing that even the best models\nshow significant room for improvement, with most achieving only moderate\nresults. Our experiments reveal that HVSBench presents a new and significant\nchallenge for cutting-edge MLLMs. Diverse human participants attained strong\nperformance, significantly outperforming MLLMs, which further underscores the\nbenchmark's high quality. We believe that HVSBench will facilitate research on\nhuman-aligned and explainable MLLMs, marking a key step in understanding how\nMLLMs perceive and process visual information.\n","authors":["Jiaying Lin","Shuquan Ye","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2412.09603v2.pdf","comment":"Project page: https://jiaying.link/HVSBench/"},{"id":"http://arxiv.org/abs/2503.21778v1","updated":"2025-03-27T17:59:54Z","published":"2025-03-27T17:59:54Z","title":"HS-SLAM: Hybrid Representation with Structural Supervision for Improved\n  Dense SLAM","summary":"  NeRF-based SLAM has recently achieved promising results in tracking and\nreconstruction. However, existing methods face challenges in providing\nsufficient scene representation, capturing structural information, and\nmaintaining global consistency in scenes emerging significant movement or being\nforgotten. To this end, we present HS-SLAM to tackle these problems. To enhance\nscene representation capacity, we propose a hybrid encoding network that\ncombines the complementary strengths of hash-grid, tri-planes, and one-blob,\nimproving the completeness and smoothness of reconstruction. Additionally, we\nintroduce structural supervision by sampling patches of non-local pixels rather\nthan individual rays to better capture the scene structure. To ensure global\nconsistency, we implement an active global bundle adjustment (BA) to eliminate\ncamera drifts and mitigate accumulative errors. Experimental results\ndemonstrate that HS-SLAM outperforms the baselines in tracking and\nreconstruction accuracy while maintaining the efficiency required for robotics.\n","authors":["Ziren Gong","Fabio Tosi","Youmin Zhang","Stefano Mattoccia","Matteo Poggi"],"pdf_url":"https://arxiv.org/pdf/2503.21778v1.pdf","comment":"ICRA 2025. Project Page: https://zorangong.github.io/HS-SLAM/"},{"id":"http://arxiv.org/abs/2503.21777v1","updated":"2025-03-27T17:59:52Z","published":"2025-03-27T17:59:52Z","title":"Test-Time Visual In-Context Tuning","summary":"  Visual in-context learning (VICL), as a new paradigm in computer vision,\nallows the model to rapidly adapt to various tasks with only a handful of\nprompts and examples. While effective, the existing VICL paradigm exhibits poor\ngeneralizability under distribution shifts. In this work, we propose test-time\nVisual In-Context Tuning (VICT), a method that can adapt VICL models on the fly\nwith a single test sample. Specifically, we flip the role between the task\nprompts and the test sample and use a cycle consistency loss to reconstruct the\noriginal task prompt output. Our key insight is that a model should be aware of\na new test distribution if it can successfully recover the original task\nprompts. Extensive experiments on six representative vision tasks ranging from\nhigh-level visual understanding to low-level image processing, with 15 common\ncorruptions, demonstrate that our VICT can improve the generalizability of VICL\nto unseen new domains. In addition, we show the potential of applying VICT for\nunseen tasks at test time. Code: https://github.com/Jiahao000/VICT.\n","authors":["Jiahao Xie","Alessio Tonioni","Nathalie Rauschmayr","Federico Tombari","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2503.21777v1.pdf","comment":"CVPR 2025. Code: https://github.com/Jiahao000/VICT"},{"id":"http://arxiv.org/abs/2503.21776v1","updated":"2025-03-27T17:59:51Z","published":"2025-03-27T17:59:51Z","title":"Video-R1: Reinforcing Video Reasoning in MLLMs","summary":"  Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\nwithin multimodal large language models (MLLMs). However, directly applying RL\ntraining with the GRPO algorithm to video reasoning presents two primary\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\nscarcity of high-quality video-reasoning data. To address these issues, we\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\ninformation in videos for reasoning. Additionally, instead of relying solely on\nvideo data, we incorporate high-quality image-reasoning data into the training\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\nand Video-R1-260k for RL training, both comprising image and video data.\nExperimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncodes, models, data are released.\n","authors":["Kaituo Feng","Kaixiong Gong","Bohao Li","Zonghao Guo","Yibing Wang","Tianshuo Peng","Benyou Wang","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2503.21776v1.pdf","comment":"Project page: https://github.com/tulerfeng/Video-R1"},{"id":"http://arxiv.org/abs/2503.21774v1","updated":"2025-03-27T17:59:46Z","published":"2025-03-27T17:59:46Z","title":"Optimal Stepsize for Diffusion Sampling","summary":"  Diffusion models achieve remarkable generation quality but suffer from\ncomputational intensive sampling due to suboptimal step discretization. While\nexisting works focus on optimizing denoising directions, we address the\nprincipled design of stepsize schedules. This paper proposes Optimal Stepsize\nDistillation, a dynamic programming framework that extracts theoretically\noptimal schedules by distilling knowledge from reference trajectories. By\nreformulating stepsize optimization as recursive error minimization, our method\nguarantees global discretization bounds through optimal substructure\nexploitation. Crucially, the distilled schedules demonstrate strong robustness\nacross architectures, ODE solvers, and noise schedules. Experiments show 10x\naccelerated text-to-image generation while preserving 99.4% performance on\nGenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.\n","authors":["Jianning Pei","Han Hu","Shuyang Gu"],"pdf_url":"https://arxiv.org/pdf/2503.21774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21775v1","updated":"2025-03-27T17:59:46Z","published":"2025-03-27T17:59:46Z","title":"StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross\n  Fusion","summary":"  We present StyleMotif, a novel Stylized Motion Latent Diffusion model,\ngenerating motion conditioned on both content and style from multiple\nmodalities. Unlike existing approaches that either focus on generating diverse\nmotion content or transferring style from sequences, StyleMotif seamlessly\nsynthesizes motion across a wide range of content while incorporating stylistic\ncues from multi-modal inputs, including motion, text, image, video, and audio.\nTo achieve this, we introduce a style-content cross fusion mechanism and align\na style encoder with a pre-trained multi-modal model, ensuring that the\ngenerated motion accurately captures the reference style while preserving\nrealism. Extensive experiments demonstrate that our framework surpasses\nexisting methods in stylized motion generation and exhibits emergent\ncapabilities for multi-modal motion stylization, enabling more nuanced motion\nsynthesis. Source code and pre-trained models will be released upon acceptance.\nProject Page: https://stylemotif.github.io\n","authors":["Ziyu Guo","Young Yoon Lee","Joseph Liu","Yizhak Ben-Shabat","Victor Zordan","Mubbasir Kapadia"],"pdf_url":"https://arxiv.org/pdf/2503.21775v1.pdf","comment":"Project Page: https://stylemotif.github.io"},{"id":"http://arxiv.org/abs/2503.21772v1","updated":"2025-03-27T17:59:44Z","published":"2025-03-27T17:59:44Z","title":"LOCORE: Image Re-ranking with Long-Context Sequence Modeling","summary":"  We introduce LOCORE, Long-Context Re-ranker, a model that takes as input\nlocal descriptors corresponding to an image query and a list of gallery images\nand outputs similarity scores between the query and each gallery image. This\nmodel is used for image retrieval, where typically a first ranking is performed\nwith an efficient similarity measure, and then a shortlist of top-ranked images\nis re-ranked based on a more fine-grained similarity measure. Compared to\nexisting methods that perform pair-wise similarity estimation with local\ndescriptors or list-wise re-ranking with global descriptors, LOCORE is the\nfirst method to perform list-wise re-ranking with local descriptors. To achieve\nthis, we leverage efficient long-context sequence models to effectively capture\nthe dependencies between query and gallery images at the local-descriptor\nlevel. During testing, we process long shortlists with a sliding window\nstrategy that is tailored to overcome the context size limitations of sequence\nmodels. Our approach achieves superior performance compared with other\nre-rankers on established image retrieval benchmarks of landmarks (ROxf and\nRPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200)\nwhile having comparable latency to the pair-wise local descriptor re-rankers.\n","authors":["Zilin Xiao","Pavel Suma","Ayush Sachdeva","Hao-Jen Wang","Giorgos Kordopatis-Zilos","Giorgos Tolias","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2503.21772v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.21771v1","updated":"2025-03-27T17:59:43Z","published":"2025-03-27T17:59:43Z","title":"A Unified Image-Dense Annotation Generation Model for Underwater Scenes","summary":"  Underwater dense prediction, especially depth estimation and semantic\nsegmentation, is crucial for gaining a comprehensive understanding of\nunderwater scenes. Nevertheless, high-quality and large-scale underwater\ndatasets with dense annotations remain scarce because of the complex\nenvironment and the exorbitant data collection costs. This paper proposes a\nunified Text-to-Image and DEnse annotation generation method (TIDE) for\nunderwater scenes. It relies solely on text as input to simultaneously generate\nrealistic underwater images and multiple highly consistent dense annotations.\nSpecifically, we unify the generation of text-to-image and text-to-dense\nannotations within a single model. The Implicit Layout Sharing mechanism (ILS)\nand cross-modal interaction method called Time Adaptive Normalization (TAN) are\nintroduced to jointly optimize the consistency between image and dense\nannotations. We synthesize a large-scale underwater dataset using TIDE to\nvalidate the effectiveness of our method in underwater dense prediction tasks.\nThe results demonstrate that our method effectively improves the performance of\nexisting underwater dense prediction models and mitigates the scarcity of\nunderwater data with dense annotations. We hope our method can offer new\nperspectives on alleviating data scarcity issues in other fields. The code is\navailable at https: //github.com/HongkLin/TIDE.\n","authors":["Hongkai Lin","Dingkang Liang","Zhenghao Qi","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2503.21771v1.pdf","comment":"Accepted by CVPR 2025. The code is available at https:\n  //github.com/HongkLin/TIDE"},{"id":"http://arxiv.org/abs/2503.21770v1","updated":"2025-03-27T17:59:33Z","published":"2025-03-27T17:59:33Z","title":"Visual Jenga: Discovering Object Dependencies via Counterfactual\n  Inpainting","summary":"  This paper proposes a novel scene understanding task called Visual Jenga.\nDrawing inspiration from the game Jenga, the proposed task involves\nprogressively removing objects from a single image until only the background\nremains. Just as Jenga players must understand structural dependencies to\nmaintain tower stability, our task reveals the intrinsic relationships between\nscene elements by systematically exploring which objects can be removed while\npreserving scene coherence in both physical and geometric sense. As a starting\npoint for tackling the Visual Jenga task, we propose a simple, data-driven,\ntraining-free approach that is surprisingly effective on a range of real-world\nimages. The principle behind our approach is to utilize the asymmetry in the\npairwise relationships between objects within a scene and employ a large\ninpainting model to generate a set of counterfactuals to quantify the\nasymmetry.\n","authors":["Anand Bhattad","Konpat Preechakul","Alexei A. Efros"],"pdf_url":"https://arxiv.org/pdf/2503.21770v1.pdf","comment":"project page: https://visualjenga.github.io/"},{"id":"http://arxiv.org/abs/2404.07977v2","updated":"2025-03-27T17:59:25Z","published":"2024-04-11T17:57:19Z","title":"Gaga: Group Any Gaussians via 3D-aware Memory Bank","summary":"  We introduce Gaga, a framework that reconstructs and segments open-world 3D\nscenes by leveraging inconsistent 2D masks predicted by zero-shot\nclass-agnostic segmentation models. Contrasted to prior 3D scene segmentation\napproaches that rely on video object tracking or contrastive learning methods,\nGaga utilizes spatial information and effectively associates object masks\nacross diverse camera poses through a novel 3D-aware memory bank. By\neliminating the assumption of continuous view changes in training images, Gaga\ndemonstrates robustness to variations in camera poses, particularly beneficial\nfor sparsely sampled images, ensuring precise mask label consistency.\nFurthermore, Gaga accommodates 2D segmentation masks from diverse sources and\ndemonstrates robust performance with different open-world zero-shot\nclass-agnostic segmentation models, significantly enhancing its versatility.\nExtensive qualitative and quantitative evaluations demonstrate that Gaga\nperforms favorably against state-of-the-art methods, emphasizing its potential\nfor real-world applications such as 3D scene understanding and manipulation.\n","authors":["Weijie Lyu","Xueting Li","Abhijit Kundu","Yi-Hsuan Tsai","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2404.07977v2.pdf","comment":"Project Page: https://weijielyu.github.io/Gaga"},{"id":"http://arxiv.org/abs/2503.21767v1","updated":"2025-03-27T17:59:05Z","published":"2025-03-27T17:59:05Z","title":"Semantic Consistent Language Gaussian Splatting for Point-Level\n  Open-vocabulary Querying","summary":"  Open-vocabulary querying in 3D Gaussian Splatting aims to identify\nsemantically relevant regions within a 3D Gaussian representation based on a\ngiven text query. Prior work, such as LangSplat, addressed this task by\nretrieving these regions in the form of segmentation masks on 2D renderings.\nMore recently, OpenGaussian introduced point-level querying, which directly\nselects a subset of 3D Gaussians. In this work, we propose a point-level\nquerying method that builds upon LangSplat's framework. Our approach improves\nthe framework in two key ways: (a) we leverage masklets from the Segment\nAnything Model 2 (SAM2) to establish semantic consistent ground-truth for\ndistilling the language Gaussians; (b) we introduces a novel two-step querying\napproach that first retrieves the distilled ground-truth and subsequently uses\nthe ground-truth to query the individual Gaussians. Experimental evaluations on\nthree benchmark datasets demonstrate that the proposed method achieves better\nperformance compared to state-of-the-art approaches. For instance, our method\nachieves an mIoU improvement of +20.42 on the 3D-OVS dataset.\n","authors":["Hairong Yin","Huangying Zhan","Yi Xu","Raymond A. Yeh"],"pdf_url":"https://arxiv.org/pdf/2503.21767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21766v1","updated":"2025-03-27T17:59:02Z","published":"2025-03-27T17:59:02Z","title":"Stable-SCore: A Stable Registration-based Framework for 3D Shape\n  Correspondence","summary":"  Establishing character shape correspondence is a critical and fundamental\ntask in computer vision and graphics, with diverse applications including\nre-topology, attribute transfer, and shape interpolation. Current dominant\nfunctional map methods, while effective in controlled scenarios, struggle in\nreal situations with more complex challenges such as non-isometric shape\ndiscrepancies. In response, we revisit registration-for-correspondence methods\nand tap their potential for more stable shape correspondence estimation. To\novercome their common issues including unstable deformations and the necessity\nfor careful pre-alignment or high-quality initial 3D correspondences, we\nintroduce Stable-SCore: A Stable Registration-based Framework for 3D Shape\nCorrespondence. We first re-purpose a foundation model for 2D character\ncorrespondence that ensures reliable and stable 2D mappings. Crucially, we\npropose a novel Semantic Flow Guided Registration approach that leverages 2D\ncorrespondence to guide mesh deformations. Our framework significantly\nsurpasses existing methods in challenging scenarios, and brings possibilities\nfor a wide array of real applications, as demonstrated in our results.\n","authors":["Haolin Liu","Xiaohang Zhan","Zizheng Yan","Zhongjin Luo","Yuxin Wen","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2503.21766v1.pdf","comment":"Accepted by CVPR 2025. Homepage:\n  https://haolinliu97.github.io/Stable-Score/"},{"id":"http://arxiv.org/abs/2503.21765v1","updated":"2025-03-27T17:58:33Z","published":"2025-03-27T17:58:33Z","title":"Exploring the Evolution of Physics Cognition in Video Generation: A\n  Survey","summary":"  Recent advancements in video generation have witnessed significant progress,\nespecially with the rapid advancement of diffusion models. Despite this, their\ndeficiencies in physical cognition have gradually received widespread attention\n- generated content often violates the fundamental laws of physics, falling\ninto the dilemma of ''visual realism but physical absurdity\". Researchers began\nto increasingly recognize the importance of physical fidelity in video\ngeneration and attempted to integrate heuristic physical cognition such as\nmotion representations and physical knowledge into generative systems to\nsimulate real-world dynamic scenarios. Considering the lack of a systematic\noverview in this field, this survey aims to provide a comprehensive summary of\narchitecture designs and their applications to fill this gap. Specifically, we\ndiscuss and organize the evolutionary process of physical cognition in video\ngeneration from a cognitive science perspective, while proposing a three-tier\ntaxonomy: 1) basic schema perception for generation, 2) passive cognition of\nphysical knowledge for generation, and 3) active cognition for world\nsimulation, encompassing state-of-the-art methods, classical paradigms, and\nbenchmarks. Subsequently, we emphasize the inherent key challenges in this\ndomain and delineate potential pathways for future research, contributing to\nadvancing the frontiers of discussion in both academia and industry. Through\nstructured review and interdisciplinary analysis, this survey aims to provide\ndirectional guidance for developing interpretable, controllable, and physically\nconsistent video generation paradigms, thereby propelling generative models\nfrom the stage of ''visual mimicry'' towards a new phase of ''human-like\nphysical comprehension''.\n","authors":["Minghui Lin","Xiang Wang","Yishan Wang","Shu Wang","Fengqi Dai","Pengxiang Ding","Cunxiang Wang","Zhengrong Zuo","Nong Sang","Siteng Huang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2503.21765v1.pdf","comment":"A comprehensive list of papers studied in this survey is available at\n  https://github.com/minnie-lin/Awesome-Physics-Cognition-based-Video-Generation"},{"id":"http://arxiv.org/abs/2502.15682v2","updated":"2025-03-27T17:57:43Z","published":"2025-02-21T18:59:57Z","title":"ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval","summary":"  The objective in this paper is to improve the performance of text-to-image\nretrieval. To this end, we introduce a new framework that can boost the\nperformance of large-scale pre-trained vision-language models, so that they can\nbe used for text-to-image re-ranking. The approach, Enhanced Language-Image\nPre-training (ELIP), uses the text query, via a simple MLP mapping network, to\npredict a set of visual prompts to condition the ViT image encoding. ELIP can\neasily be applied to the commonly used CLIP, SigLIP and BLIP-2 networks. To\ntrain the architecture with limited computing resources, we develop a 'student\nfriendly' best practice, involving global hard sample mining, and curation of a\nlarge-scale dataset. On the evaluation side, we set up two new\nout-of-distribution (OOD) benchmarks, Occluded COCO and ImageNet-R, to assess\nthe zero-shot generalisation of the models to different domains. The results\ndemonstrate that ELIP significantly boosts CLIP/SigLIP/SigLIP-2 text-to-image\nretrieval performance and outperforms BLIP-2 on several benchmarks, as well as\nproviding an easy means to adapt to OOD datasets.\n","authors":["Guanqi Zhan","Yuanpei Liu","Kai Han","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2502.15682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21761v1","updated":"2025-03-27T17:57:32Z","published":"2025-03-27T17:57:32Z","title":"Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single\n  Video","summary":"  This paper presents a unified approach to understanding dynamic scenes from\ncasual videos. Large pretrained vision foundation models, such as\nvision-language, video depth prediction, motion tracking, and segmentation\nmodels, offer promising capabilities. However, training a single model for\ncomprehensive 4D understanding remains challenging. We introduce Uni4D, a\nmulti-stage optimization framework that harnesses multiple pretrained models to\nadvance dynamic 3D modeling, including static/dynamic reconstruction, camera\npose estimation, and dense 3D motion tracking. Our results show\nstate-of-the-art performance in dynamic 4D modeling with superior visual\nquality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the\neffectiveness of repurposing visual foundation models for 4D understanding.\n","authors":["David Yifan Yao","Albert J. Zhai","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.21761v1.pdf","comment":"CVPR 2025. Project page (with code):\n  https://davidyao99.github.io/uni4d"},{"id":"http://arxiv.org/abs/2503.21757v1","updated":"2025-03-27T17:57:07Z","published":"2025-03-27T17:57:07Z","title":"Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck","summary":"  In this work, we aim to compress the vision tokens of a Large Vision Language\nModel (LVLM) into a representation that is simultaneously suitable for (a)\ngenerative and (b) discriminative tasks, (c) is nearly lossless, and (d) is\nstorage-efficient. We propose a novel compression approach, called Fwd2Bot,\nthat uses the LVLM itself to compress the visual information in a task-agnostic\nmanner. At the core of Fwd2bot there exists a \"double-forward pass\" training\nstrategy, whereby, during the first forward pass, the LLM (of the LVLM) creates\na bottleneck by condensing the visual information into a small number of\nsummary tokens. Then, using the same LLM, the second forward pass processes the\nlanguage instruction(s) alongside the summary tokens, used as a direct\nreplacement for the image ones. The training signal is provided by two losses:\nan autoregressive one applied after the second pass that provides a direct\noptimization objective for compression, and a contrastive loss, applied after\nthe first pass, that further boosts the representation strength, especially for\ndiscriminative tasks. The training is further enhanced by stage-specific\nadapters. We accompany the proposed method by an in-depth ablation study.\nOverall, Fwd2Bot results in highly-informative compressed representations\nsuitable for both generative and discriminative tasks. For generative tasks, we\noffer a 2x higher compression rate without compromising the generative\ncapabilities, setting a new state-of-the-art result. For discriminative tasks,\nwe set a new state-of-the-art on image retrieval and compositionality.\n","authors":["Adrian Bulat","Yassine Ouali","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2503.21757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21758v1","updated":"2025-03-27T17:57:07Z","published":"2025-03-27T17:57:07Z","title":"Lumina-Image 2.0: A Unified and Efficient Image Generative Framework","summary":"  We introduce Lumina-Image 2.0, an advanced text-to-image generation framework\nthat achieves significant progress compared to previous work, Lumina-Next.\nLumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts\na unified architecture (Unified Next-DiT) that treats text and image tokens as\na joint sequence, enabling natural cross-modal interactions and allowing\nseamless task expansion. Besides, since high-quality captioners can provide\nsemantically well-aligned text-image training pairs, we introduce a unified\ncaptioning system, Unified Captioner (UniCap), specifically designed for T2I\ngeneration tasks. UniCap excels at generating comprehensive and accurate\ncaptions, accelerating convergence and enhancing prompt adherence. (2)\nEfficiency - to improve the efficiency of our proposed model, we develop\nmulti-stage progressive training strategies and introduce inference\nacceleration techniques without compromising image quality. Extensive\nevaluations on academic benchmarks and public text-to-image arenas show that\nLumina-Image 2.0 delivers strong performances even with only 2.6B parameters,\nhighlighting its scalability and design efficiency. We have released our\ntraining details, code, and models at\nhttps://github.com/Alpha-VLLM/Lumina-Image-2.0.\n","authors":["Qi Qin","Le Zhuo","Yi Xin","Ruoyi Du","Zhen Li","Bin Fu","Yiting Lu","Jiakang Yuan","Xinyue Li","Dongyang Liu","Xiangyang Zhu","Manyuan Zhang","Will Beddow","Erwann Millon","Victor Perez","Wenhai Wang","Conghui He","Bo Zhang","Xiaohong Liu","Hongsheng Li","Yu Qiao","Chang Xu","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2503.21758v1.pdf","comment":"Tech Report, 21 pages, 12 figures"},{"id":"http://arxiv.org/abs/2503.21755v1","updated":"2025-03-27T17:57:01Z","published":"2025-03-27T17:57:01Z","title":"VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness","summary":"  Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.\n","authors":["Dian Zheng","Ziqi Huang","Hongbo Liu","Kai Zou","Yinan He","Fan Zhang","Yuanhan Zhang","Jingwen He","Wei-Shi Zheng","Yu Qiao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2503.21755v1.pdf","comment":"Equal contributions from first two authors. Project page:\n  https://vchitect.github.io/VBench-2.0-project/ Code:\n  https://github.com/Vchitect/VBench"},{"id":"http://arxiv.org/abs/2406.12831v3","updated":"2025-03-27T17:56:31Z","published":"2024-06-18T17:51:37Z","title":"VIA: Unified Spatiotemporal Video Adaptation Framework for Global and\n  Local Video Editing","summary":"  Video editing serves as a fundamental pillar of digital media, spanning\napplications in entertainment, education, and professional communication.\nHowever, previous methods often overlook the necessity of comprehensively\nunderstanding both global and local contexts, leading to inaccurate and\ninconsistent edits in the spatiotemporal dimension, especially for long videos.\nIn this paper, we introduce VIA, a unified spatiotemporal Video Adaptation\nframework for global and local video editing, pushing the limits of\nconsistently editing minute-long videos. First, to ensure local consistency\nwithin individual frames, we designed test-time editing adaptation to adapt a\npre-trained image editing model for improving consistency between potential\nediting directions and the text instruction, and adapts masked latent variables\nfor precise local control. Furthermore, to maintain global consistency over the\nvideo sequence, we introduce spatiotemporal adaptation that recursively gather\nconsistent attention variables in key frames and strategically applies them\nacross the whole sequence to realize the editing effects. Extensive experiments\ndemonstrate that, compared to baseline methods, our VIA approach produces edits\nthat are more faithful to the source videos, more coherent in the\nspatiotemporal context, and more precise in local control. More importantly, we\nshow that VIA can achieve consistent long video editing in minutes, unlocking\nthe potential for advanced video editing tasks over long video sequences.\n","authors":["Jing Gu","Yuwei Fang","Ivan Skorokhodov","Peter Wonka","Xinya Du","Sergey Tulyakov","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.12831v3.pdf","comment":"18 pages, 16 figures"},{"id":"http://arxiv.org/abs/2503.21751v1","updated":"2025-03-27T17:56:24Z","published":"2025-03-27T17:56:24Z","title":"Reconstructing Humans with a Biomechanically Accurate Skeleton","summary":"  In this paper, we introduce a method for reconstructing 3D humans from a\nsingle image using a biomechanically accurate skeleton model. To achieve this,\nwe train a transformer that takes an image as input and estimates the\nparameters of the model. Due to the lack of training data for this task, we\nbuild a pipeline to produce pseudo ground truth model parameters for single\nimages and implement a training procedure that iteratively refines these pseudo\nlabels. Compared to state-of-the-art methods for 3D human mesh recovery, our\nmodel achieves competitive performance on standard benchmarks, while it\nsignificantly outperforms them in settings with extreme 3D poses and\nviewpoints. Additionally, we show that previous reconstruction methods\nfrequently violate joint angle limits, leading to unnatural rotations. In\ncontrast, our approach leverages the biomechanically plausible degrees of\nfreedom making more realistic joint rotation estimates. We validate our\napproach across multiple human pose estimation benchmarks. We make the code,\nmodels and data available at: https://isshikihugh.github.io/HSMR/\n","authors":["Yan Xia","Xiaowei Zhou","Etienne Vouga","Qixing Huang","Georgios Pavlakos"],"pdf_url":"https://arxiv.org/pdf/2503.21751v1.pdf","comment":"CVPR 2025. Project Webpage: https://isshikihugh.github.io/HSMR/"},{"id":"http://arxiv.org/abs/2503.21749v1","updated":"2025-03-27T17:56:15Z","published":"2025-03-27T17:56:15Z","title":"LeX-Art: Rethinking Text Generation via Scalable High-Quality Data\n  Synthesis","summary":"  We introduce LeX-Art, a comprehensive suite for high-quality text-image\nsynthesis that systematically bridges the gap between prompt expressiveness and\ntext rendering fidelity. Our approach follows a data-centric paradigm,\nconstructing a high-quality data synthesis pipeline based on Deepseek-R1 to\ncurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined\n1024$\\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer,\na robust prompt enrichment model, and train two text-to-image models, LeX-FLUX\nand LeX-Lumina, achieving state-of-the-art text rendering performance. To\nsystematically evaluate visual text generation, we introduce LeX-Bench, a\nbenchmark that assesses fidelity, aesthetics, and alignment, complemented by\nPairwise Normalized Edit Distance (PNED), a novel metric for robust text\naccuracy evaluation. Experiments demonstrate significant improvements, with\nLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX\noutperforming baselines in color (+3.18%), positional (+4.45%), and font\naccuracy (+3.81%). Our codes, models, datasets, and demo are publicly\navailable.\n","authors":["Shitian Zhao","Qilong Wu","Xinyue Li","Bo Zhang","Ming Li","Qi Qin","Dongyang Liu","Kaipeng Zhang","Hongsheng Li","Yu Qiao","Peng Gao","Bin Fu","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2503.21749v1.pdf","comment":"Project page: https://zhaoshitian.github.io/lexart/"},{"id":"http://arxiv.org/abs/2503.21747v1","updated":"2025-03-27T17:53:50Z","published":"2025-03-27T17:53:50Z","title":"CTRL-O: Language-Controllable Object-Centric Visual Representation\n  Learning","summary":"  Object-centric representation learning aims to decompose visual scenes into\nfixed-size vectors called \"slots\" or \"object files\", where each slot captures a\ndistinct object. Current state-of-the-art object-centric models have shown\nremarkable success in object discovery in diverse domains, including complex\nreal-world scenes. However, these models suffer from a key limitation: they\nlack controllability. Specifically, current object-centric models learn\nrepresentations based on their preconceived understanding of objects, without\nallowing user input to guide which objects are represented. Introducing\ncontrollability into object-centric models could unlock a range of useful\ncapabilities, such as the ability to extract instance-specific representations\nfrom a scene. In this work, we propose a novel approach for user-directed\ncontrol over slot representations by conditioning slots on language\ndescriptions. The proposed ConTRoLlable Object-centric representation learning\napproach, which we term CTRL-O, achieves targeted object-language binding in\ncomplex real-world scenes without requiring mask supervision. Next, we apply\nthese controllable slot representations on two downstream vision language\ntasks: text-to-image generation and visual question answering. The proposed\napproach enables instance-specific text-to-image generation and also achieves\nstrong performance on visual question answering.\n","authors":["Aniket Didolkar","Andrii Zadaianchuk","Rabiul Awal","Maximilian Seitzer","Efstratios Gavves","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2503.21747v1.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2503.21745v1","updated":"2025-03-27T17:53:00Z","published":"2025-03-27T17:53:00Z","title":"3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models","summary":"  3D generation is experiencing rapid advancements, while the development of 3D\nevaluation has not kept pace. How to keep automatic evaluation equitably\naligned with human perception has become a well-recognized challenge. Recent\nadvances in the field of language and image generation have explored human\npreferences and showcased respectable fitting ability. However, the 3D domain\nstill lacks such a comprehensive preference dataset over generative models. To\nmitigate this absence, we develop 3DGen-Arena, an integrated platform in a\nbattle manner. Then, we carefully design diverse text and image prompts and\nleverage the arena platform to gather human preferences from both public users\nand expert annotators, resulting in a large-scale multi-dimension human\npreference dataset 3DGen-Bench. Using this dataset, we further train a\nCLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator,\n3DGen-Eval. These two models innovatively unify the quality evaluation of\ntext-to-3D and image-to-3D generation, and jointly form our automated\nevaluation system with their respective strengths. Extensive experiments\ndemonstrate the efficacy of our scoring model in predicting human preferences,\nexhibiting a superior correlation with human ranks compared to existing\nmetrics. We believe that our 3DGen-Bench dataset and automated evaluation\nsystem will foster a more equitable evaluation in the field of 3D generation,\nfurther promoting the development of 3D generative models and their downstream\napplications.\n","authors":["Yuhan Zhang","Mengchen Zhang","Tong Wu","Tengfei Wang","Gordon Wetzstein","Dahua Lin","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2503.21745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21732v1","updated":"2025-03-27T17:46:42Z","published":"2025-03-27T17:46:42Z","title":"SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling","summary":"  Creating high-fidelity 3D meshes with arbitrary topology, including open\nsurfaces and complex interiors, remains a significant challenge. Existing\nimplicit field methods often require costly and detail-degrading watertight\nconversion, while other approaches struggle with high resolutions. This paper\nintroduces SparseFlex, a novel sparse-structured isosurface representation that\nenables differentiable mesh reconstruction at resolutions up to $1024^3$\ndirectly from rendering losses. SparseFlex combines the accuracy of Flexicubes\nwith a sparse voxel structure, focusing computation on surface-adjacent regions\nand efficiently handling open surfaces. Crucially, we introduce a frustum-aware\nsectional voxel training strategy that activates only relevant voxels during\nrendering, dramatically reducing memory consumption and enabling\nhigh-resolution training. This also allows, for the first time, the\nreconstruction of mesh interiors using only rendering supervision. Building\nupon this, we demonstrate a complete shape modeling pipeline by training a\nvariational autoencoder (VAE) and a rectified flow transformer for high-quality\n3D shape generation. Our experiments show state-of-the-art reconstruction\naccuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in\nF-score compared to previous methods, and demonstrate the generation of\nhigh-resolution, detailed 3D shapes with arbitrary topology. By enabling\nhigh-resolution, differentiable mesh reconstruction and generation with\nrendering losses, SparseFlex significantly advances the state-of-the-art in 3D\nshape representation and modeling.\n","authors":["Xianglong He","Zi-Xin Zou","Chia-Hao Chen","Yuan-Chen Guo","Ding Liang","Chun Yuan","Wanli Ouyang","Yan-Pei Cao","Yangguang Li"],"pdf_url":"https://arxiv.org/pdf/2503.21732v1.pdf","comment":"Project page: https://xianglonghe.github.io/TripoSF"},{"id":"http://arxiv.org/abs/2410.14770v2","updated":"2025-03-27T17:45:43Z","published":"2024-10-18T17:53:07Z","title":"A Survey on Computational Solutions for Reconstructing Complete Objects\n  by Reassembling Their Fractured Parts","summary":"  Reconstructing a complete object from its parts is a fundamental problem in\nmany scientific domains. The purpose of this article is to provide a systematic\nsurvey on this topic. The reassembly problem requires understanding the\nattributes of individual pieces and establishing matches between different\npieces. Many approaches also model priors of the underlying complete object.\nExisting approaches are tightly connected problems of shape segmentation, shape\nmatching, and learning shape priors. We provide existing algorithms in this\ncontext and emphasize their similarities and differences to general-purpose\napproaches. We also survey the trends from early non-deep learning approaches\nto more recent deep learning approaches. In addition to algorithms, this survey\nwill also describe existing datasets, open-source software packages, and\napplications. To the best of our knowledge, this is the first comprehensive\nsurvey on this topic in computer graphics.\n","authors":["Jiaxin Lu","Yongqing Liang","Huijun Han","Jiacheng Hua","Junfeng Jiang","Xin Li","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14770v2.pdf","comment":"36 pages, 22 figures"},{"id":"http://arxiv.org/abs/2501.05510v2","updated":"2025-03-27T17:40:09Z","published":"2025-01-09T19:00:01Z","title":"OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video\n  Understanding?","summary":"  Temporal Awareness, the ability to reason dynamically based on the timestamp\nwhen a question is raised, is the key distinction between offline and online\nvideo LLMs. Unlike offline models, which rely on complete videos for static,\npost hoc analysis, online models process video streams incrementally and\ndynamically adapt their responses based on the timestamp at which the question\nis posed. Despite its significance, temporal awareness has not been adequately\nevaluated in existing benchmarks. To fill this gap, we present OVO-Bench\n(Online-VideO-Benchmark), a novel video benchmark that emphasizes the\nimportance of timestamps for advanced online video understanding capability\nbenchmarking. OVO-Bench evaluates the ability of video LLMs to reason and\nrespond to events occurring at specific timestamps under three distinct\nscenarios: (1) Backward tracing: trace back to past events to answer the\nquestion. (2) Real-time understanding: understand and respond to events as they\nunfold at the current timestamp. (3) Forward active responding: delay the\nresponse until sufficient future information becomes available to answer the\nquestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos\nand approximately human-curated 2,800 fine-grained meta-annotations with\nprecise timestamps. We combine automated generation pipelines with human\ncuration. With these high-quality samples, we further developed an evaluation\npipeline to systematically query video LLMs along the video timeline.\nEvaluations of nine Video-LLMs reveal that, despite advancements on traditional\nbenchmarks, current models struggle with online video understanding, showing a\nsignificant gap compared to human agents. We hope OVO-Bench will drive progress\nin video LLMs and inspire future research in online video reasoning. Our\nbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.\n","authors":["Yifei Li","Junbo Niu","Ziyang Miao","Chunjiang Ge","Yuanhang Zhou","Qihao He","Xiaoyi Dong","Haodong Duan","Shuangrui Ding","Rui Qian","Pan Zhang","Yuhang Zang","Yuhang Cao","Conghui He","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2501.05510v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2409.18119v2","updated":"2025-03-27T17:39:55Z","published":"2024-09-26T17:56:59Z","title":"Multi-View and Multi-Scale Alignment for Contrastive Language-Image\n  Pre-training in Mammography","summary":"  Contrastive Language-Image Pre-training (CLIP) demonstrates strong potential\nin medical image analysis but requires substantial data and computational\nresources. Due to these restrictions, existing CLIP applications in medical\nimaging focus mainly on modalities like chest X-rays that have abundant\nimage-report data available, leaving many other important modalities\nunderexplored. Here, we propose one of the first adaptations of the full CLIP\nmodel to mammography, which presents significant challenges due to labeled data\nscarcity, high-resolution images with small regions of interest, and class-wise\nimbalance. We first develop a specialized supervision framework for mammography\nthat leverages its multi-view nature. Furthermore, we design a symmetric local\nalignment module to better focus on detailed features in high-resolution\nimages. Lastly, we incorporate a parameter-efficient fine-tuning approach for\nlarge language models pre-trained with medical knowledge to address data\nlimitations. Our multi-view and multi-scale alignment (MaMA) method outperforms\nstate-of-the-art baselines for three different tasks on two large real-world\nmammography datasets, EMBED and RSNA-Mammo, with only 52% model size compared\nwith the largest baseline. The code is available at\nhttps://github.com/XYPB/MaMA\n","authors":["Yuexi Du","John Onofrey","Nicha C. Dvornek"],"pdf_url":"https://arxiv.org/pdf/2409.18119v2.pdf","comment":"This paper is accepted by IPMI 2025 for Oral Presentation"},{"id":"http://arxiv.org/abs/2503.21723v1","updated":"2025-03-27T17:36:55Z","published":"2025-03-27T17:36:55Z","title":"OccRobNet : Occlusion Robust Network for Accurate 3D Interacting\n  Hand-Object Pose Estimation","summary":"  Occlusion is one of the challenging issues when estimating 3D hand pose. This\nproblem becomes more prominent when hand interacts with an object or two hands\nare involved. In the past works, much attention has not been given to these\noccluded regions. But these regions contain important and beneficial\ninformation that is vital for 3D hand pose estimation. Thus, in this paper, we\npropose an occlusion robust and accurate method for the estimation of 3D\nhand-object pose from the input RGB image. Our method includes first localising\nthe hand joints using a CNN based model and then refining them by extracting\ncontextual information. The self attention transformer then identifies the\nspecific joints along with the hand identity. This helps the model to identify\nthe hand belongingness of a particular joint which helps to detect the joint\neven in the occluded region. Further, these joints with hand identity are then\nused to estimate the pose using cross attention mechanism. Thus, by identifying\nthe joints in the occluded region, the obtained network becomes robust to\nocclusion. Hence, this network achieves state-of-the-art results when evaluated\non the InterHand2.6M, HO3D and H$_2$O3D datasets.\n","authors":["Mallika Garg","Debashis Ghosh","Pyari Mohan Pradhan"],"pdf_url":"https://arxiv.org/pdf/2503.21723v1.pdf","comment":"Accepted in NATIONAL CONFERENCE ON COMMUNICATIONS (NCC) 2025"},{"id":"http://arxiv.org/abs/2503.21721v1","updated":"2025-03-27T17:35:14Z","published":"2025-03-27T17:35:14Z","title":"Evaluating Text-to-Image Synthesis with a Conditional Fréchet\n  Distance","summary":"  Evaluating text-to-image synthesis is challenging due to misalignment between\nestablished metrics and human preferences. We propose cFreD, a metric based on\nthe notion of Conditional Fr\\'echet Distance that explicitly accounts for both\nvisual fidelity and text-prompt alignment. Existing metrics such as Inception\nScore (IS), Fr\\'echet Inception Distance (FID) and CLIPScore assess either\nimage quality or image-text alignment but not both which limits their\ncorrelation with human preferences. Scoring models explicitly trained to\nreplicate human preferences require constant updates and may not generalize to\nnovel generation techniques or out-of-domain inputs. Through extensive\nexperiments across multiple recently proposed text-to-image models and diverse\nprompt datasets, we demonstrate that cFreD exhibits a higher correlation with\nhuman judgments compared to statistical metrics, including metrics trained with\nhuman preferences. Our findings validate cFreD as a robust, future-proof metric\nfor the systematic evaluation of text-to-image models, standardizing\nbenchmarking in this rapidly evolving field. We release our evaluation toolkit\nand benchmark in the appendix.\n","authors":["Jaywon Koo","Jefferson Hernandez","Moayed Haji-Ali","Ziyan Yang","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2503.21721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18943v2","updated":"2025-03-27T17:34:06Z","published":"2025-03-24T17:59:07Z","title":"SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding","summary":"  We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. We incorporate the two-stream SlowFast mechanism\ninto a streamlined training pipeline, and perform joint video-image training on\na carefully curated data mixture of only publicly available datasets. Our\nprimary focus is on highly efficient model scales (1B and 3B), demonstrating\nthat even relatively small Video LLMs can achieve state-of-the-art performance\non video understanding, meeting the demand for mobile-friendly models.\nExperimental results demonstrate that SF-LLaVA-1.5 achieves superior\nperformance on a wide range of video and image tasks, with robust results at\nall model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales across various video benchmarks.\n","authors":["Mingze Xu","Mingfei Gao","Shiyu Li","Jiasen Lu","Zhe Gan","Zhengfeng Lai","Meng Cao","Kai Kang","Yinfei Yang","Afshin Dehghan"],"pdf_url":"https://arxiv.org/pdf/2503.18943v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2502.06608v3","updated":"2025-03-27T17:25:50Z","published":"2025-02-10T16:07:54Z","title":"TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models","summary":"  Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprecedented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data processing, and insufficient exploration of\nadvanced techniques in the 3D domain. Current approaches to 3D shape generation\nface substantial challenges in terms of output quality, generalization\ncapability, and alignment with input conditions. We present TripoSG, a new\nstreamlined shape diffusion paradigm capable of generating high-fidelity 3D\nmeshes with precise correspondence to input images. Specifically, we propose:\n1) A large-scale rectified flow transformer for 3D shape generation, achieving\nstate-of-the-art fidelity through training on extensive, high-quality data. 2)\nA hybrid supervised training strategy combining SDF, normal, and eikonal losses\nfor 3D VAE, achieving high-quality 3D reconstruction performance. 3) A data\nprocessing pipeline to generate 2 million high-quality 3D samples, highlighting\nthe crucial rules for data quality and quantity in training 3D generative\nmodels. Through comprehensive experiments, we have validated the effectiveness\nof each component in our new framework. The seamless integration of these parts\nhas enabled TripoSG to achieve state-of-the-art performance in 3D shape\ngeneration. The resulting 3D shapes exhibit enhanced detail due to\nhigh-resolution capabilities and demonstrate exceptional fidelity to input\nimages. Moreover, TripoSG demonstrates improved versatility in generating 3D\nmodels from diverse image styles and contents, showcasing strong generalization\ncapabilities. To foster progress and innovation in the field of 3D generation,\nwe will make our model publicly available.\n","authors":["Yangguang Li","Zi-Xin Zou","Zexiang Liu","Dehu Wang","Yuan Liang","Zhipeng Yu","Xingchao Liu","Yuan-Chen Guo","Ding Liang","Wanli Ouyang","Yan-Pei Cao"],"pdf_url":"https://arxiv.org/pdf/2502.06608v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03314v2","updated":"2025-03-27T17:06:25Z","published":"2024-07-03T17:55:27Z","title":"BACON: Improving Clarity of Image Captions via Bag-of-Concept Graphs","summary":"  Advancements in large Vision-Language Models have brought precise, accurate\nimage captioning, vital for advancing multi-modal image understanding and\nprocessing. Yet these captions often carry lengthy, intertwined contexts that\nare difficult to parse and frequently overlook essential cues, posing a great\nbarrier for models like GroundingDINO and SDXL, which lack the strong text\nencoding and syntax analysis needed to fully leverage dense captions. To\naddress this, we propose BACON, a prompting method that breaks down\nVLM-generated captions into disentangled, structured elements such as objects,\nrelationships, styles, and themes. This approach not only minimizes confusion\nfrom handling complex contexts but also allows for efficient transfer into a\nJSON dictionary, enabling models without linguistic processing capabilities to\neasily access key information. We annotated 100,000 image-caption pairs using\nBACON with GPT-4V and trained an LLaVA captioner on this dataset, enabling it\nto produce BACON-style captions without relying on costly GPT-4V. Evaluations\nof overall quality, precision, and recall-as well as user studies-demonstrate\nthat the resulting caption model consistently outperforms other SOTA VLM models\nin generating high-quality captions. Besides, we show that BACON-style captions\nexhibit better clarity when applied to various models, enabling them to\naccomplish previously unattainable tasks or surpass existing SOTA solutions\nwithout training. For example, BACON-style captions help GroundingDINO achieve\n1.51x higher recall scores on open-vocabulary object detection tasks compared\nto leading methods.\n","authors":["Zhantao Yang","Ruili Feng","Keyu Yan","Huangji Wang","Zhicai Wang","Shangwen Zhu","Han Zhang","Jie Xiao","Pingyu Wu","Kai Zhu","Jixuan Chen","Chen-Wei Xie","Yue Yang","Hongyang Zhang","Yu Liu","Fan Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.03314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21699v1","updated":"2025-03-27T17:04:33Z","published":"2025-03-27T17:04:33Z","title":"MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX","summary":"  Frontier models have either been language-only or have primarily focused on\nvision and language modalities. Although recent advancements in models with\nvision and audio understanding capabilities have shown substantial progress,\nthe field lacks a standardized evaluation framework for thoroughly assessing\ntheir cross-modality perception performance. We introduce MAVERIX~(Multimodal\nAudio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and\n2,556 questions explicitly designed to evaluate multimodal models through tasks\nthat necessitate close integration of video and audio information. MAVERIX\nuniquely provides models with audiovisual tasks, closely mimicking the\nmultimodal perceptual experiences available to humans during inference and\ndecision-making processes. To our knowledge, MAVERIX is the first benchmark\naimed explicitly at assessing comprehensive audiovisual integration.\nExperiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show\nperformance approaching human levels (around 70% accuracy), while human experts\nreach near-ceiling performance (95.1%). With standardized evaluation protocols,\na rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a\nchallenging testbed for advancing audiovisual multimodal intelligence.\n","authors":["Liuyue Xie","George Z. Wei","Avik Kuthiala","Ce Zheng","Ananya Bal","Mosam Dabhi","Liting Wen","Taru Rustagi","Ethan Lai","Sushil Khyalia","Rohan Choudhury","Morteza Ziyadi","Xu Zhang","Hao Yang","László A. Jeni"],"pdf_url":"https://arxiv.org/pdf/2503.21699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21696v1","updated":"2025-03-27T17:00:51Z","published":"2025-03-27T17:00:51Z","title":"Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks","summary":"  Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.\n","authors":["Wenqi Zhang","Mengna Wang","Gangao Liu","Xu Huixin","Yiwei Jiang","Yongliang Shen","Guiyang Hou","Zhe Zheng","Hang Zhang","Xin Li","Weiming Lu","Peng Li","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2503.21696v1.pdf","comment":"Code: https://github.com/zwq2018/embodied_reasoner Dataset:\n  https://huggingface.co/datasets/zwq2018/embodied_reasoner"},{"id":"http://arxiv.org/abs/2503.21695v1","updated":"2025-03-27T16:59:39Z","published":"2025-03-27T16:59:39Z","title":"AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model\n  for High-Fidelity Histology Nuclei Segmentation","summary":"  Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches.\n","authors":["Jiahe Qian","Yaoyu Fang","Jinkui Hao","Bo Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.21695v1.pdf","comment":"13 pages, 4 tables, 2 figures"},{"id":"http://arxiv.org/abs/2503.21694v1","updated":"2025-03-27T16:59:15Z","published":"2025-03-27T16:59:15Z","title":"Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data","summary":"  It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only $2.5\\%$\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.\n","authors":["Zhiyuan Ma","Xinyue Liang","Rongyuan Wu","Xiangyu Zhu","Zhen Lei","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.21694v1.pdf","comment":"Accepted to CVPR 2025.\n  Code:https://github.com/theEricMa/TriplaneTurbo.\n  Demo:https://huggingface.co/spaces/ZhiyuanthePony/TriplaneTurbo"},{"id":"http://arxiv.org/abs/2503.21692v1","updated":"2025-03-27T16:57:33Z","published":"2025-03-27T16:57:33Z","title":"RapidPoseTriangulation: Multi-view Multi-person Whole-body Human Pose\n  Triangulation in a Millisecond","summary":"  The integration of multi-view imaging and pose estimation represents a\nsignificant advance in computer vision applications, offering new possibilities\nfor understanding human movement and interactions. This work presents a new\nalgorithm that improves multi-view multi-person pose estimation, focusing on\nfast triangulation speeds and good generalization capabilities. The approach\nextends to whole-body pose estimation, capturing details from facial\nexpressions to finger movements across multiple individuals and viewpoints.\nAdaptability to different settings is demonstrated through strong performance\nacross unseen datasets and configurations. To support further progress in this\nfield, all of this work is publicly accessible.\n","authors":["Daniel Bermuth","Alexander Poeppel","Wolfgang Reif"],"pdf_url":"https://arxiv.org/pdf/2503.21692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21690v1","updated":"2025-03-27T16:55:32Z","published":"2025-03-27T16:55:32Z","title":"CMED: A Child Micro-Expression Dataset","summary":"  Micro-expressions are short bursts of emotion that are difficult to hide.\nTheir detection in children is an important cue to assist psychotherapists in\nconducting better therapy. However, existing research on the detection of\nmicro-expressions has focused on adults, whose expressions differ in their\ncharacteristics from those of children. The lack of research is a direct\nconsequence of the lack of a child-based micro-expressions dataset as it is\nmuch more challenging to capture children's facial expressions due to the lack\nof predictability and controllability. This study compiles a dataset of\nspontaneous child micro-expression videos, the first of its kind, to the best\nof the authors knowledge. The dataset is captured in the wild using video\nconferencing software. This dataset enables us to then explore key features and\ndifferences between adult and child micro-expressions. This study also\nestablishes a baseline for the automated spotting and recognition of\nmicro-expressions in children using three approaches comprising of hand-created\nand learning-based approaches.\n","authors":[" Nikin~Matharaarachchi","Muhammad~Fermi Pasha"," Sonya~Coleman","Kah PengWong"],"pdf_url":"https://arxiv.org/pdf/2503.21690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11867v2","updated":"2025-03-27T16:45:04Z","published":"2024-09-18T10:48:10Z","title":"StableMamba: Distillation-free Scaling of Large SSMs for Images and\n  Videos","summary":"  State-space models (SSMs), exemplified by S4, have introduced a novel context\nmodeling method by integrating state-space techniques into deep learning.\nHowever, they struggle with global context modeling due to their\ndata-independent matrices. The Mamba model addressed this with data-dependent\nvariants via the S6 selective-scan algorithm, enhancing context modeling,\nespecially for long sequences. However, Mamba-based architectures are difficult\nto scale with respect to the number of parameters, which is a major limitation\nfor vision applications. This paper addresses the scalability issue of large\nSSMs for image classification and action recognition without requiring\nadditional techniques like knowledge distillation. We analyze the distinct\ncharacteristics of Mamba-based and Attention-based models, proposing a\nMamba-Attention interleaved architecture that enhances scalability, robustness,\nand performance. We demonstrate that the stable and efficient interleaved\narchitecture resolves the scalability issue of Mamba-based architectures for\nimages and videos and increases robustness to common artifacts like JPEG\ncompression. Our thorough evaluation on the ImageNet-1K, Kinetics-400 and\nSomething-Something-v2 benchmarks demonstrates that our approach improves the\naccuracy of state-of-the-art Mamba-based architectures by up to $+1.7$.\n","authors":["Hamid Suleman","Syed Talal Wasim","Muzammal Naseer","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2409.11867v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03006v2","updated":"2025-03-27T16:36:58Z","published":"2024-07-03T11:05:19Z","title":"Frequency-Controlled Diffusion Model for Versatile Text-Guided\n  Image-to-Image Translation","summary":"  Recently, large-scale text-to-image (T2I) diffusion models have emerged as a\npowerful tool for image-to-image translation (I2I), allowing open-domain image\ntranslation via user-provided text prompts. This paper proposes\nfrequency-controlled diffusion model (FCDiffusion), an end-to-end\ndiffusion-based framework that contributes a novel solution to text-guided I2I\nfrom a frequency-domain perspective. At the heart of our framework is a\nfeature-space frequency-domain filtering module based on Discrete Cosine\nTransform, which filters the latent features of the source image in the DCT\ndomain, yielding filtered image features bearing different DCT spectral bands\nas different control signals to the pre-trained Latent Diffusion Model. We\nreveal that control signals of different DCT spectral bands bridge the source\nimage and the T2I generated image in different correlations (e.g., style,\nstructure, layout, contour, etc.), and thus enable versatile I2I applications\nemphasizing different I2I correlations, including style-guided content\ncreation, image semantic manipulation, image scene translation, and image style\ntranslation. Different from related approaches, FCDiffusion establishes a\nunified text-guided I2I framework suitable for diverse image translation tasks\nsimply by switching among different frequency control branches at inference\ntime. The effectiveness and superiority of our method for text-guided I2I are\ndemonstrated with extensive experiments both qualitatively and quantitatively.\nOur project is publicly available at:\nhttps://xianggao1102.github.io/FCDiffusion/.\n","authors":["Xiang Gao","Zhengbo Xu","Junhan Zhao","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2407.03006v2.pdf","comment":"Proceedings of the 38th AAAI Conference on Artificial Intelligence\n  (AAAI 2024)"},{"id":"http://arxiv.org/abs/2503.21668v1","updated":"2025-03-27T16:35:02Z","published":"2025-03-27T16:35:02Z","title":"Cognitive Science-Inspired Evaluation of Core Capabilities for Object\n  Understanding in AI","summary":"  One of the core components of our world models is 'intuitive physics' - an\nunderstanding of objects, space, and causality. This capability enables us to\npredict events, plan action and navigate environments, all of which rely on a\ncomposite sense of objecthood. Despite its importance, there is no single,\nunified account of objecthood, though multiple theoretical frameworks provide\ninsights. In the first part of this paper, we present a comprehensive overview\nof the main theoretical frameworks in objecthood research - Gestalt psychology,\nenactive cognition, and developmental psychology - and identify the core\ncapabilities each framework attributes to object understanding, as well as what\nfunctional roles they play in shaping world models in biological agents. Given\nthe foundational role of objecthood in world modelling, understanding\nobjecthood is also essential in AI. In the second part of the paper, we\nevaluate how current AI paradigms approach and test objecthood capabilities\ncompared to those in cognitive science. We define an AI paradigm as a\ncombination of how objecthood is conceptualised, the methods used for studying\nobjecthood, the data utilised, and the evaluation techniques. We find that,\nwhilst benchmarks can detect that AI systems model isolated aspects of\nobjecthood, the benchmarks cannot detect when AI systems lack functional\nintegration across these capabilities, not solving the objecthood challenge\nfully. Finally, we explore novel evaluation approaches that align with the\nintegrated vision of objecthood outlined in this paper. These methods are\npromising candidates for advancing from isolated object capabilities toward\ngeneral-purpose AI with genuine object understanding in real-world contexts.\n","authors":["Danaja Rutar","Alva Markelius","Konstantinos Voudouris","José Hernández-Orallo","Lucy Cheke"],"pdf_url":"https://arxiv.org/pdf/2503.21668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21659v1","updated":"2025-03-27T16:23:15Z","published":"2025-03-27T16:23:15Z","title":"InteractionMap: Improving Online Vectorized HDMap Construction with\n  Interaction","summary":"  Vectorized high-definition (HD) maps are essential for an autonomous driving\nsystem. Recently, state-of-the-art map vectorization methods are mainly based\non DETR-like framework to generate HD maps in an end-to-end manner. In this\npaper, we propose InteractionMap, which improves previous map vectorization\nmethods by fully leveraging local-to-global information interaction in both\ntime and space. Firstly, we explore enhancing DETR-like detectors by explicit\nposition relation prior from point-level to instance-level, since map elements\ncontain strong shape priors. Secondly, we propose a key-frame-based\nhierarchical temporal fusion module, which interacts temporal information from\nlocal to global. Lastly, the separate classification branch and regression\nbranch lead to the problem of misalignment in the output distribution. We\ninteract semantic information with geometric information by introducing a novel\ngeometric-aware classification loss in optimization and a geometric-aware\nmatching cost in label assignment. InteractionMap achieves state-of-the-art\nperformance on both nuScenes and Argoverse2 benchmarks.\n","authors":["Kuang Wu","Chuan Yang","Zhanbin Li"],"pdf_url":"https://arxiv.org/pdf/2503.21659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15272v4","updated":"2025-03-27T16:21:06Z","published":"2024-09-23T17:59:05Z","title":"OmniBench: Towards The Future of Universal Omni-Language Models","summary":"  Recent advancements in multimodal large language models (MLLMs) have focused\non integrating multiple modalities, yet their ability to simultaneously process\nand reason across different inputs remains underexplored. We introduce\nOmniBench, a novel benchmark designed to evaluate models' ability to recognize,\ninterpret, and reason across visual, acoustic, and textual inputs\nsimultaneously. We define language models capable of such tri-modal processing\nas omni-language models (OLMs). OmniBench features high-quality human\nannotations that require integrated understanding across all modalities. Our\nevaluation reveals that: i) open-source OLMs show significant limitations in\ninstruction-following and reasoning in tri-modal contexts; and ii) most\nbaseline models perform poorly (around 50% accuracy) even with textual\nalternatives to image/audio inputs. To address these limitations, we develop\nOmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. We\nadvocate for developing more robust tri-modal integration techniques and\ntraining strategies to enhance OLM performance. Codes and data could be found\nat our repo (https://github.com/multimodal-art-projection/OmniBench).\n","authors":["Yizhi Li","Ge Zhang","Yinghao Ma","Ruibin Yuan","Kang Zhu","Hangyu Guo","Yiming Liang","Jiaheng Liu","Zekun Wang","Jian Yang","Siwei Wu","Xingwei Qu","Jinjie Shi","Xinyue Zhang","Zhenzhu Yang","Xiangzhou Wang","Zhaoxiang Zhang","Zachary Liu","Emmanouil Benetos","Wenhao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2409.15272v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06581v6","updated":"2025-03-27T16:16:09Z","published":"2024-07-09T06:20:17Z","title":"Vision language models are blind: Failing to translate detailed visual\n  features into words","summary":"  While large language models with vision capabilities (VLMs), e.g., GPT-4o and\nGemini 1.5 Pro, score high on many vision-understanding benchmarks, they are\nstill struggling with low-level vision tasks that are easy to humans.\nSpecifically, on BlindTest, our suite of 7 very simple tasks, including\nidentifying (a) whether two circles overlap; (b) how many times two lines\nintersect; (c) which letter is being circled in a word; and (d) the number of\ncircles in an Olympic-like logo, four state-of-the-art VLMs are only 58.07%\naccurate on average. Claude 3.5 Sonnet performs the best at 77.84% accuracy,\nfar from the human expected accuracy of 100%. Across different image\nresolutions and line widths, VLMs including slow-thinking models consistently\nstruggle with those tasks that require precise spatial information when\ngeometric primitives overlap or are close. Yet, VLMs perform at near-100%\naccuracy when much more space is added to separate shapes and letters. Linear\nprobing experiments show that vision encoders contain sufficient visual\ninformation to solve BlindTest and that language models fail to decode this\ninformation into correct answers. Code and data are at:\nhttps://vlmsareblind.github.io\n","authors":["Pooyan Rahmanzadehgervi","Logan Bolton","Mohammad Reza Taesiri","Anh Totti Nguyen"],"pdf_url":"https://arxiv.org/pdf/2407.06581v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15260v2","updated":"2025-03-27T15:59:24Z","published":"2024-07-21T20:24:21Z","title":"On the Viability of Semi-Supervised Segmentation Methods for Statistical\n  Shape Modeling","summary":"  Statistical Shape Models (SSMs) excel at identifying population level\nanatomical variations, which is at the core of various clinical and biomedical\napplications, including morphology-based diagnostics and surgical planning.\nHowever, the effectiveness of SSM is often constrained by the necessity for\nexpert-driven manual segmentation, a process that is both time-intensive and\nexpensive, thereby restricting their broader application and utility. Recent\ndeep learning approaches enable the direct estimation of Statistical Shape\nModels (SSMs) from unsegmented images. While these models can predict SSMs\nwithout segmentation during deployment, they do not address the challenge of\nacquiring the manual annotations needed for training, particularly in\nresource-limited settings. Semi-supervised models for anatomy segmentation can\nmitigate the annotation burden. Yet, despite the abundance of available\napproaches, there are no established guidelines to inform end-users on their\neffectiveness for the downstream task of constructing SSMs. In this study, we\nsystematically evaluate the potential of semi-supervised methods as viable\nalternatives to manual segmentations for building SSMs. We establish a new\nperformance benchmark by employing various semi-supervised methods for anatomy\nsegmentation under low annotation settings, utilizing the predicted\nsegmentations for the task of SSM. Our results indicate that some methods\nproduce noisy segmentation, which is very unfavorable for SSM tasks, while\nothers can capture the correct modes of variations in the population cohort\nwith 60-80% reduction in required manual annotation\n","authors":["Asma Khan","Tushar Kataria","Janmesh Ukey","Shireen Y. Elhabian"],"pdf_url":"https://arxiv.org/pdf/2407.15260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11593v2","updated":"2025-03-27T15:57:57Z","published":"2024-09-17T22:58:20Z","title":"Self-Contrastive Forward-Forward Algorithm","summary":"  Agents that operate autonomously benefit from lifelong learning capabilities.\nHowever, compatible training algorithms must comply with the decentralized\nnature of these systems, which imposes constraints on both the parameter counts\nand the computational resources. The Forward-Forward (FF) algorithm is one of\nthese. FF relies only on feedforward operations, the same used for inference,\nfor optimizing layer-wise objectives. This purely forward approach eliminates\nthe need for transpose operations required in traditional backpropagation.\nDespite its potential, FF has failed to reach state-of-the-art performance on\nmost standard benchmark tasks, in part due to unreliable negative data\ngeneration methods for unsupervised learning.\n  In this work, we propose the Self-Contrastive Forward-Forward (SCFF)\nalgorithm, a competitive training method aimed at closing this performance gap.\nInspired by standard self-supervised contrastive learning for vision tasks,\nSCFF generates positive and negative inputs applicable across various datasets.\nThe method demonstrates superior performance compared to existing unsupervised\nlocal learning algorithms on several benchmark datasets, including MNIST,\nCIFAR-10, STL-10, and Tiny ImageNet. We extend FF's application to training\nrecurrent neural networks, expanding its utility to sequential data tasks.\nThese findings pave the way for high-accuracy, real-time learning on\nresource-constrained edge devices.\n","authors":["Xing Chen","Dongshu Liu","Jeremie Laydevant","Julie Grollier"],"pdf_url":"https://arxiv.org/pdf/2409.11593v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21634v1","updated":"2025-03-27T15:56:55Z","published":"2025-03-27T15:56:55Z","title":"When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in\n  Morocco","summary":"  The accurate determination of the beginning of each Hijri month is essential\nfor religious, cultural, and administrative purposes. Manazel (The code and\ndatasets are available at https://github.com/lairgiyassir/manazel) addresses\nthis challenge in Morocco by leveraging 13 years of crescent visibility data to\nrefine the ODEH criterion, a widely used standard for lunar crescent visibility\nprediction. The study integrates two key features, the Arc of Vision (ARCV) and\nthe total width of the crescent (W), to enhance the accuracy of lunar\nvisibility assessments. A machine learning approach utilizing the Logistic\nRegression algorithm is employed to classify crescent visibility conditions,\nachieving a predictive accuracy of 98.83%. This data-driven methodology offers\na robust and reliable framework for determining the start of the Hijri month,\ncomparing different data classification tools, and improving the consistency of\nlunar calendar calculations in Morocco. The findings demonstrate the\neffectiveness of machine learning in astronomical applications and highlight\nthe potential for further enhancements in the modeling of crescent visibility.\n","authors":["Yassir Lairgi"],"pdf_url":"https://arxiv.org/pdf/2503.21634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16822v2","updated":"2025-03-27T15:42:18Z","published":"2024-12-22T02:04:17Z","title":"Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for\n  Efficient Diffusion Transformers","summary":"  Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image\ngeneration quality but suffer from high latency and memory inefficiency, making\nthem difficult to deploy on resource-constrained devices. One major efficiency\nbottleneck is that existing DiTs apply equal computation across all regions of\nan image. However, not all image tokens are equally important, and certain\nlocalized areas require more computation, such as objects. To address this, we\npropose DiffCR, a dynamic DiT inference framework with differentiable\ncompression ratios, which automatically learns to dynamically route computation\nacross layers and timesteps for each image token, resulting in efficient DiTs.\nSpecifically, DiffCR integrates three features: (1) A token-level routing\nscheme where each DiT layer includes a router that is fine-tuned jointly with\nmodel weights to predict token importance scores. In this way, unimportant\ntokens bypass the entire layer's computation; (2) A layer-wise differentiable\nratio mechanism where different DiT layers automatically learn varying\ncompression ratios from a zero initialization, resulting in large compression\nratios in redundant layers while others remain less compressed or even\nuncompressed; (3) A timestep-wise differentiable ratio mechanism where each\ndenoising timestep learns its own compression ratio. The resulting pattern\nshows higher ratios for noisier timesteps and lower ratios as the image becomes\nclearer. Extensive experiments on text-to-image and inpainting tasks show that\nDiffCR effectively captures dynamism across token, layer, and timestep axes,\nachieving superior trade-offs between generation quality and efficiency\ncompared to prior works. The project website is available at\nhttps://www.haoranyou.com/diffcr.\n","authors":["Haoran You","Connelly Barnes","Yuqian Zhou","Yan Kang","Zhenbang Du","Wei Zhou","Lingzhi Zhang","Yotam Nitzan","Xiaoyang Liu","Zhe Lin","Eli Shechtman","Sohrab Amirghodsi","Yingyan Celine Lin"],"pdf_url":"https://arxiv.org/pdf/2412.16822v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.21622v1","updated":"2025-03-27T15:41:46Z","published":"2025-03-27T15:41:46Z","title":"The MVTec AD 2 Dataset: Advanced Scenarios for Unsupervised Anomaly\n  Detection","summary":"  In recent years, performance on existing anomaly detection benchmarks like\nMVTec AD and VisA has started to saturate in terms of segmentation AU-PRO, with\nstate-of-the-art models often competing in the range of less than one\npercentage point. This lack of discriminatory power prevents a meaningful\ncomparison of models and thus hinders progress of the field, especially when\nconsidering the inherent stochastic nature of machine learning results. We\npresent MVTec AD 2, a collection of eight anomaly detection scenarios with more\nthan 8000 high-resolution images. It comprises challenging and highly relevant\nindustrial inspection use cases that have not been considered in previous\ndatasets, including transparent and overlapping objects, dark-field and back\nlight illumination, objects with high variance in the normal data, and\nextremely small defects. We provide comprehensive evaluations of\nstate-of-the-art methods and show that their performance remains below 60%\naverage AU-PRO. Additionally, our dataset provides test scenarios with lighting\ncondition changes to assess the robustness of methods under real-world\ndistribution shifts. We host a publicly accessible evaluation server that holds\nthe pixel-precise ground truth of the test set (https://benchmark.mvtec.com/).\nAll image data is available at\nhttps://www.mvtec.com/company/research/datasets/mvtec-ad-2.\n","authors":["Lars Heckler-Kram","Jan-Hendrik Neudeck","Ulla Scheler","Rebecca König","Carsten Steger"],"pdf_url":"https://arxiv.org/pdf/2503.21622v1.pdf","comment":"paper under review; dataset first released for the VAND3.0 challenge\n  @ CVPR 2025 https://sites.google.com/view/vand30cvpr2025/challenge"},{"id":"http://arxiv.org/abs/2503.21616v1","updated":"2025-03-27T15:37:16Z","published":"2025-03-27T15:37:16Z","title":"Audio-driven Gesture Generation via Deviation Feature in the Latent\n  Space","summary":"  Gestures are essential for enhancing co-speech communication, offering visual\nemphasis and complementing verbal interactions. While prior work has\nconcentrated on point-level motion or fully supervised data-driven methods, we\nfocus on co-speech gestures, advocating for weakly supervised learning and\npixel-level motion deviations. We introduce a weakly supervised framework that\nlearns latent representation deviations, tailored for co-speech gesture video\ngeneration. Our approach employs a diffusion model to integrate latent motion\nfeatures, enabling more precise and nuanced gesture representation. By\nleveraging weakly supervised deviations in latent space, we effectively\ngenerate hand gestures and mouth movements, crucial for realistic video\nproduction. Experiments show our method significantly improves video quality,\nsurpassing current state-of-the-art techniques.\n","authors":["Jiahui Chen","Yang Huan","Runhua Shi","Chanfan Ding","Xiaoqi Mo","Siyu Xiong","Yinong He"],"pdf_url":"https://arxiv.org/pdf/2503.21616v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2307.15220v4","updated":"2025-03-27T15:37:03Z","published":"2023-07-27T22:38:12Z","title":"Learning Multi-modal Representations by Watching Hundreds of Surgical\n  Video Lectures","summary":"  Recent advancements in surgical computer vision applications have been driven\nby vision-only models, which do not explicitly integrate the rich semantics of\nlanguage into their design. These methods rely on manually annotated surgical\nvideos to predict a fixed set of object categories, limiting their\ngeneralizability to unseen surgical procedures and downstream tasks. In this\nwork, we put forward the idea that the surgical video lectures available\nthrough open surgical e-learning platforms can provide effective vision and\nlanguage supervisory signals for multi-modal representation learning without\nrelying on manual annotations. We address the surgery-specific linguistic\nchallenges present in surgical video lectures by employing multiple\ncomplementary automatic speech recognition systems to generate text\ntranscriptions. We then present a novel method, SurgVLP - Surgical Vision\nLanguage Pre-training, for multi-modal representation learning. Extensive\nexperiments across diverse surgical procedures and tasks demonstrate that the\nmulti-modal representations learned by SurgVLP exhibit strong transferability\nand adaptability in surgical video analysis. Furthermore, our zero-shot\nevaluations highlight SurgVLP's potential as a general-purpose foundation model\nfor surgical workflow analysis, reducing the reliance on extensive manual\nannotations for downstream tasks, and facilitating adaptation methods such as\nfew-shot learning to build a scalable and data-efficient solution for various\ndownstream surgical applications. The [training\ncode](https://github.com/CAMMA-public/SurgVLP) and\n[weights](https://github.com/CAMMA-public/PeskaVLP) are public.\n","authors":["Kun Yuan","Vinkle Srivastav","Tong Yu","Joel L. Lavanchy","Jacques Marescaux","Pietro Mascagni","Nassir Navab","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2307.15220v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14522v2","updated":"2025-03-27T15:24:29Z","published":"2024-11-21T18:59:36Z","title":"GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A\n  Comprehensive Multimodal Dataset Towards General Medical AI","summary":"  Despite significant advancements in general AI, its effectiveness in the\nmedical domain is limited by the lack of specialized medical knowledge. To\naddress this, we formulate GMAI-VL-5.5M, a multimodal medical dataset created\nby converting hundreds of specialized medical datasets with various annotations\ninto high-quality image-text pairs. This dataset offers comprehensive task\ncoverage, diverse modalities, and rich image-text data. Building upon this\ndataset, we develop GMAI-VL, a general medical vision-language model, with a\nthree-stage training strategy that enhances the integration of visual and\ntextual information. This approach significantly improves the model's ability\nto process multimodal data, supporting accurate diagnoses and clinical\ndecision-making. Experiments show that GMAI-VL achieves state-of-the-art\nperformance across various multimodal medical tasks, including visual question\nanswering and medical image diagnosis.\n","authors":["Tianbin Li","Yanzhou Su","Wei Li","Bin Fu","Zhe Chen","Ziyan Huang","Guoan Wang","Chenglong Ma","Ying Chen","Ming Hu","Yanjun Li","Pengcheng Chen","Xiaowei Hu","Zhongying Deng","Yuanfeng Ji","Jin Ye","Yu Qiao","Junjun He"],"pdf_url":"https://arxiv.org/pdf/2411.14522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21595v1","updated":"2025-03-27T15:14:03Z","published":"2025-03-27T15:14:03Z","title":"FusionSegReID: Advancing Person Re-Identification with Multimodal\n  Retrieval and Precise Segmentation","summary":"  Person re-identification (ReID) plays a critical role in applications like\nsecurity surveillance and criminal investigations by matching individuals\nacross large image galleries captured by non-overlapping cameras. Traditional\nReID methods rely on unimodal inputs, typically images, but face limitations\ndue to challenges like occlusions, lighting changes, and pose variations. While\nadvancements in image-based and text-based ReID systems have been made, the\nintegration of both modalities has remained under-explored. This paper presents\nFusionSegReID, a multimodal model that combines both image and text inputs for\nenhanced ReID performance. By leveraging the complementary strengths of these\nmodalities, our model improves matching accuracy and robustness, particularly\nin complex, real-world scenarios where one modality may struggle. Our\nexperiments show significant improvements in Top-1 accuracy and mean Average\nPrecision (mAP) for ReID, as well as better segmentation results in challenging\nscenarios like occlusion and low-quality images. Ablation studies further\nconfirm that multimodal fusion and segmentation modules contribute to enhanced\nre-identification and mask accuracy. The results show that FusionSegReID\noutperforms traditional unimodal models, offering a more robust and flexible\nsolution for real-world person ReID tasks.\n","authors":["Jincheng Yan","Yun Wang","Xiaoyan Luo","Yu-Wing Tai"],"pdf_url":"https://arxiv.org/pdf/2503.21595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11309v2","updated":"2025-03-27T15:13:53Z","published":"2024-07-16T01:50:43Z","title":"Gaussian Splatting Lucas-Kanade","summary":"  Gaussian Splatting and its dynamic extensions are effective for\nreconstructing 3D scenes from 2D images when there is significant camera\nmovement to facilitate motion parallax and when scene objects remain relatively\nstatic. However, in many real-world scenarios, these conditions are not met. As\na consequence, data-driven semantic and geometric priors have been favored as\nregularizers, despite their bias toward training data and their neglect of\nbroader movement dynamics.\n  Departing from this practice, we propose a novel analytical approach that\nadapts the classical Lucas-Kanade method to dynamic Gaussian splatting. By\nleveraging the intrinsic properties of the forward warp field network, we\nderive an analytical velocity field that, through time integration, facilitates\naccurate scene flow computation. This enables the precise enforcement of motion\nconstraints on warp fields, thus constraining both 2D motion and 3D positions\nof the Gaussians. Our method excels in reconstructing highly dynamic scenes\nwith minimal camera movement, as demonstrated through experiments on both\nsynthetic and real-world scenes.\n","authors":["Liuyue Xie","Joel Julin","Koichiro Niinuma","Laszlo A. Jeni"],"pdf_url":"https://arxiv.org/pdf/2407.11309v2.pdf","comment":"International Conference on Learning Representations"},{"id":"http://arxiv.org/abs/2411.04844v3","updated":"2025-03-27T15:00:57Z","published":"2024-11-07T16:32:29Z","title":"Discretized Gaussian Representation for Tomographic Reconstruction","summary":"  Computed Tomography (CT) is a widely used imaging technique that provides\ndetailed cross-sectional views of objects. Over the past decade, Deep\nLearning-based Reconstruction (DLR) methods have led efforts to enhance image\nquality and reduce noise, yet they often require large amounts of data and are\ncomputationally intensive. Inspired by recent advancements in scene\nreconstruction, some approaches have adapted NeRF and 3D Gaussian Splatting\n(3DGS) techniques for CT reconstruction. However, these methods are not ideal\nfor direct 3D volume reconstruction. In this paper, we propose a novel\nDiscretized Gaussian Representation (DGR) for CT reconstruction, which directly\nreconstructs the 3D volume using a set of discretized Gaussian functions in an\nend-to-end manner. To further enhance computational efficiency, we introduce a\nFast Volume Reconstruction technique that aggregates the contributions of these\nGaussians into a discretized volume in a highly parallelized fashion. Our\nextensive experiments on both real-world and synthetic datasets demonstrate\nthat DGR achieves superior reconstruction quality and significantly improved\ncomputational efficiency compared to existing DLR and instance reconstruction\nmethods. Our code has been provided for review purposes and will be made\npublicly available upon publication.\n","authors":["Shaokai Wu","Yuxiang Lu","Wei Ji","Suizhi Huang","Fengyu Yang","Shalayiding Sirejiding","Qichen He","Jing Tong","Yanbiao Ji","Yue Ding","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2411.04844v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21581v1","updated":"2025-03-27T14:59:59Z","published":"2025-03-27T14:59:59Z","title":"AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion","summary":"  Accurate camera calibration is a fundamental task for 3D perception,\nespecially when dealing with real-world, in-the-wild environments where complex\noptical distortions are common. Existing methods often rely on pre-rectified\nimages or calibration patterns, which limits their applicability and\nflexibility. In this work, we introduce a novel framework that addresses these\nchallenges by jointly modeling camera intrinsic and extrinsic parameters using\na generic ray camera model. Unlike previous approaches, AlignDiff shifts focus\nfrom semantic to geometric features, enabling more accurate modeling of local\ndistortions. We propose AlignDiff, a diffusion model conditioned on geometric\npriors, enabling the simultaneous estimation of camera distortions and scene\ngeometry. To enhance distortion prediction, we incorporate edge-aware\nattention, focusing the model on geometric features around image edges, rather\nthan semantic content. Furthermore, to enhance generalizability to real-world\ncaptures, we incorporate a large database of ray-traced lenses containing over\nthree thousand samples. This database characterizes the distortion inherent in\na diverse variety of lens forms. Our experiments demonstrate that the proposed\nmethod significantly reduces the angular error of estimated ray bundles by ~8.2\ndegrees and overall calibration accuracy, outperforming existing approaches on\nchallenging, real-world datasets.\n","authors":["Liuyue Xie","Jiancong Guo","Ozan Cakmakci","Andre Araujo","Laszlo A. Jeni","Zhiheng Jia"],"pdf_url":"https://arxiv.org/pdf/2503.21581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12922v2","updated":"2025-03-27T14:51:25Z","published":"2024-03-19T17:27:55Z","title":"Contextual AD Narration with Interleaved Multimodal Sequence","summary":"  The Audio Description (AD) task aims to generate descriptions of visual\nelements for visually impaired individuals to help them access long-form video\ncontent, like movies. With video feature, text, character bank and context\ninformation as inputs, the generated ADs are able to correspond to the\ncharacters by name and provide reasonable, contextual descriptions to help\naudience understand the storyline of movie. To achieve this goal, we propose to\nleverage pre-trained foundation models through a simple and unified framework\nto generate ADs with interleaved multimodal sequence as input, termed as\nUni-AD. To enhance the alignment of features across various modalities with\nfiner granularity, we introduce a simple and lightweight module that maps video\nfeatures into the textual feature space. Moreover, we also propose a\ncharacter-refinement module to provide more precise information by identifying\nthe main characters who play more significant roles in the video context. With\nthese unique designs, we further incorporate contextual information and a\ncontrastive loss into our architecture to generate smoother and more\ncontextually appropriate ADs. Experiments on multiple AD datasets show that\nUni-AD performs well on AD generation, which demonstrates the effectiveness of\nour approach. Our code is available at: https://github.com/ant-research/UniAD.\n","authors":["Hanlin Wang","Zhan Tong","Kecheng Zheng","Yujun Shen","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21566v1","updated":"2025-03-27T14:49:56Z","published":"2025-03-27T14:49:56Z","title":"Bearing fault diagnosis based on multi-scale spectral images and\n  convolutional neural network","summary":"  To address the challenges of low diagnostic accuracy in traditional bearing\nfault diagnosis methods, this paper proposes a novel fault diagnosis approach\nbased on multi-scale spectrum feature images and deep learning. Firstly, the\nvibration signal are preprocessed through mean removal and then converted to\nmulti-length spectrum with fast Fourier transforms (FFT). Secondly, a novel\nfeature called multi-scale spectral image (MSSI) is constructed by multi-length\nspectrum paving scheme. Finally, a deep learning framework, convolutional\nneural network (CNN), is formulated to diagnose the bearing faults. Two\nexperimental cases are utilized to verify the effectiveness of the proposed\nmethod. Experimental results demonstrate that the proposed method significantly\nimproves the accuracy of fault diagnosis.\n","authors":["Tongchao Luo","Mingquan Qiu","Zhenyu Wu","Zebo Zhao","Dingyou Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.21566v1.pdf","comment":"12pages, 10 figures and 8 tables"},{"id":"http://arxiv.org/abs/2410.02619v2","updated":"2025-03-27T14:48:33Z","published":"2024-10-03T15:58:18Z","title":"GI-GS: Global Illumination Decomposition on Gaussian Splatting for\n  Inverse Rendering","summary":"  We present GI-GS, a novel inverse rendering framework that leverages 3D\nGaussian Splatting (3DGS) and deferred shading to achieve photo-realistic novel\nview synthesis and relighting. In inverse rendering, accurately modeling the\nshading processes of objects is essential for achieving high-fidelity results.\nTherefore, it is critical to incorporate global illumination to account for\nindirect lighting that reaches an object after multiple bounces across the\nscene. Previous 3DGS-based methods have attempted to model indirect lighting by\ncharacterizing indirect illumination as learnable lighting volumes or\nadditional attributes of each Gaussian, while using baked occlusion to\nrepresent shadow effects. These methods, however, fail to accurately model the\ncomplex physical interactions between light and objects, making it impossible\nto construct realistic indirect illumination during relighting. To address this\nlimitation, we propose to calculate indirect lighting using efficient path\ntracing with deferred shading. In our framework, we first render a G-buffer to\ncapture the detailed geometry and material properties of the scene. Then, we\nperform physically-based rendering (PBR) only for direct lighting. With the\nG-buffer and previous rendering results, the indirect lighting can be\ncalculated through a lightweight path tracing. Our method effectively models\nindirect lighting under any given lighting conditions, thereby achieving better\nnovel view synthesis and competitive relighting. Quantitative and qualitative\nresults show that our GI-GS outperforms existing baselines in both rendering\nquality and efficiency.\n","authors":["Hongze Chen","Zehong Lin","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02619v2.pdf","comment":"Camera-ready version. Project page:\n  https://stopaimme.github.io/GI-GS-site/"},{"id":"http://arxiv.org/abs/2503.21562v1","updated":"2025-03-27T14:47:05Z","published":"2025-03-27T14:47:05Z","title":"uLayout: Unified Room Layout Estimation for Perspective and Panoramic\n  Images","summary":"  We present uLayout, a unified model for estimating room layout geometries\nfrom both perspective and panoramic images, whereas traditional solutions\nrequire different model designs for each image type. The key idea of our\nsolution is to unify both domains into the equirectangular projection,\nparticularly, allocating perspective images into the most suitable latitude\ncoordinate to effectively exploit both domains seamlessly. To address the\nField-of-View (FoV) difference between the input domains, we design uLayout\nwith a shared feature extractor with an extra 1D-Convolution layer to condition\neach domain input differently. This conditioning allows us to efficiently\nformulate a column-wise feature regression problem regardless of the FoV input.\nThis simple yet effective approach achieves competitive performance with\ncurrent state-of-the-art solutions and shows for the first time a single\nend-to-end model for both domains. Extensive experiments in the real-world\ndatasets, LSUN, Matterport3D, PanoContext, and Stanford 2D-3D evidence the\ncontribution of our approach. Code is available at\nhttps://github.com/JonathanLee112/uLayout.\n","authors":["Jonathan Lee","Bolivar Solarte","Chin-Hsuan Wu","Jin-Cheng Jhang","Fu-En Wang","Yi-Hsuan Tsai","Min Sun"],"pdf_url":"https://arxiv.org/pdf/2503.21562v1.pdf","comment":"Accepted to WACV-2025"},{"id":"http://arxiv.org/abs/2503.20652v2","updated":"2025-03-27T14:46:42Z","published":"2025-03-26T15:47:50Z","title":"Imitating Radiological Scrolling: A Global-Local Attention Model for 3D\n  Chest CT Volumes Multi-Label Anomaly Classification","summary":"  The rapid increase in the number of Computed Tomography (CT) scan\nexaminations has created an urgent need for automated tools, such as organ\nsegmentation, anomaly classification, and report generation, to assist\nradiologists with their growing workload. Multi-label classification of\nThree-Dimensional (3D) CT scans is a challenging task due to the volumetric\nnature of the data and the variety of anomalies to be detected. Existing deep\nlearning methods based on Convolutional Neural Networks (CNNs) struggle to\ncapture long-range dependencies effectively, while Vision Transformers require\nextensive pre-training, posing challenges for practical use. Additionally,\nthese existing methods do not explicitly model the radiologist's navigational\nbehavior while scrolling through CT scan slices, which requires both global\ncontext understanding and local detail awareness. In this study, we present\nCT-Scroll, a novel global-local attention model specifically designed to\nemulate the scrolling behavior of radiologists during the analysis of 3D CT\nscans. Our approach is evaluated on two public datasets, demonstrating its\nefficacy through comprehensive experiments and an ablation study that\nhighlights the contribution of each model component.\n","authors":["Theo Di Piazza","Carole Lazarus","Olivier Nempont","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2503.20652v2.pdf","comment":"13 pages, 4 figures. Accepted for MIDL 2025"},{"id":"http://arxiv.org/abs/2405.01105v3","updated":"2025-03-27T14:44:30Z","published":"2024-05-02T09:14:38Z","title":"Image segmentation of treated and untreated tumor spheroids by Fully\n  Convolutional Networks","summary":"  Multicellular tumor spheroids (MCTS) are advanced cell culture systems for\nassessing the impact of combinatorial radio(chemo)therapy. They exhibit\ntherapeutically relevant in-vivo-like characteristics from 3D cell-cell and\ncell-matrix interactions to radial pathophysiological gradients related to\nproliferative activity and nutrient/oxygen supply, altering cellular\nradioresponse. State-of-the-art assays quantify long-term curative endpoints\nbased on collected brightfield image time series from large treated spheroid\npopulations per irradiation dose and treatment arm. Here, spheroid control\nprobabilities are documented analogous to in-vivo tumor control probabilities\nbased on Kaplan-Meier curves. This analyses require laborious spheroid\nsegmentation of up to 100.000 images per treatment arm to extract relevant\nstructural information from the images, e.g., diameter, area, volume and\ncircularity. While several image analysis algorithms are available for spheroid\nsegmentation, they all focus on compact MCTS with clearly distinguishable outer\nrim throughout growth. However, treated MCTS may partly be detached and\ndestroyed and are usually obscured by dead cell debris. We successfully train\ntwo Fully Convolutional Networks, UNet and HRNet, and optimize their\nhyperparameters to develop an automatic segmentation for both untreated and\ntreated MCTS. We systematically validate the automatic segmentation on larger,\nindependent data sets of spheroids derived from two human head-and-neck cancer\ncell lines. We find an excellent overlap between manual and automatic\nsegmentation for most images, quantified by Jaccard indices at around 90%. For\nimages with smaller overlap of the segmentations, we demonstrate that this\nerror is comparable to the variations across segmentations from different\nbiological experts, suggesting that these images represent biologically unclear\nor ambiguous cases.\n","authors":["Matthias Streller","Soňa Michlíková","Willy Ciecior","Katharina Lönnecke","Leoni A. Kunz-Schughart","Steffen Lange","Anja Voss-Böhme"],"pdf_url":"https://arxiv.org/pdf/2405.01105v3.pdf","comment":"30 pages, 23 figures"},{"id":"http://arxiv.org/abs/2501.04765v2","updated":"2025-03-27T14:42:53Z","published":"2025-01-08T18:38:25Z","title":"TREAD: Token Routing for Efficient Architecture-agnostic Diffusion\n  Training","summary":"  Diffusion models have emerged as the mainstream approach for visual\ngeneration. However, these models typically suffer from sample inefficiency and\nhigh training costs. Consequently, methods for efficient finetuning, inference\nand personalization were quickly adopted by the community. However, training\nthese models in the first place remains very costly. While several recent\napproaches - including masking, distillation, and architectural modifications -\nhave been proposed to improve training efficiency, each of these methods comes\nwith a tradeoff: they achieve enhanced performance at the expense of increased\ncomputational cost or vice versa. In contrast, this work aims to improve\ntraining efficiency as well as generative performance at the same time through\nroutes that act as a transport mechanism for randomly selected tokens from\nearly layers to deeper layers of the model. Our method is not limited to the\ncommon transformer-based model - it can also be applied to state-space models\nand achieves this without architectural modifications or additional parameters.\nFinally, we show that TREAD reduces computational cost and simultaneously\nboosts model performance on the standard ImageNet-256 benchmark in\nclass-conditional synthesis. Both of these benefits multiply to a convergence\nspeedup of 14x at 400K training iterations compared to DiT and 37x compared to\nthe best benchmark performance of DiT at 7M training iterations. Furthermore,\nwe achieve a competitive FID of 2.09 in a guided and 3.93 in an unguided\nsetting, which improves upon the DiT, without architectural changes.\n","authors":["Felix Krause","Timy Phan","Ming Gui","Stefan Andreas Baumann","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2501.04765v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21555v1","updated":"2025-03-27T14:40:53Z","published":"2025-03-27T14:40:53Z","title":"SyncSDE: A Probabilistic Framework for Diffusion Synchronization","summary":"  There have been many attempts to leverage multiple diffusion models for\ncollaborative generation, extending beyond the original domain. A prominent\napproach involves synchronizing multiple diffusion trajectories by mixing the\nestimated scores to artificially correlate the generation processes. However,\nexisting methods rely on naive heuristics, such as averaging, without\nconsidering task specificity. These approaches do not clarify why such methods\nwork and often fail when a heuristic suitable for one task is blindly applied\nto others. In this paper, we present a probabilistic framework for analyzing\nwhy diffusion synchronization works and reveal where heuristics should be\nfocused - modeling correlations between multiple trajectories and adapting them\nto each specific task. We further identify optimal correlation models per task,\nachieving better results than previous approaches that apply a single heuristic\nacross all tasks without justification.\n","authors":["Hyunjun Lee","Hyunsoo Lee","Sookwan Han"],"pdf_url":"https://arxiv.org/pdf/2503.21555v1.pdf","comment":"Accepted to CVPR2025"},{"id":"http://arxiv.org/abs/2412.18609v2","updated":"2025-03-27T14:40:40Z","published":"2024-12-24T18:59:56Z","title":"Video-Panda: Parameter-efficient Alignment for Encoder-free\n  Video-Language Models","summary":"  We present an efficient encoder-free approach for video-language\nunderstanding that achieves competitive performance while significantly\nreducing computational overhead. Current video-language models typically rely\non heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4B\nparameters), creating a substantial computational burden when processing\nmulti-frame videos. Our method introduces a novel Spatio-Temporal Alignment\nBlock (STAB) that directly processes video inputs without requiring pre-trained\nencoders while using only 45M parameters for visual processing - at least a\n6.5$\\times$ reduction compared to traditional approaches. The STAB architecture\ncombines Local Spatio-Temporal Encoding for fine-grained feature extraction,\nefficient spatial downsampling through learned attention and separate\nmechanisms for modeling frame-level and video-level relationships. Our model\nachieves comparable or superior performance to encoder-based approaches for\nopen-ended video question answering on standard benchmarks. The fine-grained\nvideo question-answering evaluation demonstrates our model's effectiveness,\noutperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in key\naspects like correctness and temporal understanding. Extensive ablation studies\nvalidate our architectural choices and demonstrate the effectiveness of our\nspatio-temporal modeling approach while achieving 3-4$\\times$ faster processing\nspeeds than previous methods. Code is available at\nhttps://jh-yi.github.io/Video-Panda.\n","authors":["Jinhui Yi","Syed Talal Wasim","Yanan Luo","Muzammal Naseer","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2412.18609v2.pdf","comment":"CVPR 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2503.21541v1","updated":"2025-03-27T14:32:17Z","published":"2025-03-27T14:32:17Z","title":"LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized\n  Text-Guided Image Editing","summary":"  Text-guided image editing aims to modify specific regions of an image\naccording to natural language instructions while maintaining the general\nstructure and the background fidelity. Existing methods utilize masks derived\nfrom cross-attention maps generated from diffusion models to identify the\ntarget regions for modification. However, since cross-attention mechanisms\nfocus on semantic relevance, they struggle to maintain the image integrity. As\na result, these methods often lack spatial consistency, leading to editing\nartifacts and distortions. In this work, we address these limitations and\nintroduce LOCATEdit, which enhances cross-attention maps through a graph-based\napproach utilizing self-attention-derived patch relationships to maintain\nsmooth, coherent attention across image regions, ensuring that alterations are\nlimited to the designated items while retaining the surrounding structure.\n\\method consistently and substantially outperforms existing baselines on\nPIE-Bench, demonstrating its state-of-the-art performance and effectiveness on\nvarious editing tasks. Code can be found on\nhttps://github.com/LOCATEdit/LOCATEdit/\n","authors":["Achint Soni","Meet Soni","Sirisha Rambhatla"],"pdf_url":"https://arxiv.org/pdf/2503.21541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20711v2","updated":"2025-03-27T14:28:31Z","published":"2025-03-26T16:47:14Z","title":"Demand Estimation with Text and Image Data","summary":"  We propose a demand estimation method that leverages unstructured text and\nimage data to infer substitution patterns. Using pre-trained deep learning\nmodels, we extract embeddings from product images and textual descriptions and\nincorporate them into a random coefficients logit model. This approach enables\nresearchers to estimate demand even when they lack data on product attributes\nor when consumers value hard-to-quantify attributes, such as visual design or\nfunctional benefits. Using data from a choice experiment, we show that our\napproach outperforms standard attribute-based models in counterfactual\npredictions of consumers' second choices. We also apply it across 40 product\ncategories on Amazon and consistently find that text and image data help\nidentify close substitutes within each category.\n","authors":["Giovanni Compiani","Ilya Morozov","Stephan Seiler"],"pdf_url":"https://arxiv.org/pdf/2503.20711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12691v2","updated":"2025-03-27T14:26:49Z","published":"2024-08-22T19:08:08Z","title":"Quantization-aware Matrix Factorization for Low Bit Rate Image\n  Compression","summary":"  Lossy image compression is essential for efficient transmission and storage.\nTraditional compression methods mainly rely on discrete cosine transform (DCT)\nor singular value decomposition (SVD), both of which represent image data in\ncontinuous domains and, therefore, necessitate carefully designed quantizers.\nNotably, these methods consider quantization as a separate step, where\nquantization errors cannot be incorporated into the compression process. The\nsensitivity of these methods, especially SVD-based ones, to quantization errors\nsignificantly degrades reconstruction quality. To address this issue, we\nintroduce a quantization-aware matrix factorization (QMF) to develop a novel\nlossy image compression method. QMF provides a low-rank representation of the\nimage data as a product of two smaller factor matrices, with elements\nconstrained to bounded integer values, thereby effectively integrating\nquantization with low-rank approximation. We propose an efficient, provably\nconvergent iterative algorithm for QMF using a block coordinate descent (BCD)\nscheme, with subproblems having closed-form solutions. Our experiments on the\nKodak and CLIC 2024 datasets demonstrate that our QMF compression method\nconsistently outperforms JPEG at low bit rates below 0.25 bits per pixel (bpp)\nand remains comparable at higher bit rates. We also assessed our method's\ncapability to preserve visual semantics by evaluating an ImageNet pre-trained\nclassifier on compressed images. Remarkably, our method improved top-1 accuracy\nby over 5 percentage points compared to JPEG at bit rates under 0.25 bpp. The\nproject is available at https://github.com/pashtari/lrf .\n","authors":["Pooya Ashtari","Pourya Behmandpoor","Fateme Nateghi Haredasht","Jonathan H. Chen","Panagiotis Patrinos","Sabine Van Huffel"],"pdf_url":"https://arxiv.org/pdf/2408.12691v2.pdf","comment":"22 pages, 6 figures, 1 table, 1 algorithm"},{"id":"http://arxiv.org/abs/2501.01855v2","updated":"2025-03-27T14:17:42Z","published":"2025-01-03T15:11:14Z","title":"UAV-DETR: Efficient End-to-End Object Detection for Unmanned Aerial\n  Vehicle Imagery","summary":"  Unmanned aerial vehicle object detection (UAV-OD) has been widely used in\nvarious scenarios. However, most existing UAV-OD algorithms rely on manually\ndesigned components, which require extensive tuning. End-to-end models that do\nnot depend on such manually designed components are mainly designed for natural\nimages, which are less effective for UAV imagery. To address such challenges,\nthis paper proposes an efficient detection transformer (DETR) framework\ntailored for UAV imagery, i.e., UAV-DETR. The framework includes a multi-scale\nfeature fusion with frequency enhancement module, which captures both spatial\nand frequency information at different scales. In addition, a frequency-focused\ndown-sampling module is presented to retain critical spatial details during\ndown-sampling. A semantic alignment and calibration module is developed to\nalign and fuse features from different fusion paths. Experimental results\ndemonstrate the effectiveness and generalization of our approach across various\nUAV imagery datasets. On the VisDrone dataset, our method improves AP by 3.1\\%\nand $\\text{AP}_{50}$ by 4.2\\% over the baseline. Similar enhancements are\nobserved on the UAVVaste dataset. The project page:\nhttps://github.com/ValiantDiligent/UAV-DETR\n","authors":["Huaxiang Zhang","Kai Liu","Zhongxue Gan","Guo-Niu Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.01855v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11890v2","updated":"2025-03-27T14:15:45Z","published":"2024-12-16T15:38:25Z","title":"SegMAN: Omni-scale Context Modeling with State Space Models and Local\n  Attention for Semantic Segmentation","summary":"  High-quality semantic segmentation relies on three key capabilities: global\ncontext modeling, local detail encoding, and multi-scale feature extraction.\nHowever, recent methods struggle to possess all these capabilities\nsimultaneously. Hence, we aim to empower segmentation networks to\nsimultaneously carry out efficient global context modeling, high-quality local\ndetail encoding, and rich multi-scale feature representation for varying input\nresolutions. In this paper, we introduce SegMAN, a novel linear-time model\ncomprising a hybrid feature encoder dubbed SegMAN Encoder, and a decoder based\non state space models. Specifically, the SegMAN Encoder synergistically\nintegrates sliding local attention with dynamic state space models, enabling\nhighly efficient global context modeling while preserving fine-grained local\ndetails. Meanwhile, the MMSCopE module in our decoder enhances multi-scale\ncontext feature extraction and adaptively scales with the input resolution. Our\nSegMAN-B Encoder achieves 85.1% ImageNet-1k accuracy (+1.5% over VMamba-S with\nfewer parameters). When paired with our decoder, the full SegMAN-B model\nachieves 52.6% mIoU on ADE20K (+1.6% over SegNeXt-L with 15% fewer GFLOPs),\n83.8% mIoU on Cityscapes (+2.1% over SegFormer-B3 with half the GFLOPs), and\n1.6% higher mIoU than VWFormer-B3 on COCO-Stuff with lower GFLOPs. Our code is\navailable at https://github.com/yunxiangfu2001/SegMAN.\n","authors":["Yunxiang Fu","Meng Lou","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2412.11890v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.21525v1","updated":"2025-03-27T14:13:31Z","published":"2025-03-27T14:13:31Z","title":"ICG-MVSNet: Learning Intra-view and Cross-view Relationships for\n  Guidance in Multi-View Stereo","summary":"  Multi-view Stereo (MVS) aims to estimate depth and reconstruct 3D point\nclouds from a series of overlapping images. Recent learning-based MVS\nframeworks overlook the geometric information embedded in features and\ncorrelations, leading to weak cost matching. In this paper, we propose\nICG-MVSNet, which explicitly integrates intra-view and cross-view relationships\nfor depth estimation. Specifically, we develop an intra-view feature fusion\nmodule that leverages the feature coordinate correlations within a single image\nto enhance robust cost matching. Additionally, we introduce a lightweight\ncross-view aggregation module that efficiently utilizes the contextual\ninformation from volume correlations to guide regularization. Our method is\nevaluated on the DTU dataset and Tanks and Temples benchmark, consistently\nachieving competitive performance against state-of-the-art works, while\nrequiring lower computational resources.\n","authors":["Yuxi Hu","Jun Zhang","Zhe Zhang","Rafael Weilharter","Yuchen Rao","Kuangyi Chen","Runze Yuan","Friedrich Fraundorfer"],"pdf_url":"https://arxiv.org/pdf/2503.21525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13255v3","updated":"2025-03-27T14:03:25Z","published":"2024-02-20T18:59:57Z","title":"How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey","summary":"  Over the past two decades, research in the field of Simultaneous Localization\nand Mapping (SLAM) has undergone a significant evolution, highlighting its\ncritical role in enabling autonomous exploration of unknown environments. This\nevolution ranges from hand-crafted methods, through the era of deep learning,\nto more recent developments focused on Neural Radiance Fields (NeRFs) and 3D\nGaussian Splatting (3DGS) representations. Recognizing the growing body of\nresearch and the absence of a comprehensive survey on the topic, this paper\naims to provide the first comprehensive overview of SLAM progress through the\nlens of the latest advancements in radiance fields. It sheds light on the\nbackground, evolutionary path, inherent strengths and limitations, and serves\nas a fundamental reference to highlight the dynamic progress and specific\nchallenges.\n","authors":["Fabio Tosi","Youmin Zhang","Ziren Gong","Erik Sandström","Stefano Mattoccia","Martin R. Oswald","Matteo Poggi"],"pdf_url":"https://arxiv.org/pdf/2402.13255v3.pdf","comment":"Updated to November 2024"},{"id":"http://arxiv.org/abs/2503.21510v1","updated":"2025-03-27T13:59:19Z","published":"2025-03-27T13:59:19Z","title":"Uncertainty-aware Bayesian machine learning modelling of land cover\n  classification","summary":"  Land cover classification involves the production of land cover maps, which\ndetermine the type of land through remote sensing imagery. Over recent years,\nsuch classification is being performed by machine learning classification\nmodels, which can give highly accurate predictions on land cover per pixel\nusing large quantities of input training data. However, such models do not\ncurrently take account of input measurement uncertainty, which is vital for\ntraceability in metrology. In this work we propose a Bayesian classification\nframework using generative modelling to take account of input measurement\nuncertainty. We take the specific case of Bayesian quadratic discriminant\nanalysis, and apply it to land cover datasets from Copernicus Sentinel-2 in\n2020 and 2021. We benchmark the performance of the model against more popular\nclassification models used in land cover maps such as random forests and neural\nnetworks. We find that such Bayesian models are more trustworthy, in the sense\nthat they are more interpretable, explicitly model the input measurement\nuncertainty, and maintain predictive performance of class probability outputs\nacross datasets of different years and sizes, whilst also being computationally\nefficient.\n","authors":["Samuel Bilson","Anna Pustogvar"],"pdf_url":"https://arxiv.org/pdf/2503.21510v1.pdf","comment":"31 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.20349v2","updated":"2025-03-27T13:59:15Z","published":"2025-03-26T09:20:42Z","title":"Consistency Trajectory Matching for One-Step Generative Super-Resolution","summary":"  Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency.\n","authors":["Weiyi You","Mingyang Zhang","Leheng Zhang","Xingyu Zhou","Kexuan Shi","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2503.20349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01477v2","updated":"2025-03-27T13:57:53Z","published":"2024-12-02T13:24:43Z","title":"Improving Object Detection by Modifying Synthetic Data with Explainable\n  AI","summary":"  Limited real-world data severely impacts model performance in many computer\nvision domains, particularly for samples that are underrepresented in training.\nSynthetically generated images are a promising solution, but 1) it remains\nunclear how to design synthetic training data to optimally improve model\nperformance (e.g, whether and where to introduce more realism or more\nabstraction) and 2) the domain expertise, time and effort required from human\noperators for this design and optimisation process represents a major practical\nchallenge. Here we propose a novel conceptual approach to improve the\nefficiency of designing synthetic images, by using robust Explainable AI (XAI)\ntechniques to guide a human-in-the-loop process of modifying 3D mesh models\nused to generate these images. Importantly, this framework allows both\nmodifications that increase and decrease realism in synthetic data, which can\nboth improve model performance. We illustrate this concept using a real-world\nexample where data are sparse; detection of vehicles in infrared imagery. We\nfine-tune an initial YOLOv8 model on the ATR DSIAC infrared dataset and\nsynthetic images generated from 3D mesh models in the Unity gaming engine, and\nthen use XAI saliency maps to guide modification of our Unity models. We show\nthat synthetic data can improve detection of vehicles in orientations unseen in\ntraining by 4.6% (to mAP50 = 94.6%). We further improve performance by an\nadditional 1.5% (to 96.1%) through our new XAI-guided approach, which reduces\nmisclassifications through both increasing and decreasing the realism of\ndifferent parts of the synthetic data. Our proof-of-concept results pave the\nway for fine, XAI-controlled curation of synthetic datasets tailored to improve\nobject detection performance, whilst simultaneously reducing the burden on\nhuman operators in designing and optimising these datasets.\n","authors":["Nitish Mital","Simon Malzard","Richard Walters","Celso M. De Melo","Raghuveer Rao","Victoria Nockles"],"pdf_url":"https://arxiv.org/pdf/2412.01477v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14847v2","updated":"2025-03-27T13:49:30Z","published":"2024-11-22T10:47:47Z","title":"Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly 4D\n  Reconstruction","summary":"  The recent development of 3D Gaussian Splatting (3DGS) has led to great\ninterest in 4D dynamic spatial reconstruction. Existing approaches mainly rely\non full-length multi-view videos, while there has been limited exploration of\nonline reconstruction methods that enable on-the-fly training and per-timestep\nstreaming. Current 3DGS-based streaming methods treat the Gaussian primitives\nuniformly and constantly renew the densified Gaussians, thereby overlooking the\ndifference between dynamic and static features as well as neglecting the\ntemporal continuity in the scene. To address these limitations, we propose a\nnovel three-stage pipeline for iterative streamable 4D dynamic spatial\nreconstruction. Our pipeline comprises a selective inheritance stage to\npreserve temporal continuity, a dynamics-aware shift stage to distinguish\ndynamic and static primitives and optimize their movements, and an error-guided\ndensification stage to accommodate emerging objects. Our method achieves\nstate-of-the-art performance in online 4D reconstruction, demonstrating the\nfastest on-the-fly training, superior representation quality, and real-time\nrendering capability. Project page: https://www.liuzhening.top/DASS\n","authors":["Zhening Liu","Yingdong Hu","Xinjie Zhang","Rui Song","Jiawei Shao","Zehong Lin","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14847v2.pdf","comment":"Project page: https://www.liuzhening.top/DASS"},{"id":"http://arxiv.org/abs/2503.21505v1","updated":"2025-03-27T13:45:47Z","published":"2025-03-27T13:45:47Z","title":"Fine-Grained Evaluation of Large Vision-Language Models in Autonomous\n  Driving","summary":"  Existing benchmarks for Vision-Language Model (VLM) on autonomous driving\n(AD) primarily assess interpretability through open-form visual question\nanswering (QA) within coarse-grained tasks, which remain insufficient to assess\ncapabilities in complex driving scenarios. To this end, we introduce\n$\\textbf{VLADBench}$, a challenging and fine-grained dataset featuring\nclose-form QAs that progress from static foundational knowledge and elements to\nadvanced reasoning for dynamic on-road situations. The elaborate\n$\\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,\nGeneral Element Recognition, Traffic Graph Generation, Target Attribute\nComprehension, and Ego Decision-Making and Planning. These domains are further\nbroken down into 11 secondary aspects and 29 tertiary tasks for a granular\nevaluation. A thorough assessment of general and domain-specific (DS) VLMs on\nthis benchmark reveals both their strengths and critical limitations in AD\ncontexts. To further exploit the cognitive and reasoning interactions among the\n5 domains for AD understanding, we start from a small-scale VLM and train the\nDS models on individual domain datasets (collected from 1.4M DS QAs across\npublic sources). The experimental results demonstrate that the proposed\nbenchmark provides a crucial step toward a more comprehensive assessment of\nVLMs in AD, paving the way for the development of more cognitively\nsophisticated and reasoning-capable AD systems.\n","authors":["Yue Li","Meng Tian","Zhenyu Lin","Jiangtong Zhu","Dechang Zhu","Haiqiang Liu","Zining Wang","Yueyi Zhang","Zhiwei Xiong","Xinhai Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.21505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21504v1","updated":"2025-03-27T13:45:35Z","published":"2025-03-27T13:45:35Z","title":"Keyword-Oriented Multimodal Modeling for Euphemism Identification","summary":"  Euphemism identification deciphers the true meaning of euphemisms, such as\nlinking \"weed\" (euphemism) to \"marijuana\" (target keyword) in illicit texts,\naiding content moderation and combating underground markets. While existing\nmethods are primarily text-based, the rise of social media highlights the need\nfor multimodal analysis, incorporating text, images, and audio. However, the\nlack of multimodal datasets for euphemisms limits further research. To address\nthis, we regard euphemisms and their corresponding target keywords as keywords\nand first introduce a keyword-oriented multimodal corpus of euphemisms\n(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including\ntext, images, and speech. We further propose a keyword-oriented multimodal\neuphemism identification method (KOM-EI), which uses cross-modal feature\nalignment and dynamic fusion modules to explicitly utilize the visual and audio\nfeatures of the keywords for efficient euphemism identification. Extensive\nexperiments demonstrate that KOM-EI outperforms state-of-the-art models and\nlarge language models, and show the importance of our multimodal datasets.\n","authors":["Yuxue Hu","Junsong Li","Meixuan Chen","Dongyu Su","Tongguan Wang","Ying Sha"],"pdf_url":"https://arxiv.org/pdf/2503.21504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19721v2","updated":"2025-03-27T13:41:35Z","published":"2025-03-25T14:46:45Z","title":"EventMamba: Enhancing Spatio-Temporal Locality with State Space Models\n  for Event-Based Video Reconstruction","summary":"  Leveraging its robust linear global modeling capability, Mamba has notably\nexcelled in computer vision. Despite its success, existing Mamba-based vision\nmodels have overlooked the nuances of event-driven tasks, especially in video\nreconstruction. Event-based video reconstruction (EBVR) demands spatial\ntranslation invariance and close attention to local event relationships in the\nspatio-temporal domain. Unfortunately, conventional Mamba algorithms apply\nstatic window partitions and standard reshape scanning methods, leading to\nsignificant losses in local connectivity. To overcome these limitations, we\nintroduce EventMamba--a specialized model designed for EBVR tasks. EventMamba\ninnovates by incorporating random window offset (RWO) in the spatial domain,\nmoving away from the restrictive fixed partitioning. Additionally, it features\na new consistent traversal serialization approach in the spatio-temporal\ndomain, which maintains the proximity of adjacent events both spatially and\ntemporally. These enhancements enable EventMamba to retain Mamba's robust\nmodeling capabilities while significantly preserving the spatio-temporal\nlocality of event data. Comprehensive testing on multiple datasets shows that\nEventMamba markedly enhances video reconstruction, drastically improving\ncomputation speed while delivering superior visual quality compared to\nTransformer-based methods.\n","authors":["Chengjie Ge","Xueyang Fu","Peng He","Kunyu Wang","Chengzhi Cao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2503.19721v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21501v1","updated":"2025-03-27T13:40:49Z","published":"2025-03-27T13:40:49Z","title":"Double Blind Imaging with Generative Modeling","summary":"  Blind inverse problems in imaging arise from uncertainties in the system used\nto collect (noisy) measurements of images. Recovering clean images from these\nmeasurements typically requires identifying the imaging system, either\nimplicitly or explicitly. A common solution leverages generative models as\npriors for both the images and the imaging system parameters (e.g., a class of\npoint spread functions). To learn these priors in a straightforward manner\nrequires access to a dataset of clean images as well as samples of the imaging\nsystem. We propose an AmbientGAN-based generative technique to identify the\ndistribution of parameters in unknown imaging systems, using only unpaired\nclean images and corrupted measurements. This learned distribution can then be\nused in model-based recovery algorithms to solve blind inverse problems such as\nblind deconvolution. We successfully demonstrate our technique for learning\nGaussian blur and motion blur priors from noisy measurements and show their\nutility in solving blind deconvolution with diffusion posterior sampling.\n","authors":["Brett Levac","Ajil Jalal","Kannan Ramchandran","Jonathan I. Tamir"],"pdf_url":"https://arxiv.org/pdf/2503.21501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21489v1","updated":"2025-03-27T13:25:40Z","published":"2025-03-27T13:25:40Z","title":"Shape Modeling of Longitudinal Medical Images: From Diffeomorphic Metric\n  Mapping to Deep Learning","summary":"  Living biological tissue is a complex system, constantly growing and changing\nin response to external and internal stimuli. These processes lead to\nremarkable and intricate changes in shape. Modeling and understanding both\nnatural and pathological (or abnormal) changes in the shape of anatomical\nstructures is highly relevant, with applications in diagnostic, prognostic, and\ntherapeutic healthcare. Nevertheless, modeling the longitudinal shape change of\nbiological tissue is a non-trivial task due to its inherent nonlinear nature.\nIn this review, we highlight several existing methodologies and tools for\nmodeling longitudinal shape change (i.e., spatiotemporal shape modeling). These\nmethods range from diffeomorphic metric mapping to deep-learning based\napproaches (e.g., autoencoders, generative networks, recurrent neural networks,\netc.). We discuss the synergistic combinations of existing technologies and\npotential directions for future research, underscoring key deficiencies in the\ncurrent research landscape.\n","authors":["Edwin Tay","Nazli Tümer","Amir A. Zadpoor"],"pdf_url":"https://arxiv.org/pdf/2503.21489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21486v1","updated":"2025-03-27T13:22:40Z","published":"2025-03-27T13:22:40Z","title":"Invert2Restore: Zero-Shot Degradation-Blind Image Restoration","summary":"  Two of the main challenges of image restoration in real-world scenarios are\nthe accurate characterization of an image prior and the precise modeling of the\nimage degradation operator. Pre-trained diffusion models have been very\nsuccessfully used as image priors in zero-shot image restoration methods.\nHowever, how to best handle the degradation operator is still an open problem.\nIn real-world data, methods that rely on specific parametric assumptions about\nthe degradation model often face limitations in their applicability. To address\nthis, we introduce Invert2Restore, a zero-shot, training-free method that\noperates in both fully blind and partially blind settings -- requiring no prior\nknowledge of the degradation model or only partial knowledge of its parametric\nform without known parameters. Despite this, Invert2Restore achieves\nhigh-fidelity results and generalizes well across various types of image\ndegradation. It leverages a pre-trained diffusion model as a deterministic\nmapping between normal samples and undistorted image samples. The key insight\nis that the input noise mapped by a diffusion model to a degraded image lies in\na low-probability density region of the standard normal distribution. Thus, we\ncan restore the degraded image by carefully guiding its input noise toward a\nhigher-density region. We experimentally validate Invert2Restore across several\nimage restoration tasks, demonstrating that it achieves state-of-the-art\nperformance in scenarios where the degradation operator is either unknown or\npartially known.\n","authors":["Hamadi Chihaoui","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2503.21486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21483v1","updated":"2025-03-27T13:18:40Z","published":"2025-03-27T13:18:40Z","title":"BOLT: Boost Large Vision-Language Model Without Training for Long-form\n  Video Understanding","summary":"  Large video-language models (VLMs) have demonstrated promising progress in\nvarious video understanding tasks. However, their effectiveness in long-form\nvideo analysis is constrained by limited context windows. Traditional\napproaches, such as uniform frame sampling, often inevitably allocate resources\nto irrelevant content, diminishing their effectiveness in real-world scenarios.\nIn this paper, we introduce BOLT, a method to BOost Large VLMs without\nadditional Training through a comprehensive study of frame selection\nstrategies. First, to enable a more realistic evaluation of VLMs in long-form\nvideo understanding, we propose a multi-source retrieval evaluation setting.\nOur findings reveal that uniform sampling performs poorly in noisy contexts,\nunderscoring the importance of selecting the right frames. Second, we explore\nseveral frame selection strategies based on query-frame similarity and analyze\ntheir effectiveness at inference time. Our results show that inverse transform\nsampling yields the most significant performance improvement, increasing\naccuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from\n58.9% to 63.4%. Our code is available at https://github.com/sming256/BOLT.\n","authors":["Shuming Liu","Chen Zhao","Tianqi Xu","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2503.21483v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2410.14379v2","updated":"2025-03-27T13:09:07Z","published":"2024-10-18T11:07:12Z","title":"AnomalyNCD: Towards Novel Anomaly Class Discovery in Industrial\n  Scenarios","summary":"  Recently, multi-class anomaly classification has garnered increasing\nattention. Previous methods directly cluster anomalies but often struggle due\nto the lack of anomaly-prior knowledge. Acquiring this knowledge faces two\nissues: the non-prominent and weak-semantics anomalies. In this paper, we\npropose AnomalyNCD, a multi-class anomaly classification network compatible\nwith different anomaly detection methods. To address the non-prominence of\nanomalies, we design main element binarization (MEBin) to obtain\nanomaly-centered images, ensuring anomalies are learned while avoiding the\nimpact of incorrect detections. Next, to learn anomalies with weak semantics,\nwe design mask-guided representation learning, which focuses on isolated\nanomalies guided by masks and reduces confusion from erroneous inputs through\ncorrected pseudo labels. Finally, to enable flexible classification at both\nregion and image levels, we develop a region merging strategy that determines\nthe overall image category based on the classified anomaly regions. Our method\noutperforms the state-of-the-art works on the MVTec AD and MTD datasets.\nCompared with the current methods, AnomalyNCD combined with zero-shot anomaly\ndetection method achieves a 10.8% $F_1$ gain, 8.8% NMI gain, and 9.5% ARI gain\non MVTec AD, and 12.8% $F_1$ gain, 5.7% NMI gain, and 10.8% ARI gain on MTD.\nCode is available at https://github.com/HUST-SLOW/AnomalyNCD.\n","authors":["Ziming Huang","Xurui Li","Haotian Liu","Feng Xue","Yuzhe Wang","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.14379v2.pdf","comment":"Accepted at CVPR2025"},{"id":"http://arxiv.org/abs/2503.21477v1","updated":"2025-03-27T13:06:57Z","published":"2025-03-27T13:06:57Z","title":"Fine-Grained Behavior and Lane Constraints Guided Trajectory Prediction\n  Method","summary":"  Trajectory prediction, as a critical component of autonomous driving systems,\nhas attracted the attention of many researchers. Existing prediction algorithms\nfocus on extracting more detailed scene features or selecting more reasonable\ntrajectory destinations. However, in the face of dynamic and evolving future\nmovements of the target vehicle, these algorithms cannot provide a fine-grained\nand continuous description of future behaviors and lane constraints, which\ndegrades the prediction accuracy. To address this challenge, we present BLNet,\na novel dualstream architecture that synergistically integrates behavioral\nintention recognition and lane constraint modeling through parallel attention\nmechanisms. The framework generates fine-grained behavior state queries\n(capturing spatial-temporal movement patterns) and lane queries (encoding lane\ntopology constraints), supervised by two auxiliary losses, respectively.\nSubsequently, a two-stage decoder first produces trajectory proposals, then\nperforms point-level refinement by jointly incorporating both the continuity of\npassed lanes and future motion features. Extensive experiments on two large\ndatasets, nuScenes and Argoverse, show that our network exhibits significant\nperformance gains over existing direct regression and goal-based algorithms.\n","authors":["Wenyi Xiong","Jian Chen","Ziheng Qi"],"pdf_url":"https://arxiv.org/pdf/2503.21477v1.pdf","comment":"This work has been submitted to the IEEE TIM for possible publication"},{"id":"http://arxiv.org/abs/2503.18940v2","updated":"2025-03-27T13:05:19Z","published":"2025-03-24T17:59:02Z","title":"Training-free Diffusion Acceleration with Bottleneck Sampling","summary":"  Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3$\\times$ for image generation and 2.5$\\times$ for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics.\n","authors":["Ye Tian","Xin Xia","Yuxi Ren","Shanchuan Lin","Xing Wang","Xuefeng Xiao","Yunhai Tong","Ling Yang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2503.18940v2.pdf","comment":"Project Page: https://tyfeld.github.io/BottleneckSampling.github.io/"},{"id":"http://arxiv.org/abs/2503.21469v1","updated":"2025-03-27T13:01:53Z","published":"2025-03-27T13:01:53Z","title":"Embedding Compression Distortion in Video Coding for Machines","summary":"  Currently, video transmission serves not only the Human Visual System (HVS)\nfor viewing but also machine perception for analysis. However, existing codecs\nare primarily optimized for pixel-domain and HVS-perception metrics rather than\nthe needs of machine vision tasks. To address this issue, we propose a\nCompression Distortion Representation Embedding (CDRE) framework, which\nextracts machine-perception-related distortion representation and embeds it\ninto downstream models, addressing the information lost during compression and\nimproving task performance. Specifically, to better analyze the\nmachine-perception-related distortion, we design a compression-sensitive\nextractor that identifies compression degradation in the feature domain. For\nefficient transmission, a lightweight distortion codec is introduced to\ncompress the distortion information into a compact representation.\nSubsequently, the representation is progressively embedded into the downstream\nmodel, enabling it to be better informed about compression degradation and\nenhancing performance. Experiments across various codecs and downstream tasks\ndemonstrate that our framework can effectively boost the rate-task performance\nof existing codecs with minimal overhead in terms of bitrate, execution time,\nand number of parameters. Our codes and supplementary materials are released in\nhttps://github.com/Ws-Syx/CDRE/.\n","authors":["Yuxiao Sun","Yao Zhao","Meiqin Liu","Chao Yao","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2503.21469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21465v1","updated":"2025-03-27T12:55:07Z","published":"2025-03-27T12:55:07Z","title":"Retinal Fundus Multi-Disease Image Classification using Hybrid\n  CNN-Transformer-Ensemble Architectures","summary":"  Our research is motivated by the urgent global issue of a large population\naffected by retinal diseases, which are evenly distributed but underserved by\nspecialized medical expertise, particularly in non-urban areas. Our primary\nobjective is to bridge this healthcare gap by developing a comprehensive\ndiagnostic system capable of accurately predicting retinal diseases solely from\nfundus images. However, we faced significant challenges due to limited, diverse\ndatasets and imbalanced class distributions. To overcome these issues, we have\ndevised innovative strategies. Our research introduces novel approaches,\nutilizing hybrid models combining deeper Convolutional Neural Networks (CNNs),\nTransformer encoders, and ensemble architectures sequentially and in parallel\nto classify retinal fundus images into 20 disease labels. Our overarching goal\nis to assess these advanced models' potential in practical applications, with a\nstrong focus on enhancing retinal disease diagnosis accuracy across a broader\nspectrum of conditions. Importantly, our efforts have surpassed baseline model\nresults, with the C-Tran ensemble model emerging as the leader, achieving a\nremarkable model score of 0.9166, surpassing the baseline score of 0.9.\nAdditionally, experiments with the IEViT model showcased equally promising\noutcomes with improved computational efficiency. We've also demonstrated the\neffectiveness of dynamic patch extraction and the integration of domain\nknowledge in computer vision tasks. In summary, our research strives to\ncontribute significantly to retinal disease diagnosis, addressing the critical\nneed for accessible healthcare solutions in underserved regions while aiming\nfor comprehensive and accurate disease prediction.\n","authors":["Deependra Singh","Saksham Agarwal","Subhankar Mishra"],"pdf_url":"https://arxiv.org/pdf/2503.21465v1.pdf","comment":"17 pages, 3 figures, 7 tables. Conference paper presented at the\n  International Health Informatics Conference (IHIC 2023)"},{"id":"http://arxiv.org/abs/2503.21459v1","updated":"2025-03-27T12:49:09Z","published":"2025-03-27T12:49:09Z","title":"RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event\n  Understanding from Social Video Narratives","summary":"  We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for\ngeneric road event understanding from social media narratives. Unlike existing\ndatasets limited by regional bias, viewpoint bias and expert-driven\nannotations, RoadSocial captures the global complexity of road events with\nvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich social\ndiscourse. Our scalable semi-automatic annotation framework leverages Text LLMs\nand Video LLMs to generate comprehensive question-answer pairs across 12\nchallenging QA tasks, pushing the boundaries of road event understanding.\nRoadSocial is derived from social media videos spanning 14M frames and 414K\nsocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260K\nhigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,\ndriving-specific and general-purpose) on our road event understanding\nbenchmark. We also demonstrate RoadSocial's utility in improving road event\nunderstanding capabilities of general-purpose Video LLMs.\n","authors":["Chirag Parikh","Deepti Rawat","Rakshitha R. T.","Tathagata Ghosh","Ravi Kiran Sarvadevabhatla"],"pdf_url":"https://arxiv.org/pdf/2503.21459v1.pdf","comment":"Accepted at CVPR 2025; Project Page: https://roadsocial.github.io/"},{"id":"http://arxiv.org/abs/2503.21457v1","updated":"2025-03-27T12:45:44Z","published":"2025-03-27T12:45:44Z","title":"FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for\n  Benchmarking Face Perception MLLMs","summary":"  Multimodal large language models (MLLMs) have demonstrated remarkable\ncapabilities in various tasks. However, effectively evaluating these MLLMs on\nface perception remains largely unexplored. To address this gap, we introduce\nFaceBench, a dataset featuring hierarchical multi-view and multi-level\nattributes specifically designed to assess the comprehensive face perception\nabilities of MLLMs. Initially, we construct a hierarchical facial attribute\nstructure, which encompasses five views with up to three levels of attributes,\ntotaling over 210 attributes and 700 attribute values. Based on the structure,\nthe proposed FaceBench consists of 49,919 visual question-answering (VQA) pairs\nfor evaluation and 23,841 pairs for fine-tuning. Moreover, we further develop a\nrobust face perception MLLM baseline, Face-LLaVA, by training with our proposed\nface VQA data. Extensive experiments on various mainstream MLLMs and Face-LLaVA\nare conducted to test their face perception ability, with results also compared\nagainst human performance. The results reveal that, the existing MLLMs are far\nfrom satisfactory in understanding the fine-grained facial attributes, while\nour Face-LLaVA significantly outperforms existing open-source models with a\nsmall amount of training data and is comparable to commercial ones like GPT-4o\nand Gemini. The dataset will be released at\nhttps://github.com/CVI-SZU/FaceBench.\n","authors":["Xiaoqin Wang","Xusen Ma","Xianxu Hou","Meidan Ding","Yudong Li","Junliang Chen","Wenting Chen","Xiaoyang Peng","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2503.21457v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.21449v1","updated":"2025-03-27T12:41:42Z","published":"2025-03-27T12:41:42Z","title":"Towards Generating Realistic 3D Semantic Training Data for Autonomous\n  Driving","summary":"  Semantic scene understanding is crucial for robotics and computer vision\napplications. In autonomous driving, 3D semantic segmentation plays an\nimportant role for enabling safe navigation. Despite significant advances in\nthe field, the complexity of collecting and annotating 3D data is a bottleneck\nin this developments. To overcome that data annotation limitation, synthetic\nsimulated data has been used to generate annotated data on demand. There is\nstill however a domain gap between real and simulated data. More recently,\ndiffusion models have been in the spotlight, enabling close-to-real data\nsynthesis. Those generative models have been recently applied to the 3D data\ndomain for generating scene-scale data with semantic annotations. Still, those\nmethods either rely on image projection or decoupled models trained with\ndifferent resolutions in a coarse-to-fine manner. Such intermediary\nrepresentations impact the generated data quality due to errors added in those\ntransformations. In this work, we propose a novel approach able to generate 3D\nsemantic scene-scale data without relying on any projection or decoupled\ntrained multi-resolution models, achieving more realistic semantic scene data\ngeneration compared to previous state-of-the-art methods. Besides improving 3D\nsemantic scene-scale data synthesis, we thoroughly evaluate the use of the\nsynthetic scene samples as labeled data to train a semantic segmentation\nnetwork. In our experiments, we show that using the synthetic annotated data\ngenerated by our method as training data together with the real semantic\nsegmentation labels, leads to an improvement in the semantic segmentation model\nperformance. Our results show the potential of generated scene-scale point\nclouds to generate more training data to extend existing datasets, reducing the\ndata annotation effort. Our code is available at\nhttps://github.com/PRBonn/3DiSS.\n","authors":["Lucas Nunes","Rodrigo Marcuzzi","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2503.21449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20519v2","updated":"2025-03-27T12:39:55Z","published":"2025-03-26T13:00:51Z","title":"MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D\n  Generation","summary":"  Recent advances in auto-regressive transformers have revolutionized\ngenerative modeling across different domains, from language processing to\nvisual generation, demonstrating remarkable capabilities. However, applying\nthese advances to 3D generation presents three key challenges: the unordered\nnature of 3D data conflicts with sequential next-token prediction paradigm,\nconventional vector quantization approaches incur substantial compression loss\nwhen applied to 3D meshes, and the lack of efficient scaling strategies for\nhigher resolution latent prediction. To address these challenges, we introduce\nMAR-3D, which integrates a pyramid variational autoencoder with a cascaded\nmasked auto-regressive transformer (Cascaded MAR) for progressive latent\nupscaling in the continuous space. Our architecture employs random masking\nduring training and auto-regressive denoising in random order during inference,\nnaturally accommodating the unordered property of 3D latent tokens.\nAdditionally, we propose a cascaded training strategy with condition\naugmentation that enables efficiently up-scale the latent token resolution with\nfast convergence. Extensive experiments demonstrate that MAR-3D not only\nachieves superior performance and generalization capabilities compared to\nexisting methods but also exhibits enhanced scaling capabilities compared to\njoint distribution modeling approaches (e.g., diffusion transformers).\n","authors":["Jinnan Chen","Lingting Zhu","Zeyu Hu","Shengju Qian","Yugang Chen","Xin Wang","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2503.20519v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.21443v1","updated":"2025-03-27T12:36:20Z","published":"2025-03-27T12:36:20Z","title":"Sparse Bayesian Learning for Label Efficiency in Cardiac Real-Time MRI","summary":"  Cardiac real-time magnetic resonance imaging (MRI) is an emerging technology\nthat images the heart at up to 50 frames per second, offering insight into the\nrespiratory effects on the heartbeat. However, this method significantly\nincreases the number of images that must be segmented to derive critical health\nindicators. Although neural networks perform well on inner slices, predictions\non outer slices are often unreliable.\n  This work proposes sparse Bayesian learning (SBL) to predict the ventricular\nvolume on outer slices with minimal manual labeling to address this challenge.\nThe ventricular volume over time is assumed to be dominated by sparse\nfrequencies corresponding to the heart and respiratory rates. Moreover, SBL\nidentifies these sparse frequencies on well-segmented inner slices by\noptimizing hyperparameters via type -II likelihood, automatically pruning\nirrelevant components. The identified sparse frequencies guide the selection of\nouter slice images for labeling, minimizing posterior variance.\n  This work provides performance guarantees for the greedy algorithm. Testing\non patient data demonstrates that only a few labeled images are necessary for\naccurate volume prediction. The labeling procedure effectively avoids selecting\ninefficient images. Furthermore, the Bayesian approach provides uncertainty\nestimates, highlighting unreliable predictions (e.g., when choosing suboptimal\nlabels).\n","authors":["Felix Terhag","Philipp Knechtges","Achim Basermann","Anja Bach","Darius Gerlach","Jens Tank","Raúl Tempone"],"pdf_url":"https://arxiv.org/pdf/2503.21443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21442v1","updated":"2025-03-27T12:35:03Z","published":"2025-03-27T12:35:03Z","title":"RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian\n  Splatting","summary":"  We consider the problem of adding dynamic rain effects to in-the-wild scenes\nin a physically-correct manner. Recent advances in scene modeling have made\nsignificant progress, with NeRF and 3DGS techniques emerging as powerful tools\nfor reconstructing complex scenes. However, while effective for novel view\nsynthesis, these methods typically struggle with challenging scene editing\ntasks, such as physics-based rain simulation. In contrast, traditional\nphysics-based simulations can generate realistic rain effects, such as\nraindrops and splashes, but they often rely on skilled artists to carefully set\nup high-fidelity scenes. This process lacks flexibility and scalability,\nlimiting its applicability to broader, open-world environments. In this work,\nwe introduce RainyGS, a novel approach that leverages the strengths of both\nphysics-based modeling and 3DGS to generate photorealistic, dynamic rain\neffects in open-world scenes with physical accuracy. At the core of our method\nis the integration of physically-based raindrop and shallow water simulation\ntechniques within the fast 3DGS rendering framework, enabling realistic and\nefficient simulations of raindrop behavior, splashes, and reflections. Our\nmethod supports synthesizing rain effects at over 30 fps, offering users\nflexible control over rain intensity -- from light drizzles to heavy downpours.\nWe demonstrate that RainyGS performs effectively for both real-world outdoor\nscenes and large-scale driving scenarios, delivering more photorealistic and\nphysically-accurate rain effects compared to state-of-the-art methods. Project\npage can be found at https://pku-vcl-geometry.github.io/RainyGS/\n","authors":["Qiyu Dai","Xingyu Ni","Qianfan Shen","Wenzheng Chen","Baoquan Chen","Mengyu Chu"],"pdf_url":"https://arxiv.org/pdf/2503.21442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21438v1","updated":"2025-03-27T12:25:20Z","published":"2025-03-27T12:25:20Z","title":"Dual-Task Learning for Dead Tree Detection and Segmentation with Hybrid\n  Self-Attention U-Nets in Aerial Imagery","summary":"  Mapping standing dead trees is critical for assessing forest health,\nmonitoring biodiversity, and mitigating wildfire risks, for which aerial\nimagery has proven useful. However, dense canopy structures, spectral overlaps\nbetween living and dead vegetation, and over-segmentation errors limit the\nreliability of existing methods. This study introduces a hybrid postprocessing\nframework that refines deep learning-based tree segmentation by integrating\nwatershed algorithms with adaptive filtering, enhancing boundary delineation,\nand reducing false positives in complex forest environments. Tested on\nhigh-resolution aerial imagery from boreal forests, the framework improved\ninstance-level segmentation accuracy by 41.5% and reduced positional errors by\n57%, demonstrating robust performance in densely vegetated regions. By\nbalancing detection accuracy and over-segmentation artifacts, the method\nenabled the precise identification of individual dead trees, which is critical\nfor ecological monitoring. The framework's computational efficiency supports\nscalable applications, such as wall-to-wall tree mortality mapping over large\ngeographic regions using aerial or satellite imagery. These capabilities\ndirectly benefit wildfire risk assessment (identifying fuel accumulations),\ncarbon stock estimation (tracking emissions from decaying biomass), and\nprecision forestry (targeting salvage loggings). By bridging advanced remote\nsensing techniques with practical forest management needs, this work advances\ntools for large-scale ecological conservation and climate resilience planning.\n","authors":["Anis Ur Rahman","Einari Heinaro","Mete Ahishali","Samuli Junttila"],"pdf_url":"https://arxiv.org/pdf/2503.21438v1.pdf","comment":"11 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.02482v2","updated":"2025-03-27T12:12:46Z","published":"2024-09-04T07:18:26Z","title":"Volumetric Surfaces: Representing Fuzzy Geometries with Layered Meshes","summary":"  High-quality view synthesis relies on volume rendering, splatting, or surface\nrendering. While surface rendering is typically the fastest, it struggles to\naccurately model fuzzy geometry like hair. In turn, alpha-blending techniques\nexcel at representing fuzzy materials but require an unbounded number of\nsamples per ray (P1). Further overheads are induced by empty space skipping in\nvolume rendering (P2) and sorting input primitives in splatting (P3). We\npresent a novel representation for real-time view synthesis where the (P1)\nnumber of sampling locations is small and bounded, (P2) sampling locations are\nefficiently found via rasterization, and (P3) rendering is sorting-free. We\nachieve this by representing objects as semi-transparent multi-layer meshes\nrendered in a fixed order. First, we model surface layers as signed distance\nfunction (SDF) shells with optimal spacing learned during training. Then, we\nbake them as meshes and fit UV textures. Unlike single-surface methods, our\nmulti-layer representation effectively models fuzzy objects. In contrast to\nvolume and splatting-based methods, our approach enables real-time rendering on\nlow-power laptops and smartphones.\n","authors":["Stefano Esposito","Anpei Chen","Christian Reiser","Samuel Rota Bulò","Lorenzo Porzi","Katja Schwarz","Christian Richardt","Michael Zollhöfer","Peter Kontschieder","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2409.02482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21425v1","updated":"2025-03-27T12:10:51Z","published":"2025-03-27T12:10:51Z","title":"STAMICS: Splat, Track And Map with Integrated Consistency and Semantics\n  for Dense RGB-D SLAM","summary":"  Simultaneous Localization and Mapping (SLAM) is a critical task in robotics,\nenabling systems to autonomously navigate and understand complex environments.\nCurrent SLAM approaches predominantly rely on geometric cues for mapping and\nlocalization, but they often fail to ensure semantic consistency, particularly\nin dynamic or densely populated scenes. To address this limitation, we\nintroduce STAMICS, a novel method that integrates semantic information with 3D\nGaussian representations to enhance both localization and mapping accuracy.\nSTAMICS consists of three key components: a 3D Gaussian-based scene\nrepresentation for high-fidelity reconstruction, a graph-based clustering\ntechnique that enforces temporal semantic consistency, and an open-vocabulary\nsystem that allows for the classification of unseen objects. Extensive\nexperiments show that STAMICS significantly improves camera pose estimation and\nmap quality, outperforming state-of-the-art methods while reducing\nreconstruction errors. Code will be public available.\n","authors":["Yongxu Wang","Xu Cao","Weiyun Yi","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2503.21425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07776v2","updated":"2025-03-27T11:57:50Z","published":"2024-12-10T18:59:58Z","title":"Video Motion Transfer with Diffusion Transformers","summary":"  We propose DiTFlow, a method for transferring the motion of a reference video\nto a newly synthesized one, designed specifically for Diffusion Transformers\n(DiT). We first process the reference video with a pre-trained DiT to analyze\ncross-frame attention maps and extract a patch-wise motion signal called the\nAttention Motion Flow (AMF). We guide the latent denoising process in an\noptimization-based, training-free, manner by optimizing latents with our AMF\nloss to generate videos reproducing the motion of the reference one. We also\napply our optimization strategy to transformer positional embeddings, granting\nus a boost in zero-shot motion transfer capabilities. We evaluate DiTFlow\nagainst recently published methods, outperforming all across multiple metrics\nand human evaluation.\n","authors":["Alexander Pondaven","Aliaksandr Siarohin","Sergey Tulyakov","Philip Torr","Fabio Pizzati"],"pdf_url":"https://arxiv.org/pdf/2412.07776v2.pdf","comment":"CVPR 2025 - Project page: https://ditflow.github.io/"},{"id":"http://arxiv.org/abs/2502.06352v2","updated":"2025-03-27T11:53:23Z","published":"2025-02-10T11:05:18Z","title":"LANTERN++: Enhancing Relaxed Speculative Decoding with Static Tree\n  Drafting for Visual Auto-regressive Models","summary":"  Speculative decoding has been widely used to accelerate auto-regressive (AR)\ntext generation. However, its effectiveness for visual AR models remains\nlimited due to token selection ambiguity, where multiple tokens share similarly\nlow probabilities and thus reduce acceptance rates. Recently, relaxed\nspeculative decoding with dynamic tree drafting was proposed to mitigate this\nambiguity, demonstrating promising results in accelerating visual AR models.\nHowever, we observe that token selection ambiguity still negatively affects\ndynamic tree drafting, resulting in shallow draft trees and limited\nacceleration. To overcome this issue, we introduce LANTERN++, a refined\nframework that integrates static tree drafting with a tailored relaxed\nacceptance condition, allowing drafts to be selected independently of\nlow-confidence predictions. This enables the acceptance of deeper sequences,\nimproving decoding efficiency while preserving image quality. Extensive\nexperiments on state-of-the-art visual AR models demonstrate that LANTERN++\nsignificantly accelerates inference, achieving up to $\\mathbf{\\times 2.56}$\nspeedup over standard AR decoding while maintaining high image quality. The\ncode is publicly available at https://github.com/jadohu/LANTERN.\n","authors":["Sihwan Park","Doohyuk Jang","Sungyub Kim","Souvik Kundu","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2502.06352v2.pdf","comment":"ICLR 2025 Workshop at SCOPE (Oral), 16 pages, 5 figures, short paper\n  (6 pages exclude reference and appendix)"},{"id":"http://arxiv.org/abs/2503.21410v1","updated":"2025-03-27T11:52:37Z","published":"2025-03-27T11:52:37Z","title":"Diffusion Image Prior","summary":"  Zero-shot image restoration (IR) methods based on pretrained diffusion models\nhave recently achieved significant success. These methods typically require at\nleast a parametric form of the degradation model. However, in real-world\nscenarios, the degradation may be too complex to define explicitly. To handle\nthis general case, we introduce the Diffusion Image Prior (DIIP). We take\ninspiration from the Deep Image Prior (DIP)[16], since it can be used to remove\nartifacts without the need for an explicit degradation model. However, in\ncontrast to DIP, we find that pretrained diffusion models offer a much stronger\nprior, despite being trained without knowledge from corrupted data. We show\nthat, the optimization process in DIIP first reconstructs a clean version of\nthe image before eventually overfitting to the degraded input, but it does so\nfor a broader range of degradations than DIP. In light of this result, we\npropose a blind image restoration (IR) method based on early stopping, which\ndoes not require prior knowledge of the degradation model. We validate DIIP on\nvarious degradation-blind IR tasks, including JPEG artifact removal, waterdrop\nremoval, denoising and super-resolution with state-of-the-art results.\n","authors":["Hamadi Chihaoui","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2503.21410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21408v1","updated":"2025-03-27T11:52:08Z","published":"2025-03-27T11:52:08Z","title":"VALLR: Visual ASR Language Model for Lip Reading","summary":"  Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex\ntask requiring the interpretation of spoken language exclusively from visual\ncues, primarily lip movements and facial expressions. This task is especially\nchallenging due to the absence of auditory information and the inherent\nambiguity when visually distinguishing phonemes that have overlapping visemes\nwhere different phonemes appear identical on the lips. Current methods\ntypically attempt to predict words or characters directly from these visual\ncues, but this approach frequently encounters high error rates due to\ncoarticulation effects and viseme ambiguity. We propose a novel two-stage,\nphoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that\naddresses these longstanding challenges. First, our model predicts a compact\nsequence of phonemes from visual inputs using a Video Transformer with a CTC\nhead, thereby reducing the task complexity and achieving robust speaker\ninvariance. This phoneme output then serves as the input to a fine-tuned Large\nLanguage Model (LLM), which reconstructs coherent words and sentences by\nleveraging broader linguistic context. Unlike existing methods that either\npredict words directly-often faltering on visually similar phonemes-or rely on\nlarge-scale multimodal pre-training, our approach explicitly encodes\nintermediate linguistic structure while remaining highly data efficient. We\ndemonstrate state-of-the-art performance on two challenging datasets, LRS2 and\nLRS3, where our method achieves significant reductions in Word Error Rate (WER)\nachieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data\nthan the next best approach.\n","authors":["Marshall Thomas","Edward Fish","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2503.21408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03708v3","updated":"2025-03-27T11:46:22Z","published":"2025-03-05T17:59:19Z","title":"Rethinking Video Tokenization: A Conditioned Diffusion-based Approach","summary":"  Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.\n","authors":["Nianzu Yang","Pandeng Li","Liming Zhao","Yang Li","Chen-Wei Xie","Yehui Tang","Xudong Lu","Zhihang Liu","Yun Zheng","Yu Liu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2503.03708v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21397v1","updated":"2025-03-27T11:39:55Z","published":"2025-03-27T11:39:55Z","title":"ProHOC: Probabilistic Hierarchical Out-of-Distribution Classification\n  via Multi-Depth Networks","summary":"  Out-of-distribution (OOD) detection in deep learning has traditionally been\nframed as a binary task, where samples are either classified as belonging to\nthe known classes or marked as OOD, with little attention given to the semantic\nrelationships between OOD samples and the in-distribution (ID) classes. We\npropose a framework for detecting and classifying OOD samples in a given class\nhierarchy. Specifically, we aim to predict OOD data to their correct internal\nnodes of the class hierarchy, whereas the known ID classes should be predicted\nas their corresponding leaf nodes. Our approach leverages the class hierarchy\nto create a probabilistic model and we implement this model by using networks\ntrained for ID classification at multiple hierarchy depths. We conduct\nexperiments on three datasets with predefined class hierarchies and show the\neffectiveness of our method. Our code is available at\nhttps://github.com/walline/prohoc.\n","authors":["Erik Wallin","Fredrik Kahl","Lars Hammarstrand"],"pdf_url":"https://arxiv.org/pdf/2503.21397v1.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2503.14001v3","updated":"2025-03-27T11:37:28Z","published":"2025-03-18T08:09:19Z","title":"Multimodal Feature-Driven Deep Learning for the Prediction of Duck Body\n  Dimensions and Weight","summary":"  Accurate body dimension and weight measurements are critical for optimizing\npoultry management, health assessment, and economic efficiency. This study\nintroduces an innovative deep learning-based model leveraging multimodal\ndata-2D RGB images from different views, depth images, and 3D point clouds-for\nthe non-invasive estimation of duck body dimensions and weight. A dataset of\n1,023 Linwu ducks, comprising over 5,000 samples with diverse postures and\nconditions, was collected to support model training. The proposed method\ninnovatively employs PointNet++ to extract key feature points from point\nclouds, extracts and computes corresponding 3D geometric features, and fuses\nthem with multi-view convolutional 2D features. A Transformer encoder is then\nutilized to capture long-range dependencies and refine feature interactions,\nthereby enhancing prediction robustness. The model achieved a mean absolute\npercentage error (MAPE) of 6.33% and an R2 of 0.953 across eight morphometric\nparameters, demonstrating strong predictive capability. Unlike conventional\nmanual measurements, the proposed model enables high-precision estimation while\neliminating the necessity for physical handling, thereby reducing animal stress\nand broadening its application scope. This study marks the first application of\ndeep learning techniques to poultry body dimension and weight estimation,\nproviding a valuable reference for the intelligent and precise management of\nthe livestock industry with far-reaching practical significance.\n","authors":["Yi Xiao","Qiannan Han","Gang Shu","Guiping Liang","Hongyan Zhang","Song Wang","Zhihao Xu","Weican Wan","Chuang Li","Guitao Jiang","Wenbo Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.14001v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17132v2","updated":"2025-03-27T11:35:37Z","published":"2025-03-21T13:31:16Z","title":"Temporal-Guided Spiking Neural Networks for Event-Based Human Action\n  Recognition","summary":"  This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.\n","authors":["Siyuan Yang","Shilin Lu","Shizheng Wang","Meng Hwa Er","Zengwei Zheng","Alex C. Kot"],"pdf_url":"https://arxiv.org/pdf/2503.17132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07091v3","updated":"2025-03-27T11:23:24Z","published":"2025-03-10T09:14:47Z","title":"FaceID-6M: A Large-Scale, Open-Source FaceID Customization Dataset","summary":"  Due to the data-driven nature of current face identity (FaceID) customization\nmethods, all state-of-the-art models rely on large-scale datasets containing\nmillions of high-quality text-image pairs for training. However, none of these\ndatasets are publicly available, which restricts transparency and hinders\nfurther advancements in the field.\n  To address this issue, in this paper, we collect and release FaceID-6M, the\nfirst large-scale, open-source FaceID dataset containing 6 million high-quality\ntext-image pairs. Filtered from LAION-5B \\cite{schuhmann2022laion}, FaceID-6M\nundergoes a rigorous image and text filtering steps to ensure dataset quality,\nincluding resolution filtering to maintain high-quality images and faces, face\nfiltering to remove images that lack human faces, and keyword-based strategy to\nretain descriptions containing human-related terms (e.g., nationality,\nprofessions and names). Through these cleaning processes, FaceID-6M provides a\nhigh-quality dataset optimized for training powerful FaceID customization\nmodels, facilitating advancements in the field by offering an open resource for\nresearch and development.\n  We conduct extensive experiments to show the effectiveness of our FaceID-6M,\ndemonstrating that models trained on our FaceID-6M dataset achieve performance\nthat is comparable to, and slightly better than currently available industrial\nmodels. Additionally, to support and advance research in the FaceID\ncustomization community, we make our code, datasets, and models fully publicly\navailable. Our codes, models, and datasets are available at:\nhttps://github.com/ShuheSH/FaceID-6M.\n","authors":["Shuhe Wang","Xiaoya Li","Jiwei Li","Guoyin Wang","Xiaofei Sun","Bob Zhu","Han Qiu","Mo Yu","Shengjie Shen","Tianwei Zhang","Eduard Hovy"],"pdf_url":"https://arxiv.org/pdf/2503.07091v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2501.15407"},{"id":"http://arxiv.org/abs/2412.15215v2","updated":"2025-03-27T11:12:07Z","published":"2024-12-19T18:59:57Z","title":"EnvGS: Modeling View-Dependent Appearance with Environment Gaussian","summary":"  Reconstructing complex reflections in real-world scenes from 2D images is\nessential for achieving photorealistic novel view synthesis. Existing methods\nthat utilize environment maps to model reflections from distant lighting often\nstruggle with high-frequency reflection details and fail to account for\nnear-field reflections. In this work, we introduce EnvGS, a novel approach that\nemploys a set of Gaussian primitives as an explicit 3D representation for\ncapturing reflections of environments. These environment Gaussian primitives\nare incorporated with base Gaussian primitives to model the appearance of the\nwhole scene. To efficiently render these environment Gaussian primitives, we\ndeveloped a ray-tracing-based renderer that leverages the GPU's RT core for\nfast rendering. This allows us to jointly optimize our model for high-quality\nreconstruction while maintaining real-time rendering speeds. Results from\nmultiple real-world and synthetic datasets demonstrate that our method produces\nsignificantly more detailed reflections, achieving the best rendering quality\nin real-time novel view synthesis. The code is available at\nhttps://zju3dv.github.io/envgs.\n","authors":["Tao Xie","Xi Chen","Zhen Xu","Yiman Xie","Yudong Jin","Yujun Shen","Sida Peng","Hujun Bao","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.15215v2.pdf","comment":"Project page: https://zju3dv.github.io/envgs"},{"id":"http://arxiv.org/abs/2503.21377v1","updated":"2025-03-27T11:09:58Z","published":"2025-03-27T11:09:58Z","title":"Unsupervised Real-World Denoising: Sparsity is All You Need","summary":"  Supervised training for real-world denoising presents challenges due to the\ndifficulty of collecting large datasets of paired noisy and clean images.\nRecent methods have attempted to address this by utilizing unpaired datasets of\nclean and noisy images. Some approaches leverage such unpaired data to train\ndenoisers in a supervised manner by generating synthetic clean-noisy pairs.\nHowever, these methods often fall short due to the distribution gap between\nsynthetic and real noisy images. To mitigate this issue, we propose a solution\nbased on input sparsification, specifically using random input masking. Our\nmethod, which we refer to as Mask, Inpaint and Denoise (MID), trains a denoiser\nto simultaneously denoise and inpaint synthetic clean-noisy pairs. On one hand,\ninput sparsification reduces the gap between synthetic and real noisy images.\nOn the other hand, an inpainter trained in a supervised manner can still\naccurately reconstruct sparse inputs by predicting missing clean pixels using\nthe remaining unmasked pixels. Our approach begins with a synthetic Gaussian\nnoise sampler and iteratively refines it using a noise dataset derived from the\ndenoiser's predictions. The noise dataset is created by subtracting predicted\npseudo-clean images from real noisy images at each iteration. The core\nintuition is that improving the denoiser results in a more accurate noise\ndataset and, consequently, a better noise sampler. We validate our method\nthrough extensive experiments on real-world noisy image datasets, demonstrating\ncompetitive performance compared to existing unsupervised denoising methods.\n","authors":["Hamadi Chihaoui","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2503.21377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21367v1","updated":"2025-03-27T10:58:45Z","published":"2025-03-27T10:58:45Z","title":"Multimodal surface defect detection from wooden logs for sawing\n  optimization","summary":"  We propose a novel, good-quality, and less demanding method for detecting\nknots on the surface of wooden logs using multimodal data fusion. Knots are a\nprimary factor affecting the quality of sawn timber, making their detection\nfundamental to any timber grading or cutting optimization system. While X-ray\ncomputed tomography provides accurate knot locations and internal structures,\nit is often too slow or expensive for practical use. An attractive alternative\nis to use fast and cost-effective log surface measurements, such as laser\nscanners or RGB cameras, to detect surface knots and estimate the internal\nstructure of wood. However, due to the small size of knots and noise caused by\nfactors, such as bark and other natural variations, detection accuracy often\nremains low when only one measurement modality is used. In this paper, we\ndemonstrate that by using a data fusion pipeline consisting of separate streams\nfor RGB and point cloud data, combined by a late fusion module, higher knot\ndetection accuracy can be achieved compared to using either modality alone. We\nfurther propose a simple yet efficient sawing angle optimization method that\nutilizes surface knot detections and cross-correlation to minimize the amount\nof unwanted arris knots, demonstrating its benefits over randomized sawing\nangles.\n","authors":["Bořek Reich","Matej Kunda","Fedor Zolotarev","Tuomas Eerola","Pavel Zemčík","Tomi Kauppi"],"pdf_url":"https://arxiv.org/pdf/2503.21367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21364v1","updated":"2025-03-27T10:55:36Z","published":"2025-03-27T10:55:36Z","title":"LandMarkSystem Technical Report","summary":"  3D reconstruction is vital for applications in autonomous driving, virtual\nreality, augmented reality, and the metaverse. Recent advancements such as\nNeural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed\nthe field, yet traditional deep learning frameworks struggle to meet the\nincreasing demands for scene quality and scale. This paper introduces\nLandMarkSystem, a novel computing framework designed to enhance multi-scale\nscene reconstruction and rendering. By leveraging a componentized model\nadaptation layer, LandMarkSystem supports various NeRF and 3DGS structures\nwhile optimizing computational efficiency through distributed parallel\ncomputing and model parameter offloading. Our system addresses the limitations\nof existing frameworks, providing dedicated operators for complex 3D sparse\ncomputations, thus facilitating efficient training and rapid inference over\nextensive scenes. Key contributions include a modular architecture, a dynamic\nloading strategy for limited resources, and proven capabilities across multiple\nrepresentative algorithms.This comprehensive solution aims to advance the\nefficiency and effectiveness of 3D reconstruction tasks.To facilitate further\nresearch and collaboration, the source code and documentation for the\nLandMarkSystem project are publicly available in an open-source repository,\naccessing the repository at: https://github.com/InternLandMark/LandMarkSystem.\n","authors":["Zhenxiang Ma","Zhenyu Yang","Miao Tao","Yuanzhen Zhou","Zeyu He","Yuchang Zhang","Rong Fu","Hengjie Li"],"pdf_url":"https://arxiv.org/pdf/2503.21364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00493v2","updated":"2025-03-27T10:30:42Z","published":"2024-11-30T14:28:53Z","title":"Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene\n  Understanding","summary":"  The rapid advancement of Multimodal Large Language Models (MLLMs) has\nsignificantly impacted various multimodal tasks. However, these models face\nchallenges in tasks that require spatial understanding within 3D environments.\nEfforts to enhance MLLMs, such as incorporating point cloud features, have been\nmade, yet a considerable gap remains between the models' learned\nrepresentations and the inherent complexity of 3D scenes. This discrepancy\nlargely stems from the training of MLLMs on predominantly 2D data, which\nrestricts their effectiveness in comprehending 3D spaces. To address this\nissue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM,\nfor 3D scene understanding. By treating 3D scenes as dynamic videos and\nincorporating 3D position encoding into these representations, our Video-3D LLM\naligns video representations with real-world spatial contexts more accurately.\nIn addition, we have implemented a maximum coverage sampling technique to\noptimize the trade-off between computational cost and performance. Extensive\nexperiments demonstrate that our model achieves state-of-the-art performance on\nseveral 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer,\nScan2Cap, ScanQA, and SQA3D.\n","authors":["Duo Zheng","Shijia Huang","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.00493v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2411.16199v5","updated":"2025-03-27T10:17:44Z","published":"2024-11-25T08:55:41Z","title":"VIRES: Video Instance Repainting via Sketch and Text Guided Generation","summary":"  We introduce VIRES, a video instance repainting method with sketch and text\nguidance, enabling video instance repainting, replacement, generation, and\nremoval. Existing approaches struggle with temporal consistency and accurate\nalignment with the provided sketch sequence. VIRES leverages the generative\npriors of text-to-video models to maintain temporal consistency and produce\nvisually pleasing results. We propose the Sequential ControlNet with the\nstandardized self-scaling, which effectively extracts structure layouts and\nadaptively captures high-contrast sketch details. We further augment the\ndiffusion transformer backbone with the sketch attention to interpret and\ninject fine-grained sketch semantics. A sketch-aware encoder ensures that\nrepainted results are aligned with the provided sketch sequence. Additionally,\nwe contribute the VireSet, a dataset with detailed annotations tailored for\ntraining and evaluating video instance editing methods. Experimental results\ndemonstrate the effectiveness of VIRES, which outperforms state-of-the-art\nmethods in visual quality, temporal consistency, condition alignment, and human\nratings. Project page: https://hjzheng.net/projects/VIRES/\n","authors":["Shuchen Weng","Haojie Zheng","Peixuan Zhan","Yuchen Hong","Han Jiang","Si Li","Boxin Shi"],"pdf_url":"https://arxiv.org/pdf/2411.16199v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21338v1","updated":"2025-03-27T10:14:46Z","published":"2025-03-27T10:14:46Z","title":"UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based\n  on Uncertainty-Guided NeRF Augmentation","summary":"  Visual place recognition (VPR) is crucial for robots to identify previously\nvisited locations, playing an important role in autonomous navigation in both\nindoor and outdoor environments. However, most existing VPR datasets are\nlimited to single-viewpoint scenarios, leading to reduced recognition accuracy,\nparticularly in multi-directional driving or feature-sparse scenes. Moreover,\nobtaining additional data to mitigate these limitations is often expensive.\nThis paper introduces a novel training paradigm to improve the performance of\nexisting VPR networks by enhancing multi-view diversity within current datasets\nthrough uncertainty estimation and NeRF-based data augmentation. Specifically,\nwe initially train NeRF using the existing VPR dataset. Then, our devised\nself-supervised uncertainty estimation network identifies places with high\nuncertainty. The poses of these uncertain places are input into NeRF to\ngenerate new synthetic observations for further training of VPR networks.\nAdditionally, we propose an improved storage method for efficient organization\nof augmented and original training data. We conducted extensive experiments on\nthree datasets and tested three different VPR backbone networks. The results\ndemonstrate that our proposed training paradigm significantly improves VPR\nperformance by fully utilizing existing data, outperforming other training\napproaches. We further validated the effectiveness of our approach on\nself-recorded indoor and outdoor datasets, consistently demonstrating superior\nresults. Our dataset and code have been released at\n\\href{https://github.com/nubot-nudt/UGNA-VPR}{https://github.com/nubot-nudt/UGNA-VPR}.\n","authors":["Yehui Shen","Lei Zhang","Qingqiu Li","Xiongwei Zhao","Yue Wang","Huimin Lu","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2503.21338v1.pdf","comment":"Accepted to IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2503.19654v2","updated":"2025-03-27T10:11:22Z","published":"2025-03-25T13:43:47Z","title":"RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of\n  Vision Language Models","summary":"  We introduce RGB-Th-Bench, the first benchmark designed to evaluate the\nability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.\nWhile VLMs have demonstrated remarkable progress in visual reasoning and\nmultimodal understanding, their evaluation has been predominantly limited to\nRGB-based benchmarks, leaving a critical gap in assessing their capabilities in\ninfrared vision tasks. Existing visible-infrared datasets are either\ntask-specific or lack high-quality annotations necessary for rigorous model\nevaluation. To address these limitations, RGB-Th-Bench provides a comprehensive\nevaluation framework covering 14 distinct skill dimensions, with a total of\n1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy\nmetrics: a standard question-level accuracy and a stricter skill-level\naccuracy, which evaluates model robustness across multiple questions within\neach skill dimension. This design ensures a thorough assessment of model\nperformance, including resilience to adversarial and hallucinated responses. We\nconduct extensive evaluations on 19 state-of-the-art VLMs, revealing\nsignificant performance gaps in RGB-Thermal understanding. Our results show\nthat even the strongest models struggle with thermal image comprehension, with\nperformance heavily constrained by their RGB-based capabilities. Additionally,\nthe lack of large-scale application-specific and expert-annotated\nthermal-caption-pair datasets in pre-training is an important reason of the\nobserved performance gap. RGB-Th-Bench highlights the urgent need for further\nadvancements in multimodal learning to bridge the gap between visible and\nthermal image understanding. The dataset is available through this link, and\nthe evaluation code will also be made publicly available.\n","authors":["Mehdi Moshtaghi","Siavash H. Khajavi","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2503.19654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21323v1","updated":"2025-03-27T10:02:30Z","published":"2025-03-27T10:02:30Z","title":"DuckSegmentation: A segmentation model based on the AnYue Hemp Duck\n  Dataset","summary":"  The modernization of smart farming is a way to improve agricultural\nproduction efficiency, and improve the agricultural production environment.\nAlthough many large models have achieved high accuracy in the task of object\nrecognition and segmentation, they cannot really be put into use in the farming\nindustry due to their own poor interpretability and limitations in\ncomputational volume. In this paper, we built AnYue Shelduck Dateset, which\ncontains a total of 1951 Shelduck datasets, and performed target detection and\nsegmentation annotation with the help of professional annotators. Based on\nAnYue ShelduckDateset, this paper describes DuckProcessing, an efficient and\npowerful module for duck identification based on real shelduckfarms. First of\nall, using the YOLOv8 module designed to divide the mahjong between them,\nPrecision reached 98.10%, Recall reached 96.53% and F1 score reached 0.95 on\nthe test set. Again using the DuckSegmentation segmentation model,\nDuckSegmentation reached 96.43% mIoU. Finally, the excellent DuckSegmentation\nwas used as the teacher model, and through knowledge distillation, Deeplabv3\nr50 was used as the student model, and the final student model achieved 94.49%\nmIoU on the test set. The method provides a new way of thinking in practical\nsisal duck smart farming.\n","authors":["Ling Feng","Tianyu Xie","Wei Ma","Ruijie Fu","Yingxiao Zhang","Jun Li","Bei Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.21323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03215v2","updated":"2025-03-27T09:59:41Z","published":"2024-12-04T11:08:32Z","title":"Beyond [cls]: Exploring the true potential of Masked Image Modeling\n  representations","summary":"  Masked Image Modeling (MIM) has emerged as a promising approach for\nSelf-Supervised Learning (SSL) of visual representations. However, the\nout-of-the-box performance of MIMs is typically inferior to competing\napproaches. Most users cannot afford fine-tuning due to the need for large\namounts of data, high GPU consumption, and specialized user knowledge.\nTherefore, the practical use of MIM representations is limited. In this paper\nwe ask what is the reason for the poor out-of-the-box performance of MIMs. Is\nit due to weaker features produced by MIM models, or is it due to suboptimal\nusage? Through detailed analysis, we show that attention in MIMs is spread\nalmost uniformly over many patches, leading to ineffective aggregation by the\n[cls] token. Based on this insight, we propose Selective Aggregation to better\ncapture the rich semantic information retained in patch tokens, which\nsignificantly improves the out-of-the-box performance of MIM.\n","authors":["Marcin Przewięźlikowski","Randall Balestriero","Wojciech Jasiński","Marek Śmieja","Bartosz Zieliński"],"pdf_url":"https://arxiv.org/pdf/2412.03215v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07067v2","updated":"2025-03-27T09:57:17Z","published":"2024-09-11T07:35:02Z","title":"Structure Modeling Activation Free Fourier Network for Spacecraft Image\n  Denoising","summary":"  Spacecraft image denoising is a crucial fundamental technology closely\nrelated to aerospace research. However, the existing deep learning-based image\ndenoising methods are primarily designed for natural image and fail to\nadequately consider the characteristics of spacecraft image(e.g. low-light\nconditions, repetitive periodic structures), resulting in suboptimal\nperformance in the spacecraft image denoising task. To address the\naforementioned problems, we propose a Structure modeling Activation Free\nFourier Network (SAFFN), which is an efficient spacecraft image denoising\nmethod including Structure Modeling Block (SMB) and Activation Free Fourier\nBlock (AFFB). We present SMB to effectively extract edge information and model\nthe structure for better identification of spacecraft components from dark\nregions in spacecraft noise image. We present AFFB and utilize an improved Fast\nFourier block to extract repetitive periodic features and long-range\ninformation in noisy spacecraft image. Extensive experimental results\ndemonstrate that our SAFFN performs competitively compared to the\nstate-of-the-art methods on spacecraft noise image datasets. The codes are\navailable at: https://github.com/shenduke/SAFFN.\n","authors":["Jingfan Yang","Hu Gao","Ying Zhang","Bowen Ma","Depeng Dang"],"pdf_url":"https://arxiv.org/pdf/2409.07067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07534v3","updated":"2025-03-27T09:50:26Z","published":"2024-12-10T14:15:32Z","title":"ReCap: Better Gaussian Relighting with Cross-Environment Captures","summary":"  Accurate 3D objects relighting in diverse unseen environments is crucial for\nrealistic virtual object placement. Due to the albedo-lighting ambiguity,\nexisting methods often fall short in producing faithful relights. Without\nproper constraints, observed training views can be explained by numerous\ncombinations of lighting and material attributes, lacking physical\ncorrespondence with the actual environment maps used for relighting. In this\nwork, we present ReCap, treating cross-environment captures as multi-task\ntarget to provide the missing supervision that cuts through the entanglement.\nSpecifically, ReCap jointly optimizes multiple lighting representations that\nshare a common set of material attributes. This naturally harmonizes a coherent\nset of lighting representations around the mutual material attributes,\nexploiting commonalities and differences across varied object appearances. Such\ncoherence enables physically sound lighting reconstruction and robust material\nestimation - both essential for accurate relighting. Together with a\nstreamlined shading function and effective post-processing, ReCap outperforms\nall leading competitors on an expanded relighting benchmark.\n","authors":["Jingzhi Li","Zongwei Wu","Eduard Zamfir","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2412.07534v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21313v1","updated":"2025-03-27T09:45:09Z","published":"2025-03-27T09:45:09Z","title":"HORT: Monocular Hand-held Objects Reconstruction with Transformers","summary":"  Reconstructing hand-held objects in 3D from monocular images remains a\nsignificant challenge in computer vision. Most existing approaches rely on\nimplicit 3D representations, which produce overly smooth reconstructions and\nare time-consuming to generate explicit 3D shapes. While more recent methods\ndirectly reconstruct point clouds with diffusion models, the multi-step\ndenoising makes high-resolution reconstruction inefficient. To address these\nlimitations, we propose a transformer-based model to efficiently reconstruct\ndense 3D point clouds of hand-held objects. Our method follows a coarse-to-fine\nstrategy, first generating a sparse point cloud from the image and\nprogressively refining it into a dense representation using pixel-aligned image\nfeatures. To enhance reconstruction accuracy, we integrate image features with\n3D hand geometry to jointly predict the object point cloud and its pose\nrelative to the hand. Our model is trained end-to-end for optimal performance.\nExperimental results on both synthetic and real datasets demonstrate that our\nmethod achieves state-of-the-art accuracy with much faster inference speed,\nwhile generalizing well to in-the-wild images.\n","authors":["Zerui Chen","Rolandos Alexandros Potamias","Shizhe Chen","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2503.21313v1.pdf","comment":"Project Page: https://zerchen.github.io/projects/hort.html"},{"id":"http://arxiv.org/abs/2503.19923v2","updated":"2025-03-27T09:41:43Z","published":"2025-03-12T08:40:39Z","title":"Mapping fMRI Signal and Image Stimuli in an Artificial Neural Network\n  Latent Space: Bringing Artificial and Natural Minds Together","summary":"  The goal of this study is to investigate whether latent space representations\nof visual stimuli and fMRI data share common information. Decoding and\nreconstructing stimuli from fMRI data remains a challenge in AI and\nneuroscience, with significant implications for understanding neural\nrepresentations and improving the interpretability of Artificial Neural\nNetworks (ANNs). In this preliminary study, we investigate the feasibility of\nsuch reconstruction by examining the similarity between the latent spaces of\none autoencoder (AE) and one vision transformer (ViT) trained on fMRI and image\ndata, respectively. Using representational similarity analysis (RSA), we found\nthat the latent spaces of the two domains appear different. However, these\ninitial findings are inconclusive, and further research is needed to explore\nthis relationship more thoroughly.\n","authors":["Cesare Maria Dalbagno","Manuel de Castro Ribeiro Jardim","Mihnea Angheluţă"],"pdf_url":"https://arxiv.org/pdf/2503.19923v2.pdf","comment":"4 pages, 3 figures"},{"id":"http://arxiv.org/abs/2405.15668v4","updated":"2025-03-27T09:41:01Z","published":"2024-05-24T16:05:15Z","title":"What Do You See? Enhancing Zero-Shot Image Classification with\n  Multimodal Large Language Models","summary":"  Large language models (LLMs) have been effectively used for many computer\nvision tasks, including image classification. In this paper, we present a\nsimple yet effective approach for zero-shot image classification using\nmultimodal LLMs. Using multimodal LLMs, we generate comprehensive textual\nrepresentations from input images. These textual representations are then\nutilized to generate fixed-dimensional features in a cross-modal embedding\nspace. Subsequently, these features are fused together to perform zero-shot\nclassification using a linear classifier. Our method does not require prompt\nengineering for each dataset; instead, we use a single, straightforward set of\nprompts across all datasets. We evaluated our method on several datasets and\nour results demonstrate its remarkable effectiveness, surpassing benchmark\naccuracy on multiple datasets. On average, for ten benchmarks, our method\nachieved an accuracy gain of 6.2 percentage points, with an increase of 6.8\npercentage points on the ImageNet dataset, compared to prior methods\nre-evaluated with the same setup. Our findings highlight the potential of\nmultimodal LLMs to enhance computer vision tasks such as zero-shot image\nclassification, offering a significant improvement over traditional methods.\n","authors":["Abdelrahman Abdelhamed","Mahmoud Afifi","Alec Go"],"pdf_url":"https://arxiv.org/pdf/2405.15668v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05037v2","updated":"2025-03-27T09:39:11Z","published":"2025-01-09T07:51:14Z","title":"LongViTU: Instruction Tuning for Long-Form Video Understanding","summary":"  This paper introduces LongViTU, a large-scale (~121k QA pairs, ~900h videos),\nautomatically generated dataset for long-form video understanding. We propose a\nsystematic approach that organizes videos into a hierarchical tree structure\nfor QA generation and incorporates self-revision mechanisms to ensure\nhigh-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context\n(average certificate length of 4.6 minutes); 2) rich knowledge and condensed\nreasoning (commonsense, causality, planning, etc.)). We also offer explicit\ntimestamp annotations of relevant events for each QA pair. We have conducted\nextensive human studies on LongViTU, and the results prove the quality of our\ndataset. To better evaluate the challenges posed by LongViTU's emphasis on\nlong-term context and condensed reasoning, we manually curate a subset of\nLongViTU into a benchmark. Evaluations using a state-of-the-art open-source\nmodel (LongVU), a proprietary model (Gemini-1.5-Pro), and human annotators\nyield GPT-4 scores of 49.9, 52.3, and 81.0, respectively, underscoring the\nsubstantial difficulty presented by LongViTU questions. Performing supervised\nfine-tuning (SFT) of LongVU and LLaVA-Video on LongViTU data results in average\nperformance gains of 2.5% and 3.7%, respectively, across a suite of long video\nunderstanding benchmarks (EgoSchema, VideoMME-Long, MLVU, LVBench).\n","authors":["Rujie Wu","Xiaojian Ma","Hai Ci","Yue Fan","Yuxuan Wang","Haozhe Zhao","Qing Li","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2501.05037v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21309v1","updated":"2025-03-27T09:34:21Z","published":"2025-03-27T09:34:21Z","title":"FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for\n  Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) facilitates image retrieval through a\nmultimodal query consisting of a reference image and modification text. The\nreference image defines the retrieval context, while the modification text\nspecifies desired alterations. However, existing CIR datasets predominantly\nemploy coarse-grained modification text (CoarseMT), which inadequately captures\nfine-grained retrieval intents. This limitation introduces two key challenges:\n(1) ignoring detailed differences leads to imprecise positive samples, and (2)\ngreater ambiguity arises when retrieving visually similar images. These issues\ndegrade retrieval accuracy, necessitating manual result filtering or repeated\nqueries. To address these limitations, we develop a robust fine-grained CIR\ndata annotation pipeline that minimizes imprecise positive samples and enhances\nCIR systems' ability to discern modification intents accurately. Using this\npipeline, we refine the FashionIQ and CIRR datasets to create two fine-grained\nCIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR,\nthe first CIR framework explicitly designed to parse the modification text.\nFineCIR effectively captures fine-grained modification semantics and aligns\nthem with ambiguous visual entities, enhancing retrieval precision. Extensive\nexperiments demonstrate that FineCIR consistently outperforms state-of-the-art\nCIR baselines on both fine-grained and traditional CIR benchmark datasets. Our\nFineCIR code and fine-grained CIR datasets are available at\nhttps://github.com/SDU-L/FineCIR.git.\n","authors":["Zixu Li","Zhiheng Fu","Yupeng Hu","Zhiwei Chen","Haokun Wen","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2503.21309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21307v1","updated":"2025-03-27T09:31:35Z","published":"2025-03-27T09:31:35Z","title":"InternVL-X: Advancing and Accelerating InternVL Series with Efficient\n  Visual Token Compression","summary":"  Most multimodal large language models (MLLMs) treat visual tokens as \"a\nsequence of text\", integrating them with text tokens into a large language\nmodel (LLM). However, a great quantity of visual tokens significantly increases\nthe demand for computational resources and time. In this paper, we propose\nInternVL-X, which outperforms the InternVL model in both performance and\nefficiency by incorporating three visual token compression methods. First, we\npropose a novel vision-language projector, PVTC. This component integrates\nadjacent visual embeddings to form a local query and utilizes the transformed\nCLS token as a global query, then performs point-to-region cross-attention\nthrough these local and global queries to more effectively convert visual\nfeatures. Second, we present a layer-wise visual token compression module,\nLVTC, which compresses tokens in the LLM shallow layers and then expands them\nthrough upsampling and residual connections in the deeper layers. This\nsignificantly enhances the model computational efficiency. Futhermore, we\npropose an efficient high resolution slicing method, RVTC, which dynamically\nadjusts the number of visual tokens based on image area or length filtering.\nRVTC greatly enhances training efficiency with only a slight reduction in\nperformance. By utilizing 20% or fewer visual tokens, InternVL-X achieves\nstate-of-the-art performance on 7 public MLLM benchmarks, and improves the\naverage metric by 2.34% across 12 tasks.\n","authors":["Dongchen Lu","Yuyao Sun","Zilu Zhang","Leping Huang","Jianliang Zeng","Mao Shu","Huo Cao"],"pdf_url":"https://arxiv.org/pdf/2503.21307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01263v2","updated":"2025-03-27T09:28:13Z","published":"2025-03-03T07:41:41Z","title":"Generalizable Prompt Learning of CLIP: A Brief Overview","summary":"  Existing vision-language models (VLMs) such as CLIP have showcased an\nimpressive capability to generalize well across various downstream tasks. These\nmodels leverage the synergy between visual and textual information, enabling\nthem to understand and reason about the content present in images and text in a\nunified manner. This article provides a brief overview of CLIP based on\nfew-shot prompt learning, including experimental data and technical\ncharacteristics of some methods. The purpose of this review is to provide a\nreference for researchers who have just started their research in generalizable\nprompting of CLIP through few-shot training for classification across 15\ndatasets and also to facilitate the integration of this field by researchers in\nother downstream tasks.\n","authors":["Fangming Cui","Yonggang Zhang","Xuan Wang","Xule Wang","Liang Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.01263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13470v2","updated":"2025-03-27T09:23:15Z","published":"2025-01-23T08:40:54Z","title":"Leveraging Textual Anatomical Knowledge for Class-Imbalanced\n  Semi-Supervised Multi-Organ Segmentation","summary":"  Annotating 3D medical images demands substantial time and expertise, driving\nthe adoption of semi-supervised learning (SSL) for segmentation tasks. However,\nthe complex anatomical structures of organs often lead to significant class\nimbalances, posing major challenges for deploying SSL in real-world scenarios.\nDespite the availability of valuable prior information, such as inter-organ\nrelative positions and organ shape priors, existing SSL methods have yet to\nfully leverage these insights. To address this gap, we propose a novel approach\nthat integrates textual anatomical knowledge (TAK) into the segmentation model.\nSpecifically, we use GPT-4o to generate textual descriptions of anatomical\npriors, which are then encoded using a CLIP-based model. These encoded priors\nare injected into the segmentation model as parameters of the segmentation\nhead. Additionally, contrastive learning is employed to enhance the alignment\nbetween textual priors and visual features. Extensive experiments demonstrate\nthe superior performance of our method, significantly surpassing\nstate-of-the-art approaches. The source code will be available at:\nhttps://github.com/Lunn88/TAK-Semi.\n","authors":["Yuliang Gu","Weilun Tsao","Bo Du","Thierry Géraud","Yongchao Xu"],"pdf_url":"https://arxiv.org/pdf/2501.13470v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.16604v2","updated":"2025-03-27T09:20:25Z","published":"2024-12-21T12:33:08Z","title":"OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional\n  Images with Editable Capabilities","summary":"  Feed-forward 3D Gaussian splatting (3DGS) models have gained significant\npopularity due to their ability to generate scenes immediately without needing\nper-scene optimization. Although omnidirectional images are becoming more\npopular since they reduce the computation required for image stitching to\ncomposite a holistic scene, existing feed-forward models are only designed for\nperspective images. The unique optical properties of omnidirectional images\nmake it difficult for feature encoders to correctly understand the context of\nthe image and make the Gaussian non-uniform in space, which hinders the image\nquality synthesized from novel views. We propose OmniSplat, a training-free\nfast feed-forward 3DGS generation framework for omnidirectional images. We\nadopt a Yin-Yang grid and decompose images based on it to reduce the domain gap\nbetween omnidirectional and perspective images. The Yin-Yang grid can use the\nexisting CNN structure as it is, but its quasi-uniform characteristic allows\nthe decomposed image to be similar to a perspective image, so it can exploit\nthe strong prior knowledge of the learned feed-forward network. OmniSplat\ndemonstrates higher reconstruction accuracy than existing feed-forward networks\ntrained on perspective images. Our project page is available on:\nhttps://robot0321.github.io/omnisplat/index.html.\n","authors":["Suyoung Lee","Jaeyoung Chung","Kihoon Kim","Jaeyoo Huh","Gunhee Lee","Minsoo Lee","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2412.16604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15128v2","updated":"2025-03-27T09:10:42Z","published":"2025-01-25T08:30:15Z","title":"MAP-based Problem-Agnostic diffusion model for Inverse Problems","summary":"  Diffusion models have indeed shown great promise in solving inverse problems\nin image processing. In this paper, we propose a novel, problem-agnostic\ndiffusion model called the maximum a posteriori (MAP)-based guided term\nestimation method for inverse problems. To leverage unconditionally pretrained\ndiffusion models to address conditional generation tasks, we divide the\nconditional score function into two terms according to Bayes' rule: an\nunconditional score function (approximated by a pretrained score network) and a\nguided term, which is estimated using a novel MAP-based method that\nincorporates a Gaussian-type prior of natural images. This innovation allows us\nto better capture the intrinsic properties of the data, leading to improved\nperformance. Numerical results demonstrate that our method preserves contents\nmore effectively compared to state-of-the-art methods--for example, maintaining\nthe structure of glasses in super-resolution tasks and producing more coherent\nresults in the neighborhood of masked regions during inpainting.\n","authors":["Pingping Tao","Haixia Liu","Jing Su","Xiaochen Yang","Hongchen Tan"],"pdf_url":"https://arxiv.org/pdf/2501.15128v2.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.21284v1","updated":"2025-03-27T09:08:39Z","published":"2025-03-27T09:08:39Z","title":"Multi-Scale Invertible Neural Network for Wide-Range Variable-Rate\n  Learned Image Compression","summary":"  Autoencoder-based structures have dominated recent learned image compression\nmethods. However, the inherent information loss associated with autoencoders\nlimits their rate-distortion performance at high bit rates and restricts their\nflexibility of rate adaptation. In this paper, we present a variable-rate image\ncompression model based on invertible transform to overcome these limitations.\nSpecifically, we design a lightweight multi-scale invertible neural network,\nwhich bijectively maps the input image into multi-scale latent representations.\nTo improve the compression efficiency, a multi-scale spatial-channel context\nmodel with extended gain units is devised to estimate the entropy of the latent\nrepresentation from high to low levels. Experimental results demonstrate that\nthe proposed method achieves state-of-the-art performance compared to existing\nvariable-rate methods, and remains competitive with recent multi-model\napproaches. Notably, our method is the first learned image compression solution\nthat outperforms VVC across a very wide range of bit rates using a single\nmodel, especially at high bit rates.The source code is available at\n\\href{https://github.com/hytu99/MSINN-VRLIC}{https://github.com/hytu99/MSINN-VRLIC}.\n","authors":["Hanyue Tu","Siqi Wu","Li Li","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2503.21284v1.pdf","comment":"Accepted to IEEE Transactions on Multimedia 2025"},{"id":"http://arxiv.org/abs/2403.18886v3","updated":"2025-03-27T08:58:57Z","published":"2024-03-27T17:59:21Z","title":"Self-Expansion of Pre-trained Models with Mixture of Adapters for\n  Continual Learning","summary":"  Continual learning (CL) aims to continually accumulate knowledge from a\nnon-stationary data stream without catastrophic forgetting of learned\nknowledge, requiring a balance between stability and adaptability. Relying on\nthe generalizable representation in pre-trained models (PTMs), PTM-based CL\nmethods perform effective continual adaptation on downstream tasks by adding\nlearnable adapters or prompts upon the frozen PTMs. However, many existing\nPTM-based CL methods use restricted adaptation on a fixed set of these modules\nto avoid forgetting, suffering from limited CL ability. Periodically adding\ntask-specific modules results in linear model growth rate and impaired\nknowledge reuse. We propose Self-Expansion of pre-trained models with\nModularized Adaptation (SEMA), a novel approach to enhance the control of\nstability-plasticity balance in PTM-based CL. SEMA automatically decides to\nreuse or add adapter modules on demand in CL, depending on whether significant\ndistribution shift that cannot be handled is detected at different\nrepresentation levels. We design modular adapter consisting of a functional\nadapter and a representation descriptor. The representation descriptors are\ntrained as a distribution shift indicator and used to trigger self-expansion\nsignals. For better composing the adapters, an expandable weighting router is\nlearned jointly for mixture of adapter outputs. SEMA enables better knowledge\nreuse and sub-linear expansion rate. Extensive experiments demonstrate the\neffectiveness of the proposed self-expansion method, achieving state-of-the-art\nperformance compared to PTM-based CL methods without memory rehearsal. Code is\navailable at https://github.com/huiyiwang01/SEMA-CL.\n","authors":["Huiyi Wang","Haodong Lu","Lina Yao","Dong Gong"],"pdf_url":"https://arxiv.org/pdf/2403.18886v3.pdf","comment":"Code available at https: https://github.com/huiyiwang01/SEMA-CL"},{"id":"http://arxiv.org/abs/2503.07101v2","updated":"2025-03-27T08:58:54Z","published":"2025-03-10T09:23:14Z","title":"SimROD: A Simple Baseline for Raw Object Detection with Global and Local\n  Enhancements","summary":"  Most visual models are designed for sRGB images, yet RAW data offers\nsignificant advantages for object detection by preserving sensor information\nbefore ISP processing. This enables improved detection accuracy and more\nefficient hardware designs by bypassing the ISP. However, RAW object detection\nis challenging due to limited training data, unbalanced pixel distributions,\nand sensor noise. To address this, we propose SimROD, a lightweight and\neffective approach for RAW object detection. We introduce a Global Gamma\nEnhancement (GGE) module, which applies a learnable global gamma transformation\nwith only four parameters, improving feature representation while keeping the\nmodel efficient. Additionally, we leverage the green channel's richer signal to\nenhance local details, aligning with the human eye's sensitivity and Bayer\nfilter design. Extensive experiments on multiple RAW object detection datasets\nand detectors demonstrate that SimROD outperforms state-of-the-art methods like\nRAW-Adapter and DIAP while maintaining efficiency. Our work highlights the\npotential of RAW data for real-world object detection. Code is available at\nhttps://ocean146.github.io/SimROD2025/.\n","authors":["Haiyang Xie","Xi Shen","Shihua Huang","Qirui Wang","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2503.07101v2.pdf","comment":"Code is available at https://ocean146.github.io/SimROD2025/"},{"id":"http://arxiv.org/abs/2411.03055v3","updated":"2025-03-27T08:57:30Z","published":"2024-11-05T12:42:42Z","title":"ATM: Improving Model Merging by Alternating Tuning and Merging","summary":"  Model merging has recently emerged as a cost-efficient paradigm for\nmulti-task learning. Among current approaches, task arithmetic stands out for\nits simplicity and effectiveness. In this paper, we motivate the effectiveness\nof task vectors by linking them to multi-task gradients. We show that in a\nsingle-epoch scenario, if the optimization is performed via gradient descent,\ntask vectors are after one step mathematically equivalent to the gradients\nobtained via gradient descent in a multi-task setting, and still approximate\nthese gradients in subsequent epochs. Furthermore, we show that the\neffectiveness of task vectors is largely driven by the first epoch's gradient.\nGiven this parallel between task vectors and gradients, we propose viewing\nmodel merging as a single step in an iterative process that alternates between\ntuning and merging (ATM). We then propose two ways to utilize ATM. The first is\nto replace multi-task learning with ATM in scenarios where data sharing is\nprohibited, such as federated learning. The second is to improve the outcome of\nany model merging algorithm by applying a few post-hoc iterations of ATM on a\nsmall validation dataset, which is commonly available for hyperparameter\ntuning. Finally, we provide both empirical and theoretical support for the\neffectiveness of ATM, demonstrating that it minimizes an upper bound on the\nloss obtained by jointly finetuning all tasks.\n","authors":["Luca Zhou","Daniele Solombrino","Donato Crisostomi","Maria Sofia Bucarelli","Fabrizio Silvestri","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2411.03055v3.pdf","comment":"Main paper: 9 Pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2503.21277v1","updated":"2025-03-27T08:56:33Z","published":"2025-03-27T08:56:33Z","title":"Zero-Shot Visual Concept Blending Without Text Guidance","summary":"  We propose a novel, zero-shot image generation technique called \"Visual\nConcept Blending\" that provides fine-grained control over which features from\nmultiple reference images are transferred to a source image. If only a single\nreference image is available, it is difficult to isolate which specific\nelements should be transferred. However, using multiple reference images, the\nproposed approach distinguishes between common and unique features by\nselectively incorporating them into a generated output. By operating within a\npartially disentangled Contrastive Language-Image Pre-training (CLIP) embedding\nspace (from IP-Adapter), our method enables the flexible transfer of texture,\nshape, motion, style, and more abstract conceptual transformations without\nrequiring additional training or text prompts. We demonstrate its effectiveness\nacross a diverse range of tasks, including style transfer, form metamorphosis,\nand conceptual transformations, showing how subtle or abstract attributes\n(e.g., brushstroke style, aerodynamic lines, and dynamism) can be seamlessly\ncombined into a new image. In a user study, participants accurately recognized\nwhich features were intended to be transferred. Its simplicity, flexibility,\nand high-level control make Visual Concept Blending valuable for creative\nfields such as art, design, and content creation, where combining specific\nvisual qualities from multiple inspirations is crucial.\n","authors":["Hiroya Makino","Takahiro Yamaguchi","Hiroyuki Sakai"],"pdf_url":"https://arxiv.org/pdf/2503.21277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21269v1","updated":"2025-03-27T08:50:40Z","published":"2025-03-27T08:50:40Z","title":"Delving Deep into Semantic Relation Distillation","summary":"  Knowledge distillation has become a cornerstone technique in deep learning,\nfacilitating the transfer of knowledge from complex models to lightweight\ncounterparts. Traditional distillation approaches focus on transferring\nknowledge at the instance level, but fail to capture nuanced semantic\nrelationships within the data. In response, this paper introduces a novel\nmethodology, Semantics-based Relation Knowledge Distillation (SeRKD), which\nreimagines knowledge distillation through a semantics-relation lens among each\nsample. By leveraging semantic components, \\ie, superpixels, SeRKD enables a\nmore comprehensive and context-aware transfer of knowledge, which skillfully\nintegrates superpixel-based semantic extraction with relation-based knowledge\ndistillation for a sophisticated model compression and distillation.\nParticularly, the proposed method is naturally relevant in the domain of Vision\nTransformers (ViTs), where visual tokens serve as fundamental units of\nrepresentation. Experimental evaluations on benchmark datasets demonstrate the\nsuperiority of SeRKD over existing methods, underscoring its efficacy in\nenhancing model performance and generalization capabilities.\n","authors":["Zhaoyi Yan","Kangjun Liu","Qixiang Ye"],"pdf_url":"https://arxiv.org/pdf/2503.21269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21268v1","updated":"2025-03-27T08:49:33Z","published":"2025-03-27T08:49:33Z","title":"ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World\n  Coordinate","summary":"  Human Motion Recovery (HMR) research mainly focuses on ground-based motions\nsuch as running. The study on capturing climbing motion, an off-ground motion,\nis sparse. This is partly due to the limited availability of climbing motion\ndatasets, especially large-scale and challenging 3D labeled datasets. To\naddress the insufficiency of climbing motion datasets, we collect AscendMotion,\na large-scale well-annotated, and challenging climbing motion dataset. It\nconsists of 412k RGB, LiDAR frames, and IMU measurements, including the\nchallenging climbing motions of 22 skilled climbing coaches across 12 different\nrock walls. Capturing the climbing motions is challenging as it requires\nprecise recovery of not only the complex pose but also the global position of\nclimbers. Although multiple global HMR methods have been proposed, they cannot\nfaithfully capture climbing motions. To address the limitations of HMR methods\nfor climbing, we propose ClimbingCap, a motion recovery method that\nreconstructs continuous 3D human climbing motion in a global coordinate system.\nOne key insight is to use the RGB and LiDAR modalities to separately\nreconstruct motions in camera coordinates and global coordinates and to\noptimize them jointly. We demonstrate the quality of the AscendMotion dataset\nand present promising results from ClimbingCap. The AscendMotion dataset and\nsource code release publicly at \\href{this\nlink}{http://www.lidarhumanmotion.net/climbingcap/}\n","authors":["Ming Yan","Xincheng Lin","Yuhua Luo","Shuqi Fan","Yudi Dai","Qixin Zhong","Lincai Zhong","Yuexin Ma","Lan Xu","Chenglu Wen","Siqi Shen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2503.21268v1.pdf","comment":"CVPR2025, project in \\href{this\n  link}{http://www.lidarhumanmotion.net/climbingcap/}"},{"id":"http://arxiv.org/abs/2312.07669v3","updated":"2025-03-27T08:47:12Z","published":"2023-12-12T19:03:04Z","title":"GMTalker: Gaussian Mixture-based Audio-Driven Emotional Talking Video\n  Portraits","summary":"  Synthesizing high-fidelity and emotion-controllable talking video portraits,\nwith audio-lip sync, vivid expressions, realistic head poses, and eye blinks,\nhas been an important and challenging task in recent years. Most existing\nmethods suffer in achieving personalized and precise emotion control, smooth\ntransitions between different emotion states, and the generation of diverse\nmotions. To tackle these challenges, we present GMTalker, a Gaussian\nmixture-based emotional talking portraits generation framework. Specifically,\nwe propose a Gaussian mixture-based expression generator that can construct a\ncontinuous and disentangled latent space, achieving more flexible emotion\nmanipulation. Furthermore, we introduce a normalizing flow-based motion\ngenerator pretrained on a large dataset with a wide-range motion to generate\ndiverse head poses, blinks, and eyeball movements. Finally, we propose a\npersonalized emotion-guided head generator with an emotion mapping network that\ncan synthesize high-fidelity and faithful emotional video portraits. Both\nquantitative and qualitative experiments demonstrate our method outperforms\nprevious methods in image quality, photo-realism, emotion accuracy, and motion\ndiversity.\n","authors":["Yibo Xia","Lizhen Wang","Xiang Deng","Xiaoyan Luo","Yunhong Wang","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2312.07669v3.pdf","comment":"Project page: https://bob35buaa.github.io/GMTalker. This work has\n  been submitted to the IEEE journal for possible publication"},{"id":"http://arxiv.org/abs/2503.21262v1","updated":"2025-03-27T08:39:58Z","published":"2025-03-27T08:39:58Z","title":"vGamba: Attentive State Space Bottleneck for efficient Long-range\n  Dependencies in Visual Recognition","summary":"  Capturing long-range dependencies efficiently is essential for visual\nrecognition tasks, yet existing methods face limitations. Convolutional neural\nnetworks (CNNs) struggle with restricted receptive fields, while Vision\nTransformers (ViTs) achieve global context and long-range modeling at a high\ncomputational cost. State-space models (SSMs) offer an alternative, but their\napplication in vision remains underexplored. This work introduces vGamba, a\nhybrid vision backbone that integrates SSMs with attention mechanisms to\nenhance efficiency and expressiveness. At its core, the Gamba bottleneck block\nthat includes, Gamba Cell, an adaptation of Mamba for 2D spatial structures,\nalongside a Multi-Head Self-Attention (MHSA) mechanism and a Gated Fusion\nModule for effective feature representation. The interplay of these components\nensures that vGamba leverages the low computational demands of SSMs while\nmaintaining the accuracy of attention mechanisms for modeling long-range\ndependencies in vision tasks. Additionally, the Fusion module enables seamless\ninteraction between these components. Extensive experiments on classification,\ndetection, and segmentation tasks demonstrate that vGamba achieves a superior\ntrade-off between accuracy and computational efficiency, outperforming several\nexisting models.\n","authors":["Yunusa Haruna","Adamu Lawan"],"pdf_url":"https://arxiv.org/pdf/2503.21262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21259v1","updated":"2025-03-27T08:35:10Z","published":"2025-03-27T08:35:10Z","title":"Reducing CT Metal Artifacts by Learning Latent Space Alignment with\n  Gemstone Spectral Imaging Data","summary":"  Metal artifacts in CT slices have long posed challenges in medical\ndiagnostics. These artifacts degrade image quality, resulting in suboptimal\nvisualization and complicating the accurate interpretation of tissues adjacent\nto metal implants. To address these issues, we introduce the Latent Gemstone\nSpectral Imaging (GSI) Alignment Framework, which effectively reduces metal\nartifacts while avoiding the introduction of noise information. Our work is\nbased on a key finding that even artifact-affected ordinary CT sequences\ncontain sufficient information to discern detailed structures. The challenge\nlies in the inability to clearly represent this information. To address this\nissue, we developed an Alignment Framework that adjusts the representation of\nordinary CT images to match GSI CT sequences. GSI is an advanced imaging\ntechnique using multiple energy levels to mitigate artifacts caused by metal\nimplants. By aligning the representation to GSI data, we can effectively\nsuppress metal artifacts while clearly revealing detailed structure, without\nintroducing extraneous information into CT sequences. To facilitate the\napplication, we propose a new dataset, Artifacts-GSI, captured from real\npatients with metal implants, and establish a new benchmark based on this\ndataset. Experimental results show that our method significantly reduces metal\nartifacts and greatly enhances the readability of CT slices. All our code and\ndata are available at: https://um-lab.github.io/GSI-MAR/\n","authors":["Wencheng Han","Dongqian Guo","Xiao Chen","Pang Lyu","Yi Jin","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2503.21259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19835v3","updated":"2025-03-27T08:34:04Z","published":"2024-11-29T16:45:25Z","title":"Feedback-driven object detection and iterative model improvement","summary":"  Automated object detection has become increasingly valuable across diverse\napplications, yet efficient, high-quality annotation remains a persistent\nchallenge. In this paper, we present the development and evaluation of a\nplatform designed to interactively improve object detection models. The\nplatform allows uploading and annotating images as well as fine-tuning object\ndetection models. Users can then manually review and refine annotations,\nfurther creating improved snapshots that are used for automatic object\ndetection on subsequent image uploads - a process we refer to as semi-automatic\nannotation resulting in a significant gain in annotation efficiency.\n  Whereas iterative refinement of model results to speed up annotation has\nbecome common practice, we are the first to quantitatively evaluate its\nbenefits with respect to time, effort, and interaction savings. Our\nexperimental results show clear evidence for a significant time reduction of up\nto 53% for semi-automatic compared to manual annotation. Importantly, these\nefficiency gains did not compromise annotation quality, while matching or\noccasionally even exceeding the accuracy of manual annotations. These findings\ndemonstrate the potential of our lightweight annotation platform for creating\nhigh-quality object detection datasets and provide best practices to guide\nfuture development of annotation platforms.\n  The platform is open-source, with the frontend and backend repositories\navailable on GitHub. To support the understanding of our labeling process, we\nhave created an explanatory video demonstrating the methodology using\nmicroscopy images of E. coli bacteria as an example.\n","authors":["Sönke Tenckhoff","Mario Koddenbrock","Erik Rodner"],"pdf_url":"https://arxiv.org/pdf/2411.19835v3.pdf","comment":"Code: https://github.com/ml-lab-htw/iterative-annotate Video:\n  https://www.youtube.com/watch?v=CM9uhE8NN5E"},{"id":"http://arxiv.org/abs/2503.21258v1","updated":"2025-03-27T08:31:46Z","published":"2025-03-27T08:31:46Z","title":"Learn by Reasoning: Analogical Weight Generation for Few-Shot\n  Class-Incremental Learning","summary":"  Few-shot class-incremental Learning (FSCIL) enables models to learn new\nclasses from limited data while retaining performance on previously learned\nclasses. Traditional FSCIL methods often require fine-tuning parameters with\nlimited new class data and suffer from a separation between learning new\nclasses and utilizing old knowledge. Inspired by the analogical learning\nmechanisms of the human brain, we propose a novel analogical generative method.\nOur approach includes the Brain-Inspired Analogical Generator (BiAG), which\nderives new class weights from existing classes without parameter fine-tuning\nduring incremental stages. BiAG consists of three components: Weight\nSelf-Attention Module (WSA), Weight & Prototype Analogical Attention Module\n(WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory\nfor semantic conversion, WSA supplements new class weights, and WPAA computes\nanalogies to generate new class weights. Experiments on miniImageNet, CUB-200,\nand CIFAR-100 datasets demonstrate that our method achieves higher final and\naverage accuracy compared to SOTA methods.\n","authors":["Jizhou Han","Chenhao Ding","Yuhang He","Songlin Dong","Qiang Wang","Xinyuan Gao","Yihong Gong"],"pdf_url":"https://arxiv.org/pdf/2503.21258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21254v1","updated":"2025-03-27T08:21:54Z","published":"2025-03-27T08:21:54Z","title":"Vision-to-Music Generation: A Survey","summary":"  Vision-to-music Generation, including video-to-music and image-to-music\ntasks, is a significant branch of multimodal artificial intelligence\ndemonstrating vast application prospects in fields such as film scoring, short\nvideo creation, and dance music synthesis. However, compared to the rapid\ndevelopment of modalities like text and images, research in vision-to-music is\nstill in its preliminary stage due to its complex internal structure and the\ndifficulty of modeling dynamic relationships with video. Existing surveys focus\non general music generation without comprehensive discussion on\nvision-to-music. In this paper, we systematically review the research progress\nin the field of vision-to-music generation. We first analyze the technical\ncharacteristics and core challenges for three input types: general videos,\nhuman movement videos, and images, as well as two output types of symbolic\nmusic and audio music. We then summarize the existing methodologies on\nvision-to-music generation from the architecture perspective. A detailed review\nof common datasets and evaluation metrics is provided. Finally, we discuss\ncurrent challenges and promising directions for future research. We hope our\nsurvey can inspire further innovation in vision-to-music generation and the\nbroader field of multimodal generation in academic research and industrial\napplications. To follow latest works and foster further innovation in this\nfield, we are continuously maintaining a GitHub repository at\nhttps://github.com/wzk1015/Awesome-Vision-to-Music-Generation.\n","authors":["Zhaokai Wang","Chenxi Bao","Le Zhuo","Jingrui Han","Yang Yue","Yihong Tang","Victor Shea-Jay Huang","Yue Liao"],"pdf_url":"https://arxiv.org/pdf/2503.21254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01739v3","updated":"2025-03-27T08:15:46Z","published":"2024-11-04T01:42:41Z","title":"Not Just Object, But State: Compositional Incremental Learning without\n  Forgetting","summary":"  Most incremental learners excessively prioritize coarse classes of objects\nwhile neglecting various kinds of states (e.g. color and material) attached to\nthe objects. As a result, they are limited in the ability to reason\nfine-grained compositionality of state-object pairs. To remedy this limitation,\nwe propose a novel task called Compositional Incremental Learning\n(composition-IL), enabling the model to recognize state-object compositions as\na whole in an incremental learning fashion. Since the lack of suitable\nbenchmarks, we re-organize two existing datasets and make them tailored for\ncomposition-IL. Then, we propose a prompt-based Composition Incremental Learner\n(CompILer), to overcome the ambiguous composition boundary problem which\nchallenges composition-IL largely. Specifically, we exploit multi-pool prompt\nlearning, which is regularized by inter-pool prompt discrepancy and intra-pool\nprompt diversity. Besides, we devise object-injected state prompting by using\nobject prompts to guide the selection of state prompts. Furthermore, we fuse\nthe selected prompts by a generalized-mean strategy, to eliminate irrelevant\ninformation learned in the prompts. Extensive experiments on two datasets\nexhibit state-of-the-art performance achieved by CompILer.\n","authors":["Yanyi Zhang","Binglin Qiu","Qi Jia","Yu Liu","Ran He"],"pdf_url":"https://arxiv.org/pdf/2411.01739v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2503.21250v1","updated":"2025-03-27T08:14:56Z","published":"2025-03-27T08:14:56Z","title":"Orange Quality Grading with Deep Learning","summary":"  Orange grading is a crucial step in the fruit industry, as it helps to sort\noranges according to different criteria such as size, quality, ripeness, and\nhealth condition, ensuring safety for human consumption and better price\nallocation and client satisfaction. Automated grading enables faster\nprocessing, precision, and reduced human labor. In this paper, we implement a\ndeep learning-based solution for orange grading via machine vision. Unlike\ntypical grading systems that analyze fruits from a single view, we capture\nmultiview images of each single orange in order to enable a richer\nrepresentation. Afterwards, we compose the acquired images into one collage.\nThis enables the analysis of the whole orange skin. We train a convolutional\nneural network (CNN) on the composed images to grade the oranges into three\nclasses, namely good, bad, and undefined. We also evaluate the performance with\ntwo different CNNs (ResNet-18 and SqueezeNet). We show experimentally that\nmulti-view grading is superior to single view grading.\n","authors":["Mohamed Lamine Mekhalfi","Paul Chippendale","Francisco Fraile","Marcos Rico"],"pdf_url":"https://arxiv.org/pdf/2503.21250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16180v2","updated":"2025-03-27T08:14:32Z","published":"2024-11-25T08:23:38Z","title":"Event-boosted Deformable 3D Gaussians for Dynamic Scene Reconstruction","summary":"  Deformable 3D Gaussian Splatting (3D-GS) is limited by missing intermediate\nmotion information due to the low temporal resolution of RGB cameras. To\naddress this, we introduce the first approach combining event cameras, which\ncapture high-temporal-resolution, continuous motion data, with deformable 3D-GS\nfor dynamic scene reconstruction. We observe that threshold modeling for events\nplays a crucial role in achieving high-quality reconstruction. Therefore, we\npropose a GS-Threshold Joint Modeling strategy, creating a mutually reinforcing\nprocess that greatly improves both 3D reconstruction and threshold modeling.\nMoreover, we introduce a Dynamic-Static Decomposition strategy that first\nidentifies dynamic areas by exploiting the inability of static Gaussians to\nrepresent motions, then applies a buffer-based soft decomposition to separate\ndynamic and static areas. This strategy accelerates rendering by avoiding\nunnecessary deformation in static areas, and focuses on dynamic areas to\nenhance fidelity. Additionally, we contribute the first event-inclusive 4D\nbenchmark with synthetic and real-world dynamic scenes, on which our method\nachieves state-of-the-art performance.\n","authors":["Wenhao Xu","Wenming Weng","Yueyi Zhang","Ruikang Xu","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2411.16180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00279v2","updated":"2025-03-27T08:11:57Z","published":"2024-08-01T04:39:36Z","title":"MESA: Effective Matching Redundancy Reduction by Semantic Area\n  Segmentation","summary":"  We propose MESA and DMESA as novel feature matching methods, which utilize\nSegment Anything Model (SAM) to effectively mitigate matching redundancy. The\nkey insight of our methods is to establish implicit-semantic area matching\nprior to point matching, based on advanced image understanding of SAM. Then,\ninformative area matches with consistent internal semantic are able to undergo\ndense feature comparison, facilitating precise inside-area point matching.\nSpecifically, MESA adopts a sparse matching framework and first obtains\ncandidate areas from SAM results through a novel Area Graph (AG). Then, area\nmatching among the candidates is formulated as graph energy minimization and\nsolved by graphical models derived from AG. To address the efficiency issue of\nMESA, we further propose DMESA as its dense counterpart, applying a dense\nmatching framework. After candidate areas are identified by AG, DMESA\nestablishes area matches through generating dense matching distributions. The\ndistributions are produced from off-the-shelf patch matching utilizing the\nGaussian Mixture Model and refined via the Expectation Maximization. With less\nrepetitive computation, DMESA showcases a speed improvement of nearly five\ntimes compared to MESA, while maintaining competitive accuracy. Our methods are\nextensively evaluated on five datasets encompassing indoor and outdoor scenes.\nThe results illustrate consistent performance improvements from our methods for\nfive distinct point matching baselines across all datasets. Furthermore, our\nmethods exhibit promise generalization and improved robustness against image\nresolution variations. The code is publicly available at\nhttps://github.com/Easonyesheng/A2PM-MESA.\n","authors":["Yesheng Zhang","Shuhan Shen","Xu Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.00279v2.pdf","comment":"18pages+suppl"},{"id":"http://arxiv.org/abs/2503.21246v1","updated":"2025-03-27T08:07:45Z","published":"2025-03-27T08:07:45Z","title":"DynamiCtrl: Rethinking the Basic Structure and the Role of Text for\n  High-quality Human Image Animation","summary":"  Human image animation has recently gained significant attention due to\nadvancements in generative models. However, existing methods still face two\nmajor challenges: (1) architectural limitations, most models rely on U-Net,\nwhich underperforms compared to the MM-DiT; and (2) the neglect of textual\ninformation, which can enhance controllability. In this work, we introduce\nDynamiCtrl, a novel framework that not only explores different pose-guided\ncontrol structures in MM-DiT, but also reemphasizes the crucial role of text in\nthis task. Specifically, we employ a Shared VAE encoder for both reference\nimages and driving pose videos, eliminating the need for an additional pose\nencoder and simplifying the overall framework. To incorporate pose features\ninto the full attention blocks, we propose Pose-adaptive Layer Norm (PadaLN),\nwhich utilizes adaptive layer normalization to encode sparse pose features. The\nencoded features are directly added to the visual input, preserving the\nspatiotemporal consistency of the backbone while effectively introducing pose\ncontrol into MM-DiT. Furthermore, within the full attention mechanism, we align\ntextual and visual features to enhance controllability. By leveraging text, we\nnot only enable fine-grained control over the generated content, but also, for\nthe first time, achieve simultaneous control over both background and motion.\nExperimental results verify the superiority of DynamiCtrl on benchmark\ndatasets, demonstrating its strong identity preservation, heterogeneous\ncharacter driving, background controllability, and high-quality synthesis. The\nproject page is available at https://gulucaptain.github.io/DynamiCtrl/.\n","authors":["Haoyu Zhao","Zhongang Qi","Cong Wang","Qingping Zheng","Guansong Lu","Fei Chen","Hang Xu","Zuxuan Wu"],"pdf_url":"https://arxiv.org/pdf/2503.21246v1.pdf","comment":"11 pages, 10 figures"}]}}