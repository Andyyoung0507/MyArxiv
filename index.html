<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-27T00:00:00Z">2025-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">36</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Underwater Navigation through Cross-Correlation-Aware Deep
  INS/DVL Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Cohen, Itzik Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate navigation of autonomous underwater vehicles critically depends
on the precision of Doppler velocity log (DVL) velocity measurements. Recent
advancements in deep learning have demonstrated significant potential in
improving DVL outputs by leveraging spatiotemporal dependencies across multiple
sensor modalities. However, integrating these estimates into model-based
filters, such as the extended Kalman filter, introduces statistical
inconsistencies, most notably, cross-correlations between process and
measurement noise. This paper addresses this challenge by proposing a
cross-correlation-aware deep INS/DVL fusion framework. Building upon BeamsNet,
a convolutional neural network designed to estimate AUV velocity using DVL and
inertial data, we integrate its output into a navigation filter that explicitly
accounts for the cross-correlation induced between the noise sources. This
approach improves filter consistency and better reflects the underlying sensor
error structure. Evaluated on two real-world underwater trajectories, the
proposed method outperforms both least squares and cross-correlation-neglecting
approaches in terms of state uncertainty. Notably, improvements exceed 10% in
velocity and misalignment angle confidence metrics. Beyond demonstrating
empirical performance, this framework provides a theoretically principled
mechanism for embedding deep learning outputs within stochastic filters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span> and Analysis of Long-Term Skill Acquisition in <span class="highlight-title">Robo</span>t-Assisted
  Minimally Invasive Surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yarden Sharon, Alex Geftler, Hanna Kossowsky Lev, Ilana Nisky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: We aim to investigate long-term robotic surgical skill acquisition
among surgical residents and the effects of training intervals and fatigue on
performance. Methods: For six months, surgical residents participated in three
training sessions once a month, surrounding a single 26-hour hospital shift. In
each shift, they participated in training sessions scheduled before, during,
and after the shift. In each training session, they performed three dry-lab
training tasks: Ring Tower Transfer, Knot-Tying, and Suturing. We collected a
comprehensive dataset, including videos synchronized with kinematic data,
activity tracking, and scans of the suturing pads. Results: We collected a
dataset of 972 trials performed by 18 residents of different surgical
specializations. Participants demonstrated consistent performance improvement
across all tasks. In addition, we found variations in between-shift learning
and forgetting across metrics and tasks, and hints for possible effects of
fatigue. Conclusion: The findings from our first analysis shed light on the
long-term learning processes of robotic surgical skills with extended intervals
and varying levels of fatigue. Significance: This study lays the groundwork for
future research aimed at optimizing training protocols and enhancing AI
applications in surgery, ultimately contributing to improved patient outcomes.
The dataset will be made available upon acceptance of our journal submission.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooking Task Planning using LLM and Verified by Graph Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryunosuke Takebayashi, Vitor Hideyo Isume, Takuya Kiyokawa, Weiwei Wan, Kensuke Harada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooking tasks remain a challenging problem for robotics due to their
complexity. Videos of people cooking are a valuable source of information for
such task, but introduces a lot of variability in terms of how to translate
this data to a robotic environment. This research aims to streamline this
process, focusing on the task plan generation step, by using a Large Language
Model (LLM)-based Task and Motion Planning (TAMP) framework to autonomously
generate cooking task plans from videos with subtitles, and execute them.
Conventional LLM-based task planning methods are not well-suited for
interpreting the cooking video data due to uncertainty in the videos, and the
risk of hallucination in its output. To address both of these problems, we
explore using LLMs in combination with Functional Object-Oriented Networks
(FOON), to validate the plan and provide feedback in case of failure. This
combination can generate task sequences with manipulation motions that are
logically correct and executable by a robot. We compare the execution of the
generated plans for 5 cooking recipes from our approach against the plans
generated by a few-shot LLM-only approach for a dual-arm robot setup. It could
successfully execute 4 of the plans generated by our approach, whereas only 1
of the plans generated by solely using the LLM could be executed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Contact-Aware Control Method for Real-Time Deformable Tool
  <span class="highlight-title">Manipulation</span>: A Case Study in the Environmental Swabbing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siavash Mahmoudi, Amirreza Davar, Dongyi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable Object Manipulation (DOM) remains a critical challenge in robotics
due to the complexities of developing suitable model-based control strategies.
Deformable Tool Manipulation (DTM) further complicates this task by introducing
additional uncertainties between the robot and its environment. While humans
effortlessly manipulate deformable tools using touch and experience, robotic
systems struggle to maintain stability and precision. To address these
challenges, we present a novel State-Adaptive Koopman LQR (SA-KLQR) control
framework for real-time deformable tool manipulation, demonstrated through a
case study in environmental swab sampling for food safety. This method
leverages Koopman operator-based control to linearize nonlinear dynamics while
adapting to state-dependent variations in tool deformation and contact forces.
A tactile-based feedback system dynamically estimates and regulates the swab
tool's angle, contact pressure, and surface coverage, ensuring compliance with
food safety standards. Additionally, a sensor-embedded contact pad monitors
force distribution to mitigate tool pivoting and deformation, improving
stability during dynamic interactions. Experimental results validate the
SA-KLQR approach, demonstrating accurate contact angle estimation, robust
trajectory tracking, and reliable force regulation. The proposed framework
enhances precision, adaptability, and real-time control in deformable tool
manipulation, bridging the gap between data-driven learning and optimal control
in robotic interaction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for Journal Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAMICS: Splat, Track And Map with Integrated Consistency and Semantics
  for Dense RGB-D SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxu Wang, Xu Cao, Weiyun Yi, Zhaoxin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) is a critical task in robotics,
enabling systems to autonomously navigate and understand complex environments.
Current SLAM approaches predominantly rely on geometric cues for mapping and
localization, but they often fail to ensure semantic consistency, particularly
in dynamic or densely populated scenes. To address this limitation, we
introduce STAMICS, a novel method that integrates semantic information with 3D
Gaussian representations to enhance both localization and mapping accuracy.
STAMICS consists of three key components: a 3D Gaussian-based scene
representation for high-fidelity reconstruction, a graph-based clustering
technique that enforces temporal semantic consistency, and an open-vocabulary
system that allows for the classification of unseen objects. Extensive
experiments show that STAMICS significantly improves camera pose estimation and
map quality, outperforming state-of-the-art methods while reducing
reconstruction errors. Code will be public available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for
  Skill Learning <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Keller, Daniel Tanneberg, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning is a popular method for teaching robots new behaviors.
However, most existing methods focus on teaching short, isolated skills rather
than long, multi-step tasks. To bridge this gap, imitation learning algorithms
must not only learn individual skills but also an abstract understanding of how
to sequence these skills to perform extended tasks effectively. This paper
addresses this challenge by proposing a neuro-symbolic imitation learning
framework. Using task demonstrations, the system first learns a symbolic
representation that abstracts the low-level state-action space. The learned
representation decomposes a task into easier subtasks and allows the system to
leverage symbolic planning to generate abstract plans. Subsequently, the system
utilizes this task decomposition to learn a set of neural skills capable of
refining abstract plans into actionable robot commands. Experimental results in
three simulated robotic environments demonstrate that, compared to baselines,
our neuro-symbolic approach increases data efficiency, improves generalization
capabilities, and facilitates interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Conference on Robotics and Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AcL: Action Learner for Fault-Tolerant Quadruped Locomotion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Xu, Yaoyu Cheng, Pinxi Shen, Lin Zhao,  Electrical, Computer Engineering, National University of Singapore,  Singapore, Mechanical Engineering, National University of Singapore,  Singapore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadrupedal robots can learn versatile locomotion skills but remain
vulnerable when one or more joints lose power. In contrast, dogs and cats can
adopt limping gaits when injured, demonstrating their remarkable ability to
adapt to physical conditions. Inspired by such adaptability, this paper
presents Action Learner (AcL), a novel teacher-student reinforcement learning
framework that enables quadrupeds to autonomously adapt their gait for stable
walking under multiple joint faults. Unlike conventional teacher-student
approaches that enforce strict imitation, AcL leverages teacher policies to
generate style rewards, guiding the student policy without requiring precise
replication. We train multiple teacher policies, each corresponding to a
different fault condition, and subsequently distill them into a single student
policy with an encoder-decoder architecture. While prior works primarily
address single-joint faults, AcL enables quadrupeds to walk with up to four
faulty joints across one or two legs, autonomously switching between different
limping gaits when faults occur. We validate AcL on a real Go2 quadruped robot
under single- and double-joint faults, demonstrating fault-tolerant, stable
walking, smooth gait transitions between normal and lamb gaits, and robustness
against external disturbances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Data-Driven Method for INS/DVL Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Damari, Itzik Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous underwater vehicles (AUVs) are sophisticated robotic platforms
crucial for a wide range of applications. The accuracy of AUV navigation
systems is critical to their success. Inertial sensors and Doppler velocity
logs (DVL) fusion is a promising solution for long-range underwater navigation.
However, the effectiveness of this fusion depends heavily on an accurate
alignment between the inertial sensors and the DVL. While current alignment
methods show promise, there remains significant room for improvement in terms
of accuracy, convergence time, and alignment trajectory efficiency. In this
research we propose an end-to-end deep learning framework for the alignment
process. By leveraging deep-learning capabilities, such as noise reduction and
capture of nonlinearities in the data, we show using simulative data, that our
proposed approach enhances both alignment accuracy and reduces convergence time
beyond current model-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UGNA-VPR: A Novel Training Paradigm for <span class="highlight-title">Visual</span> Place Recognition Based
  on Uncertainty-Guided NeRF Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehui Shen, Lei Zhang, Qingqiu Li, Xiongwei Zhao, Yue Wang, Huimin Lu, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition (VPR) is crucial for robots to identify previously
visited locations, playing an important role in autonomous navigation in both
indoor and outdoor environments. However, most existing VPR datasets are
limited to single-viewpoint scenarios, leading to reduced recognition accuracy,
particularly in multi-directional driving or feature-sparse scenes. Moreover,
obtaining additional data to mitigate these limitations is often expensive.
This paper introduces a novel training paradigm to improve the performance of
existing VPR networks by enhancing multi-view diversity within current datasets
through uncertainty estimation and NeRF-based data augmentation. Specifically,
we initially train NeRF using the existing VPR dataset. Then, our devised
self-supervised uncertainty estimation network identifies places with high
uncertainty. The poses of these uncertain places are input into NeRF to
generate new synthetic observations for further training of VPR networks.
Additionally, we propose an improved storage method for efficient organization
of augmented and original training data. We conducted extensive experiments on
three datasets and tested three different VPR backbone networks. The results
demonstrate that our proposed training paradigm significantly improves VPR
performance by fully utilizing existing data, outperforming other training
approaches. We further validated the effectiveness of our approach on
self-recorded indoor and outdoor datasets, consistently demonstrating superior
results. Our dataset and code have been released at
\href{https://github.com/nubot-nudt/UGNA-VPR}{https://github.com/nubot-nudt/UGNA-VPR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Robotics and Automation Letters (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Lidar</span>-only Odometry based on Multiple Scan-to-Scan Alignments over a
  Moving Window 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Kurda, Simon Steuernagel, Marcus Baum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lidar-only odometry considers the pose estimation of a mobile robot based on
the accumulation of motion increments extracted from consecutive lidar scans.
Many existing approaches to the problem use a scan-to-map registration, which
neglects the accumulation of errors within the maintained map due to drift.
Other methods use a refinement step that jointly optimizes the local map on a
feature basis. We propose a solution that avoids this by using multiple
independent scan-to-scan Iterative Closest Points (ICP) registrations to
previous scans in order to derive constraints for a pose graph. The
optimization of the pose graph then not only yields an accurate estimate for
the latest pose, but also enables the refinement of previous scans in the
optimization window. By avoiding the need to recompute the scan-to-scan
alignments, the computational load is minimized. Extensive evaluation on the
public KITTI and MulRan datasets as well as on a custom automotive lidar
dataset is carried out. Results show that the proposed approach achieves
state-of-the-art estimation accuracy, while alleviating the mentioned issues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An analysis of higher-order kinematics formalisms for an innovative
  surgical parallel <span class="highlight-title">robo</span>t 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Calin Vaida, Iosif Birlescu, Bogdan Gherman, Daniel Condurache, Damien Chablat, Doina Pisla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents a novel modular hybrid parallel robot for pancreatic
surgery and its higher-order kinematics derived based on various formalisms.
The classical vector, homogeneous transformation matrices and dual quaternion
approaches are studied for the kinematic functions using both classical
differentiation and multidual algebra. The algorithms for inverse kinematics
for all three studied formalisms are presented for both differentiation and
multidual algebra approaches. Furthermore, these algorithms are compared based
on numerical stability, execution times and number and type of mathematical
functions and operators contained in each algorithm. A statistical analysis
shows that there is significant improvement in execution time for the
algorithms implemented using multidual algebra, while the numerical stability
is appropriate for all algorithms derived based on differentiation and
multidual algebra. While the implementation of the kinematic algorithms using
multidual algebra shows positive results when benchmarked on a standard PC,
further work is required to evaluate the multidual algorithms on
hardware/software used for the modular parallel robot command and control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Haptic bilateral teleoperation system for free-hand dental procedures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Pagliara, Enrico Ferrentino, Andrea Chiacchio, Giovanni Russo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Free-hand dental procedures are typically repetitive, time-consuming and
require high precision and manual dexterity. Dental robots can play a key role
in improving procedural accuracy and safety, enhancing patient comfort, and
reducing operator workload. However, robotic solutions for free-hand procedures
remain limited or completely lacking, and their acceptance is still low. To
address this gap, we develop a haptic bilateral teleoperation system (HBTS) for
free-hand dental procedures. The system includes a dedicated mechanical
end-effector, compatible with standard clinical tools, and equipped with an
endoscopic camera for improved visibility of the intervention site. By ensuring
motion and force correspondence between the operator's actions and the robot's
movements, monitored through visual feedback, we enhance the operator's sensory
awareness and motor accuracy. Furthermore, recognizing the need to ensure
procedural safety, we limit interaction forces by scaling the motion references
provided to the admittance controller based solely on measured contact forces.
This ensures effective force limitation in all contact states without requiring
prior knowledge of the environment. The proposed HBTS is validated in a dental
scaling procedure using a dental phantom. The results show that the system
improves the naturalness, safety, and accuracy of teleoperation, highlighting
its potential to enhance free-hand dental procedures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Output-Feedback Boundary Control of Thermally and Flow-Induced
  Vibrations in Slender Timoshenko Beams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyi Wang, Ji Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work is motivated by the engineering challenge of suppressing vibrations
in turbine blades of aero engines, which often operate under extreme thermal
conditions and high-Mach aerodynamic environments that give rise to complex
vibration phenomena, commonly referred to as thermally-induced and flow-induced
vibrations. Using Hamilton's variational principle, the system is modeled as a
rotating slender Timoshenko beam under thermal and aerodynamic loads, described
by a mixed hyperbolic-parabolic PDE system where instabilities occur both
within the PDE domain and at the uncontrolled boundary, and the two types of
PDEs are cascaded in the domain. For such a system, we present the
state-feedback control design based on the PDE backstepping method. Recognizing
that the distributed temperature gradients and structural vibrations in the
Timoshenko beam are typically unmeasurable in practice, we design a state
observer for the mixed hyperbolic-parabolic PDE system. Based on this observer,
an output-feedback controller is then built to regulate the overall system
using only available boundary measurements. In the closed-loop system, the
state of the uncontrolled boundary, i.e., the furthest state from the control
input, is proved to be exponentially convergent to zero, and all signals are
proved as uniformly ultimately bounded. The proposed control design is
validated on an aero-engine flexible blade under extreme thermal and
aerodynamic conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OminiAdapt: Learning Cross-Task Invariance for Robust and
  Environment-Aware <span class="highlight-title">Robo</span>tic <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxu Wang, Weiyun Yi, Xinhao Kong, Wanting Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of embodied intelligence, leveraging large-scale
human data for high-level imitation learning on humanoid robots has become a
focal point of interest in both academia and industry. However, applying
humanoid robots to precision operation domains remains challenging due to the
complexities they face in perception and control processes, the long-standing
physical differences in morphology and actuation mechanisms between humanoid
robots and humans, and the lack of task-relevant features obtained from
egocentric vision. To address the issue of covariate shift in imitation
learning, this paper proposes an imitation learning algorithm tailored for
humanoid robots. By focusing on the primary task objectives, filtering out
background information, and incorporating channel feature fusion with spatial
attention mechanisms, the proposed algorithm suppresses environmental
disturbances and utilizes a dynamic weight update strategy to significantly
improve the success rate of humanoid robots in accomplishing target tasks.
Experimental results demonstrate that the proposed method exhibits robustness
and scalability across various typical task scenarios, providing new ideas and
approaches for autonomous learning and control in humanoid robots. The project
will be open-sourced on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dimensional optimization of single-DOF planar rigid link-flapping
  mechanisms for high lift and low power 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shyam Sunder Nishad, Anupam Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rigid link flapping mechanisms remain the most practical choice for flapping
wing micro-aerial vehicles (MAVs) to carry useful payloads and onboard
batteries for free flight due to their long-term durability and reliability.
However, to achieve high agility and maneuverability-like insects-MAVs with
these mechanisms require significant weight reduction. One approach involves
using single-DOF planar rigid linkages, which are rarely optimized
dimensionally for high lift and low power so that smaller motors and batteries
could be used. We integrated a mechanism simulator based on a quasistatic
nonlinear finite element method with an unsteady vortex lattice method-based
aerodynamic analysis tool within an optimization routine. We optimized three
different mechanism topologies from the literature. As a result, significant
power savings were observed up to 42% in some cases, due to increased amplitude
and higher lift coefficients resulting from optimized asymmetric sweeping
velocity profiles. We also conducted an uncertainty analysis that revealed the
need for high manufacturing tolerances to ensure reliable mechanism
performance. The presented unified computational tool also facilitates the
optimal selection of MAV components based on the payload and flight time
requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAGA: A Tangent-Based Reactive Approach for Socially Compliant <span class="highlight-title">Robo</span>t
  Navigation Around Human Groups <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utsha Kumar Roy, Sejuti Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot navigation in densely populated environments presents significant
challenges, particularly regarding the interplay between individual and group
dynamics. Current navigation models predominantly address interactions with
individual pedestrians while failing to account for human groups that naturally
form in real-world settings. Conversely, the limited models implementing
group-aware navigation typically prioritize group dynamics at the expense of
individual interactions, both of which are essential for socially appropriate
navigation. This research extends an existing simulation framework to
incorporate both individual pedestrians and human groups. We present Tangent
Action for Group Avoidance (TAGA), a modular reactive mechanism that can be
integrated with existing navigation frameworks to enhance their group-awareness
capabilities. TAGA dynamically modifies robot trajectories using tangent
action-based avoidance strategies while preserving the underlying model's
capacity to navigate around individuals. Additionally, we introduce Group
Collision Rate (GCR), a novel metric to quantitatively assess how effectively
robots maintain group integrity during navigation. Through comprehensive
simulation-based benchmarking, we demonstrate that integrating TAGA with
state-of-the-art navigation models (ORCA, Social Force, DS-RNN, and AG-RL)
reduces group intrusions by 45.7-78.6% while maintaining comparable success
rates and navigation efficiency. Future work will focus on real-world
implementation and validation of this approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures. Submitted as a conference paper in IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Human <span class="highlight-title">Robo</span>t Navigation in Warehouse Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth Farrell, Chenghao Li, Hongzhan Yu, Ryo Yoshimitsu, Sicun Gao, Henrik I. Christensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of autonomous mobile robots (AMRs) in industrial
environments, particularly warehouses, has revolutionized logistics and
operational efficiency. However, ensuring the safety of human workers in
dynamic, shared spaces remains a critical challenge. This work proposes a novel
methodology that leverages control barrier functions (CBFs) to enhance safety
in warehouse navigation. By integrating learning-based CBFs with the Open
Robotics Middleware Framework (OpenRMF), the system achieves adaptive and
safety-enhanced controls in multi-robot, multi-agent scenarios. Experiments
conducted using various robot platforms demonstrate the efficacy of the
proposed approach in avoiding static and dynamic obstacles, including human
pedestrians. Our experiments evaluate different scenarios in which the number
of robots, robot platforms, speed, and number of obstacles are varied, from
which we achieve promising performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuzzy-Logic-based model predictive control: A paradigm integrating
  optimal and common-sense decision making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Surma, Anahita Jamshidnejad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel concept, fuzzy-logic-based model predictive
control (FLMPC), along with a multi-robot control approach for exploring
unknown environments and locating targets. Traditional model predictive control
(MPC) methods rely on Bayesian theory to represent environmental knowledge and
optimize a stochastic cost function, often leading to high computational costs
and lack of effectiveness in locating all the targets. Our approach instead
leverages FLMPC and extends it to a bi-level parent-child architecture for
enhanced coordination and extended decision making horizon. Extracting
high-level information from probability distributions and local observations,
FLMPC simplifies the optimization problem and significantly extends its
operational horizon compared to other MPC methods. We conducted extensive
simulations in unknown 2-dimensional environments with randomly placed
obstacles and humans. We compared the performance and computation time of FLMPC
against MPC with a stochastic cost function, then evaluated the impact of
integrating the high-level parent FLMPC layer. The results indicate that our
approaches significantly improve both performance and computation time,
enhancing coordination of robots and reducing the impact of uncertainty in
large-scale search and rescue environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 Pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Immersive and Wearable Thermal Rendering for Augmented Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Watkins, Ritam Ghosh, Evan Chow, Nilanjan Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In augmented reality (AR), where digital content is overlaid onto the real
world, realistic thermal feedback has been shown to enhance immersion. Yet
current thermal feedback devices, heavily influenced by the needs of virtual
reality, often hinder physical interactions and are ineffective for immersion
in AR. To bridge this gap, we have identified three design considerations
relevant for AR thermal feedback: indirect feedback to maintain dexterity,
thermal passthrough to preserve real-world temperature perception, and
spatiotemporal rendering for dynamic sensations. We then created a unique and
innovative thermal feedback device that satisfies these criteria. Human subject
experiments assessing perceptual sensitivity, object temperature matching,
spatial pattern recognition, and moving thermal stimuli demonstrated the impact
of our design, enabling realistic temperature discrimination, virtual object
perception, and enhanced immersion. These findings demonstrate that carefully
designed thermal feedback systems can bridge the sensory gap between physical
and virtual interactions, enhancing AR realism and usability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-Predictive Trajectory Generation for Aerial Search and Coverage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Matias, Daniel Silvestre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a trajectory planning algorithm for search and coverage
missions with an Unmanned Aerial Vehicle (UAV) based on an uncertainty map that
represents prior knowledge of the target region, modeled by a Gaussian Mixture
Model (GMM). The trajectory planning problem is formulated as an Optimal
Control Problem (OCP), which aims to maximize the uncertainty reduction within
a specified mission duration. However, this results in an intractable OCP whose
objective functional cannot be expressed in closed form. To address this, we
propose a Model Predictive Control (MPC) algorithm based on a relaxed
formulation of the objective function to approximate the optimal solutions.
This relaxation promotes efficient map exploration by penalizing overlaps in
the UAV's visibility regions along the trajectory. The algorithm can produce
efficient and smooth trajectories, and it can be efficiently implemented using
standard Nonlinear Programming solvers, being suitable for real-time planning.
Unlike traditional methods, which often rely on discretizing the mission space
and using complex mixed-integer formulations, our approach is computationally
efficient and easier to implement. The MPC algorithm is initially assessed in
MATLAB, followed by Gazebo simulations and actual experimental tests conducted
in an outdoor environment. The results demonstrate that the proposed strategy
can generate efficient and smooth trajectories for search and coverage
missions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Optimizing a Convex Cover of Collision-Free Space for Trajectory
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09631v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09631v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Wu, Igor Spasojevic, Pratik Chaudhari, Vijay Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an online iterative algorithm to optimize a convex cover to
under-approximate the free space for autonomous navigation to delineate Safe
Flight Corridors (SFC). The convex cover consists of a set of polytopes such
that the union of the polytopes represents obstacle-free space, allowing us to
find trajectories for robots that lie within the convex cover. In order to find
the SFC that facilitates trajectory optimization, we iteratively find
overlapping polytopes of maximum volumes that include specified waypoints
initialized by a geometric or kinematic planner. Constraints at waypoints
appear in two alternating stages of a joint optimization problem, which is
solved by a novel heuristic-based iterative algorithm with partially
distributed variables. We validate the effectiveness of our proposed algorithm
using a range of parameterized environments and show its applications for
two-stage motion planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13255v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13255v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandström, Stefano Mattoccia, Martin R. Oswald, Matteo Poggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past two decades, research in the field of Simultaneous Localization
and Mapping (SLAM) has undergone a significant evolution, highlighting its
critical role in enabling autonomous exploration of unknown environments. This
evolution ranges from hand-crafted methods, through the era of deep learning,
to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D
Gaussian Splatting (3DGS) representations. Recognizing the growing body of
research and the absence of a comprehensive survey on the topic, this paper
aims to provide the first comprehensive overview of SLAM progress through the
lens of the latest advancements in radiance fields. It sheds light on the
background, evolutionary path, inherent strengths and limitations, and serves
as a fundamental reference to highlight the dynamic progress and specific
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated to November 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Continual Adaptation of Pretrained <span class="highlight-title">Robo</span>tic Policy with Online
  Meta-Learned Adapters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Zhu, Endong Sun, Guanhe Huang, Oya Celiktutan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual adaptation is essential for general autonomous agents. For example,
a household robot pretrained with a repertoire of skills must still adapt to
unseen tasks specific to each household. Motivated by this, building upon
parameter-efficient fine-tuning in language models, prior works have explored
lightweight adapters to adapt pretrained policies, which can preserve learned
features from the pretraining phase and demonstrate good adaptation
performances. However, these approaches treat task learning separately,
limiting knowledge transfer between tasks. In this paper, we propose Online
Meta-Learned adapters (OMLA). Instead of applying adapters directly, OMLA can
facilitate knowledge transfer from previously learned tasks to current learning
tasks through a novel meta-learning objective. Extensive experiments in both
simulated and real-world environments demonstrate that OMLA can lead to better
adaptation performances compared to the baseline methods. The project link:
https://ricky-zhu.github.io/OMLA/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project link: https://ricky-zhu.github.io/OMLA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating Naturalistic Insights in Objective Multi-Vehicle Safety
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico Del Re, Amirhesam Aghanouri, Cristina Olaverri-Monreal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As autonomous vehicle technology advances, the precise assessment of safety
in complex traffic scenarios becomes crucial, especially in mixed-vehicle
environments where human perception of safety must be taken into account. This
paper presents a framework designed for assessing traffic safety in
multi-vehicle situations, facilitating the simultaneous utilization of diverse
objective safety metrics. Additionally, it allows the integration of subjective
perception of safety by adjusting model parameters. The framework was applied
to evaluate various model configurations in car-following scenarios on a
highway, utilizing naturalistic driving datasets. The evaluation of the model
showed an outstanding performance, particularly when integrating multiple
objective safety measures. Furthermore, the performance was significantly
enhanced when considering all surrounding vehicles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online POMDP Planning with Anytime Deterministic Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moran Barenboim, Vadim Indelman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making under uncertainty is a critical aspect of many practical
autonomous systems due to incomplete information. Partially Observable Markov
Decision Processes (POMDPs) offer a mathematically principled framework for
formulating decision-making problems under such conditions. However, finding an
optimal solution for a POMDP is generally intractable. In recent years, there
has been a significant progress of scaling approximate solvers from small to
moderately sized problems, using online tree search solvers. Often, such
approximate solvers are limited to probabilistic or asymptotic guarantees
towards the optimal solution. In this paper, we derive a deterministic
relationship for discrete POMDPs between an approximated and the optimal
solution. We show that at any time, we can derive bounds that relate between
the existing solution and the optimal one. We show that our derivations provide
an avenue for a new set of algorithms and can be attached to existing
algorithms that have a certain structure to provide them with deterministic
guarantees with marginal computational overhead. In return, not only do we
certify the solution quality, but we demonstrate that making a decision based
on the deterministic guarantee may result in superior performance compared to
the original algorithm without the deterministic certification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Risk-Aware <span class="highlight-title">Reinforcement Learning</span> for Autonomous Driving: Improving
  Safety When Driving through Intersection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Leng, Ran Yu, Wei Han, Lu Xiong, Zhuoren Li, Hailong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying reinforcement learning to autonomous driving has garnered widespread
attention. However, classical reinforcement learning methods optimize policies
by maximizing expected rewards but lack sufficient safety considerations, often
putting agents in hazardous situations. This paper proposes a risk-aware
reinforcement learning approach for autonomous driving to improve the safety
performance when crossing the intersection. Safe critics are constructed to
evaluate driving risk and work in conjunction with the reward critic to update
the actor. Based on this, a Lagrangian relaxation method and cyclic gradient
iteration are combined to project actions into a feasible safe region.
Furthermore, a Multi-hop and Multi-layer perception (MLP) mixed Attention
Mechanism (MMAM) is incorporated into the actor-critic network, enabling the
policy to adapt to dynamic traffic and overcome permutation sensitivity
challenges. This allows the policy to focus more effectively on surrounding
potential risks while enhancing the identification of passing opportunities.
Simulation tests are conducted on different tasks at unsignalized
intersections. The results show that the proposed approach effectively reduces
collision rates and improves crossing efficiency in comparison to baseline
algorithms. Additionally, our ablation experiments demonstrate the benefits of
incorporating risk-awareness and MMAM into RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constrained Nonlinear Kaczmarz Projection on Intersections of Manifolds
  for Coordinated Multi-<span class="highlight-title">Robo</span>t Mobile <span class="highlight-title">Manipulation</span> <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshaya Agrawal, Parker Mayer, Zachary Kingston, Geoffrey A. Hollinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperative manipulation tasks impose various structure-, task-, and
robot-specific constraints on mobile manipulators. However, current methods
struggle to model and solve these myriad constraints simultaneously. We propose
a twofold solution: first, we model constraints as a family of manifolds
amenable to simultaneous solving. Second, we introduce the constrained
nonlinear Kaczmarz (cNKZ) projection technique to produce constraint-satisfying
solutions. Experiments show that cNKZ dramatically outperforms baseline
approaches, which cannot find solutions at all. We integrate cNKZ with a
sampling-based motion planning algorithm to generate complex, coordinated
motions for 3 to 6 mobile manipulators (18--36 DoF), with cNKZ solving up to 80
nonlinear constraints simultaneously and achieving up to a 92% success rate in
cluttered environments. We also demonstrate our approach on hardware using
three Turtlebot3 Waffle Pi robots with OpenMANIPULATOR-X arms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at IEEE International Conference on Robotics
  and Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MUSE: A Real-Time Multi-Sensor State Estimator for Quadruped <span class="highlight-title">Robo</span>ts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ylenia Nisticò, João Carlos Virgolino Soares, Lorenzo Amatucci, Geoff Fink, Claudio Semini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative state estimator, MUSE (MUlti-sensor State
Estimator), designed to enhance state estimation's accuracy and real-time
performance in quadruped robot navigation. The proposed state estimator builds
upon our previous work presented in [1]. It integrates data from a range of
onboard sensors, including IMUs, encoders, cameras, and LiDARs, to deliver a
comprehensive and reliable estimation of the robot's pose and motion, even in
slippery scenarios. We tested MUSE on a Unitree Aliengo robot, successfully
closing the locomotion control loop in difficult scenarios, including slippery
and uneven terrain. Benchmarking against Pronto [2] and VILENS [3] showed 67.6%
and 26.7% reductions in translational errors, respectively. Additionally, MUSE
outperformed DLIO [4], a LiDAR-inertial odometry system in rotational errors
and frequency, while the proprioceptive version of MUSE (P-MUSE) outperformed
TSIF [5], with a 45.9% reduction in absolute trajectory error (ATE).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mirroring the Parking Target: An Optimal-Control-Based Parking Motion
  Planner with Strengthened Parking Reliability and Faster Parking Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Hu, Yongwei Feng, Shuoyuan Li, Haoran Wang, Jaehyun So, Junnian Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated Parking Assist (APA) systems are now facing great challenges of low
adoption in applications, due to users' concerns about parking capability,
reliability, and completion efficiency. To upgrade the conventional APA
planners and enhance user's acceptance, this research proposes an
optimal-control-based parking motion planner. Its highlight lies in its control
logic: planning trajectories by mirroring the parking target. This method
enables: i) parking capability in narrow spaces; ii) better parking reliability
by expanding Operation Design Domain (ODD); iii) faster completion of parking
process; iv) enhanced computational efficiency; v) universal to all types of
parking. A comprehensive evaluation is conducted. Results demonstrate the
proposed planner does enhance parking success rate by 40.6%, improve parking
completion efficiency by 18.0%, and expand ODD by 86.1%. It shows its
superiority in difficult parking cases, such as the parallel parking scenario
and narrow spaces. Moreover, the average computation time of the proposed
planner is 74 milliseconds. Results indicate that the proposed planner is ready
for real-time commercial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Intelligent Transportation Systems (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safety-Aware Human-Lead Vehicle Platooning by Proactively Reacting to
  Uncertain Human Behaving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07556v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07556v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Hu, Shuhan Wang, Yiming Zhang, Haoran Wang, Zhilong Liu, Guangzhi Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as a
promising vehicle platooning technology in real-world implementation. By
utilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reduces
the cost and enhances the reliability of perception and decision-making.
However, state-of-the-art HL-CACC technology still has a great limitation on
driving safety due to the lack of considering the leading human driver's
uncertain behavior. In this study, a HL-CACC controller is designed based on
Stochastic Model Predictive Control (SMPC). It is enabled to predict the
driving intention of the leading Connected Human-Driven Vehicle (CHV). The
proposed controller has the following features: i) enhanced perceived safety in
oscillating traffic; ii) guaranteed safety against hard brakes; iii)
computational efficiency for real-time implementation. The proposed controller
is evaluated on a PreScan&Simulink simulation platform. Real vehicle trajectory
data is collected for the calibration of the simulation. Results reveal that
the proposed controller: i) improves perceived safety by 19.17% in oscillating
traffic; ii) enhances actual safety by 7.76% against hard brakes; iii) is
confirmed with string stability. The computation time is approximately 3.2
milliseconds when running on a laptop equipped with an Intel i5-13500H CPU.
This indicates the proposed controller is ready for real-time implementation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AlphaSpace: Enabling <span class="highlight-title">Robo</span>tic Actions through Semantic Tokenization and
  Symbolic Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Dao, Dinh Bach Vu, Bui Quang Huy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents AlphaSpace, a novel methodology designed to enhance the
spatial reasoning capabilities of language models for robotic manipulation in
3D Cartesian space. AlphaSpace employs a hierarchical semantics-based
tokenization strategy that encodes spatial information at both coarse and
fine-grained levels. Our approach represents objects with their attributes,
positions, and height information through structured tokens, enabling precise
spatial reasoning without relying on traditional vision-based embeddings. This
approach enables LLMs to accurately manipulate objects by positioning them at
specific (x, y, z) coordinates. Experimental results suggest that AlphaSpace
demonstrates promising potential for improving manipulation tasks, achieving a
total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude
3.5 Sonnet. These results demonstrate the potential of structured spatial
encoding for manipulation tasks and warrant further exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in
  <span class="highlight-title">Reinforcement Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17125v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17125v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Kim, Seung-Woo Seo, Seong-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) has demonstrated strong performance in
robotic control but remains susceptible to out-of-distribution (OOD) states,
often resulting in unreliable actions and task failure. While previous methods
have focused on minimizing or preventing OOD occurrences, they largely neglect
recovery once an agent encounters such states. Although the latest research has
attempted to address this by guiding agents back to in-distribution states,
their reliance on uncertainty estimation hinders scalability in complex
environments. To overcome this limitation, we introduce Language Models for
Out-of-Distribution Recovery (LaMOuR), which enables recovery learning without
relying on uncertainty estimation. LaMOuR generates dense reward codes that
guide the agent back to a state where it can successfully perform its original
task, leveraging the capabilities of LVLMs in image description, logical
reasoning, and code generation. Experimental results show that LaMOuR
substantially enhances recovery efficiency across diverse locomotion tasks and
even generalizes effectively to complex environments, including humanoid
locomotion and mobile manipulation, where existing methods struggle. The code
and supplementary materials are available at https://lamour-rl.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is currently under security review and will be re-released
  once the review is complete</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DexForce: Extracting Force-informed Actions from Kinesthetic
  Demonstrations for Dexterous <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claire Chen, Zhongchun Yu, Hojung Choi, Mark Cutkosky, Jeannette Bohg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning requires high-quality demonstrations consisting of
sequences of state-action pairs. For contact-rich dexterous manipulation tasks
that require dexterity, the actions in these state-action pairs must produce
the right forces. Current widely-used methods for collecting dexterous
manipulation demonstrations are difficult to use for demonstrating contact-rich
tasks due to unintuitive human-to-robot motion retargeting and the lack of
direct haptic feedback. Motivated by these concerns, we propose DexForce.
DexForce leverages contact forces, measured during kinesthetic demonstrations,
to compute force-informed actions for policy learning. We collect
demonstrations for six tasks and show that policies trained on our
force-informed actions achieve an average success rate of 76% across all tasks.
In contrast, policies trained directly on actions that do not account for
contact forces have near-zero success rates. We also conduct a study ablating
the inclusion of force data in policy observations. We find that while using
force data never hurts policy performance, it helps most for tasks that require
advanced levels of precision and coordination, like opening an AirPods case and
unscrewing a nut.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Videos can be found here:
  https://clairelc.github.io/dexforce.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GR00T N1: An Open Foundation Model for Generalist Humanoid <span class="highlight-title">Robo</span>ts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         NVIDIA,  :, Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi "Jim" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose robots need a versatile body and an intelligent mind. Recent
advancements in humanoid robots have shown great promise as a hardware platform
for building generalist autonomy in the human world. A robot foundation model,
trained on massive and diverse data sources, is essential for enabling the
robots to reason about novel situations, robustly handle real-world
variability, and rapidly learn new tasks. To this end, we introduce GR00T N1,
an open foundation model for humanoid robots. GR00T N1 is a
Vision-Language-Action (VLA) model with a dual-system architecture. The
vision-language module (System 2) interprets the environment through vision and
language instructions. The subsequent diffusion transformer module (System 1)
generates fluid motor actions in real time. Both modules are tightly coupled
and jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture
of real-robot trajectories, human videos, and synthetically generated datasets.
We show that our generalist robot model GR00T N1 outperforms the
state-of-the-art imitation learning baselines on standard simulation benchmarks
across multiple robot embodiments. Furthermore, we deploy our model on the
Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation
tasks, achieving strong performance with high data efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Authors are listed alphabetically. Project leads are Linxi "Jim" Fan
  and Yuke Zhu. For more information, see
  https://developer.nvidia.com/isaac/gr00t</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyBimanual: Transferring Unimanual Policy for General Bimanual
  <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06779v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06779v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanxing Lu, Tengbo Yu, Haoyuan Deng, Season Si Chen, Yansong Tang, Ziwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performing general language-conditioned bimanual manipulation tasks is of
great importance for many applications ranging from household service to
industrial assembly. However, collecting bimanual manipulation data is
expensive due to the high-dimensional action space, which poses challenges for
conventional methods to handle general bimanual manipulation tasks. In
contrast, unimanual policy has recently demonstrated impressive
generalizability across a wide range of tasks because of scaled model
parameters and training data, which can provide sharable manipulation knowledge
for bimanual systems. To this end, we propose a plug-and-play method named
AnyBimanual, which transfers pre-trained unimanual policy to general bimanual
manipulation policy with few bimanual demonstrations. Specifically, we first
introduce a skill manager to dynamically schedule the skill representations
discovered from pre-trained unimanual policy for bimanual manipulation tasks,
which linearly combines skill primitives with task-oriented compensation to
represent the bimanual manipulation instruction. To mitigate the observation
discrepancy between unimanual and bimanual systems, we present a visual aligner
to generate soft masks for visual embedding of the workspace, which aims to
align visual input of unimanual policy model for each arm with those during
pretraining stage. AnyBimanual shows superiority on 12 simulated tasks from
RLBench2 with a sizable 12.67% improvement in success rate over previous
methods. Experiments on 9 real-world tasks further verify its practicality with
an average success rate of 84.62%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://anybimanual.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> SyncDiff: Synchronized Motion <span class="highlight-title">Diffusion</span> for Multi-Body Human-Object
  Interaction Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20104v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20104v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkun He, Yun Liu, Ruitao Liu, <span class="highlight-author">Li Yi</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing realistic human-object interaction motions is a critical problem
in VR/AR and human animation. Unlike the commonly studied scenarios involving a
single human or hand interacting with one object, we address a more generic
multi-body setting with arbitrary numbers of humans, hands, and objects. This
complexity introduces significant challenges in synchronizing motions due to
the high correlations and mutual influences among bodies. To address these
challenges, we introduce SyncDiff, a novel method for multi-body interaction
synthesis using a synchronized motion diffusion strategy. SyncDiff employs a
single diffusion model to capture the joint distribution of multi-body motions.
To enhance motion fidelity, we propose a frequency-domain motion decomposition
scheme. Additionally, we introduce a new set of alignment scores to emphasize
the synchronization of different body motions. SyncDiff jointly optimizes both
data sample likelihood and alignment likelihood through an explicit
synchronization strategy. Extensive experiments across four datasets with
various multi-body configurations demonstrate the superiority of SyncDiff over
existing state-of-the-art motion synthesis methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Library Adaptation: LoRA Retrieval and Fusion for
  Open-Vocabulary Semantic Segmentation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Qorbani, Gianluca Villani, Theodoros Panagiotakopoulos, Marc Botet Colomer, Linus Härenstam-Nielsen, Mattia Segu, Pier Luigi Dovesi, Jussi Karlgren, Daniel Cremers, Federico Tombari, Matteo Poggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary semantic segmentation models associate vision and text to
label pixels from an undefined set of classes using textual queries, providing
versatile performance on novel datasets. However, large shifts between training
and test domains degrade their performance, requiring fine-tuning for effective
real-world applications. We introduce Semantic Library Adaptation (SemLA), a
novel framework for training-free, test-time domain adaptation. SemLA leverages
a library of LoRA-based adapters indexed with CLIP embeddings, dynamically
merging the most relevant adapters based on proximity to the target domain in
the embedding space. This approach constructs an ad-hoc model tailored to each
specific input without additional training. Our method scales efficiently,
enhances explainability by tracking adapter contributions, and inherently
protects data privacy, making it ideal for sensitive applications.
Comprehensive experiments on a 20-domain benchmark built over 10 standard
datasets demonstrate SemLA's superior adaptability and performance across
diverse settings, establishing a new standard in domain adaptation for
open-vocabulary semantic segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Project page: https://thegoodailab.org/semla Code:
  https://github.com/rezaqorbani/SemLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoMage: Multi-Subject and Motion Customization of Text-to-Video
  <span class="highlight-title">Diffusion</span> Models <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-Pin Huang, Yen-Siang Wu, Hung-Kai Chung, Kai-Po Chang, Fu-En Yang, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customized text-to-video generation aims to produce high-quality videos that
incorporate user-specified subject identities or motion patterns. However,
existing methods mainly focus on personalizing a single concept, either subject
identity or motion pattern, limiting their effectiveness for multiple subjects
with the desired motion patterns. To tackle this challenge, we propose a
unified framework VideoMage for video customization over both multiple subjects
and their interactive motions. VideoMage employs subject and motion LoRAs to
capture personalized content from user-provided images and videos, along with
an appearance-agnostic motion learning approach to disentangle motion patterns
from visual appearance. Furthermore, we develop a spatial-temporal composition
scheme to guide interactions among subjects within the desired motion patterns.
Extensive experiments demonstrate that VideoMage outperforms existing methods,
generating coherent, user-controlled videos with consistent subject identities
and interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Project Page:
  https://jasper0314-huang.github.io/videomage-customization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Shaker, Muhammad Maaz, Chenhui Gou, Hamid Rezatofighi, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video understanding models often struggle with high computational
requirements, extensive parameter counts, and slow inference speed, making them
inefficient for practical use. To tackle these challenges, we propose
Mobile-VideoGPT, an efficient multimodal framework designed to operate with
fewer than a billion parameters. Unlike traditional video large multimodal
models (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders,
efficient projectors, and a small language model (SLM), enabling real-time
throughput. To further improve efficiency, we present an Attention-Based Frame
Scoring mechanism to select the key-frames, along with an efficient token
projector that prunes redundant visual tokens and preserves essential
contextual cues. We evaluate our model across well-established six video
understanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest).
Our results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per
second while outperforming existing state-of-the-art 0.5B-parameter models by 6
points on average with 40% fewer parameters and more than 2x higher throughput.
Our code and models are publicly available at:
https://github.com/Amshaker/Mobile-VideoGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. Project Page:
  https://amshaker.github.io/Mobile-VideoGPT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time
  Tomographic Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Yu, Yuanhao Cai, Ruyi Zha, Zhiwen Fan, Chenxin Li, Yixuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Four-dimensional computed tomography (4D CT) reconstruction is crucial for
capturing dynamic anatomical changes but faces inherent limitations from
conventional phase-binning workflows. Current methods discretize temporal
resolution into fixed phases with respiratory gating devices, introducing
motion misalignment and restricting clinical practicality. In this paper, We
propose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT
reconstruction by integrating dynamic radiative Gaussian splatting with
self-supervised respiratory motion learning. Our approach models anatomical
dynamics through a spatiotemporal encoder-decoder architecture that predicts
time-varying Gaussian deformations, eliminating phase discretization. To remove
dependency on external gating devices, we introduce a physiology-driven
periodic consistency loss that learns patient-specific breathing cycles
directly from projections via differentiable optimization. Extensive
experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR
gain over traditional methods and 2.25 dB improvement against prior Gaussian
splatting techniques. By unifying continuous motion modeling with hardware-free
period learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for
dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://x2-gaussian.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HS-SLAM: Hybrid Representation with Structural Super<span class="highlight-title">vision</span> for Improved
  Dense SLAM <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziren Gong, Fabio Tosi, Youmin Zhang, Stefano Mattoccia, Matteo Poggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  NeRF-based SLAM has recently achieved promising results in tracking and
reconstruction. However, existing methods face challenges in providing
sufficient scene representation, capturing structural information, and
maintaining global consistency in scenes emerging significant movement or being
forgotten. To this end, we present HS-SLAM to tackle these problems. To enhance
scene representation capacity, we propose a hybrid encoding network that
combines the complementary strengths of hash-grid, tri-planes, and one-blob,
improving the completeness and smoothness of reconstruction. Additionally, we
introduce structural supervision by sampling patches of non-local pixels rather
than individual rays to better capture the scene structure. To ensure global
consistency, we implement an active global bundle adjustment (BA) to eliminate
camera drifts and mitigate accumulative errors. Experimental results
demonstrate that HS-SLAM outperforms the baselines in tracking and
reconstruction accuracy while maintaining the efficiency required for robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025. Project Page: https://zorangong.github.io/HS-SLAM/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-Time <span class="highlight-title">Visual</span> In-Context Tuning <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Xie, Alessio Tonioni, Nathalie Rauschmayr, Federico Tombari, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual in-context learning (VICL), as a new paradigm in computer vision,
allows the model to rapidly adapt to various tasks with only a handful of
prompts and examples. While effective, the existing VICL paradigm exhibits poor
generalizability under distribution shifts. In this work, we propose test-time
Visual In-Context Tuning (VICT), a method that can adapt VICL models on the fly
with a single test sample. Specifically, we flip the role between the task
prompts and the test sample and use a cycle consistency loss to reconstruct the
original task prompt output. Our key insight is that a model should be aware of
a new test distribution if it can successfully recover the original task
prompts. Extensive experiments on six representative vision tasks ranging from
high-level visual understanding to low-level image processing, with 15 common
corruptions, demonstrate that our VICT can improve the generalizability of VICL
to unseen new domains. In addition, we show the potential of applying VICT for
unseen tasks at test time. Code: https://github.com/Jiahao000/VICT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Code: https://github.com/Jiahao000/VICT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video-R1: Reinforcing Video Reasoning in MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by DeepSeek-R1's success in eliciting reasoning abilities through
rule-based reinforcement learning (RL), we introduce Video-R1 as the first
attempt to systematically explore the R1 paradigm for eliciting video reasoning
within multimodal large language models (MLLMs). However, directly applying RL
training with the GRPO algorithm to video reasoning presents two primary
challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the
scarcity of high-quality video-reasoning data. To address these issues, we
first propose the T-GRPO algorithm, which encourages models to utilize temporal
information in videos for reasoning. Additionally, instead of relying solely on
video data, we incorporate high-quality image-reasoning data into the training
process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start
and Video-R1-260k for RL training, both comprising image and video data.
Experimental results demonstrate that Video-R1 achieves significant
improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as
well as on general video benchmarks including MVBench and TempCompass, etc.
Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning
benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All
codes, models, data are released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/tulerfeng/Video-R1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Stepsize for <span class="highlight-title">Diffusion</span> Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianning Pei, Han Hu, Shuyang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models achieve remarkable generation quality but suffer from
computational intensive sampling due to suboptimal step discretization. While
existing works focus on optimizing denoising directions, we address the
principled design of stepsize schedules. This paper proposes Optimal Stepsize
Distillation, a dynamic programming framework that extracts theoretically
optimal schedules by distilling knowledge from reference trajectories. By
reformulating stepsize optimization as recursive error minimization, our method
guarantees global discretization bounds through optimal substructure
exploitation. Crucially, the distilled schedules demonstrate strong robustness
across architectures, ODE solvers, and noise schedules. Experiments show 10x
accelerated text-to-image generation while preserving 99.4% performance on
GenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Guo, Young Yoon Lee, Joseph Liu, Yizhak Ben-Shabat, Victor Zordan, Mubbasir Kapadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present StyleMotif, a novel Stylized Motion Latent Diffusion model,
generating motion conditioned on both content and style from multiple
modalities. Unlike existing approaches that either focus on generating diverse
motion content or transferring style from sequences, StyleMotif seamlessly
synthesizes motion across a wide range of content while incorporating stylistic
cues from multi-modal inputs, including motion, text, image, video, and audio.
To achieve this, we introduce a style-content cross fusion mechanism and align
a style encoder with a pre-trained multi-modal model, ensuring that the
generated motion accurately captures the reference style while preserving
realism. Extensive experiments demonstrate that our framework surpasses
existing methods in stylized motion generation and exhibits emergent
capabilities for multi-modal motion stylization, enabling more nuanced motion
synthesis. Source code and pre-trained models will be released upon acceptance.
Project Page: https://stylemotif.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://stylemotif.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOCORE: Image Re-ranking with Long-Context Sequence Modeling <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilin Xiao, Pavel Suma, Ayush Sachdeva, Hao-Jen Wang, Giorgos Kordopatis-Zilos, Giorgos Tolias, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LOCORE, Long-Context Re-ranker, a model that takes as input
local descriptors corresponding to an image query and a list of gallery images
and outputs similarity scores between the query and each gallery image. This
model is used for image retrieval, where typically a first ranking is performed
with an efficient similarity measure, and then a shortlist of top-ranked images
is re-ranked based on a more fine-grained similarity measure. Compared to
existing methods that perform pair-wise similarity estimation with local
descriptors or list-wise re-ranking with global descriptors, LOCORE is the
first method to perform list-wise re-ranking with local descriptors. To achieve
this, we leverage efficient long-context sequence models to effectively capture
the dependencies between query and gallery images at the local-descriptor
level. During testing, we process long shortlists with a sliding window
strategy that is tailored to overcome the context size limitations of sequence
models. Our approach achieves superior performance compared with other
re-rankers on established image retrieval benchmarks of landmarks (ROxf and
RPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200)
while having comparable latency to the pair-wise local descriptor re-rankers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Image-Dense Annotation Generation Model for Underwater Scenes <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongkai Lin, Dingkang Liang, Zhenghao Qi, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater dense prediction, especially depth estimation and semantic
segmentation, is crucial for gaining a comprehensive understanding of
underwater scenes. Nevertheless, high-quality and large-scale underwater
datasets with dense annotations remain scarce because of the complex
environment and the exorbitant data collection costs. This paper proposes a
unified Text-to-Image and DEnse annotation generation method (TIDE) for
underwater scenes. It relies solely on text as input to simultaneously generate
realistic underwater images and multiple highly consistent dense annotations.
Specifically, we unify the generation of text-to-image and text-to-dense
annotations within a single model. The Implicit Layout Sharing mechanism (ILS)
and cross-modal interaction method called Time Adaptive Normalization (TAN) are
introduced to jointly optimize the consistency between image and dense
annotations. We synthesize a large-scale underwater dataset using TIDE to
validate the effectiveness of our method in underwater dense prediction tasks.
The results demonstrate that our method effectively improves the performance of
existing underwater dense prediction models and mitigates the scarcity of
underwater data with dense annotations. We hope our method can offer new
perspectives on alleviating data scarcity issues in other fields. The code is
available at https: //github.com/HongkLin/TIDE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025. The code is available at https:
  //github.com/HongkLin/TIDE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Visual</span> Jenga: Discovering Object Dependencies via Counterfactual
  Inpainting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anand Bhattad, Konpat Preechakul, Alexei A. Efros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel scene understanding task called Visual Jenga.
Drawing inspiration from the game Jenga, the proposed task involves
progressively removing objects from a single image until only the background
remains. Just as Jenga players must understand structural dependencies to
maintain tower stability, our task reveals the intrinsic relationships between
scene elements by systematically exploring which objects can be removed while
preserving scene coherence in both physical and geometric sense. As a starting
point for tackling the Visual Jenga task, we propose a simple, data-driven,
training-free approach that is surprisingly effective on a range of real-world
images. The principle behind our approach is to utilize the asymmetry in the
pairwise relationships between objects within a scene and employ a large
inpainting model to generate a set of counterfactuals to quantify the
asymmetry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://visualjenga.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Consistent Language Gaussian Splatting for Point-Level
  Open-vocabulary Querying 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hairong Yin, Huangying Zhan, Yi Xu, Raymond A. Yeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary querying in 3D Gaussian Splatting aims to identify
semantically relevant regions within a 3D Gaussian representation based on a
given text query. Prior work, such as LangSplat, addressed this task by
retrieving these regions in the form of segmentation masks on 2D renderings.
More recently, OpenGaussian introduced point-level querying, which directly
selects a subset of 3D Gaussians. In this work, we propose a point-level
querying method that builds upon LangSplat's framework. Our approach improves
the framework in two key ways: (a) we leverage masklets from the Segment
Anything Model 2 (SAM2) to establish semantic consistent ground-truth for
distilling the language Gaussians; (b) we introduces a novel two-step querying
approach that first retrieves the distilled ground-truth and subsequently uses
the ground-truth to query the individual Gaussians. Experimental evaluations on
three benchmark datasets demonstrate that the proposed method achieves better
performance compared to state-of-the-art approaches. For instance, our method
achieves an mIoU improvement of +20.42 on the 3D-OVS dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable-SCore: A Stable Registration-based Framework for 3D Shape
  Correspondence <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Liu, Xiaohang Zhan, Zizheng Yan, Zhongjin Luo, Yuxin Wen, Xiaoguang Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Establishing character shape correspondence is a critical and fundamental
task in computer vision and graphics, with diverse applications including
re-topology, attribute transfer, and shape interpolation. Current dominant
functional map methods, while effective in controlled scenarios, struggle in
real situations with more complex challenges such as non-isometric shape
discrepancies. In response, we revisit registration-for-correspondence methods
and tap their potential for more stable shape correspondence estimation. To
overcome their common issues including unstable deformations and the necessity
for careful pre-alignment or high-quality initial 3D correspondences, we
introduce Stable-SCore: A Stable Registration-based Framework for 3D Shape
Correspondence. We first re-purpose a foundation model for 2D character
correspondence that ensures reliable and stable 2D mappings. Crucially, we
propose a novel Semantic Flow Guided Registration approach that leverages 2D
correspondence to guide mesh deformations. Our framework significantly
surpasses existing methods in challenging scenarios, and brings possibilities
for a wide array of real applications, as demonstrated in our results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025. Homepage:
  https://haolinliu97.github.io/Stable-Score/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Evolution of Physics Cognition in Video Generation: A
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghui Lin, Xiang Wang, Yishan Wang, Shu Wang, Fengqi Dai, Pengxiang Ding, Cunxiang Wang, Zhengrong Zuo, Nong Sang, Siteng Huang, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in video generation have witnessed significant progress,
especially with the rapid advancement of diffusion models. Despite this, their
deficiencies in physical cognition have gradually received widespread attention
- generated content often violates the fundamental laws of physics, falling
into the dilemma of ''visual realism but physical absurdity". Researchers began
to increasingly recognize the importance of physical fidelity in video
generation and attempted to integrate heuristic physical cognition such as
motion representations and physical knowledge into generative systems to
simulate real-world dynamic scenarios. Considering the lack of a systematic
overview in this field, this survey aims to provide a comprehensive summary of
architecture designs and their applications to fill this gap. Specifically, we
discuss and organize the evolutionary process of physical cognition in video
generation from a cognitive science perspective, while proposing a three-tier
taxonomy: 1) basic schema perception for generation, 2) passive cognition of
physical knowledge for generation, and 3) active cognition for world
simulation, encompassing state-of-the-art methods, classical paradigms, and
benchmarks. Subsequently, we emphasize the inherent key challenges in this
domain and delineate potential pathways for future research, contributing to
advancing the frontiers of discussion in both academia and industry. Through
structured review and interdisciplinary analysis, this survey aims to provide
directional guidance for developing interpretable, controllable, and physically
consistent video generation paradigms, thereby propelling generative models
from the stage of ''visual mimicry'' towards a new phase of ''human-like
physical comprehension''.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A comprehensive list of papers studied in this survey is available at
  https://github.com/minnie-lin/Awesome-Physics-Cognition-based-Video-Generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni4D: Unifying <span class="highlight-title">Visual</span> Foundation Models for 4D Modeling from a Single
  Video <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Yifan Yao, Albert J. Zhai, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a unified approach to understanding dynamic scenes from
casual videos. Large pretrained vision foundation models, such as
vision-language, video depth prediction, motion tracking, and segmentation
models, offer promising capabilities. However, training a single model for
comprehensive 4D understanding remains challenging. We introduce Uni4D, a
multi-stage optimization framework that harnesses multiple pretrained models to
advance dynamic 3D modeling, including static/dynamic reconstruction, camera
pose estimation, and dense 3D motion tracking. Our results show
state-of-the-art performance in dynamic 4D modeling with superior visual
quality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the
effectiveness of repurposing visual foundation models for 4D understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Project page (with code):
  https://davidyao99.github.io/uni4d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fwd2Bot: LVLM <span class="highlight-title">Visual</span> Token Compression with Double Forward Bottleneck 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Bulat, Yassine Ouali, Georgios Tzimiropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we aim to compress the vision tokens of a Large Vision Language
Model (LVLM) into a representation that is simultaneously suitable for (a)
generative and (b) discriminative tasks, (c) is nearly lossless, and (d) is
storage-efficient. We propose a novel compression approach, called Fwd2Bot,
that uses the LVLM itself to compress the visual information in a task-agnostic
manner. At the core of Fwd2bot there exists a "double-forward pass" training
strategy, whereby, during the first forward pass, the LLM (of the LVLM) creates
a bottleneck by condensing the visual information into a small number of
summary tokens. Then, using the same LLM, the second forward pass processes the
language instruction(s) alongside the summary tokens, used as a direct
replacement for the image ones. The training signal is provided by two losses:
an autoregressive one applied after the second pass that provides a direct
optimization objective for compression, and a contrastive loss, applied after
the first pass, that further boosts the representation strength, especially for
discriminative tasks. The training is further enhanced by stage-specific
adapters. We accompany the proposed method by an in-depth ablation study.
Overall, Fwd2Bot results in highly-informative compressed representations
suitable for both generative and discriminative tasks. For generative tasks, we
offer a 2x higher compression rate without compromising the generative
capabilities, setting a new state-of-the-art result. For discriminative tasks,
we set a new state-of-the-art on image retrieval and compositionality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lumina-Image 2.0: A Unified and Efficient Image Generative Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, Xiangyang Zhu, Manyuan Zhang, Will Beddow, Erwann Millon, Victor Perez, Wenhai Wang, Conghui He, Bo Zhang, Xiaohong Liu, Hongsheng Li, Yu Qiao, Chang Xu, Peng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Lumina-Image 2.0, an advanced text-to-image generation framework
that achieves significant progress compared to previous work, Lumina-Next.
Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts
a unified architecture (Unified Next-DiT) that treats text and image tokens as
a joint sequence, enabling natural cross-modal interactions and allowing
seamless task expansion. Besides, since high-quality captioners can provide
semantically well-aligned text-image training pairs, we introduce a unified
captioning system, Unified Captioner (UniCap), specifically designed for T2I
generation tasks. UniCap excels at generating comprehensive and accurate
captions, accelerating convergence and enhancing prompt adherence. (2)
Efficiency - to improve the efficiency of our proposed model, we develop
multi-stage progressive training strategies and introduce inference
acceleration techniques without compromising image quality. Extensive
evaluations on academic benchmarks and public text-to-image arenas show that
Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters,
highlighting its scalability and design efficiency. We have released our
training details, code, and models at
https://github.com/Alpha-VLLM/Lumina-Image-2.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report, 21 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic
  Faithfulness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video generation has advanced significantly, evolving from producing
unrealistic outputs to generating videos that appear visually convincing and
temporally coherent. To evaluate these video generative models, benchmarks such
as VBench have been developed to assess their faithfulness, measuring factors
like per-frame aesthetics, temporal consistency, and basic prompt adherence.
However, these aspects mainly represent superficial faithfulness, which focus
on whether the video appears visually convincing rather than whether it adheres
to real-world principles. While recent models perform increasingly well on
these metrics, they still struggle to generate videos that are not just
visually plausible but fundamentally realistic. To achieve real "world models"
through video generation, the next frontier lies in intrinsic faithfulness to
ensure that generated videos adhere to physical laws, commonsense reasoning,
anatomical correctness, and compositional integrity. Achieving this level of
realism is essential for applications such as AI-assisted filmmaking and
simulated world modeling. To bridge this gap, we introduce VBench-2.0, a
next-generation benchmark designed to automatically evaluate video generative
models for their intrinsic faithfulness. VBench-2.0 assesses five key
dimensions: Human Fidelity, Controllability, Creativity, Physics, and
Commonsense, each further broken down into fine-grained capabilities. Tailored
for individual dimensions, our evaluation framework integrates generalists such
as state-of-the-art VLMs and LLMs, and specialists, including anomaly detection
methods proposed for video generation. We conduct extensive annotations to
ensure alignment with human judgment. By pushing beyond superficial
faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new
standard for the next generation of video generative models in pursuit of
intrinsic faithfulness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contributions from first two authors. Project page:
  https://vchitect.github.io/VBench-2.0-project/ Code:
  https://github.com/Vchitect/VBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconstructing Humans with a Biomechanically Accurate Skeleton <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Xia, Xiaowei Zhou, Etienne Vouga, Qixing Huang, Georgios Pavlakos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a method for reconstructing 3D humans from a
single image using a biomechanically accurate skeleton model. To achieve this,
we train a transformer that takes an image as input and estimates the
parameters of the model. Due to the lack of training data for this task, we
build a pipeline to produce pseudo ground truth model parameters for single
images and implement a training procedure that iteratively refines these pseudo
labels. Compared to state-of-the-art methods for 3D human mesh recovery, our
model achieves competitive performance on standard benchmarks, while it
significantly outperforms them in settings with extreme 3D poses and
viewpoints. Additionally, we show that previous reconstruction methods
frequently violate joint angle limits, leading to unnatural rotations. In
contrast, our approach leverages the biomechanically plausible degrees of
freedom making more realistic joint rotation estimates. We validate our
approach across multiple human pose estimation benchmarks. We make the code,
models and data available at: https://isshikihugh.github.io/HSMR/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Project Webpage: https://isshikihugh.github.io/HSMR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LeX-Art: Rethinking Text Generation via Scalable High-Quality Data
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shitian Zhao, Qilong Wu, Xinyue Li, Bo Zhang, Ming Li, Qi Qin, Dongyang Liu, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Peng Gao, Bin Fu, Zhen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LeX-Art, a comprehensive suite for high-quality text-image
synthesis that systematically bridges the gap between prompt expressiveness and
text rendering fidelity. Our approach follows a data-centric paradigm,
constructing a high-quality data synthesis pipeline based on Deepseek-R1 to
curate LeX-10K, a dataset of 10K high-resolution, aesthetically refined
1024$\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer,
a robust prompt enrichment model, and train two text-to-image models, LeX-FLUX
and LeX-Lumina, achieving state-of-the-art text rendering performance. To
systematically evaluate visual text generation, we introduce LeX-Bench, a
benchmark that assesses fidelity, aesthetics, and alignment, complemented by
Pairwise Normalized Edit Distance (PNED), a novel metric for robust text
accuracy evaluation. Experiments demonstrate significant improvements, with
LeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX
outperforming baselines in color (+3.18%), positional (+4.45%), and font
accuracy (+3.81%). Our codes, models, datasets, and demo are publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zhaoshitian.github.io/lexart/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTRL-O: Language-Controllable Object-Centric <span class="highlight-title">Visual</span> Representation
  Learning <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Didolkar, Andrii Zadaianchuk, Rabiul Awal, Maximilian Seitzer, Efstratios Gavves, Aishwarya Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric representation learning aims to decompose visual scenes into
fixed-size vectors called "slots" or "object files", where each slot captures a
distinct object. Current state-of-the-art object-centric models have shown
remarkable success in object discovery in diverse domains, including complex
real-world scenes. However, these models suffer from a key limitation: they
lack controllability. Specifically, current object-centric models learn
representations based on their preconceived understanding of objects, without
allowing user input to guide which objects are represented. Introducing
controllability into object-centric models could unlock a range of useful
capabilities, such as the ability to extract instance-specific representations
from a scene. In this work, we propose a novel approach for user-directed
control over slot representations by conditioning slots on language
descriptions. The proposed ConTRoLlable Object-centric representation learning
approach, which we term CTRL-O, achieves targeted object-language binding in
complex real-world scenes without requiring mask supervision. Next, we apply
these controllable slot representations on two downstream vision language
tasks: text-to-image generation and visual question answering. The proposed
approach enables instance-specific text-to-image generation and also achieves
strong performance on visual question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Zhang, Mengchen Zhang, Tong Wu, Tengfei Wang, Gordon Wetzstein, Dahua Lin, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D generation is experiencing rapid advancements, while the development of 3D
evaluation has not kept pace. How to keep automatic evaluation equitably
aligned with human perception has become a well-recognized challenge. Recent
advances in the field of language and image generation have explored human
preferences and showcased respectable fitting ability. However, the 3D domain
still lacks such a comprehensive preference dataset over generative models. To
mitigate this absence, we develop 3DGen-Arena, an integrated platform in a
battle manner. Then, we carefully design diverse text and image prompts and
leverage the arena platform to gather human preferences from both public users
and expert annotators, resulting in a large-scale multi-dimension human
preference dataset 3DGen-Bench. Using this dataset, we further train a
CLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator,
3DGen-Eval. These two models innovatively unify the quality evaluation of
text-to-3D and image-to-3D generation, and jointly form our automated
evaluation system with their respective strengths. Extensive experiments
demonstrate the efficacy of our scoring model in predicting human preferences,
exhibiting a superior correlation with human ranks compared to existing
metrics. We believe that our 3DGen-Bench dataset and automated evaluation
system will foster a more equitable evaluation in the field of 3D generation,
further promoting the development of 3D generative models and their downstream
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, Yangguang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating high-fidelity 3D meshes with arbitrary topology, including open
surfaces and complex interiors, remains a significant challenge. Existing
implicit field methods often require costly and detail-degrading watertight
conversion, while other approaches struggle with high resolutions. This paper
introduces SparseFlex, a novel sparse-structured isosurface representation that
enables differentiable mesh reconstruction at resolutions up to $1024^3$
directly from rendering losses. SparseFlex combines the accuracy of Flexicubes
with a sparse voxel structure, focusing computation on surface-adjacent regions
and efficiently handling open surfaces. Crucially, we introduce a frustum-aware
sectional voxel training strategy that activates only relevant voxels during
rendering, dramatically reducing memory consumption and enabling
high-resolution training. This also allows, for the first time, the
reconstruction of mesh interiors using only rendering supervision. Building
upon this, we demonstrate a complete shape modeling pipeline by training a
variational autoencoder (VAE) and a rectified flow transformer for high-quality
3D shape generation. Our experiments show state-of-the-art reconstruction
accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in
F-score compared to previous methods, and demonstrate the generation of
high-resolution, detailed 3D shapes with arbitrary topology. By enabling
high-resolution, differentiable mesh reconstruction and generation with
rendering losses, SparseFlex significantly advances the state-of-the-art in 3D
shape representation and modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://xianglonghe.github.io/TripoSF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OccRobNet : Occlusion Robust Network for Accurate 3D Interacting
  Hand-Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mallika Garg, Debashis Ghosh, Pyari Mohan Pradhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Occlusion is one of the challenging issues when estimating 3D hand pose. This
problem becomes more prominent when hand interacts with an object or two hands
are involved. In the past works, much attention has not been given to these
occluded regions. But these regions contain important and beneficial
information that is vital for 3D hand pose estimation. Thus, in this paper, we
propose an occlusion robust and accurate method for the estimation of 3D
hand-object pose from the input RGB image. Our method includes first localising
the hand joints using a CNN based model and then refining them by extracting
contextual information. The self attention transformer then identifies the
specific joints along with the hand identity. This helps the model to identify
the hand belongingness of a particular joint which helps to detect the joint
even in the occluded region. Further, these joints with hand identity are then
used to estimate the pose using cross attention mechanism. Thus, by identifying
the joints in the occluded region, the obtained network becomes robust to
occlusion. Hence, this network achieves state-of-the-art results when evaluated
on the InterHand2.6M, HO3D and H$_2$O3D datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NATIONAL CONFERENCE ON COMMUNICATIONS (NCC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Text-to-Image Synthesis with a Conditional Fréchet
  Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaywon Koo, Jefferson Hernandez, Moayed Haji-Ali, Ziyan Yang, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating text-to-image synthesis is challenging due to misalignment between
established metrics and human preferences. We propose cFreD, a metric based on
the notion of Conditional Fr\'echet Distance that explicitly accounts for both
visual fidelity and text-prompt alignment. Existing metrics such as Inception
Score (IS), Fr\'echet Inception Distance (FID) and CLIPScore assess either
image quality or image-text alignment but not both which limits their
correlation with human preferences. Scoring models explicitly trained to
replicate human preferences require constant updates and may not generalize to
novel generation techniques or out-of-domain inputs. Through extensive
experiments across multiple recently proposed text-to-image models and diverse
prompt datasets, we demonstrate that cFreD exhibits a higher correlation with
human judgments compared to statistical metrics, including metrics trained with
human preferences. Our findings validate cFreD as a robust, future-proof metric
for the systematic evaluation of text-to-image models, standardizing
benchmarking in this rapidly evolving field. We release our evaluation toolkit
and benchmark in the appendix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAVERIX: Multimodal Audio-<span class="highlight-title">Visual</span> Evaluation Reasoning IndeX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liuyue Xie, George Z. Wei, Avik Kuthiala, Ce Zheng, Ananya Bal, Mosam Dabhi, Liting Wen, Taru Rustagi, Ethan Lai, Sushil Khyalia, Rohan Choudhury, Morteza Ziyadi, Xu Zhang, Hao Yang, László A. Jeni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frontier models have either been language-only or have primarily focused on
vision and language modalities. Although recent advancements in models with
vision and audio understanding capabilities have shown substantial progress,
the field lacks a standardized evaluation framework for thoroughly assessing
their cross-modality perception performance. We introduce MAVERIX~(Multimodal
Audio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and
2,556 questions explicitly designed to evaluate multimodal models through tasks
that necessitate close integration of video and audio information. MAVERIX
uniquely provides models with audiovisual tasks, closely mimicking the
multimodal perceptual experiences available to humans during inference and
decision-making processes. To our knowledge, MAVERIX is the first benchmark
aimed explicitly at assessing comprehensive audiovisual integration.
Experiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show
performance approaching human levels (around 70% accuracy), while human experts
reach near-ceiling performance (95.1%). With standardized evaluation protocols,
a rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a
challenging testbed for advancing audiovisual multimodal intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied-Reasoner: Synergizing <span class="highlight-title">Visual</span> Search, Reasoning, and Action for
  Embodied Interactive Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, Weiming Lu, Peng Li, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep thinking models have demonstrated remarkable
reasoning capabilities on mathematical and coding tasks. However, their
effectiveness in embodied domains which require continuous interaction with
environments through image action interleaved trajectories remains largely
-unexplored. We present Embodied Reasoner, a model that extends o1 style
reasoning to interactive embodied search tasks. Unlike mathematical reasoning
that relies primarily on logical deduction, embodied scenarios demand spatial
understanding, temporal reasoning, and ongoing self-reflection based on
interaction history. To address these challenges, we synthesize 9.3k coherent
Observation-Thought-Action trajectories containing 64k interactive images and
90k diverse thinking processes (analysis, spatial reasoning, reflection,
planning, and verification). We develop a three-stage training pipeline that
progressively enhances the model's capabilities through imitation learning,
self-exploration via rejection sampling, and self-correction through reflection
tuning. The evaluation shows that our model significantly outperforms those
advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and
Claude-3.7 by +9\%, 24\%, and +13\%. Analysis reveals our model exhibits fewer
repeated searches and logical inconsistencies, with particular advantages in
complex long-horizon tasks. Real-world environments also show our superiority
while exhibiting fewer repeated searches and logical inconsistency cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/zwq2018/embodied_reasoner Dataset:
  https://huggingface.co/datasets/zwq2018/embodied_reasoner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model
  for High-Fidelity Histology Nuclei Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahe Qian, Yaoyu Fang, Jinkui Hao, Bo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of cell nuclei in histopathology images is essential
for numerous biomedical research and clinical applications. However, existing
cell nucleus segmentation methods only consider a single dataset (i.e., primary
domain), while neglecting to leverage supplementary data from diverse sources
(i.e., auxiliary domains) to reduce overfitting and enhance the performance.
Although incorporating multiple datasets could alleviate overfitting, it often
exacerbates performance drops caused by domain shifts. In this work, we
introduce Adversarial Multi-domain Alignment of Segment Anything Model
(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these
obstacles through two key innovations. First, we propose a Conditional Gradient
Reversal Layer (CGRL), a multi-domain alignment module that harmonizes features
from diverse domains to promote domain-invariant representation learning while
preserving crucial discriminative features for the primary dataset. Second, we
address SAM's inherent low-resolution output by designing a High-Resolution
Decoder (HR-Decoder), which directly produces fine-grained segmentation maps in
order to capture intricate nuclei boundaries in high-resolution histology
images. To the best of our knowledge, this is the first attempt to adapt SAM
for multi-dataset learning with application to histology nuclei segmentation.
We validate our method on several publicly available datasets, demonstrating
consistent and significant improvements over state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Rendering Distillation: Adapting Stable <span class="highlight-title">Diffusion</span> for
  Instant Text-to-Mesh Generation without 3D Data <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Ma, Xinyue Liang, Rongyuan Wu, Xiangyu Zhu, Zhen Lei, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is highly desirable to obtain a model that can generate high-quality 3D
meshes from text prompts in just seconds. While recent attempts have adapted
pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into
generators of 3D representations (e.g., Triplane), they often suffer from poor
quality due to the lack of sufficient high-quality 3D training data. Aiming at
overcoming the data shortage, we propose a novel training scheme, termed as
Progressive Rendering Distillation (PRD), eliminating the need for 3D
ground-truths by distilling multi-view diffusion models and adapting SD into a
native 3D generator. In each iteration of training, PRD uses the U-Net to
progressively denoise the latent from random noise for a few steps, and in each
step it decodes the denoised latent into 3D output. Multi-view diffusion
models, including MVDream and RichDreamer, are used in joint with SD to distill
text-consistent textures and geometries into the 3D outputs through score
distillation. Since PRD supports training without 3D ground-truths, we can
easily scale up the training data and improve generation quality for
challenging text prompts with creative concepts. Meanwhile, PRD can accelerate
the inference speed of the generation model in just a few steps. With PRD, we
train a Triplane generator, namely TriplaneTurbo, which adds only $2.5\%$
trainable parameters to adapt SD for Triplane generation. TriplaneTurbo
outperforms previous text-to-3D generators in both efficiency and quality.
Specifically, it can produce high-quality 3D meshes in 1.2 seconds and
generalize well for challenging text input. The code is available at
https://github.com/theEricMa/TriplaneTurbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025.
  Code:https://github.com/theEricMa/TriplaneTurbo.
  Demo:https://huggingface.co/spaces/ZhiyuanthePony/TriplaneTurbo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RapidPoseTriangulation: Multi-view Multi-person Whole-body Human Pose
  Triangulation in a Millisecond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Bermuth, Alexander Poeppel, Wolfgang Reif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of multi-view imaging and pose estimation represents a
significant advance in computer vision applications, offering new possibilities
for understanding human movement and interactions. This work presents a new
algorithm that improves multi-view multi-person pose estimation, focusing on
fast triangulation speeds and good generalization capabilities. The approach
extends to whole-body pose estimation, capturing details from facial
expressions to finger movements across multiple individuals and viewpoints.
Adaptability to different settings is demonstrated through strong performance
across unseen datasets and configurations. To support further progress in this
field, all of this work is publicly accessible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMED: A Child Micro-Expression <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Nikin~Matharaarachchi, Muhammad~Fermi Pasha,  Sonya~Coleman, Kah PengWong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Micro-expressions are short bursts of emotion that are difficult to hide.
Their detection in children is an important cue to assist psychotherapists in
conducting better therapy. However, existing research on the detection of
micro-expressions has focused on adults, whose expressions differ in their
characteristics from those of children. The lack of research is a direct
consequence of the lack of a child-based micro-expressions dataset as it is
much more challenging to capture children's facial expressions due to the lack
of predictability and controllability. This study compiles a dataset of
spontaneous child micro-expression videos, the first of its kind, to the best
of the authors knowledge. The dataset is captured in the wild using video
conferencing software. This dataset enables us to then explore key features and
differences between adult and child micro-expressions. This study also
establishes a baseline for the automated spotting and recognition of
micro-expressions in children using three approaches comprising of hand-created
and learning-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cognitive Science-Inspired Evaluation of Core Capabilities for Object
  Understanding in AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danaja Rutar, Alva Markelius, Konstantinos Voudouris, José Hernández-Orallo, Lucy Cheke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the core components of our world models is 'intuitive physics' - an
understanding of objects, space, and causality. This capability enables us to
predict events, plan action and navigate environments, all of which rely on a
composite sense of objecthood. Despite its importance, there is no single,
unified account of objecthood, though multiple theoretical frameworks provide
insights. In the first part of this paper, we present a comprehensive overview
of the main theoretical frameworks in objecthood research - Gestalt psychology,
enactive cognition, and developmental psychology - and identify the core
capabilities each framework attributes to object understanding, as well as what
functional roles they play in shaping world models in biological agents. Given
the foundational role of objecthood in world modelling, understanding
objecthood is also essential in AI. In the second part of the paper, we
evaluate how current AI paradigms approach and test objecthood capabilities
compared to those in cognitive science. We define an AI paradigm as a
combination of how objecthood is conceptualised, the methods used for studying
objecthood, the data utilised, and the evaluation techniques. We find that,
whilst benchmarks can detect that AI systems model isolated aspects of
objecthood, the benchmarks cannot detect when AI systems lack functional
integration across these capabilities, not solving the objecthood challenge
fully. Finally, we explore novel evaluation approaches that align with the
integrated vision of objecthood outlined in this paper. These methods are
promising candidates for advancing from isolated object capabilities toward
general-purpose AI with genuine object understanding in real-world contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InteractionMap: Improving Online Vectorized HDMap Construction with
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuang Wu, Chuan Yang, Zhanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vectorized high-definition (HD) maps are essential for an autonomous driving
system. Recently, state-of-the-art map vectorization methods are mainly based
on DETR-like framework to generate HD maps in an end-to-end manner. In this
paper, we propose InteractionMap, which improves previous map vectorization
methods by fully leveraging local-to-global information interaction in both
time and space. Firstly, we explore enhancing DETR-like detectors by explicit
position relation prior from point-level to instance-level, since map elements
contain strong shape priors. Secondly, we propose a key-frame-based
hierarchical temporal fusion module, which interacts temporal information from
local to global. Lastly, the separate classification branch and regression
branch lead to the problem of misalignment in the output distribution. We
interact semantic information with geometric information by introducing a novel
geometric-aware classification loss in optimization and a geometric-aware
matching cost in label assignment. InteractionMap achieves state-of-the-art
performance on both nuScenes and Argoverse2 benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in
  Morocco 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassir Lairgi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate determination of the beginning of each Hijri month is essential
for religious, cultural, and administrative purposes. Manazel (The code and
datasets are available at https://github.com/lairgiyassir/manazel) addresses
this challenge in Morocco by leveraging 13 years of crescent visibility data to
refine the ODEH criterion, a widely used standard for lunar crescent visibility
prediction. The study integrates two key features, the Arc of Vision (ARCV) and
the total width of the crescent (W), to enhance the accuracy of lunar
visibility assessments. A machine learning approach utilizing the Logistic
Regression algorithm is employed to classify crescent visibility conditions,
achieving a predictive accuracy of 98.83%. This data-driven methodology offers
a robust and reliable framework for determining the start of the Hijri month,
comparing different data classification tools, and improving the consistency of
lunar calendar calculations in Morocco. The findings demonstrate the
effectiveness of machine learning in astronomical applications and highlight
the potential for further enhancements in the modeling of crescent visibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The MVTec AD 2 <span class="highlight-title">Dataset</span>: Advanced Scenarios for Unsupervised Anomaly
  Detection <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Heckler-Kram, Jan-Hendrik Neudeck, Ulla Scheler, Rebecca König, Carsten Steger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, performance on existing anomaly detection benchmarks like
MVTec AD and VisA has started to saturate in terms of segmentation AU-PRO, with
state-of-the-art models often competing in the range of less than one
percentage point. This lack of discriminatory power prevents a meaningful
comparison of models and thus hinders progress of the field, especially when
considering the inherent stochastic nature of machine learning results. We
present MVTec AD 2, a collection of eight anomaly detection scenarios with more
than 8000 high-resolution images. It comprises challenging and highly relevant
industrial inspection use cases that have not been considered in previous
datasets, including transparent and overlapping objects, dark-field and back
light illumination, objects with high variance in the normal data, and
extremely small defects. We provide comprehensive evaluations of
state-of-the-art methods and show that their performance remains below 60%
average AU-PRO. Additionally, our dataset provides test scenarios with lighting
condition changes to assess the robustness of methods under real-world
distribution shifts. We host a publicly accessible evaluation server that holds
the pixel-precise ground truth of the test set (https://benchmark.mvtec.com/).
All image data is available at
https://www.mvtec.com/company/research/datasets/mvtec-ad-2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>paper under review; dataset first released for the VAND3.0 challenge
  @ CVPR 2025 https://sites.google.com/view/vand30cvpr2025/challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-driven Gesture Generation via Deviation Feature in the Latent
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Chen, Yang Huan, Runhua Shi, Chanfan Ding, Xiaoqi Mo, Siyu Xiong, Yinong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gestures are essential for enhancing co-speech communication, offering visual
emphasis and complementing verbal interactions. While prior work has
concentrated on point-level motion or fully supervised data-driven methods, we
focus on co-speech gestures, advocating for weakly supervised learning and
pixel-level motion deviations. We introduce a weakly supervised framework that
learns latent representation deviations, tailored for co-speech gesture video
generation. Our approach employs a diffusion model to integrate latent motion
features, enabling more precise and nuanced gesture representation. By
leveraging weakly supervised deviations in latent space, we effectively
generate hand gestures and mouth movements, crucial for realistic video
production. Experiments show our method significantly improves video quality,
surpassing current state-of-the-art techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FusionSegReID: Advancing Person Re-Identification with Multimodal
  Retrieval and Precise Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Yan, Yun Wang, Xiaoyan Luo, Yu-Wing Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person re-identification (ReID) plays a critical role in applications like
security surveillance and criminal investigations by matching individuals
across large image galleries captured by non-overlapping cameras. Traditional
ReID methods rely on unimodal inputs, typically images, but face limitations
due to challenges like occlusions, lighting changes, and pose variations. While
advancements in image-based and text-based ReID systems have been made, the
integration of both modalities has remained under-explored. This paper presents
FusionSegReID, a multimodal model that combines both image and text inputs for
enhanced ReID performance. By leveraging the complementary strengths of these
modalities, our model improves matching accuracy and robustness, particularly
in complex, real-world scenarios where one modality may struggle. Our
experiments show significant improvements in Top-1 accuracy and mean Average
Precision (mAP) for ReID, as well as better segmentation results in challenging
scenarios like occlusion and low-quality images. Ablation studies further
confirm that multimodal fusion and segmentation modules contribute to enhanced
re-identification and mask accuracy. The results show that FusionSegReID
outperforms traditional unimodal models, offering a more robust and flexible
solution for real-world person ReID tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlignDiff: Learning Physically-Grounded <span class="highlight-title">Camera</span> Alignment via <span class="highlight-title">Diffusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liuyue Xie, Jiancong Guo, Ozan Cakmakci, Andre Araujo, Laszlo A. Jeni, Zhiheng Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate camera calibration is a fundamental task for 3D perception,
especially when dealing with real-world, in-the-wild environments where complex
optical distortions are common. Existing methods often rely on pre-rectified
images or calibration patterns, which limits their applicability and
flexibility. In this work, we introduce a novel framework that addresses these
challenges by jointly modeling camera intrinsic and extrinsic parameters using
a generic ray camera model. Unlike previous approaches, AlignDiff shifts focus
from semantic to geometric features, enabling more accurate modeling of local
distortions. We propose AlignDiff, a diffusion model conditioned on geometric
priors, enabling the simultaneous estimation of camera distortions and scene
geometry. To enhance distortion prediction, we incorporate edge-aware
attention, focusing the model on geometric features around image edges, rather
than semantic content. Furthermore, to enhance generalizability to real-world
captures, we incorporate a large database of ray-traced lenses containing over
three thousand samples. This database characterizes the distortion inherent in
a diverse variety of lens forms. Our experiments demonstrate that the proposed
method significantly reduces the angular error of estimated ray bundles by ~8.2
degrees and overall calibration accuracy, outperforming existing approaches on
challenging, real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bearing fault diagnosis based on multi-scale spectral images and
  convolutional neural network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongchao Luo, Mingquan Qiu, Zhenyu Wu, Zebo Zhao, Dingyou Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the challenges of low diagnostic accuracy in traditional bearing
fault diagnosis methods, this paper proposes a novel fault diagnosis approach
based on multi-scale spectrum feature images and deep learning. Firstly, the
vibration signal are preprocessed through mean removal and then converted to
multi-length spectrum with fast Fourier transforms (FFT). Secondly, a novel
feature called multi-scale spectral image (MSSI) is constructed by multi-length
spectrum paving scheme. Finally, a deep learning framework, convolutional
neural network (CNN), is formulated to diagnose the bearing faults. Two
experimental cases are utilized to verify the effectiveness of the proposed
method. Experimental results demonstrate that the proposed method significantly
improves the accuracy of fault diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12pages, 10 figures and 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ uLayout: Unified Room Layout Estimation for Perspective and Panoramic
  Images <span class="chip">WACV-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Lee, Bolivar Solarte, Chin-Hsuan Wu, Jin-Cheng Jhang, Fu-En Wang, Yi-Hsuan Tsai, Min Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present uLayout, a unified model for estimating room layout geometries
from both perspective and panoramic images, whereas traditional solutions
require different model designs for each image type. The key idea of our
solution is to unify both domains into the equirectangular projection,
particularly, allocating perspective images into the most suitable latitude
coordinate to effectively exploit both domains seamlessly. To address the
Field-of-View (FoV) difference between the input domains, we design uLayout
with a shared feature extractor with an extra 1D-Convolution layer to condition
each domain input differently. This conditioning allows us to efficiently
formulate a column-wise feature regression problem regardless of the FoV input.
This simple yet effective approach achieves competitive performance with
current state-of-the-art solutions and shows for the first time a single
end-to-end model for both domains. Extensive experiments in the real-world
datasets, LSUN, Matterport3D, PanoContext, and Stanford 2D-3D evidence the
contribution of our approach. Code is available at
https://github.com/JonathanLee112/uLayout.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SyncSDE: A Probabilistic Framework for <span class="highlight-title">Diffusion</span> Synchronization <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunjun Lee, Hyunsoo Lee, Sookwan Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been many attempts to leverage multiple diffusion models for
collaborative generation, extending beyond the original domain. A prominent
approach involves synchronizing multiple diffusion trajectories by mixing the
estimated scores to artificially correlate the generation processes. However,
existing methods rely on naive heuristics, such as averaging, without
considering task specificity. These approaches do not clarify why such methods
work and often fail when a heuristic suitable for one task is blindly applied
to others. In this paper, we present a probabilistic framework for analyzing
why diffusion synchronization works and reveal where heuristics should be
focused - modeling correlations between multiple trajectories and adapting them
to each specific task. We further identify optimal correlation models per task,
achieving better results than previous approaches that apply a single heuristic
across all tasks without justification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized
  Text-Guided Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achint Soni, Meet Soni, Sirisha Rambhatla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided image editing aims to modify specific regions of an image
according to natural language instructions while maintaining the general
structure and the background fidelity. Existing methods utilize masks derived
from cross-attention maps generated from diffusion models to identify the
target regions for modification. However, since cross-attention mechanisms
focus on semantic relevance, they struggle to maintain the image integrity. As
a result, these methods often lack spatial consistency, leading to editing
artifacts and distortions. In this work, we address these limitations and
introduce LOCATEdit, which enhances cross-attention maps through a graph-based
approach utilizing self-attention-derived patch relationships to maintain
smooth, coherent attention across image regions, ensuring that alterations are
limited to the designated items while retaining the surrounding structure.
\method consistently and substantially outperforms existing baselines on
PIE-Bench, demonstrating its state-of-the-art performance and effectiveness on
various editing tasks. Code can be found on
https://github.com/LOCATEdit/LOCATEdit/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICG-MVSNet: Learning Intra-view and Cross-view Relationships for
  Guidance in Multi-View Stereo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Hu, Jun Zhang, Zhe Zhang, Rafael Weilharter, Yuchen Rao, Kuangyi Chen, Runze Yuan, Friedrich Fraundorfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view Stereo (MVS) aims to estimate depth and reconstruct 3D point
clouds from a series of overlapping images. Recent learning-based MVS
frameworks overlook the geometric information embedded in features and
correlations, leading to weak cost matching. In this paper, we propose
ICG-MVSNet, which explicitly integrates intra-view and cross-view relationships
for depth estimation. Specifically, we develop an intra-view feature fusion
module that leverages the feature coordinate correlations within a single image
to enhance robust cost matching. Additionally, we introduce a lightweight
cross-view aggregation module that efficiently utilizes the contextual
information from volume correlations to guide regularization. Our method is
evaluated on the DTU dataset and Tanks and Temples benchmark, consistently
achieving competitive performance against state-of-the-art works, while
requiring lower computational resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-aware Bayesian machine learning modelling of land cover
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Bilson, Anna Pustogvar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Land cover classification involves the production of land cover maps, which
determine the type of land through remote sensing imagery. Over recent years,
such classification is being performed by machine learning classification
models, which can give highly accurate predictions on land cover per pixel
using large quantities of input training data. However, such models do not
currently take account of input measurement uncertainty, which is vital for
traceability in metrology. In this work we propose a Bayesian classification
framework using generative modelling to take account of input measurement
uncertainty. We take the specific case of Bayesian quadratic discriminant
analysis, and apply it to land cover datasets from Copernicus Sentinel-2 in
2020 and 2021. We benchmark the performance of the model against more popular
classification models used in land cover maps such as random forests and neural
networks. We find that such Bayesian models are more trustworthy, in the sense
that they are more interpretable, explicitly model the input measurement
uncertainty, and maintain predictive performance of class probability outputs
across datasets of different years and sizes, whilst also being computationally
efficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Evaluation of Large <span class="highlight-title">Vision</span>-Language Models in Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Li, Meng Tian, Zhenyu Lin, Jiangtong Zhu, Dechang Zhu, Haiqiang Liu, Zining Wang, Yueyi Zhang, Zhiwei Xiong, Xinhai Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for Vision-Language Model (VLM) on autonomous driving
(AD) primarily assess interpretability through open-form visual question
answering (QA) within coarse-grained tasks, which remain insufficient to assess
capabilities in complex driving scenarios. To this end, we introduce
$\textbf{VLADBench}$, a challenging and fine-grained dataset featuring
close-form QAs that progress from static foundational knowledge and elements to
advanced reasoning for dynamic on-road situations. The elaborate
$\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,
General Element Recognition, Traffic Graph Generation, Target Attribute
Comprehension, and Ego Decision-Making and Planning. These domains are further
broken down into 11 secondary aspects and 29 tertiary tasks for a granular
evaluation. A thorough assessment of general and domain-specific (DS) VLMs on
this benchmark reveals both their strengths and critical limitations in AD
contexts. To further exploit the cognitive and reasoning interactions among the
5 domains for AD understanding, we start from a small-scale VLM and train the
DS models on individual domain datasets (collected from 1.4M DS QAs across
public sources). The experimental results demonstrate that the proposed
benchmark provides a crucial step toward a more comprehensive assessment of
VLMs in AD, paving the way for the development of more cognitively
sophisticated and reasoning-capable AD systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keyword-Oriented Multimodal Modeling for Euphemism Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxue Hu, Junsong Li, Meixuan Chen, Dongyu Su, Tongguan Wang, Ying Sha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Euphemism identification deciphers the true meaning of euphemisms, such as
linking "weed" (euphemism) to "marijuana" (target keyword) in illicit texts,
aiding content moderation and combating underground markets. While existing
methods are primarily text-based, the rise of social media highlights the need
for multimodal analysis, incorporating text, images, and audio. However, the
lack of multimodal datasets for euphemisms limits further research. To address
this, we regard euphemisms and their corresponding target keywords as keywords
and first introduce a keyword-oriented multimodal corpus of euphemisms
(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including
text, images, and speech. We further propose a keyword-oriented multimodal
euphemism identification method (KOM-EI), which uses cross-modal feature
alignment and dynamic fusion modules to explicitly utilize the visual and audio
features of the keywords for efficient euphemism identification. Extensive
experiments demonstrate that KOM-EI outperforms state-of-the-art models and
large language models, and show the importance of our multimodal datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Double Blind Imaging with Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brett Levac, Ajil Jalal, Kannan Ramchandran, Jonathan I. Tamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind inverse problems in imaging arise from uncertainties in the system used
to collect (noisy) measurements of images. Recovering clean images from these
measurements typically requires identifying the imaging system, either
implicitly or explicitly. A common solution leverages generative models as
priors for both the images and the imaging system parameters (e.g., a class of
point spread functions). To learn these priors in a straightforward manner
requires access to a dataset of clean images as well as samples of the imaging
system. We propose an AmbientGAN-based generative technique to identify the
distribution of parameters in unknown imaging systems, using only unpaired
clean images and corrupted measurements. This learned distribution can then be
used in model-based recovery algorithms to solve blind inverse problems such as
blind deconvolution. We successfully demonstrate our technique for learning
Gaussian blur and motion blur priors from noisy measurements and show their
utility in solving blind deconvolution with diffusion posterior sampling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shape Modeling of Longitudinal Medical Images: From Diffeomorphic Metric
  Mapping to Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edwin Tay, Nazli Tümer, Amir A. Zadpoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Living biological tissue is a complex system, constantly growing and changing
in response to external and internal stimuli. These processes lead to
remarkable and intricate changes in shape. Modeling and understanding both
natural and pathological (or abnormal) changes in the shape of anatomical
structures is highly relevant, with applications in diagnostic, prognostic, and
therapeutic healthcare. Nevertheless, modeling the longitudinal shape change of
biological tissue is a non-trivial task due to its inherent nonlinear nature.
In this review, we highlight several existing methodologies and tools for
modeling longitudinal shape change (i.e., spatiotemporal shape modeling). These
methods range from diffeomorphic metric mapping to deep-learning based
approaches (e.g., autoencoders, generative networks, recurrent neural networks,
etc.). We discuss the synergistic combinations of existing technologies and
potential directions for future research, underscoring key deficiencies in the
current research landscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Invert2Restore: Zero-Shot Degradation-Blind Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamadi Chihaoui, Paolo Favaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two of the main challenges of image restoration in real-world scenarios are
the accurate characterization of an image prior and the precise modeling of the
image degradation operator. Pre-trained diffusion models have been very
successfully used as image priors in zero-shot image restoration methods.
However, how to best handle the degradation operator is still an open problem.
In real-world data, methods that rely on specific parametric assumptions about
the degradation model often face limitations in their applicability. To address
this, we introduce Invert2Restore, a zero-shot, training-free method that
operates in both fully blind and partially blind settings -- requiring no prior
knowledge of the degradation model or only partial knowledge of its parametric
form without known parameters. Despite this, Invert2Restore achieves
high-fidelity results and generalizes well across various types of image
degradation. It leverages a pre-trained diffusion model as a deterministic
mapping between normal samples and undistorted image samples. The key insight
is that the input noise mapped by a diffusion model to a degraded image lies in
a low-probability density region of the standard normal distribution. Thus, we
can restore the degraded image by carefully guiding its input noise toward a
higher-density region. We experimentally validate Invert2Restore across several
image restoration tasks, demonstrating that it achieves state-of-the-art
performance in scenarios where the degradation operator is either unknown or
partially known.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOLT: Boost Large <span class="highlight-title">Vision</span>-Language Model Without Training for Long-form
  Video Understanding <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuming Liu, Chen Zhao, Tianqi Xu, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large video-language models (VLMs) have demonstrated promising progress in
various video understanding tasks. However, their effectiveness in long-form
video analysis is constrained by limited context windows. Traditional
approaches, such as uniform frame sampling, often inevitably allocate resources
to irrelevant content, diminishing their effectiveness in real-world scenarios.
In this paper, we introduce BOLT, a method to BOost Large VLMs without
additional Training through a comprehensive study of frame selection
strategies. First, to enable a more realistic evaluation of VLMs in long-form
video understanding, we propose a multi-source retrieval evaluation setting.
Our findings reveal that uniform sampling performs poorly in noisy contexts,
underscoring the importance of selecting the right frames. Second, we explore
several frame selection strategies based on query-frame similarity and analyze
their effectiveness at inference time. Our results show that inverse transform
sampling yields the most significant performance improvement, increasing
accuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from
58.9% to 63.4%. Our code is available at https://github.com/sming256/BOLT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Behavior and Lane Constraints Guided Trajectory Prediction
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyi Xiong, Jian Chen, Ziheng Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction, as a critical component of autonomous driving systems,
has attracted the attention of many researchers. Existing prediction algorithms
focus on extracting more detailed scene features or selecting more reasonable
trajectory destinations. However, in the face of dynamic and evolving future
movements of the target vehicle, these algorithms cannot provide a fine-grained
and continuous description of future behaviors and lane constraints, which
degrades the prediction accuracy. To address this challenge, we present BLNet,
a novel dualstream architecture that synergistically integrates behavioral
intention recognition and lane constraint modeling through parallel attention
mechanisms. The framework generates fine-grained behavior state queries
(capturing spatial-temporal movement patterns) and lane queries (encoding lane
topology constraints), supervised by two auxiliary losses, respectively.
Subsequently, a two-stage decoder first produces trajectory proposals, then
performs point-level refinement by jointly incorporating both the continuity of
passed lanes and future motion features. Extensive experiments on two large
datasets, nuScenes and Argoverse, show that our network exhibits significant
performance gains over existing direct regression and goal-based algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE TIM for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedding Compression Distortion in Video Coding for Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiao Sun, Yao Zhao, Meiqin Liu, Chao Yao, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, video transmission serves not only the Human Visual System (HVS)
for viewing but also machine perception for analysis. However, existing codecs
are primarily optimized for pixel-domain and HVS-perception metrics rather than
the needs of machine vision tasks. To address this issue, we propose a
Compression Distortion Representation Embedding (CDRE) framework, which
extracts machine-perception-related distortion representation and embeds it
into downstream models, addressing the information lost during compression and
improving task performance. Specifically, to better analyze the
machine-perception-related distortion, we design a compression-sensitive
extractor that identifies compression degradation in the feature domain. For
efficient transmission, a lightweight distortion codec is introduced to
compress the distortion information into a compact representation.
Subsequently, the representation is progressively embedded into the downstream
model, enabling it to be better informed about compression degradation and
enhancing performance. Experiments across various codecs and downstream tasks
demonstrate that our framework can effectively boost the rate-task performance
of existing codecs with minimal overhead in terms of bitrate, execution time,
and number of parameters. Our codes and supplementary materials are released in
https://github.com/Ws-Syx/CDRE/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retinal Fundus Multi-Disease Image Classification using Hybrid
  CNN-Transformer-Ensemble Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deependra Singh, Saksham Agarwal, Subhankar Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our research is motivated by the urgent global issue of a large population
affected by retinal diseases, which are evenly distributed but underserved by
specialized medical expertise, particularly in non-urban areas. Our primary
objective is to bridge this healthcare gap by developing a comprehensive
diagnostic system capable of accurately predicting retinal diseases solely from
fundus images. However, we faced significant challenges due to limited, diverse
datasets and imbalanced class distributions. To overcome these issues, we have
devised innovative strategies. Our research introduces novel approaches,
utilizing hybrid models combining deeper Convolutional Neural Networks (CNNs),
Transformer encoders, and ensemble architectures sequentially and in parallel
to classify retinal fundus images into 20 disease labels. Our overarching goal
is to assess these advanced models' potential in practical applications, with a
strong focus on enhancing retinal disease diagnosis accuracy across a broader
spectrum of conditions. Importantly, our efforts have surpassed baseline model
results, with the C-Tran ensemble model emerging as the leader, achieving a
remarkable model score of 0.9166, surpassing the baseline score of 0.9.
Additionally, experiments with the IEViT model showcased equally promising
outcomes with improved computational efficiency. We've also demonstrated the
effectiveness of dynamic patch extraction and the integration of domain
knowledge in computer vision tasks. In summary, our research strives to
contribute significantly to retinal disease diagnosis, addressing the critical
need for accessible healthcare solutions in underserved regions while aiming
for comprehensive and accurate disease prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures, 7 tables. Conference paper presented at the
  International Health Informatics Conference (IHIC 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoadSocial: A Diverse VideoQA <span class="highlight-title">Dataset</span> and Benchmark for Road Event
  Understanding from Social Video Narratives <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chirag Parikh, Deepti Rawat, Rakshitha R. T., Tathagata Ghosh, Ravi Kiran Sarvadevabhatla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for
generic road event understanding from social media narratives. Unlike existing
datasets limited by regional bias, viewpoint bias and expert-driven
annotations, RoadSocial captures the global complexity of road events with
varied geographies, camera viewpoints (CCTV, handheld, drones) and rich social
discourse. Our scalable semi-automatic annotation framework leverages Text LLMs
and Video LLMs to generate comprehensive question-answer pairs across 12
challenging QA tasks, pushing the boundaries of road event understanding.
RoadSocial is derived from social media videos spanning 14M frames and 414K
social comments, resulting in a dataset with 13.2K videos, 674 tags and 260K
high-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,
driving-specific and general-purpose) on our road event understanding
benchmark. We also demonstrate RoadSocial's utility in improving road event
understanding capabilities of general-purpose Video LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2025; Project Page: https://roadsocial.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaceBench: A Multi-View Multi-Level Facial Attribute VQA <span class="highlight-title">Dataset</span> for
  Benchmarking Face Perception MLLMs <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqin Wang, Xusen Ma, Xianxu Hou, Meidan Ding, Yudong Li, Junliang Chen, Wenting Chen, Xiaoyang Peng, Linlin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities in various tasks. However, effectively evaluating these MLLMs on
face perception remains largely unexplored. To address this gap, we introduce
FaceBench, a dataset featuring hierarchical multi-view and multi-level
attributes specifically designed to assess the comprehensive face perception
abilities of MLLMs. Initially, we construct a hierarchical facial attribute
structure, which encompasses five views with up to three levels of attributes,
totaling over 210 attributes and 700 attribute values. Based on the structure,
the proposed FaceBench consists of 49,919 visual question-answering (VQA) pairs
for evaluation and 23,841 pairs for fine-tuning. Moreover, we further develop a
robust face perception MLLM baseline, Face-LLaVA, by training with our proposed
face VQA data. Extensive experiments on various mainstream MLLMs and Face-LLaVA
are conducted to test their face perception ability, with results also compared
against human performance. The results reveal that, the existing MLLMs are far
from satisfactory in understanding the fine-grained facial attributes, while
our Face-LLaVA significantly outperforms existing open-source models with a
small amount of training data and is comparable to commercial ones like GPT-4o
and Gemini. The dataset will be released at
https://github.com/CVI-SZU/FaceBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Generating Realistic 3D Semantic Training Data for Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Nunes, Rodrigo Marcuzzi, Jens Behley, Cyrill Stachniss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic scene understanding is crucial for robotics and computer vision
applications. In autonomous driving, 3D semantic segmentation plays an
important role for enabling safe navigation. Despite significant advances in
the field, the complexity of collecting and annotating 3D data is a bottleneck
in this developments. To overcome that data annotation limitation, synthetic
simulated data has been used to generate annotated data on demand. There is
still however a domain gap between real and simulated data. More recently,
diffusion models have been in the spotlight, enabling close-to-real data
synthesis. Those generative models have been recently applied to the 3D data
domain for generating scene-scale data with semantic annotations. Still, those
methods either rely on image projection or decoupled models trained with
different resolutions in a coarse-to-fine manner. Such intermediary
representations impact the generated data quality due to errors added in those
transformations. In this work, we propose a novel approach able to generate 3D
semantic scene-scale data without relying on any projection or decoupled
trained multi-resolution models, achieving more realistic semantic scene data
generation compared to previous state-of-the-art methods. Besides improving 3D
semantic scene-scale data synthesis, we thoroughly evaluate the use of the
synthetic scene samples as labeled data to train a semantic segmentation
network. In our experiments, we show that using the synthetic annotated data
generated by our method as training data together with the real semantic
segmentation labels, leads to an improvement in the semantic segmentation model
performance. Our results show the potential of generated scene-scale point
clouds to generate more training data to extend existing datasets, reducing the
data annotation effort. Our code is available at
https://github.com/PRBonn/3DiSS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Bayesian Learning for Label Efficiency in Cardiac Real-Time MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Terhag, Philipp Knechtges, Achim Basermann, Anja Bach, Darius Gerlach, Jens Tank, Raúl Tempone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac real-time magnetic resonance imaging (MRI) is an emerging technology
that images the heart at up to 50 frames per second, offering insight into the
respiratory effects on the heartbeat. However, this method significantly
increases the number of images that must be segmented to derive critical health
indicators. Although neural networks perform well on inner slices, predictions
on outer slices are often unreliable.
  This work proposes sparse Bayesian learning (SBL) to predict the ventricular
volume on outer slices with minimal manual labeling to address this challenge.
The ventricular volume over time is assumed to be dominated by sparse
frequencies corresponding to the heart and respiratory rates. Moreover, SBL
identifies these sparse frequencies on well-segmented inner slices by
optimizing hyperparameters via type -II likelihood, automatically pruning
irrelevant components. The identified sparse frequencies guide the selection of
outer slice images for labeling, minimizing posterior variance.
  This work provides performance guarantees for the greedy algorithm. Testing
on patient data demonstrates that only a few labeled images are necessary for
accurate volume prediction. The labeling procedure effectively avoids selecting
inefficient images. Furthermore, the Bayesian approach provides uncertainty
estimates, highlighting unreliable predictions (e.g., when choosing suboptimal
labels).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyu Dai, Xingyu Ni, Qianfan Shen, Wenzheng Chen, Baoquan Chen, Mengyu Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of adding dynamic rain effects to in-the-wild scenes
in a physically-correct manner. Recent advances in scene modeling have made
significant progress, with NeRF and 3DGS techniques emerging as powerful tools
for reconstructing complex scenes. However, while effective for novel view
synthesis, these methods typically struggle with challenging scene editing
tasks, such as physics-based rain simulation. In contrast, traditional
physics-based simulations can generate realistic rain effects, such as
raindrops and splashes, but they often rely on skilled artists to carefully set
up high-fidelity scenes. This process lacks flexibility and scalability,
limiting its applicability to broader, open-world environments. In this work,
we introduce RainyGS, a novel approach that leverages the strengths of both
physics-based modeling and 3DGS to generate photorealistic, dynamic rain
effects in open-world scenes with physical accuracy. At the core of our method
is the integration of physically-based raindrop and shallow water simulation
techniques within the fast 3DGS rendering framework, enabling realistic and
efficient simulations of raindrop behavior, splashes, and reflections. Our
method supports synthesizing rain effects at over 30 fps, offering users
flexible control over rain intensity -- from light drizzles to heavy downpours.
We demonstrate that RainyGS performs effectively for both real-world outdoor
scenes and large-scale driving scenarios, delivering more photorealistic and
physically-accurate rain effects compared to state-of-the-art methods. Project
page can be found at https://pku-vcl-geometry.github.io/RainyGS/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Task Learning for Dead Tree Detection and Segmentation with Hybrid
  Self-Attention U-Nets in Aerial Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anis Ur Rahman, Einari Heinaro, Mete Ahishali, Samuli Junttila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping standing dead trees is critical for assessing forest health,
monitoring biodiversity, and mitigating wildfire risks, for which aerial
imagery has proven useful. However, dense canopy structures, spectral overlaps
between living and dead vegetation, and over-segmentation errors limit the
reliability of existing methods. This study introduces a hybrid postprocessing
framework that refines deep learning-based tree segmentation by integrating
watershed algorithms with adaptive filtering, enhancing boundary delineation,
and reducing false positives in complex forest environments. Tested on
high-resolution aerial imagery from boreal forests, the framework improved
instance-level segmentation accuracy by 41.5% and reduced positional errors by
57%, demonstrating robust performance in densely vegetated regions. By
balancing detection accuracy and over-segmentation artifacts, the method
enabled the precise identification of individual dead trees, which is critical
for ecological monitoring. The framework's computational efficiency supports
scalable applications, such as wall-to-wall tree mortality mapping over large
geographic regions using aerial or satellite imagery. These capabilities
directly benefit wildfire risk assessment (identifying fuel accumulations),
carbon stock estimation (tracking emissions from decaying biomass), and
precision forestry (targeting salvage loggings). By bridging advanced remote
sensing techniques with practical forest management needs, this work advances
tools for large-scale ecological conservation and climate resilience planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAMICS: Splat, Track And Map with Integrated Consistency and Semantics
  for Dense RGB-D SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxu Wang, Xu Cao, Weiyun Yi, Zhaoxin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) is a critical task in robotics,
enabling systems to autonomously navigate and understand complex environments.
Current SLAM approaches predominantly rely on geometric cues for mapping and
localization, but they often fail to ensure semantic consistency, particularly
in dynamic or densely populated scenes. To address this limitation, we
introduce STAMICS, a novel method that integrates semantic information with 3D
Gaussian representations to enhance both localization and mapping accuracy.
STAMICS consists of three key components: a 3D Gaussian-based scene
representation for high-fidelity reconstruction, a graph-based clustering
technique that enforces temporal semantic consistency, and an open-vocabulary
system that allows for the classification of unseen objects. Extensive
experiments show that STAMICS significantly improves camera pose estimation and
map quality, outperforming state-of-the-art methods while reducing
reconstruction errors. Code will be public available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Diffusion</span> Image Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamadi Chihaoui, Paolo Favaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot image restoration (IR) methods based on pretrained diffusion models
have recently achieved significant success. These methods typically require at
least a parametric form of the degradation model. However, in real-world
scenarios, the degradation may be too complex to define explicitly. To handle
this general case, we introduce the Diffusion Image Prior (DIIP). We take
inspiration from the Deep Image Prior (DIP)[16], since it can be used to remove
artifacts without the need for an explicit degradation model. However, in
contrast to DIP, we find that pretrained diffusion models offer a much stronger
prior, despite being trained without knowledge from corrupted data. We show
that, the optimization process in DIIP first reconstructs a clean version of
the image before eventually overfitting to the degraded input, but it does so
for a broader range of degradations than DIP. In light of this result, we
propose a blind image restoration (IR) method based on early stopping, which
does not require prior knowledge of the degradation model. We validate DIIP on
various degradation-blind IR tasks, including JPEG artifact removal, waterdrop
removal, denoising and super-resolution with state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VALLR: <span class="highlight-title">Visual</span> ASR Language Model for Lip Reading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marshall Thomas, Edward Fish, Richard Bowden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex
task requiring the interpretation of spoken language exclusively from visual
cues, primarily lip movements and facial expressions. This task is especially
challenging due to the absence of auditory information and the inherent
ambiguity when visually distinguishing phonemes that have overlapping visemes
where different phonemes appear identical on the lips. Current methods
typically attempt to predict words or characters directly from these visual
cues, but this approach frequently encounters high error rates due to
coarticulation effects and viseme ambiguity. We propose a novel two-stage,
phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that
addresses these longstanding challenges. First, our model predicts a compact
sequence of phonemes from visual inputs using a Video Transformer with a CTC
head, thereby reducing the task complexity and achieving robust speaker
invariance. This phoneme output then serves as the input to a fine-tuned Large
Language Model (LLM), which reconstructs coherent words and sentences by
leveraging broader linguistic context. Unlike existing methods that either
predict words directly-often faltering on visually similar phonemes-or rely on
large-scale multimodal pre-training, our approach explicitly encodes
intermediate linguistic structure while remaining highly data efficient. We
demonstrate state-of-the-art performance on two challenging datasets, LRS2 and
LRS3, where our method achieves significant reductions in Word Error Rate (WER)
achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data
than the next best approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProHOC: Probabilistic Hierarchical Out-of-Distribution Classification
  via Multi-Depth Networks <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Wallin, Fredrik Kahl, Lars Hammarstrand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection in deep learning has traditionally been
framed as a binary task, where samples are either classified as belonging to
the known classes or marked as OOD, with little attention given to the semantic
relationships between OOD samples and the in-distribution (ID) classes. We
propose a framework for detecting and classifying OOD samples in a given class
hierarchy. Specifically, we aim to predict OOD data to their correct internal
nodes of the class hierarchy, whereas the known ID classes should be predicted
as their corresponding leaf nodes. Our approach leverages the class hierarchy
to create a probabilistic model and we implement this model by using networks
trained for ID classification at multiple hierarchy depths. We conduct
experiments on three datasets with predefined class hierarchies and show the
effectiveness of our method. Our code is available at
https://github.com/walline/prohoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Real-World Denoising: Sparsity is All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamadi Chihaoui, Paolo Favaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised training for real-world denoising presents challenges due to the
difficulty of collecting large datasets of paired noisy and clean images.
Recent methods have attempted to address this by utilizing unpaired datasets of
clean and noisy images. Some approaches leverage such unpaired data to train
denoisers in a supervised manner by generating synthetic clean-noisy pairs.
However, these methods often fall short due to the distribution gap between
synthetic and real noisy images. To mitigate this issue, we propose a solution
based on input sparsification, specifically using random input masking. Our
method, which we refer to as Mask, Inpaint and Denoise (MID), trains a denoiser
to simultaneously denoise and inpaint synthetic clean-noisy pairs. On one hand,
input sparsification reduces the gap between synthetic and real noisy images.
On the other hand, an inpainter trained in a supervised manner can still
accurately reconstruct sparse inputs by predicting missing clean pixels using
the remaining unmasked pixels. Our approach begins with a synthetic Gaussian
noise sampler and iteratively refines it using a noise dataset derived from the
denoiser's predictions. The noise dataset is created by subtracting predicted
pseudo-clean images from real noisy images at each iteration. The core
intuition is that improving the denoiser results in a more accurate noise
dataset and, consequently, a better noise sampler. We validate our method
through extensive experiments on real-world noisy image datasets, demonstrating
competitive performance compared to existing unsupervised denoising methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal surface defect detection from wooden logs for sawing
  optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bořek Reich, Matej Kunda, Fedor Zolotarev, Tuomas Eerola, Pavel Zemčík, Tomi Kauppi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel, good-quality, and less demanding method for detecting
knots on the surface of wooden logs using multimodal data fusion. Knots are a
primary factor affecting the quality of sawn timber, making their detection
fundamental to any timber grading or cutting optimization system. While X-ray
computed tomography provides accurate knot locations and internal structures,
it is often too slow or expensive for practical use. An attractive alternative
is to use fast and cost-effective log surface measurements, such as laser
scanners or RGB cameras, to detect surface knots and estimate the internal
structure of wood. However, due to the small size of knots and noise caused by
factors, such as bark and other natural variations, detection accuracy often
remains low when only one measurement modality is used. In this paper, we
demonstrate that by using a data fusion pipeline consisting of separate streams
for RGB and point cloud data, combined by a late fusion module, higher knot
detection accuracy can be achieved compared to using either modality alone. We
further propose a simple yet efficient sawing angle optimization method that
utilizes surface knot detections and cross-correlation to minimize the amount
of unwanted arris knots, demonstrating its benefits over randomized sawing
angles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LandMarkSystem Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenxiang Ma, Zhenyu Yang, Miao Tao, Yuanzhen Zhou, Zeyu He, Yuchang Zhang, Rong Fu, Hengjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction is vital for applications in autonomous driving, virtual
reality, augmented reality, and the metaverse. Recent advancements such as
Neural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed
the field, yet traditional deep learning frameworks struggle to meet the
increasing demands for scene quality and scale. This paper introduces
LandMarkSystem, a novel computing framework designed to enhance multi-scale
scene reconstruction and rendering. By leveraging a componentized model
adaptation layer, LandMarkSystem supports various NeRF and 3DGS structures
while optimizing computational efficiency through distributed parallel
computing and model parameter offloading. Our system addresses the limitations
of existing frameworks, providing dedicated operators for complex 3D sparse
computations, thus facilitating efficient training and rapid inference over
extensive scenes. Key contributions include a modular architecture, a dynamic
loading strategy for limited resources, and proven capabilities across multiple
representative algorithms.This comprehensive solution aims to advance the
efficiency and effectiveness of 3D reconstruction tasks.To facilitate further
research and collaboration, the source code and documentation for the
LandMarkSystem project are publicly available in an open-source repository,
accessing the repository at: https://github.com/InternLandMark/LandMarkSystem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UGNA-VPR: A Novel Training Paradigm for <span class="highlight-title">Visual</span> Place Recognition Based
  on Uncertainty-Guided NeRF Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehui Shen, Lei Zhang, Qingqiu Li, Xiongwei Zhao, Yue Wang, Huimin Lu, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition (VPR) is crucial for robots to identify previously
visited locations, playing an important role in autonomous navigation in both
indoor and outdoor environments. However, most existing VPR datasets are
limited to single-viewpoint scenarios, leading to reduced recognition accuracy,
particularly in multi-directional driving or feature-sparse scenes. Moreover,
obtaining additional data to mitigate these limitations is often expensive.
This paper introduces a novel training paradigm to improve the performance of
existing VPR networks by enhancing multi-view diversity within current datasets
through uncertainty estimation and NeRF-based data augmentation. Specifically,
we initially train NeRF using the existing VPR dataset. Then, our devised
self-supervised uncertainty estimation network identifies places with high
uncertainty. The poses of these uncertain places are input into NeRF to
generate new synthetic observations for further training of VPR networks.
Additionally, we propose an improved storage method for efficient organization
of augmented and original training data. We conducted extensive experiments on
three datasets and tested three different VPR backbone networks. The results
demonstrate that our proposed training paradigm significantly improves VPR
performance by fully utilizing existing data, outperforming other training
approaches. We further validated the effectiveness of our approach on
self-recorded indoor and outdoor datasets, consistently demonstrating superior
results. Our dataset and code have been released at
\href{https://github.com/nubot-nudt/UGNA-VPR}{https://github.com/nubot-nudt/UGNA-VPR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Robotics and Automation Letters (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DuckSegmentation: A segmentation model based on the AnYue Hemp Duck
  <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Feng, Tianyu Xie, Wei Ma, Ruijie Fu, Yingxiao Zhang, Jun Li, Bei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The modernization of smart farming is a way to improve agricultural
production efficiency, and improve the agricultural production environment.
Although many large models have achieved high accuracy in the task of object
recognition and segmentation, they cannot really be put into use in the farming
industry due to their own poor interpretability and limitations in
computational volume. In this paper, we built AnYue Shelduck Dateset, which
contains a total of 1951 Shelduck datasets, and performed target detection and
segmentation annotation with the help of professional annotators. Based on
AnYue ShelduckDateset, this paper describes DuckProcessing, an efficient and
powerful module for duck identification based on real shelduckfarms. First of
all, using the YOLOv8 module designed to divide the mahjong between them,
Precision reached 98.10%, Recall reached 96.53% and F1 score reached 0.95 on
the test set. Again using the DuckSegmentation segmentation model,
DuckSegmentation reached 96.43% mIoU. Finally, the excellent DuckSegmentation
was used as the teacher model, and through knowledge distillation, Deeplabv3
r50 was used as the student model, and the final student model achieved 94.49%
mIoU on the test set. The method provides a new way of thinking in practical
sisal duck smart farming.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HORT: Monocular Hand-held Objects Reconstruction with Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zerui Chen, Rolandos Alexandros Potamias, Shizhe Chen, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing hand-held objects in 3D from monocular images remains a
significant challenge in computer vision. Most existing approaches rely on
implicit 3D representations, which produce overly smooth reconstructions and
are time-consuming to generate explicit 3D shapes. While more recent methods
directly reconstruct point clouds with diffusion models, the multi-step
denoising makes high-resolution reconstruction inefficient. To address these
limitations, we propose a transformer-based model to efficiently reconstruct
dense 3D point clouds of hand-held objects. Our method follows a coarse-to-fine
strategy, first generating a sparse point cloud from the image and
progressively refining it into a dense representation using pixel-aligned image
features. To enhance reconstruction accuracy, we integrate image features with
3D hand geometry to jointly predict the object point cloud and its pose
relative to the hand. Our model is trained end-to-end for optimal performance.
Experimental results on both synthetic and real datasets demonstrate that our
method achieves state-of-the-art accuracy with much faster inference speed,
while generalizing well to in-the-wild images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://zerchen.github.io/projects/hort.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for
  Composed Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixu Li, Zhiheng Fu, Yupeng Hu, Zhiwei Chen, Haokun Wen, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed Image Retrieval (CIR) facilitates image retrieval through a
multimodal query consisting of a reference image and modification text. The
reference image defines the retrieval context, while the modification text
specifies desired alterations. However, existing CIR datasets predominantly
employ coarse-grained modification text (CoarseMT), which inadequately captures
fine-grained retrieval intents. This limitation introduces two key challenges:
(1) ignoring detailed differences leads to imprecise positive samples, and (2)
greater ambiguity arises when retrieving visually similar images. These issues
degrade retrieval accuracy, necessitating manual result filtering or repeated
queries. To address these limitations, we develop a robust fine-grained CIR
data annotation pipeline that minimizes imprecise positive samples and enhances
CIR systems' ability to discern modification intents accurately. Using this
pipeline, we refine the FashionIQ and CIRR datasets to create two fine-grained
CIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR,
the first CIR framework explicitly designed to parse the modification text.
FineCIR effectively captures fine-grained modification semantics and aligns
them with ambiguous visual entities, enhancing retrieval precision. Extensive
experiments demonstrate that FineCIR consistently outperforms state-of-the-art
CIR baselines on both fine-grained and traditional CIR benchmark datasets. Our
FineCIR code and fine-grained CIR datasets are available at
https://github.com/SDU-L/FineCIR.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InternVL-X: Advancing and Accelerating InternVL Series with Efficient
  <span class="highlight-title">Visual</span> Token Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongchen Lu, Yuyao Sun, Zilu Zhang, Leping Huang, Jianliang Zeng, Mao Shu, Huo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most multimodal large language models (MLLMs) treat visual tokens as "a
sequence of text", integrating them with text tokens into a large language
model (LLM). However, a great quantity of visual tokens significantly increases
the demand for computational resources and time. In this paper, we propose
InternVL-X, which outperforms the InternVL model in both performance and
efficiency by incorporating three visual token compression methods. First, we
propose a novel vision-language projector, PVTC. This component integrates
adjacent visual embeddings to form a local query and utilizes the transformed
CLS token as a global query, then performs point-to-region cross-attention
through these local and global queries to more effectively convert visual
features. Second, we present a layer-wise visual token compression module,
LVTC, which compresses tokens in the LLM shallow layers and then expands them
through upsampling and residual connections in the deeper layers. This
significantly enhances the model computational efficiency. Futhermore, we
propose an efficient high resolution slicing method, RVTC, which dynamically
adjusts the number of visual tokens based on image area or length filtering.
RVTC greatly enhances training efficiency with only a slight reduction in
performance. By utilizing 20% or fewer visual tokens, InternVL-X achieves
state-of-the-art performance on 7 public MLLM benchmarks, and improves the
average metric by 2.34% across 12 tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Scale Invertible Neural Network for Wide-Range Variable-Rate
  Learned Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyue Tu, Siqi Wu, Li Li, Wengang Zhou, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoencoder-based structures have dominated recent learned image compression
methods. However, the inherent information loss associated with autoencoders
limits their rate-distortion performance at high bit rates and restricts their
flexibility of rate adaptation. In this paper, we present a variable-rate image
compression model based on invertible transform to overcome these limitations.
Specifically, we design a lightweight multi-scale invertible neural network,
which bijectively maps the input image into multi-scale latent representations.
To improve the compression efficiency, a multi-scale spatial-channel context
model with extended gain units is devised to estimate the entropy of the latent
representation from high to low levels. Experimental results demonstrate that
the proposed method achieves state-of-the-art performance compared to existing
variable-rate methods, and remains competitive with recent multi-model
approaches. Notably, our method is the first learned image compression solution
that outperforms VVC across a very wide range of bit rates using a single
model, especially at high bit rates.The source code is available at
\href{https://github.com/hytu99/MSINN-VRLIC}{https://github.com/hytu99/MSINN-VRLIC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Multimedia 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot <span class="highlight-title">Visual</span> Concept Blending Without Text Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroya Makino, Takahiro Yamaguchi, Hiroyuki Sakai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel, zero-shot image generation technique called "Visual
Concept Blending" that provides fine-grained control over which features from
multiple reference images are transferred to a source image. If only a single
reference image is available, it is difficult to isolate which specific
elements should be transferred. However, using multiple reference images, the
proposed approach distinguishes between common and unique features by
selectively incorporating them into a generated output. By operating within a
partially disentangled Contrastive Language-Image Pre-training (CLIP) embedding
space (from IP-Adapter), our method enables the flexible transfer of texture,
shape, motion, style, and more abstract conceptual transformations without
requiring additional training or text prompts. We demonstrate its effectiveness
across a diverse range of tasks, including style transfer, form metamorphosis,
and conceptual transformations, showing how subtle or abstract attributes
(e.g., brushstroke style, aerodynamic lines, and dynamism) can be seamlessly
combined into a new image. In a user study, participants accurately recognized
which features were intended to be transferred. Its simplicity, flexibility,
and high-level control make Visual Concept Blending valuable for creative
fields such as art, design, and content creation, where combining specific
visual qualities from multiple inspirations is crucial.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delving Deep into Semantic Relation Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyi Yan, Kangjun Liu, Qixiang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation has become a cornerstone technique in deep learning,
facilitating the transfer of knowledge from complex models to lightweight
counterparts. Traditional distillation approaches focus on transferring
knowledge at the instance level, but fail to capture nuanced semantic
relationships within the data. In response, this paper introduces a novel
methodology, Semantics-based Relation Knowledge Distillation (SeRKD), which
reimagines knowledge distillation through a semantics-relation lens among each
sample. By leveraging semantic components, \ie, superpixels, SeRKD enables a
more comprehensive and context-aware transfer of knowledge, which skillfully
integrates superpixel-based semantic extraction with relation-based knowledge
distillation for a sophisticated model compression and distillation.
Particularly, the proposed method is naturally relevant in the domain of Vision
Transformers (ViTs), where visual tokens serve as fundamental units of
representation. Experimental evaluations on benchmark datasets demonstrate the
superiority of SeRKD over existing methods, underscoring its efficacy in
enhancing model performance and generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClimbingCap: Multi-Modal <span class="highlight-title">Dataset</span> and Method for Rock Climbing in World
  Coordinate <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Yan, Xincheng Lin, Yuhua Luo, Shuqi Fan, Yudi Dai, Qixin Zhong, Lincai Zhong, Yuexin Ma, Lan Xu, Chenglu Wen, Siqi Shen, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Motion Recovery (HMR) research mainly focuses on ground-based motions
such as running. The study on capturing climbing motion, an off-ground motion,
is sparse. This is partly due to the limited availability of climbing motion
datasets, especially large-scale and challenging 3D labeled datasets. To
address the insufficiency of climbing motion datasets, we collect AscendMotion,
a large-scale well-annotated, and challenging climbing motion dataset. It
consists of 412k RGB, LiDAR frames, and IMU measurements, including the
challenging climbing motions of 22 skilled climbing coaches across 12 different
rock walls. Capturing the climbing motions is challenging as it requires
precise recovery of not only the complex pose but also the global position of
climbers. Although multiple global HMR methods have been proposed, they cannot
faithfully capture climbing motions. To address the limitations of HMR methods
for climbing, we propose ClimbingCap, a motion recovery method that
reconstructs continuous 3D human climbing motion in a global coordinate system.
One key insight is to use the RGB and LiDAR modalities to separately
reconstruct motions in camera coordinates and global coordinates and to
optimize them jointly. We demonstrate the quality of the AscendMotion dataset
and present promising results from ClimbingCap. The AscendMotion dataset and
source code release publicly at \href{this
link}{http://www.lidarhumanmotion.net/climbingcap/}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2025, project in \href{this
  link}{http://www.lidarhumanmotion.net/climbingcap/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ vGamba: Attentive State Space Bottleneck for efficient Long-range
  Dependencies in <span class="highlight-title">Visual</span> Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunusa Haruna, Adamu Lawan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing long-range dependencies efficiently is essential for visual
recognition tasks, yet existing methods face limitations. Convolutional neural
networks (CNNs) struggle with restricted receptive fields, while Vision
Transformers (ViTs) achieve global context and long-range modeling at a high
computational cost. State-space models (SSMs) offer an alternative, but their
application in vision remains underexplored. This work introduces vGamba, a
hybrid vision backbone that integrates SSMs with attention mechanisms to
enhance efficiency and expressiveness. At its core, the Gamba bottleneck block
that includes, Gamba Cell, an adaptation of Mamba for 2D spatial structures,
alongside a Multi-Head Self-Attention (MHSA) mechanism and a Gated Fusion
Module for effective feature representation. The interplay of these components
ensures that vGamba leverages the low computational demands of SSMs while
maintaining the accuracy of attention mechanisms for modeling long-range
dependencies in vision tasks. Additionally, the Fusion module enables seamless
interaction between these components. Extensive experiments on classification,
detection, and segmentation tasks demonstrate that vGamba achieves a superior
trade-off between accuracy and computational efficiency, outperforming several
existing models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing CT Metal Artifacts by Learning Latent Space Alignment with
  Gemstone Spectral Imaging Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wencheng Han, Dongqian Guo, Xiao Chen, Pang Lyu, Yi Jin, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metal artifacts in CT slices have long posed challenges in medical
diagnostics. These artifacts degrade image quality, resulting in suboptimal
visualization and complicating the accurate interpretation of tissues adjacent
to metal implants. To address these issues, we introduce the Latent Gemstone
Spectral Imaging (GSI) Alignment Framework, which effectively reduces metal
artifacts while avoiding the introduction of noise information. Our work is
based on a key finding that even artifact-affected ordinary CT sequences
contain sufficient information to discern detailed structures. The challenge
lies in the inability to clearly represent this information. To address this
issue, we developed an Alignment Framework that adjusts the representation of
ordinary CT images to match GSI CT sequences. GSI is an advanced imaging
technique using multiple energy levels to mitigate artifacts caused by metal
implants. By aligning the representation to GSI data, we can effectively
suppress metal artifacts while clearly revealing detailed structure, without
introducing extraneous information into CT sequences. To facilitate the
application, we propose a new dataset, Artifacts-GSI, captured from real
patients with metal implants, and establish a new benchmark based on this
dataset. Experimental results show that our method significantly reduces metal
artifacts and greatly enhances the readability of CT slices. All our code and
data are available at: https://um-lab.github.io/GSI-MAR/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learn by Reasoning: Analogical Weight Generation for Few-Shot
  Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizhou Han, Chenhao Ding, Yuhang He, Songlin Dong, Qiang Wang, Xinyuan Gao, Yihong Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot class-incremental Learning (FSCIL) enables models to learn new
classes from limited data while retaining performance on previously learned
classes. Traditional FSCIL methods often require fine-tuning parameters with
limited new class data and suffer from a separation between learning new
classes and utilizing old knowledge. Inspired by the analogical learning
mechanisms of the human brain, we propose a novel analogical generative method.
Our approach includes the Brain-Inspired Analogical Generator (BiAG), which
derives new class weights from existing classes without parameter fine-tuning
during incremental stages. BiAG consists of three components: Weight
Self-Attention Module (WSA), Weight & Prototype Analogical Attention Module
(WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory
for semantic conversion, WSA supplements new class weights, and WPAA computes
analogies to generate new class weights. Experiments on miniImageNet, CUB-200,
and CIFAR-100 datasets demonstrate that our method achieves higher final and
average accuracy compared to SOTA methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Vision</span>-to-Music Generation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaokai Wang, Chenxi Bao, Le Zhuo, Jingrui Han, Yang Yue, Yihong Tang, Victor Shea-Jay Huang, Yue Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-to-music Generation, including video-to-music and image-to-music
tasks, is a significant branch of multimodal artificial intelligence
demonstrating vast application prospects in fields such as film scoring, short
video creation, and dance music synthesis. However, compared to the rapid
development of modalities like text and images, research in vision-to-music is
still in its preliminary stage due to its complex internal structure and the
difficulty of modeling dynamic relationships with video. Existing surveys focus
on general music generation without comprehensive discussion on
vision-to-music. In this paper, we systematically review the research progress
in the field of vision-to-music generation. We first analyze the technical
characteristics and core challenges for three input types: general videos,
human movement videos, and images, as well as two output types of symbolic
music and audio music. We then summarize the existing methodologies on
vision-to-music generation from the architecture perspective. A detailed review
of common datasets and evaluation metrics is provided. Finally, we discuss
current challenges and promising directions for future research. We hope our
survey can inspire further innovation in vision-to-music generation and the
broader field of multimodal generation in academic research and industrial
applications. To follow latest works and foster further innovation in this
field, we are continuously maintaining a GitHub repository at
https://github.com/wzk1015/Awesome-Vision-to-Music-Generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Orange Quality Grading with Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Lamine Mekhalfi, Paul Chippendale, Francisco Fraile, Marcos Rico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Orange grading is a crucial step in the fruit industry, as it helps to sort
oranges according to different criteria such as size, quality, ripeness, and
health condition, ensuring safety for human consumption and better price
allocation and client satisfaction. Automated grading enables faster
processing, precision, and reduced human labor. In this paper, we implement a
deep learning-based solution for orange grading via machine vision. Unlike
typical grading systems that analyze fruits from a single view, we capture
multiview images of each single orange in order to enable a richer
representation. Afterwards, we compose the acquired images into one collage.
This enables the analysis of the whole orange skin. We train a convolutional
neural network (CNN) on the composed images to grade the oranges into three
classes, namely good, bad, and undefined. We also evaluate the performance with
two different CNNs (ResNet-18 and SqueezeNet). We show experimentally that
multi-view grading is superior to single view grading.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynamiCtrl: Rethinking the Basic Structure and the Role of Text for
  High-quality Human Image Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhao, Zhongang Qi, Cong Wang, Qingping Zheng, Guansong Lu, Fei Chen, Hang Xu, Zuxuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human image animation has recently gained significant attention due to
advancements in generative models. However, existing methods still face two
major challenges: (1) architectural limitations, most models rely on U-Net,
which underperforms compared to the MM-DiT; and (2) the neglect of textual
information, which can enhance controllability. In this work, we introduce
DynamiCtrl, a novel framework that not only explores different pose-guided
control structures in MM-DiT, but also reemphasizes the crucial role of text in
this task. Specifically, we employ a Shared VAE encoder for both reference
images and driving pose videos, eliminating the need for an additional pose
encoder and simplifying the overall framework. To incorporate pose features
into the full attention blocks, we propose Pose-adaptive Layer Norm (PadaLN),
which utilizes adaptive layer normalization to encode sparse pose features. The
encoded features are directly added to the visual input, preserving the
spatiotemporal consistency of the backbone while effectively introducing pose
control into MM-DiT. Furthermore, within the full attention mechanism, we align
textual and visual features to enhance controllability. By leveraging text, we
not only enable fine-grained control over the generated content, but also, for
the first time, achieve simultaneous control over both background and motion.
Experimental results verify the superiority of DynamiCtrl on benchmark
datasets, demonstrating its strong identity preservation, heterogeneous
character driving, background controllability, and high-quality synthesis. The
project page is available at https://gulucaptain.github.io/DynamiCtrl/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Multimodal Large Language Models See Like Humans? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaying Lin, Shuquan Ye, Rynson W. H. Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have achieved impressive results on
various vision tasks, leveraging recent advancements in large language models.
However, a critical question remains unaddressed: do MLLMs perceive visual
information similarly to humans? Current benchmarks lack the ability to
evaluate MLLMs from this perspective. To address this challenge, we introduce
HVSBench, a large-scale benchmark designed to assess the alignment between
MLLMs and the human visual system (HVS) on fundamental vision tasks that mirror
human vision. HVSBench curated over 85K multimodal samples, spanning 13
categories and 5 fields in HVS, including Prominence, Subitizing, Prioritizing,
Free-Viewing, and Searching. Extensive experiments demonstrate the
effectiveness of our benchmark in providing a comprehensive evaluation of
MLLMs. Specifically, we evaluate 13 MLLMs, revealing that even the best models
show significant room for improvement, with most achieving only moderate
results. Our experiments reveal that HVSBench presents a new and significant
challenge for cutting-edge MLLMs. Diverse human participants attained strong
performance, significantly outperforming MLLMs, which further underscores the
benchmark's high quality. We believe that HVSBench will facilitate research on
human-aligned and explainable MLLMs, marking a key step in understanding how
MLLMs perceive and process visual information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jiaying.link/HVSBench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaga: Group Any Gaussians via 3D-aware Memory Bank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Gaga, a framework that reconstructs and segments open-world 3D
scenes by leveraging inconsistent 2D masks predicted by zero-shot
class-agnostic segmentation models. Contrasted to prior 3D scene segmentation
approaches that rely on video object tracking or contrastive learning methods,
Gaga utilizes spatial information and effectively associates object masks
across diverse camera poses through a novel 3D-aware memory bank. By
eliminating the assumption of continuous view changes in training images, Gaga
demonstrates robustness to variations in camera poses, particularly beneficial
for sparsely sampled images, ensuring precise mask label consistency.
Furthermore, Gaga accommodates 2D segmentation masks from diverse sources and
demonstrates robust performance with different open-world zero-shot
class-agnostic segmentation models, significantly enhancing its versatility.
Extensive qualitative and quantitative evaluations demonstrate that Gaga
performs favorably against state-of-the-art methods, emphasizing its potential
for real-world applications such as 3D scene understanding and manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://weijielyu.github.io/Gaga</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELIP: Enhanced <span class="highlight-title">Visual</span>-Language Foundation Models for Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanqi Zhan, Yuanpei Liu, Kai Han, Weidi Xie, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective in this paper is to improve the performance of text-to-image
retrieval. To this end, we introduce a new framework that can boost the
performance of large-scale pre-trained vision-language models, so that they can
be used for text-to-image re-ranking. The approach, Enhanced Language-Image
Pre-training (ELIP), uses the text query, via a simple MLP mapping network, to
predict a set of visual prompts to condition the ViT image encoding. ELIP can
easily be applied to the commonly used CLIP, SigLIP and BLIP-2 networks. To
train the architecture with limited computing resources, we develop a 'student
friendly' best practice, involving global hard sample mining, and curation of a
large-scale dataset. On the evaluation side, we set up two new
out-of-distribution (OOD) benchmarks, Occluded COCO and ImageNet-R, to assess
the zero-shot generalisation of the models to different domains. The results
demonstrate that ELIP significantly boosts CLIP/SigLIP/SigLIP-2 text-to-image
retrieval performance and outperforms BLIP-2 on several benchmarks, as well as
providing an easy means to adapt to OOD datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIA: Unified Spatiotemporal Video Adaptation Framework for Global and
  Local Video Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12831v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12831v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Gu, Yuwei Fang, Ivan Skorokhodov, Peter Wonka, Xinya Du, Sergey Tulyakov, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video editing serves as a fundamental pillar of digital media, spanning
applications in entertainment, education, and professional communication.
However, previous methods often overlook the necessity of comprehensively
understanding both global and local contexts, leading to inaccurate and
inconsistent edits in the spatiotemporal dimension, especially for long videos.
In this paper, we introduce VIA, a unified spatiotemporal Video Adaptation
framework for global and local video editing, pushing the limits of
consistently editing minute-long videos. First, to ensure local consistency
within individual frames, we designed test-time editing adaptation to adapt a
pre-trained image editing model for improving consistency between potential
editing directions and the text instruction, and adapts masked latent variables
for precise local control. Furthermore, to maintain global consistency over the
video sequence, we introduce spatiotemporal adaptation that recursively gather
consistent attention variables in key frames and strategically applies them
across the whole sequence to realize the editing effects. Extensive experiments
demonstrate that, compared to baseline methods, our VIA approach produces edits
that are more faithful to the source videos, more coherent in the
spatiotemporal context, and more precise in local control. More importantly, we
show that VIA can achieve consistent long video editing in minutes, unlocking
the potential for advanced video editing tasks over long video sequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Computational Solutions for Reconstructing Complete Objects
  by Reassembling Their Fractured Parts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Lu, Yongqing Liang, Huijun Han, Jiacheng Hua, Junfeng Jiang, Xin Li, Qixing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing a complete object from its parts is a fundamental problem in
many scientific domains. The purpose of this article is to provide a systematic
survey on this topic. The reassembly problem requires understanding the
attributes of individual pieces and establishing matches between different
pieces. Many approaches also model priors of the underlying complete object.
Existing approaches are tightly connected problems of shape segmentation, shape
matching, and learning shape priors. We provide existing algorithms in this
context and emphasize their similarities and differences to general-purpose
approaches. We also survey the trends from early non-deep learning approaches
to more recent deep learning approaches. In addition to algorithms, this survey
will also describe existing datasets, open-source software packages, and
applications. To the best of our knowledge, this is the first comprehensive
survey on this topic in computer graphics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video
  Understanding? <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Awareness, the ability to reason dynamically based on the timestamp
when a question is raised, is the key distinction between offline and online
video LLMs. Unlike offline models, which rely on complete videos for static,
post hoc analysis, online models process video streams incrementally and
dynamically adapt their responses based on the timestamp at which the question
is posed. Despite its significance, temporal awareness has not been adequately
evaluated in existing benchmarks. To fill this gap, we present OVO-Bench
(Online-VideO-Benchmark), a novel video benchmark that emphasizes the
importance of timestamps for advanced online video understanding capability
benchmarking. OVO-Bench evaluates the ability of video LLMs to reason and
respond to events occurring at specific timestamps under three distinct
scenarios: (1) Backward tracing: trace back to past events to answer the
question. (2) Real-time understanding: understand and respond to events as they
unfold at the current timestamp. (3) Forward active responding: delay the
response until sufficient future information becomes available to answer the
question accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos
and approximately human-curated 2,800 fine-grained meta-annotations with
precise timestamps. We combine automated generation pipelines with human
curation. With these high-quality samples, we further developed an evaluation
pipeline to systematically query video LLMs along the video timeline.
Evaluations of nine Video-LLMs reveal that, despite advancements on traditional
benchmarks, current models struggle with online video understanding, showing a
significant gap compared to human agents. We hope OVO-Bench will drive progress
in video LLMs and inspire future research in online video reasoning. Our
benchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-View and Multi-Scale Alignment for Contrastive Language-Image
  Pre-training in Mammography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuexi Du, John Onofrey, Nicha C. Dvornek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) demonstrates strong potential
in medical image analysis but requires substantial data and computational
resources. Due to these restrictions, existing CLIP applications in medical
imaging focus mainly on modalities like chest X-rays that have abundant
image-report data available, leaving many other important modalities
underexplored. Here, we propose one of the first adaptations of the full CLIP
model to mammography, which presents significant challenges due to labeled data
scarcity, high-resolution images with small regions of interest, and class-wise
imbalance. We first develop a specialized supervision framework for mammography
that leverages its multi-view nature. Furthermore, we design a symmetric local
alignment module to better focus on detailed features in high-resolution
images. Lastly, we incorporate a parameter-efficient fine-tuning approach for
large language models pre-trained with medical knowledge to address data
limitations. Our multi-view and multi-scale alignment (MaMA) method outperforms
state-of-the-art baselines for three different tasks on two large real-world
mammography datasets, EMBED and RSNA-Mammo, with only 52% model size compared
with the largest baseline. The code is available at
https://github.com/XYPB/MaMA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by IPMI 2025 for Oral Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language
  Models for Long-Form Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, Meng Cao, Kai Kang, Yinfei Yang, Afshin Dehghan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of
video large language models (LLMs) offering a token-efficient solution for
long-form video understanding. We incorporate the two-stream SlowFast mechanism
into a streamlined training pipeline, and perform joint video-image training on
a carefully curated data mixture of only publicly available datasets. Our
primary focus is on highly efficient model scales (1B and 3B), demonstrating
that even relatively small Video LLMs can achieve state-of-the-art performance
on video understanding, meeting the demand for mobile-friendly models.
Experimental results demonstrate that SF-LLaVA-1.5 achieves superior
performance on a wide range of video and image tasks, with robust results at
all model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achieves
state-of-the-art results in long-form video understanding (e.g., LongVideoBench
and MLVU) and excels at small scales across various video benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified
  Flow Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06608v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06608v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, Yan-Pei Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion techniques have propelled image and video
generation to unprecedented levels of quality, significantly accelerating the
deployment and application of generative AI. However, 3D shape generation
technology has so far lagged behind, constrained by limitations in 3D data
scale, complexity of 3D data processing, and insufficient exploration of
advanced techniques in the 3D domain. Current approaches to 3D shape generation
face substantial challenges in terms of output quality, generalization
capability, and alignment with input conditions. We present TripoSG, a new
streamlined shape diffusion paradigm capable of generating high-fidelity 3D
meshes with precise correspondence to input images. Specifically, we propose:
1) A large-scale rectified flow transformer for 3D shape generation, achieving
state-of-the-art fidelity through training on extensive, high-quality data. 2)
A hybrid supervised training strategy combining SDF, normal, and eikonal losses
for 3D VAE, achieving high-quality 3D reconstruction performance. 3) A data
processing pipeline to generate 2 million high-quality 3D samples, highlighting
the crucial rules for data quality and quantity in training 3D generative
models. Through comprehensive experiments, we have validated the effectiveness
of each component in our new framework. The seamless integration of these parts
has enabled TripoSG to achieve state-of-the-art performance in 3D shape
generation. The resulting 3D shapes exhibit enhanced detail due to
high-resolution capabilities and demonstrate exceptional fidelity to input
images. Moreover, TripoSG demonstrates improved versatility in generating 3D
models from diverse image styles and contents, showcasing strong generalization
capabilities. To foster progress and innovation in the field of 3D generation,
we will make our model publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BACON: Improving Clarity of Image Captions via Bag-of-Concept Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhantao Yang, Ruili Feng, Keyu Yan, Huangji Wang, Zhicai Wang, Shangwen Zhu, Han Zhang, Jie Xiao, Pingyu Wu, Kai Zhu, Jixuan Chen, Chen-Wei Xie, Yue Yang, Hongyang Zhang, Yu Liu, Fan Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in large Vision-Language Models have brought precise, accurate
image captioning, vital for advancing multi-modal image understanding and
processing. Yet these captions often carry lengthy, intertwined contexts that
are difficult to parse and frequently overlook essential cues, posing a great
barrier for models like GroundingDINO and SDXL, which lack the strong text
encoding and syntax analysis needed to fully leverage dense captions. To
address this, we propose BACON, a prompting method that breaks down
VLM-generated captions into disentangled, structured elements such as objects,
relationships, styles, and themes. This approach not only minimizes confusion
from handling complex contexts but also allows for efficient transfer into a
JSON dictionary, enabling models without linguistic processing capabilities to
easily access key information. We annotated 100,000 image-caption pairs using
BACON with GPT-4V and trained an LLaVA captioner on this dataset, enabling it
to produce BACON-style captions without relying on costly GPT-4V. Evaluations
of overall quality, precision, and recall-as well as user studies-demonstrate
that the resulting caption model consistently outperforms other SOTA VLM models
in generating high-quality captions. Besides, we show that BACON-style captions
exhibit better clarity when applied to various models, enabling them to
accomplish previously unattainable tasks or surpass existing SOTA solutions
without training. For example, BACON-style captions help GroundingDINO achieve
1.51x higher recall scores on open-vocabulary object detection tasks compared
to leading methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StableMamba: Distillation-free Scaling of Large SSMs for Images and
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamid Suleman, Syed Talal Wasim, Muzammal Naseer, Juergen Gall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-space models (SSMs), exemplified by S4, have introduced a novel context
modeling method by integrating state-space techniques into deep learning.
However, they struggle with global context modeling due to their
data-independent matrices. The Mamba model addressed this with data-dependent
variants via the S6 selective-scan algorithm, enhancing context modeling,
especially for long sequences. However, Mamba-based architectures are difficult
to scale with respect to the number of parameters, which is a major limitation
for vision applications. This paper addresses the scalability issue of large
SSMs for image classification and action recognition without requiring
additional techniques like knowledge distillation. We analyze the distinct
characteristics of Mamba-based and Attention-based models, proposing a
Mamba-Attention interleaved architecture that enhances scalability, robustness,
and performance. We demonstrate that the stable and efficient interleaved
architecture resolves the scalability issue of Mamba-based architectures for
images and videos and increases robustness to common artifacts like JPEG
compression. Our thorough evaluation on the ImageNet-1K, Kinetics-400 and
Something-Something-v2 benchmarks demonstrates that our approach improves the
accuracy of state-of-the-art Mamba-based architectures by up to $+1.7$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequency-Controlled <span class="highlight-title">Diffusion</span> Model for Versatile Text-Guided
  Image-to-Image Translation <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Gao, Zhengbo Xu, Junhan Zhao, Jiaying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large-scale text-to-image (T2I) diffusion models have emerged as a
powerful tool for image-to-image translation (I2I), allowing open-domain image
translation via user-provided text prompts. This paper proposes
frequency-controlled diffusion model (FCDiffusion), an end-to-end
diffusion-based framework that contributes a novel solution to text-guided I2I
from a frequency-domain perspective. At the heart of our framework is a
feature-space frequency-domain filtering module based on Discrete Cosine
Transform, which filters the latent features of the source image in the DCT
domain, yielding filtered image features bearing different DCT spectral bands
as different control signals to the pre-trained Latent Diffusion Model. We
reveal that control signals of different DCT spectral bands bridge the source
image and the T2I generated image in different correlations (e.g., style,
structure, layout, contour, etc.), and thus enable versatile I2I applications
emphasizing different I2I correlations, including style-guided content
creation, image semantic manipulation, image scene translation, and image style
translation. Different from related approaches, FCDiffusion establishes a
unified text-guided I2I framework suitable for diverse image translation tasks
simply by switching among different frequency control branches at inference
time. The effectiveness and superiority of our method for text-guided I2I are
demonstrated with extensive experiments both qualitatively and quantitatively.
Our project is publicly available at:
https://xianggao1102.github.io/FCDiffusion/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 38th AAAI Conference on Artificial Intelligence
  (AAAI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniBench: Towards The Future of Universal Omni-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15272v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15272v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Wang, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Xiangzhou Wang, Zhaoxiang Zhang, Zachary Liu, Emmanouil Benetos, Wenhao Huang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal large language models (MLLMs) have focused
on integrating multiple modalities, yet their ability to simultaneously process
and reason across different inputs remains underexplored. We introduce
OmniBench, a novel benchmark designed to evaluate models' ability to recognize,
interpret, and reason across visual, acoustic, and textual inputs
simultaneously. We define language models capable of such tri-modal processing
as omni-language models (OLMs). OmniBench features high-quality human
annotations that require integrated understanding across all modalities. Our
evaluation reveals that: i) open-source OLMs show significant limitations in
instruction-following and reasoning in tri-modal contexts; and ii) most
baseline models perform poorly (around 50% accuracy) even with textual
alternatives to image/audio inputs. To address these limitations, we develop
OmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. We
advocate for developing more robust tri-modal integration techniques and
training strategies to enhance OLM performance. Codes and data could be found
at our repo (https://github.com/multimodal-art-projection/OmniBench).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Vision</span> language models are blind: Failing to translate detailed <span class="highlight-title">visual</span>
  features into words 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06581v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06581v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models with vision capabilities (VLMs), e.g., GPT-4o and
Gemini 1.5 Pro, score high on many vision-understanding benchmarks, they are
still struggling with low-level vision tasks that are easy to humans.
Specifically, on BlindTest, our suite of 7 very simple tasks, including
identifying (a) whether two circles overlap; (b) how many times two lines
intersect; (c) which letter is being circled in a word; and (d) the number of
circles in an Olympic-like logo, four state-of-the-art VLMs are only 58.07%
accurate on average. Claude 3.5 Sonnet performs the best at 77.84% accuracy,
far from the human expected accuracy of 100%. Across different image
resolutions and line widths, VLMs including slow-thinking models consistently
struggle with those tasks that require precise spatial information when
geometric primitives overlap or are close. Yet, VLMs perform at near-100%
accuracy when much more space is added to separate shapes and letters. Linear
probing experiments show that vision encoders contain sufficient visual
information to solve BlindTest and that language models fail to decode this
information into correct answers. Code and data are at:
https://vlmsareblind.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Viability of Semi-Supervised Segmentation Methods for Statistical
  Shape Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asma Khan, Tushar Kataria, Janmesh Ukey, Shireen Y. Elhabian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical Shape Models (SSMs) excel at identifying population level
anatomical variations, which is at the core of various clinical and biomedical
applications, including morphology-based diagnostics and surgical planning.
However, the effectiveness of SSM is often constrained by the necessity for
expert-driven manual segmentation, a process that is both time-intensive and
expensive, thereby restricting their broader application and utility. Recent
deep learning approaches enable the direct estimation of Statistical Shape
Models (SSMs) from unsegmented images. While these models can predict SSMs
without segmentation during deployment, they do not address the challenge of
acquiring the manual annotations needed for training, particularly in
resource-limited settings. Semi-supervised models for anatomy segmentation can
mitigate the annotation burden. Yet, despite the abundance of available
approaches, there are no established guidelines to inform end-users on their
effectiveness for the downstream task of constructing SSMs. In this study, we
systematically evaluate the potential of semi-supervised methods as viable
alternatives to manual segmentations for building SSMs. We establish a new
performance benchmark by employing various semi-supervised methods for anatomy
segmentation under low annotation settings, utilizing the predicted
segmentations for the task of SSM. Our results indicate that some methods
produce noisy segmentation, which is very unfavorable for SSM tasks, while
others can capture the correct modes of variations in the population cohort
with 60-80% reduction in required manual annotation
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Contrastive Forward-Forward Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Chen, Dongshu Liu, Jeremie Laydevant, Julie Grollier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents that operate autonomously benefit from lifelong learning capabilities.
However, compatible training algorithms must comply with the decentralized
nature of these systems, which imposes constraints on both the parameter counts
and the computational resources. The Forward-Forward (FF) algorithm is one of
these. FF relies only on feedforward operations, the same used for inference,
for optimizing layer-wise objectives. This purely forward approach eliminates
the need for transpose operations required in traditional backpropagation.
Despite its potential, FF has failed to reach state-of-the-art performance on
most standard benchmark tasks, in part due to unreliable negative data
generation methods for unsupervised learning.
  In this work, we propose the Self-Contrastive Forward-Forward (SCFF)
algorithm, a competitive training method aimed at closing this performance gap.
Inspired by standard self-supervised contrastive learning for vision tasks,
SCFF generates positive and negative inputs applicable across various datasets.
The method demonstrates superior performance compared to existing unsupervised
local learning algorithms on several benchmark datasets, including MNIST,
CIFAR-10, STL-10, and Tiny ImageNet. We extend FF's application to training
recurrent neural networks, expanding its utility to sequential data tasks.
These findings pave the way for high-accuracy, real-time learning on
resource-constrained edge devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for
  Efficient <span class="highlight-title">Diffusion</span> Transformers <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran You, Connelly Barnes, Yuqian Zhou, Yan Kang, Zhenbang Du, Wei Zhou, Lingzhi Zhang, Yotam Nitzan, Xiaoyang Liu, Zhe Lin, Eli Shechtman, Sohrab Amirghodsi, Yingyan Celine Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image
generation quality but suffer from high latency and memory inefficiency, making
them difficult to deploy on resource-constrained devices. One major efficiency
bottleneck is that existing DiTs apply equal computation across all regions of
an image. However, not all image tokens are equally important, and certain
localized areas require more computation, such as objects. To address this, we
propose DiffCR, a dynamic DiT inference framework with differentiable
compression ratios, which automatically learns to dynamically route computation
across layers and timesteps for each image token, resulting in efficient DiTs.
Specifically, DiffCR integrates three features: (1) A token-level routing
scheme where each DiT layer includes a router that is fine-tuned jointly with
model weights to predict token importance scores. In this way, unimportant
tokens bypass the entire layer's computation; (2) A layer-wise differentiable
ratio mechanism where different DiT layers automatically learn varying
compression ratios from a zero initialization, resulting in large compression
ratios in redundant layers while others remain less compressed or even
uncompressed; (3) A timestep-wise differentiable ratio mechanism where each
denoising timestep learns its own compression ratio. The resulting pattern
shows higher ratios for noisier timesteps and lower ratios as the image becomes
clearer. Extensive experiments on text-to-image and inpainting tasks show that
DiffCR effectively captures dynamism across token, layer, and timestep axes,
achieving superior trade-offs between generation quality and efficiency
compared to prior works. The project website is available at
https://www.haoranyou.com/diffcr.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Multi-modal Representations by Watching Hundreds of Surgical
  Video Lectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15220v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15220v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Yuan, Vinkle Srivastav, Tong Yu, Joel L. Lavanchy, Jacques Marescaux, Pietro Mascagni, Nassir Navab, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in surgical computer vision applications have been driven
by vision-only models, which do not explicitly integrate the rich semantics of
language into their design. These methods rely on manually annotated surgical
videos to predict a fixed set of object categories, limiting their
generalizability to unseen surgical procedures and downstream tasks. In this
work, we put forward the idea that the surgical video lectures available
through open surgical e-learning platforms can provide effective vision and
language supervisory signals for multi-modal representation learning without
relying on manual annotations. We address the surgery-specific linguistic
challenges present in surgical video lectures by employing multiple
complementary automatic speech recognition systems to generate text
transcriptions. We then present a novel method, SurgVLP - Surgical Vision
Language Pre-training, for multi-modal representation learning. Extensive
experiments across diverse surgical procedures and tasks demonstrate that the
multi-modal representations learned by SurgVLP exhibit strong transferability
and adaptability in surgical video analysis. Furthermore, our zero-shot
evaluations highlight SurgVLP's potential as a general-purpose foundation model
for surgical workflow analysis, reducing the reliance on extensive manual
annotations for downstream tasks, and facilitating adaptation methods such as
few-shot learning to build a scalable and data-efficient solution for various
downstream surgical applications. The [training
code](https://github.com/CAMMA-public/SurgVLP) and
[weights](https://github.com/CAMMA-public/PeskaVLP) are public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMAI-VL & GMAI-VL-5.5M: A Large <span class="highlight-title">Vision</span>-Language Model and A
  Comprehensive Multimodal <span class="highlight-title">Dataset</span> Towards General Medical AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianbin Li, Yanzhou Su, Wei Li, Bin Fu, Zhe Chen, Ziyan Huang, Guoan Wang, Chenglong Ma, Ying Chen, Ming Hu, Yanjun Li, Pengcheng Chen, Xiaowei Hu, Zhongying Deng, Yuanfeng Ji, Jin Ye, Yu Qiao, Junjun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in general AI, its effectiveness in the
medical domain is limited by the lack of specialized medical knowledge. To
address this, we formulate GMAI-VL-5.5M, a multimodal medical dataset created
by converting hundreds of specialized medical datasets with various annotations
into high-quality image-text pairs. This dataset offers comprehensive task
coverage, diverse modalities, and rich image-text data. Building upon this
dataset, we develop GMAI-VL, a general medical vision-language model, with a
three-stage training strategy that enhances the integration of visual and
textual information. This approach significantly improves the model's ability
to process multimodal data, supporting accurate diagnoses and clinical
decision-making. Experiments show that GMAI-VL achieves state-of-the-art
performance across various multimodal medical tasks, including visual question
answering and medical image diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Splatting Lucas-Kanade 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liuyue Xie, Joel Julin, Koichiro Niinuma, Laszlo A. Jeni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Splatting and its dynamic extensions are effective for
reconstructing 3D scenes from 2D images when there is significant camera
movement to facilitate motion parallax and when scene objects remain relatively
static. However, in many real-world scenarios, these conditions are not met. As
a consequence, data-driven semantic and geometric priors have been favored as
regularizers, despite their bias toward training data and their neglect of
broader movement dynamics.
  Departing from this practice, we propose a novel analytical approach that
adapts the classical Lucas-Kanade method to dynamic Gaussian splatting. By
leveraging the intrinsic properties of the forward warp field network, we
derive an analytical velocity field that, through time integration, facilitates
accurate scene flow computation. This enables the precise enforcement of motion
constraints on warp fields, thus constraining both 2D motion and 3D positions
of the Gaussians. Our method excels in reconstructing highly dynamic scenes
with minimal camera movement, as demonstrated through experiments on both
synthetic and real-world scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discretized Gaussian Representation for Tomographic Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04844v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04844v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaokai Wu, Yuxiang Lu, Wei Ji, Suizhi Huang, Fengyu Yang, Shalayiding Sirejiding, Qichen He, Jing Tong, Yanbiao Ji, Yue Ding, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed Tomography (CT) is a widely used imaging technique that provides
detailed cross-sectional views of objects. Over the past decade, Deep
Learning-based Reconstruction (DLR) methods have led efforts to enhance image
quality and reduce noise, yet they often require large amounts of data and are
computationally intensive. Inspired by recent advancements in scene
reconstruction, some approaches have adapted NeRF and 3D Gaussian Splatting
(3DGS) techniques for CT reconstruction. However, these methods are not ideal
for direct 3D volume reconstruction. In this paper, we propose a novel
Discretized Gaussian Representation (DGR) for CT reconstruction, which directly
reconstructs the 3D volume using a set of discretized Gaussian functions in an
end-to-end manner. To further enhance computational efficiency, we introduce a
Fast Volume Reconstruction technique that aggregates the contributions of these
Gaussians into a discretized volume in a highly parallelized fashion. Our
extensive experiments on both real-world and synthetic datasets demonstrate
that DGR achieves superior reconstruction quality and significantly improved
computational efficiency compared to existing DLR and instance reconstruction
methods. Our code has been provided for review purposes and will be made
publicly available upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual AD Narration with Interleaved Multimodal Sequence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Wang, Zhan Tong, Kecheng Zheng, Yujun Shen, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Audio Description (AD) task aims to generate descriptions of visual
elements for visually impaired individuals to help them access long-form video
content, like movies. With video feature, text, character bank and context
information as inputs, the generated ADs are able to correspond to the
characters by name and provide reasonable, contextual descriptions to help
audience understand the storyline of movie. To achieve this goal, we propose to
leverage pre-trained foundation models through a simple and unified framework
to generate ADs with interleaved multimodal sequence as input, termed as
Uni-AD. To enhance the alignment of features across various modalities with
finer granularity, we introduce a simple and lightweight module that maps video
features into the textual feature space. Moreover, we also propose a
character-refinement module to provide more precise information by identifying
the main characters who play more significant roles in the video context. With
these unique designs, we further incorporate contextual information and a
contrastive loss into our architecture to generate smoother and more
contextually appropriate ADs. Experiments on multiple AD datasets show that
Uni-AD performs well on AD generation, which demonstrates the effectiveness of
our approach. Our code is available at: https://github.com/ant-research/UniAD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GI-GS: Global Illumination Decomposition on Gaussian Splatting for
  Inverse Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongze Chen, Zehong Lin, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GI-GS, a novel inverse rendering framework that leverages 3D
Gaussian Splatting (3DGS) and deferred shading to achieve photo-realistic novel
view synthesis and relighting. In inverse rendering, accurately modeling the
shading processes of objects is essential for achieving high-fidelity results.
Therefore, it is critical to incorporate global illumination to account for
indirect lighting that reaches an object after multiple bounces across the
scene. Previous 3DGS-based methods have attempted to model indirect lighting by
characterizing indirect illumination as learnable lighting volumes or
additional attributes of each Gaussian, while using baked occlusion to
represent shadow effects. These methods, however, fail to accurately model the
complex physical interactions between light and objects, making it impossible
to construct realistic indirect illumination during relighting. To address this
limitation, we propose to calculate indirect lighting using efficient path
tracing with deferred shading. In our framework, we first render a G-buffer to
capture the detailed geometry and material properties of the scene. Then, we
perform physically-based rendering (PBR) only for direct lighting. With the
G-buffer and previous rendering results, the indirect lighting can be
calculated through a lightweight path tracing. Our method effectively models
indirect lighting under any given lighting conditions, thereby achieving better
novel view synthesis and competitive relighting. Quantitative and qualitative
results show that our GI-GS outperforms existing baselines in both rendering
quality and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version. Project page:
  https://stopaimme.github.io/GI-GS-site/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitating Radiological Scrolling: A Global-Local Attention Model for 3D
  Chest CT Volumes Multi-Label Anomaly Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in the number of Computed Tomography (CT) scan
examinations has created an urgent need for automated tools, such as organ
segmentation, anomaly classification, and report generation, to assist
radiologists with their growing workload. Multi-label classification of
Three-Dimensional (3D) CT scans is a challenging task due to the volumetric
nature of the data and the variety of anomalies to be detected. Existing deep
learning methods based on Convolutional Neural Networks (CNNs) struggle to
capture long-range dependencies effectively, while Vision Transformers require
extensive pre-training, posing challenges for practical use. Additionally,
these existing methods do not explicitly model the radiologist's navigational
behavior while scrolling through CT scan slices, which requires both global
context understanding and local detail awareness. In this study, we present
CT-Scroll, a novel global-local attention model specifically designed to
emulate the scrolling behavior of radiologists during the analysis of 3D CT
scans. Our approach is evaluated on two public datasets, demonstrating its
efficacy through comprehensive experiments and an ablation study that
highlights the contribution of each model component.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures. Accepted for MIDL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image segmentation of treated and untreated tumor spheroids by Fully
  Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01105v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01105v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Streller, Soňa Michlíková, Willy Ciecior, Katharina Lönnecke, Leoni A. Kunz-Schughart, Steffen Lange, Anja Voss-Böhme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multicellular tumor spheroids (MCTS) are advanced cell culture systems for
assessing the impact of combinatorial radio(chemo)therapy. They exhibit
therapeutically relevant in-vivo-like characteristics from 3D cell-cell and
cell-matrix interactions to radial pathophysiological gradients related to
proliferative activity and nutrient/oxygen supply, altering cellular
radioresponse. State-of-the-art assays quantify long-term curative endpoints
based on collected brightfield image time series from large treated spheroid
populations per irradiation dose and treatment arm. Here, spheroid control
probabilities are documented analogous to in-vivo tumor control probabilities
based on Kaplan-Meier curves. This analyses require laborious spheroid
segmentation of up to 100.000 images per treatment arm to extract relevant
structural information from the images, e.g., diameter, area, volume and
circularity. While several image analysis algorithms are available for spheroid
segmentation, they all focus on compact MCTS with clearly distinguishable outer
rim throughout growth. However, treated MCTS may partly be detached and
destroyed and are usually obscured by dead cell debris. We successfully train
two Fully Convolutional Networks, UNet and HRNet, and optimize their
hyperparameters to develop an automatic segmentation for both untreated and
treated MCTS. We systematically validate the automatic segmentation on larger,
independent data sets of spheroids derived from two human head-and-neck cancer
cell lines. We find an excellent overlap between manual and automatic
segmentation for most images, quantified by Jaccard indices at around 90%. For
images with smaller overlap of the segmentations, we demonstrate that this
error is comparable to the variations across segmentations from different
biological experts, suggesting that these images represent biologically unclear
or ambiguous cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TREAD: Token Routing for Efficient Architecture-agnostic <span class="highlight-title">Diffusion</span>
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Krause, Timy Phan, Ming Gui, Stefan Andreas Baumann, Vincent Tao Hu, Björn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as the mainstream approach for visual
generation. However, these models typically suffer from sample inefficiency and
high training costs. Consequently, methods for efficient finetuning, inference
and personalization were quickly adopted by the community. However, training
these models in the first place remains very costly. While several recent
approaches - including masking, distillation, and architectural modifications -
have been proposed to improve training efficiency, each of these methods comes
with a tradeoff: they achieve enhanced performance at the expense of increased
computational cost or vice versa. In contrast, this work aims to improve
training efficiency as well as generative performance at the same time through
routes that act as a transport mechanism for randomly selected tokens from
early layers to deeper layers of the model. Our method is not limited to the
common transformer-based model - it can also be applied to state-space models
and achieves this without architectural modifications or additional parameters.
Finally, we show that TREAD reduces computational cost and simultaneously
boosts model performance on the standard ImageNet-256 benchmark in
class-conditional synthesis. Both of these benefits multiply to a convergence
speedup of 14x at 400K training iterations compared to DiT and 37x compared to
the best benchmark performance of DiT at 7M training iterations. Furthermore,
we achieve a competitive FID of 2.09 in a guided and 3.93 in an unguided
setting, which improves upon the DiT, without architectural changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-Panda: Parameter-efficient Alignment for Encoder-free
  Video-Language Models <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18609v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18609v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhui Yi, Syed Talal Wasim, Yanan Luo, Muzammal Naseer, Juergen Gall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an efficient encoder-free approach for video-language
understanding that achieves competitive performance while significantly
reducing computational overhead. Current video-language models typically rely
on heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4B
parameters), creating a substantial computational burden when processing
multi-frame videos. Our method introduces a novel Spatio-Temporal Alignment
Block (STAB) that directly processes video inputs without requiring pre-trained
encoders while using only 45M parameters for visual processing - at least a
6.5$\times$ reduction compared to traditional approaches. The STAB architecture
combines Local Spatio-Temporal Encoding for fine-grained feature extraction,
efficient spatial downsampling through learned attention and separate
mechanisms for modeling frame-level and video-level relationships. Our model
achieves comparable or superior performance to encoder-based approaches for
open-ended video question answering on standard benchmarks. The fine-grained
video question-answering evaluation demonstrates our model's effectiveness,
outperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in key
aspects like correctness and temporal understanding. Extensive ablation studies
validate our architectural choices and demonstrate the effectiveness of our
spatio-temporal modeling approach while achieving 3-4$\times$ faster processing
speeds than previous methods. Code is available at
https://jh-yi.github.io/Video-Panda.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demand Estimation with Text and Image Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Compiani, Ilya Morozov, Stephan Seiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a demand estimation method that leverages unstructured text and
image data to infer substitution patterns. Using pre-trained deep learning
models, we extract embeddings from product images and textual descriptions and
incorporate them into a random coefficients logit model. This approach enables
researchers to estimate demand even when they lack data on product attributes
or when consumers value hard-to-quantify attributes, such as visual design or
functional benefits. Using data from a choice experiment, we show that our
approach outperforms standard attribute-based models in counterfactual
predictions of consumers' second choices. We also apply it across 40 product
categories on Amazon and consistently find that text and image data help
identify close substitutes within each category.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantization-aware Matrix Factorization for Low Bit Rate Image
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pooya Ashtari, Pourya Behmandpoor, Fateme Nateghi Haredasht, Jonathan H. Chen, Panagiotis Patrinos, Sabine Van Huffel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lossy image compression is essential for efficient transmission and storage.
Traditional compression methods mainly rely on discrete cosine transform (DCT)
or singular value decomposition (SVD), both of which represent image data in
continuous domains and, therefore, necessitate carefully designed quantizers.
Notably, these methods consider quantization as a separate step, where
quantization errors cannot be incorporated into the compression process. The
sensitivity of these methods, especially SVD-based ones, to quantization errors
significantly degrades reconstruction quality. To address this issue, we
introduce a quantization-aware matrix factorization (QMF) to develop a novel
lossy image compression method. QMF provides a low-rank representation of the
image data as a product of two smaller factor matrices, with elements
constrained to bounded integer values, thereby effectively integrating
quantization with low-rank approximation. We propose an efficient, provably
convergent iterative algorithm for QMF using a block coordinate descent (BCD)
scheme, with subproblems having closed-form solutions. Our experiments on the
Kodak and CLIC 2024 datasets demonstrate that our QMF compression method
consistently outperforms JPEG at low bit rates below 0.25 bits per pixel (bpp)
and remains comparable at higher bit rates. We also assessed our method's
capability to preserve visual semantics by evaluating an ImageNet pre-trained
classifier on compressed images. Remarkably, our method improved top-1 accuracy
by over 5 percentage points compared to JPEG at bit rates under 0.25 bpp. The
project is available at https://github.com/pashtari/lrf .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures, 1 table, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UAV-DETR: Efficient End-to-End Object Detection for Unmanned Aerial
  Vehicle Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaxiang Zhang, Kai Liu, Zhongxue Gan, Guo-Niu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicle object detection (UAV-OD) has been widely used in
various scenarios. However, most existing UAV-OD algorithms rely on manually
designed components, which require extensive tuning. End-to-end models that do
not depend on such manually designed components are mainly designed for natural
images, which are less effective for UAV imagery. To address such challenges,
this paper proposes an efficient detection transformer (DETR) framework
tailored for UAV imagery, i.e., UAV-DETR. The framework includes a multi-scale
feature fusion with frequency enhancement module, which captures both spatial
and frequency information at different scales. In addition, a frequency-focused
down-sampling module is presented to retain critical spatial details during
down-sampling. A semantic alignment and calibration module is developed to
align and fuse features from different fusion paths. Experimental results
demonstrate the effectiveness and generalization of our approach across various
UAV imagery datasets. On the VisDrone dataset, our method improves AP by 3.1\%
and $\text{AP}_{50}$ by 4.2\% over the baseline. Similar enhancements are
observed on the UAVVaste dataset. The project page:
https://github.com/ValiantDiligent/UAV-DETR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegMAN: Omni-scale Context Modeling with State Space Models and Local
  Attention for Semantic Segmentation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiang Fu, Meng Lou, Yizhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality semantic segmentation relies on three key capabilities: global
context modeling, local detail encoding, and multi-scale feature extraction.
However, recent methods struggle to possess all these capabilities
simultaneously. Hence, we aim to empower segmentation networks to
simultaneously carry out efficient global context modeling, high-quality local
detail encoding, and rich multi-scale feature representation for varying input
resolutions. In this paper, we introduce SegMAN, a novel linear-time model
comprising a hybrid feature encoder dubbed SegMAN Encoder, and a decoder based
on state space models. Specifically, the SegMAN Encoder synergistically
integrates sliding local attention with dynamic state space models, enabling
highly efficient global context modeling while preserving fine-grained local
details. Meanwhile, the MMSCopE module in our decoder enhances multi-scale
context feature extraction and adaptively scales with the input resolution. Our
SegMAN-B Encoder achieves 85.1% ImageNet-1k accuracy (+1.5% over VMamba-S with
fewer parameters). When paired with our decoder, the full SegMAN-B model
achieves 52.6% mIoU on ADE20K (+1.6% over SegNeXt-L with 15% fewer GFLOPs),
83.8% mIoU on Cityscapes (+2.1% over SegFormer-B3 with half the GFLOPs), and
1.6% higher mIoU than VWFormer-B3 on COCO-Stuff with lower GFLOPs. Our code is
available at https://github.com/yunxiangfu2001/SegMAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13255v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13255v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandström, Stefano Mattoccia, Martin R. Oswald, Matteo Poggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past two decades, research in the field of Simultaneous Localization
and Mapping (SLAM) has undergone a significant evolution, highlighting its
critical role in enabling autonomous exploration of unknown environments. This
evolution ranges from hand-crafted methods, through the era of deep learning,
to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D
Gaussian Splatting (3DGS) representations. Recognizing the growing body of
research and the absence of a comprehensive survey on the topic, this paper
aims to provide the first comprehensive overview of SLAM progress through the
lens of the latest advancements in radiance fields. It sheds light on the
background, evolutionary path, inherent strengths and limitations, and serves
as a fundamental reference to highlight the dynamic progress and specific
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated to November 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency Trajectory Matching for One-Step Generative Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyi You, Mingyang Zhang, Leheng Zhang, Xingyu Zhou, Kexuan Shi, Shuhang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current diffusion-based super-resolution (SR) approaches achieve commendable
performance at the cost of high inference overhead. Therefore, distillation
techniques are utilized to accelerate the multi-step teacher model into
one-step student model. Nevertheless, these methods significantly raise
training costs and constrain the performance of the student model by the
teacher model. To overcome these tough challenges, we propose Consistency
Trajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy
that is able to generate photo-realistic SR results in one step. Concretely, we
first formulate a Probability Flow Ordinary Differential Equation (PF-ODE)
trajectory to establish a deterministic mapping from low-resolution (LR) images
with noise to high-resolution (HR) images. Then we apply the Consistency
Training (CT) strategy to directly learn the mapping in one step, eliminating
the necessity of pre-trained diffusion model. To further enhance the
performance and better leverage the ground-truth during the training process,
we aim to align the distribution of SR results more closely with that of the
natural images. To this end, we propose to minimize the discrepancy between
their respective PF-ODE trajectories from the LR image distribution by our
meticulously designed Distribution Trajectory Matching (DTM) loss, resulting in
improved realism of our recovered HR images. Comprehensive experimental results
demonstrate that the proposed methods can attain comparable or even superior
capabilities on both synthetic and real datasets while maintaining minimal
inference latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Object Detection by Modifying Synthetic Data with Explainable
  AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitish Mital, Simon Malzard, Richard Walters, Celso M. De Melo, Raghuveer Rao, Victoria Nockles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Limited real-world data severely impacts model performance in many computer
vision domains, particularly for samples that are underrepresented in training.
Synthetically generated images are a promising solution, but 1) it remains
unclear how to design synthetic training data to optimally improve model
performance (e.g, whether and where to introduce more realism or more
abstraction) and 2) the domain expertise, time and effort required from human
operators for this design and optimisation process represents a major practical
challenge. Here we propose a novel conceptual approach to improve the
efficiency of designing synthetic images, by using robust Explainable AI (XAI)
techniques to guide a human-in-the-loop process of modifying 3D mesh models
used to generate these images. Importantly, this framework allows both
modifications that increase and decrease realism in synthetic data, which can
both improve model performance. We illustrate this concept using a real-world
example where data are sparse; detection of vehicles in infrared imagery. We
fine-tune an initial YOLOv8 model on the ATR DSIAC infrared dataset and
synthetic images generated from 3D mesh models in the Unity gaming engine, and
then use XAI saliency maps to guide modification of our Unity models. We show
that synthetic data can improve detection of vehicles in orientations unseen in
training by 4.6% (to mAP50 = 94.6%). We further improve performance by an
additional 1.5% (to 96.1%) through our new XAI-guided approach, which reduces
misclassifications through both increasing and decreasing the realism of
different parts of the synthetic data. Our proof-of-concept results pave the
way for fine, XAI-controlled curation of synthetic datasets tailored to improve
object detection performance, whilst simultaneously reducing the burden on
human operators in designing and optimising these datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly 4D
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhening Liu, Yingdong Hu, Xinjie Zhang, Rui Song, Jiawei Shao, Zehong Lin, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development of 3D Gaussian Splatting (3DGS) has led to great
interest in 4D dynamic spatial reconstruction. Existing approaches mainly rely
on full-length multi-view videos, while there has been limited exploration of
online reconstruction methods that enable on-the-fly training and per-timestep
streaming. Current 3DGS-based streaming methods treat the Gaussian primitives
uniformly and constantly renew the densified Gaussians, thereby overlooking the
difference between dynamic and static features as well as neglecting the
temporal continuity in the scene. To address these limitations, we propose a
novel three-stage pipeline for iterative streamable 4D dynamic spatial
reconstruction. Our pipeline comprises a selective inheritance stage to
preserve temporal continuity, a dynamics-aware shift stage to distinguish
dynamic and static primitives and optimize their movements, and an error-guided
densification stage to accommodate emerging objects. Our method achieves
state-of-the-art performance in online 4D reconstruction, demonstrating the
fastest on-the-fly training, superior representation quality, and real-time
rendering capability. Project page: https://www.liuzhening.top/DASS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://www.liuzhening.top/DASS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EventMamba: Enhancing Spatio-Temporal Locality with State Space Models
  for Event-Based Video Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengjie Ge, Xueyang Fu, Peng He, Kunyu Wang, Chengzhi Cao, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging its robust linear global modeling capability, Mamba has notably
excelled in computer vision. Despite its success, existing Mamba-based vision
models have overlooked the nuances of event-driven tasks, especially in video
reconstruction. Event-based video reconstruction (EBVR) demands spatial
translation invariance and close attention to local event relationships in the
spatio-temporal domain. Unfortunately, conventional Mamba algorithms apply
static window partitions and standard reshape scanning methods, leading to
significant losses in local connectivity. To overcome these limitations, we
introduce EventMamba--a specialized model designed for EBVR tasks. EventMamba
innovates by incorporating random window offset (RWO) in the spatial domain,
moving away from the restrictive fixed partitioning. Additionally, it features
a new consistent traversal serialization approach in the spatio-temporal
domain, which maintains the proximity of adjacent events both spatially and
temporally. These enhancements enable EventMamba to retain Mamba's robust
modeling capabilities while significantly preserving the spatio-temporal
locality of event data. Comprehensive testing on multiple datasets shows that
EventMamba markedly enhances video reconstruction, drastically improving
computation speed while delivering superior visual quality compared to
Transformer-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> AnomalyNCD: Towards Novel Anomaly Class Discovery in Industrial
  Scenarios <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Huang, Xurui Li, Haotian Liu, Feng Xue, Yuz<span class="highlight-author">he Wang</span>, Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, multi-class anomaly classification has garnered increasing
attention. Previous methods directly cluster anomalies but often struggle due
to the lack of anomaly-prior knowledge. Acquiring this knowledge faces two
issues: the non-prominent and weak-semantics anomalies. In this paper, we
propose AnomalyNCD, a multi-class anomaly classification network compatible
with different anomaly detection methods. To address the non-prominence of
anomalies, we design main element binarization (MEBin) to obtain
anomaly-centered images, ensuring anomalies are learned while avoiding the
impact of incorrect detections. Next, to learn anomalies with weak semantics,
we design mask-guided representation learning, which focuses on isolated
anomalies guided by masks and reduces confusion from erroneous inputs through
corrected pseudo labels. Finally, to enable flexible classification at both
region and image levels, we develop a region merging strategy that determines
the overall image category based on the classified anomaly regions. Our method
outperforms the state-of-the-art works on the MVTec AD and MTD datasets.
Compared with the current methods, AnomalyNCD combined with zero-shot anomaly
detection method achieves a 10.8% $F_1$ gain, 8.8% NMI gain, and 9.5% ARI gain
on MVTec AD, and 12.8% $F_1$ gain, 5.7% NMI gain, and 10.8% ARI gain on MTD.
Code is available at https://github.com/HUST-SLOW/AnomalyNCD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training-free <span class="highlight-title">Diffusion</span> Acceleration with Bottleneck Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Yunhai Tong, Ling Yang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable capabilities in visual content
generation but remain challenging to deploy due to their high computational
cost during inference. This computational burden primarily arises from the
quadratic complexity of self-attention with respect to image or video
resolution. While existing acceleration methods often compromise output quality
or necessitate costly retraining, we observe that most diffusion models are
pre-trained at lower resolutions, presenting an opportunity to exploit these
low-resolution priors for more efficient inference without degrading
performance. In this work, we introduce Bottleneck Sampling, a training-free
framework that leverages low-resolution priors to reduce computational overhead
while preserving output fidelity. Bottleneck Sampling follows a high-low-high
denoising workflow: it performs high-resolution denoising in the initial and
final stages while operating at lower resolutions in intermediate steps. To
mitigate aliasing and blurring artifacts, we further refine the resolution
transition points and adaptively shift the denoising timesteps at each stage.
We evaluate Bottleneck Sampling on both image and video generation tasks, where
extensive experiments demonstrate that it accelerates inference by up to
3$\times$ for image generation and 2.5$\times$ for video generation, all while
maintaining output quality comparable to the standard full-resolution sampling
process across multiple evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://tyfeld.github.io/BottleneckSampling.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D
  Generation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinnan Chen, Lingting Zhu, Zeyu Hu, Shengju Qian, Yugang Chen, Xin Wang, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in auto-regressive transformers have revolutionized
generative modeling across different domains, from language processing to
visual generation, demonstrating remarkable capabilities. However, applying
these advances to 3D generation presents three key challenges: the unordered
nature of 3D data conflicts with sequential next-token prediction paradigm,
conventional vector quantization approaches incur substantial compression loss
when applied to 3D meshes, and the lack of efficient scaling strategies for
higher resolution latent prediction. To address these challenges, we introduce
MAR-3D, which integrates a pyramid variational autoencoder with a cascaded
masked auto-regressive transformer (Cascaded MAR) for progressive latent
upscaling in the continuous space. Our architecture employs random masking
during training and auto-regressive denoising in random order during inference,
naturally accommodating the unordered property of 3D latent tokens.
Additionally, we propose a cascaded training strategy with condition
augmentation that enables efficiently up-scale the latent token resolution with
fast convergence. Extensive experiments demonstrate that MAR-3D not only
achieves superior performance and generalization capabilities compared to
existing methods but also exhibits enhanced scaling capabilities compared to
joint distribution modeling approaches (e.g., diffusion transformers).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Volumetric Surfaces: Representing Fuzzy Geometries with Layered Meshes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Esposito, Anpei Chen, Christian Reiser, Samuel Rota Bulò, Lorenzo Porzi, Katja Schwarz, Christian Richardt, Michael Zollhöfer, Peter Kontschieder, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality view synthesis relies on volume rendering, splatting, or surface
rendering. While surface rendering is typically the fastest, it struggles to
accurately model fuzzy geometry like hair. In turn, alpha-blending techniques
excel at representing fuzzy materials but require an unbounded number of
samples per ray (P1). Further overheads are induced by empty space skipping in
volume rendering (P2) and sorting input primitives in splatting (P3). We
present a novel representation for real-time view synthesis where the (P1)
number of sampling locations is small and bounded, (P2) sampling locations are
efficiently found via rasterization, and (P3) rendering is sorting-free. We
achieve this by representing objects as semi-transparent multi-layer meshes
rendered in a fixed order. First, we model surface layers as signed distance
function (SDF) shells with optimal spacing learned during training. Then, we
bake them as meshes and fit UV textures. Unlike single-surface methods, our
multi-layer representation effectively models fuzzy objects. In contrast to
volume and splatting-based methods, our approach enables real-time rendering on
low-power laptops and smartphones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Motion Transfer with <span class="highlight-title">Diffusion</span> Transformers <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07776v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07776v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Pondaven, Aliaksandr Siarohin, Sergey Tulyakov, Philip Torr, Fabio Pizzati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose DiTFlow, a method for transferring the motion of a reference video
to a newly synthesized one, designed specifically for Diffusion Transformers
(DiT). We first process the reference video with a pre-trained DiT to analyze
cross-frame attention maps and extract a patch-wise motion signal called the
Attention Motion Flow (AMF). We guide the latent denoising process in an
optimization-based, training-free, manner by optimizing latents with our AMF
loss to generate videos reproducing the motion of the reference one. We also
apply our optimization strategy to transformer positional embeddings, granting
us a boost in zero-shot motion transfer capabilities. We evaluate DiTFlow
against recently published methods, outperforming all across multiple metrics
and human evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025 - Project page: https://ditflow.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LANTERN++: Enhancing Relaxed Speculative Decoding with Static Tree
  Drafting for <span class="highlight-title">Visual</span> Auto-regressive Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihwan Park, Doohyuk Jang, Sungyub Kim, Souvik Kundu, Eunho Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding has been widely used to accelerate auto-regressive (AR)
text generation. However, its effectiveness for visual AR models remains
limited due to token selection ambiguity, where multiple tokens share similarly
low probabilities and thus reduce acceptance rates. Recently, relaxed
speculative decoding with dynamic tree drafting was proposed to mitigate this
ambiguity, demonstrating promising results in accelerating visual AR models.
However, we observe that token selection ambiguity still negatively affects
dynamic tree drafting, resulting in shallow draft trees and limited
acceleration. To overcome this issue, we introduce LANTERN++, a refined
framework that integrates static tree drafting with a tailored relaxed
acceptance condition, allowing drafts to be selected independently of
low-confidence predictions. This enables the acceptance of deeper sequences,
improving decoding efficiency while preserving image quality. Extensive
experiments on state-of-the-art visual AR models demonstrate that LANTERN++
significantly accelerates inference, achieving up to $\mathbf{\times 2.56}$
speedup over standard AR decoding while maintaining high image quality. The
code is publicly available at https://github.com/jadohu/LANTERN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Workshop at SCOPE (Oral), 16 pages, 5 figures, short paper
  (6 pages exclude reference and appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Video Tokenization: A Conditioned <span class="highlight-title">Diffusion</span>-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03708v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03708v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, Xudong Lu, Zhihang Liu, Yun Zheng, Yu Liu, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing video tokenizers typically use the traditional Variational
Autoencoder (VAE) architecture for video compression and reconstruction.
However, to achieve good performance, its training process often relies on
complex multi-stage training tricks that go beyond basic reconstruction loss
and KL regularization. Among these tricks, the most challenging is the precise
tuning of adversarial training with additional Generative Adversarial Networks
(GANs) in the final stage, which can hinder stable convergence. In contrast to
GANs, diffusion models offer more stable training processes and can generate
higher-quality results. Inspired by these advantages, we propose CDT, a novel
Conditioned Diffusion-based video Tokenizer, that replaces the GAN-based
decoder with a conditional causal diffusion model. The encoder compresses
spatio-temporal information into compact latents, while the decoder
reconstructs videos through a reverse diffusion process conditioned on these
latents. During inference, we incorporate a feature cache mechanism to generate
videos of arbitrary length while maintaining temporal continuity and adopt
sampling acceleration technique to enhance efficiency. Trained using only a
basic MSE diffusion loss for reconstruction, along with KL term and LPIPS
perceptual loss from scratch, extensive experiments demonstrate that CDT
achieves state-of-the-art performance in video reconstruction tasks with just a
single-step sampling. Even a scaled-down version of CDT (3$\times$ inference
speedup) still performs comparably with top baselines. Moreover, the latent
video generation model trained with CDT also exhibits superior performance. The
source code and pretrained weights are available at
https://github.com/ali-vilab/CDT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Feature-Driven Deep Learning for the Prediction of Duck Body
  Dimensions and Weight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14001v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14001v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xiao, Qiannan Han, Gang Shu, Guiping Liang, Hongyan Zhang, Song Wang, Zhihao Xu, Weican Wan, Chuang Li, Guitao Jiang, Wenbo Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate body dimension and weight measurements are critical for optimizing
poultry management, health assessment, and economic efficiency. This study
introduces an innovative deep learning-based model leveraging multimodal
data-2D RGB images from different views, depth images, and 3D point clouds-for
the non-invasive estimation of duck body dimensions and weight. A dataset of
1,023 Linwu ducks, comprising over 5,000 samples with diverse postures and
conditions, was collected to support model training. The proposed method
innovatively employs PointNet++ to extract key feature points from point
clouds, extracts and computes corresponding 3D geometric features, and fuses
them with multi-view convolutional 2D features. A Transformer encoder is then
utilized to capture long-range dependencies and refine feature interactions,
thereby enhancing prediction robustness. The model achieved a mean absolute
percentage error (MAPE) of 6.33% and an R2 of 0.953 across eight morphometric
parameters, demonstrating strong predictive capability. Unlike conventional
manual measurements, the proposed model enables high-precision estimation while
eliminating the necessity for physical handling, thereby reducing animal stress
and broadening its application scope. This study marks the first application of
deep learning techniques to poultry body dimension and weight estimation,
providing a valuable reference for the intelligent and precise management of
the livestock industry with far-reaching practical significance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal-Guided Spiking Neural Networks for Event-Based Human Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Yang, Shilin Lu, Shizheng Wang, Meng Hwa Er, Zengwei Zheng, Alex C. Kot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the promising interplay between spiking neural networks
(SNNs) and event-based cameras for privacy-preserving human action recognition
(HAR). The unique feature of event cameras in capturing only the outlines of
motion, combined with SNNs' proficiency in processing spatiotemporal data
through spikes, establishes a highly synergistic compatibility for event-based
HAR. Previous studies, however, have been limited by SNNs' ability to process
long-term temporal information, essential for precise HAR. In this paper, we
introduce two novel frameworks to address this: temporal segment-based SNN
(\textit{TS-SNN}) and 3D convolutional SNN (\textit{3D-SNN}). The
\textit{TS-SNN} extracts long-term temporal information by dividing actions
into shorter segments, while the \textit{3D-SNN} replaces 2D spatial elements
with 3D components to facilitate the transmission of temporal information. To
promote further research in event-based HAR, we create a dataset,
\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V
event camera $(1280 \times 800)$, comprising 7 distinct actions. Extensive
experimental results show that our proposed frameworks surpass state-of-the-art
SNN methods on our newly collected dataset and three other neuromorphic
datasets, showcasing their effectiveness in handling long-range temporal
information for event-based HAR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> FaceID-6M: A Large-Scale, Open-Source FaceID Customization <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu<span class="highlight-author">he Wang</span>, Xiaoya Li, Jiwei Li, Guoyin Wang, Xiaofei Sun, Bob Zhu, Han Qiu, Mo Yu, Shengjie Shen, Tianwei Zhang, Eduard Hovy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the data-driven nature of current face identity (FaceID) customization
methods, all state-of-the-art models rely on large-scale datasets containing
millions of high-quality text-image pairs for training. However, none of these
datasets are publicly available, which restricts transparency and hinders
further advancements in the field.
  To address this issue, in this paper, we collect and release FaceID-6M, the
first large-scale, open-source FaceID dataset containing 6 million high-quality
text-image pairs. Filtered from LAION-5B \cite{schuhmann2022laion}, FaceID-6M
undergoes a rigorous image and text filtering steps to ensure dataset quality,
including resolution filtering to maintain high-quality images and faces, face
filtering to remove images that lack human faces, and keyword-based strategy to
retain descriptions containing human-related terms (e.g., nationality,
professions and names). Through these cleaning processes, FaceID-6M provides a
high-quality dataset optimized for training powerful FaceID customization
models, facilitating advancements in the field by offering an open resource for
research and development.
  We conduct extensive experiments to show the effectiveness of our FaceID-6M,
demonstrating that models trained on our FaceID-6M dataset achieve performance
that is comparable to, and slightly better than currently available industrial
models. Additionally, to support and advance research in the FaceID
customization community, we make our code, datasets, and models fully publicly
available. Our codes, models, and datasets are available at:
https://github.com/ShuheSH/FaceID-6M.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2501.15407</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EnvGS: Modeling View-Dependent Appearance with Environment Gaussian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng, Hujun Bao, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing complex reflections in real-world scenes from 2D images is
essential for achieving photorealistic novel view synthesis. Existing methods
that utilize environment maps to model reflections from distant lighting often
struggle with high-frequency reflection details and fail to account for
near-field reflections. In this work, we introduce EnvGS, a novel approach that
employs a set of Gaussian primitives as an explicit 3D representation for
capturing reflections of environments. These environment Gaussian primitives
are incorporated with base Gaussian primitives to model the appearance of the
whole scene. To efficiently render these environment Gaussian primitives, we
developed a ray-tracing-based renderer that leverages the GPU's RT core for
fast rendering. This allows us to jointly optimize our model for high-quality
reconstruction while maintaining real-time rendering speeds. Results from
multiple real-world and synthetic datasets demonstrate that our method produces
significantly more detailed reflections, achieving the best rendering quality
in real-time novel view synthesis. The code is available at
https://zju3dv.github.io/envgs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/envgs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene
  Understanding <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duo Zheng, Shijia Huang, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Multimodal Large Language Models (MLLMs) has
significantly impacted various multimodal tasks. However, these models face
challenges in tasks that require spatial understanding within 3D environments.
Efforts to enhance MLLMs, such as incorporating point cloud features, have been
made, yet a considerable gap remains between the models' learned
representations and the inherent complexity of 3D scenes. This discrepancy
largely stems from the training of MLLMs on predominantly 2D data, which
restricts their effectiveness in comprehending 3D spaces. To address this
issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM,
for 3D scene understanding. By treating 3D scenes as dynamic videos and
incorporating 3D position encoding into these representations, our Video-3D LLM
aligns video representations with real-world spatial contexts more accurately.
In addition, we have implemented a maximum coverage sampling technique to
optimize the trade-off between computational cost and performance. Extensive
experiments demonstrate that our model achieves state-of-the-art performance on
several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer,
Scan2Cap, ScanQA, and SQA3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIRES: Video Instance Repainting via Sketch and Text Guided Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16199v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16199v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchen Weng, Haojie Zheng, Peixuan Zhan, Yuchen Hong, Han Jiang, Si Li, Boxin Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VIRES, a video instance repainting method with sketch and text
guidance, enabling video instance repainting, replacement, generation, and
removal. Existing approaches struggle with temporal consistency and accurate
alignment with the provided sketch sequence. VIRES leverages the generative
priors of text-to-video models to maintain temporal consistency and produce
visually pleasing results. We propose the Sequential ControlNet with the
standardized self-scaling, which effectively extracts structure layouts and
adaptively captures high-contrast sketch details. We further augment the
diffusion transformer backbone with the sketch attention to interpret and
inject fine-grained sketch semantics. A sketch-aware encoder ensures that
repainted results are aligned with the provided sketch sequence. Additionally,
we contribute the VireSet, a dataset with detailed annotations tailored for
training and evaluating video instance editing methods. Experimental results
demonstrate the effectiveness of VIRES, which outperforms state-of-the-art
methods in visual quality, temporal consistency, condition alignment, and human
ratings. Project page: https://hjzheng.net/projects/VIRES/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RGB-Th-Bench: A Dense benchmark for <span class="highlight-title">Visual</span>-Thermal Understanding of
  <span class="highlight-title">Vision</span> Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Moshtaghi, Siavash H. Khajavi, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce RGB-Th-Bench, the first benchmark designed to evaluate the
ability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.
While VLMs have demonstrated remarkable progress in visual reasoning and
multimodal understanding, their evaluation has been predominantly limited to
RGB-based benchmarks, leaving a critical gap in assessing their capabilities in
infrared vision tasks. Existing visible-infrared datasets are either
task-specific or lack high-quality annotations necessary for rigorous model
evaluation. To address these limitations, RGB-Th-Bench provides a comprehensive
evaluation framework covering 14 distinct skill dimensions, with a total of
1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy
metrics: a standard question-level accuracy and a stricter skill-level
accuracy, which evaluates model robustness across multiple questions within
each skill dimension. This design ensures a thorough assessment of model
performance, including resilience to adversarial and hallucinated responses. We
conduct extensive evaluations on 19 state-of-the-art VLMs, revealing
significant performance gaps in RGB-Thermal understanding. Our results show
that even the strongest models struggle with thermal image comprehension, with
performance heavily constrained by their RGB-based capabilities. Additionally,
the lack of large-scale application-specific and expert-annotated
thermal-caption-pair datasets in pre-training is an important reason of the
observed performance gap. RGB-Th-Bench highlights the urgent need for further
advancements in multimodal learning to bridge the gap between visible and
thermal image understanding. The dataset is available through this link, and
the evaluation code will also be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond [cls]: Exploring the true potential of Masked Image Modeling
  representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcin Przewięźlikowski, Randall Balestriero, Wojciech Jasiński, Marek Śmieja, Bartosz Zieliński
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Image Modeling (MIM) has emerged as a promising approach for
Self-Supervised Learning (SSL) of visual representations. However, the
out-of-the-box performance of MIMs is typically inferior to competing
approaches. Most users cannot afford fine-tuning due to the need for large
amounts of data, high GPU consumption, and specialized user knowledge.
Therefore, the practical use of MIM representations is limited. In this paper
we ask what is the reason for the poor out-of-the-box performance of MIMs. Is
it due to weaker features produced by MIM models, or is it due to suboptimal
usage? Through detailed analysis, we show that attention in MIMs is spread
almost uniformly over many patches, leading to ineffective aggregation by the
[cls] token. Based on this insight, we propose Selective Aggregation to better
capture the rich semantic information retained in patch tokens, which
significantly improves the out-of-the-box performance of MIM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure Modeling Activation Free Fourier Network for Spacecraft Image
  Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingfan Yang, Hu Gao, Ying Zhang, Bowen Ma, Depeng Dang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spacecraft image denoising is a crucial fundamental technology closely
related to aerospace research. However, the existing deep learning-based image
denoising methods are primarily designed for natural image and fail to
adequately consider the characteristics of spacecraft image(e.g. low-light
conditions, repetitive periodic structures), resulting in suboptimal
performance in the spacecraft image denoising task. To address the
aforementioned problems, we propose a Structure modeling Activation Free
Fourier Network (SAFFN), which is an efficient spacecraft image denoising
method including Structure Modeling Block (SMB) and Activation Free Fourier
Block (AFFB). We present SMB to effectively extract edge information and model
the structure for better identification of spacecraft components from dark
regions in spacecraft noise image. We present AFFB and utilize an improved Fast
Fourier block to extract repetitive periodic features and long-range
information in noisy spacecraft image. Extensive experimental results
demonstrate that our SAFFN performs competitively compared to the
state-of-the-art methods on spacecraft noise image datasets. The codes are
available at: https://github.com/shenduke/SAFFN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReCap: Better Gaussian Relighting with Cross-Environment Captures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07534v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07534v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingzhi Li, Zongwei Wu, Eduard Zamfir, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D objects relighting in diverse unseen environments is crucial for
realistic virtual object placement. Due to the albedo-lighting ambiguity,
existing methods often fall short in producing faithful relights. Without
proper constraints, observed training views can be explained by numerous
combinations of lighting and material attributes, lacking physical
correspondence with the actual environment maps used for relighting. In this
work, we present ReCap, treating cross-environment captures as multi-task
target to provide the missing supervision that cuts through the entanglement.
Specifically, ReCap jointly optimizes multiple lighting representations that
share a common set of material attributes. This naturally harmonizes a coherent
set of lighting representations around the mutual material attributes,
exploiting commonalities and differences across varied object appearances. Such
coherence enables physically sound lighting reconstruction and robust material
estimation - both essential for accurate relighting. Together with a
streamlined shading function and effective post-processing, ReCap outperforms
all leading competitors on an expanded relighting benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mapping fMRI Signal and Image Stimuli in an Artificial Neural Network
  Latent Space: Bringing Artificial and Natural Minds Together 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cesare Maria Dalbagno, Manuel de Castro Ribeiro Jardim, Mihnea Angheluţă
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this study is to investigate whether latent space representations
of visual stimuli and fMRI data share common information. Decoding and
reconstructing stimuli from fMRI data remains a challenge in AI and
neuroscience, with significant implications for understanding neural
representations and improving the interpretability of Artificial Neural
Networks (ANNs). In this preliminary study, we investigate the feasibility of
such reconstruction by examining the similarity between the latent spaces of
one autoencoder (AE) and one vision transformer (ViT) trained on fMRI and image
data, respectively. Using representational similarity analysis (RSA), we found
that the latent spaces of the two domains appear different. However, these
initial findings are inconclusive, and further research is needed to explore
this relationship more thoroughly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Do You See? Enhancing Zero-Shot Image Classification with
  Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15668v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15668v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdelhamed, Mahmoud Afifi, Alec Go
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been effectively used for many computer
vision tasks, including image classification. In this paper, we present a
simple yet effective approach for zero-shot image classification using
multimodal LLMs. Using multimodal LLMs, we generate comprehensive textual
representations from input images. These textual representations are then
utilized to generate fixed-dimensional features in a cross-modal embedding
space. Subsequently, these features are fused together to perform zero-shot
classification using a linear classifier. Our method does not require prompt
engineering for each dataset; instead, we use a single, straightforward set of
prompts across all datasets. We evaluated our method on several datasets and
our results demonstrate its remarkable effectiveness, surpassing benchmark
accuracy on multiple datasets. On average, for ten benchmarks, our method
achieved an accuracy gain of 6.2 percentage points, with an increase of 6.8
percentage points on the ImageNet dataset, compared to prior methods
re-evaluated with the same setup. Our findings highlight the potential of
multimodal LLMs to enhance computer vision tasks such as zero-shot image
classification, offering a significant improvement over traditional methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongViTU: Instruction Tuning for Long-Form Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05037v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05037v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rujie Wu, Xiaojian Ma, Hai Ci, Yue Fan, Yuxuan Wang, Haozhe Zhao, Qing Li, Yizhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces LongViTU, a large-scale (~121k QA pairs, ~900h videos),
automatically generated dataset for long-form video understanding. We propose a
systematic approach that organizes videos into a hierarchical tree structure
for QA generation and incorporates self-revision mechanisms to ensure
high-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context
(average certificate length of 4.6 minutes); 2) rich knowledge and condensed
reasoning (commonsense, causality, planning, etc.)). We also offer explicit
timestamp annotations of relevant events for each QA pair. We have conducted
extensive human studies on LongViTU, and the results prove the quality of our
dataset. To better evaluate the challenges posed by LongViTU's emphasis on
long-term context and condensed reasoning, we manually curate a subset of
LongViTU into a benchmark. Evaluations using a state-of-the-art open-source
model (LongVU), a proprietary model (Gemini-1.5-Pro), and human annotators
yield GPT-4 scores of 49.9, 52.3, and 81.0, respectively, underscoring the
substantial difficulty presented by LongViTU questions. Performing supervised
fine-tuning (SFT) of LongVU and LLaVA-Video on LongViTU data results in average
performance gains of 2.5% and 3.7%, respectively, across a suite of long video
understanding benchmarks (EgoSchema, VideoMME-Long, MLVU, LVBench).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizable Prompt Learning of CLIP: A Brief <span class="highlight-title">Overview</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangming Cui, Yonggang Zhang, Xuan Wang, Xule Wang, Liang Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing vision-language models (VLMs) such as CLIP have showcased an
impressive capability to generalize well across various downstream tasks. These
models leverage the synergy between visual and textual information, enabling
them to understand and reason about the content present in images and text in a
unified manner. This article provides a brief overview of CLIP based on
few-shot prompt learning, including experimental data and technical
characteristics of some methods. The purpose of this review is to provide a
reference for researchers who have just started their research in generalizable
prompting of CLIP through few-shot training for classification across 15
datasets and also to facilitate the integration of this field by researchers in
other downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Textual Anatomical Knowledge for Class-Imbalanced
  Semi-Supervised Multi-Organ Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Gu, Weilun Tsao, Bo Du, Thierry Géraud, Yongchao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotating 3D medical images demands substantial time and expertise, driving
the adoption of semi-supervised learning (SSL) for segmentation tasks. However,
the complex anatomical structures of organs often lead to significant class
imbalances, posing major challenges for deploying SSL in real-world scenarios.
Despite the availability of valuable prior information, such as inter-organ
relative positions and organ shape priors, existing SSL methods have yet to
fully leverage these insights. To address this gap, we propose a novel approach
that integrates textual anatomical knowledge (TAK) into the segmentation model.
Specifically, we use GPT-4o to generate textual descriptions of anatomical
priors, which are then encoded using a CLIP-based model. These encoded priors
are injected into the segmentation model as parameters of the segmentation
head. Additionally, contrastive learning is employed to enhance the alignment
between textual priors and visual features. Extensive experiments demonstrate
the superior performance of our method, significantly surpassing
state-of-the-art approaches. The source code will be available at:
https://github.com/Lunn88/TAK-Semi.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional
  Images with Editable Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suyoung Lee, Jaeyoung Chung, Kihoon Kim, Jaeyoo Huh, Gunhee Lee, Minsoo Lee, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feed-forward 3D Gaussian splatting (3DGS) models have gained significant
popularity due to their ability to generate scenes immediately without needing
per-scene optimization. Although omnidirectional images are becoming more
popular since they reduce the computation required for image stitching to
composite a holistic scene, existing feed-forward models are only designed for
perspective images. The unique optical properties of omnidirectional images
make it difficult for feature encoders to correctly understand the context of
the image and make the Gaussian non-uniform in space, which hinders the image
quality synthesized from novel views. We propose OmniSplat, a training-free
fast feed-forward 3DGS generation framework for omnidirectional images. We
adopt a Yin-Yang grid and decompose images based on it to reduce the domain gap
between omnidirectional and perspective images. The Yin-Yang grid can use the
existing CNN structure as it is, but its quasi-uniform characteristic allows
the decomposed image to be similar to a perspective image, so it can exploit
the strong prior knowledge of the learned feed-forward network. OmniSplat
demonstrates higher reconstruction accuracy than existing feed-forward networks
trained on perspective images. Our project page is available on:
https://robot0321.github.io/omnisplat/index.html.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAP-based Problem-Agnostic <span class="highlight-title">diffusion</span> model for Inverse Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingping Tao, Haixia Liu, Jing Su, Xiaochen Yang, Hongchen Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have indeed shown great promise in solving inverse problems
in image processing. In this paper, we propose a novel, problem-agnostic
diffusion model called the maximum a posteriori (MAP)-based guided term
estimation method for inverse problems. To leverage unconditionally pretrained
diffusion models to address conditional generation tasks, we divide the
conditional score function into two terms according to Bayes' rule: an
unconditional score function (approximated by a pretrained score network) and a
guided term, which is estimated using a novel MAP-based method that
incorporates a Gaussian-type prior of natural images. This innovation allows us
to better capture the intrinsic properties of the data, leading to improved
performance. Numerical results demonstrate that our method preserves contents
more effectively compared to state-of-the-art methods--for example, maintaining
the structure of glasses in super-resolution tasks and producing more coherent
results in the neighborhood of masked regions during inpainting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Expansion of Pre-trained Models with Mixture of Adapters for
  Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18886v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18886v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyi Wang, Haodong Lu, Lina Yao, Dong Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) aims to continually accumulate knowledge from a
non-stationary data stream without catastrophic forgetting of learned
knowledge, requiring a balance between stability and adaptability. Relying on
the generalizable representation in pre-trained models (PTMs), PTM-based CL
methods perform effective continual adaptation on downstream tasks by adding
learnable adapters or prompts upon the frozen PTMs. However, many existing
PTM-based CL methods use restricted adaptation on a fixed set of these modules
to avoid forgetting, suffering from limited CL ability. Periodically adding
task-specific modules results in linear model growth rate and impaired
knowledge reuse. We propose Self-Expansion of pre-trained models with
Modularized Adaptation (SEMA), a novel approach to enhance the control of
stability-plasticity balance in PTM-based CL. SEMA automatically decides to
reuse or add adapter modules on demand in CL, depending on whether significant
distribution shift that cannot be handled is detected at different
representation levels. We design modular adapter consisting of a functional
adapter and a representation descriptor. The representation descriptors are
trained as a distribution shift indicator and used to trigger self-expansion
signals. For better composing the adapters, an expandable weighting router is
learned jointly for mixture of adapter outputs. SEMA enables better knowledge
reuse and sub-linear expansion rate. Extensive experiments demonstrate the
effectiveness of the proposed self-expansion method, achieving state-of-the-art
performance compared to PTM-based CL methods without memory rehearsal. Code is
available at https://github.com/huiyiwang01/SEMA-CL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https: https://github.com/huiyiwang01/SEMA-CL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimROD: A Simple Baseline for Raw Object Detection with Global and Local
  Enhancements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Xie, Xi Shen, Shihua Huang, Qirui Wang, Zheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most visual models are designed for sRGB images, yet RAW data offers
significant advantages for object detection by preserving sensor information
before ISP processing. This enables improved detection accuracy and more
efficient hardware designs by bypassing the ISP. However, RAW object detection
is challenging due to limited training data, unbalanced pixel distributions,
and sensor noise. To address this, we propose SimROD, a lightweight and
effective approach for RAW object detection. We introduce a Global Gamma
Enhancement (GGE) module, which applies a learnable global gamma transformation
with only four parameters, improving feature representation while keeping the
model efficient. Additionally, we leverage the green channel's richer signal to
enhance local details, aligning with the human eye's sensitivity and Bayer
filter design. Extensive experiments on multiple RAW object detection datasets
and detectors demonstrate that SimROD outperforms state-of-the-art methods like
RAW-Adapter and DIAP while maintaining efficiency. Our work highlights the
potential of RAW data for real-world object detection. Code is available at
https://ocean146.github.io/SimROD2025/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://ocean146.github.io/SimROD2025/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ATM: Improving Model Merging by Alternating Tuning and Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03055v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03055v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Zhou, Daniele Solombrino, Donato Crisostomi, Maria Sofia Bucarelli, Fabrizio Silvestri, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging has recently emerged as a cost-efficient paradigm for
multi-task learning. Among current approaches, task arithmetic stands out for
its simplicity and effectiveness. In this paper, we motivate the effectiveness
of task vectors by linking them to multi-task gradients. We show that in a
single-epoch scenario, if the optimization is performed via gradient descent,
task vectors are after one step mathematically equivalent to the gradients
obtained via gradient descent in a multi-task setting, and still approximate
these gradients in subsequent epochs. Furthermore, we show that the
effectiveness of task vectors is largely driven by the first epoch's gradient.
Given this parallel between task vectors and gradients, we propose viewing
model merging as a single step in an iterative process that alternates between
tuning and merging (ATM). We then propose two ways to utilize ATM. The first is
to replace multi-task learning with ATM in scenarios where data sharing is
prohibited, such as federated learning. The second is to improve the outcome of
any model merging algorithm by applying a few post-hoc iterations of ATM on a
small validation dataset, which is commonly available for hyperparameter
tuning. Finally, we provide both empirical and theoretical support for the
effectiveness of ATM, demonstrating that it minimizes an upper bound on the
loss obtained by jointly finetuning all tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 9 Pages, 9 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMTalker: Gaussian Mixture-based Audio-Driven Emotional Talking Video
  Portraits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07669v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07669v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Xia, Lizhen Wang, Xiang Deng, Xiaoyan Luo, Yunhong Wang, Yebin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing high-fidelity and emotion-controllable talking video portraits,
with audio-lip sync, vivid expressions, realistic head poses, and eye blinks,
has been an important and challenging task in recent years. Most existing
methods suffer in achieving personalized and precise emotion control, smooth
transitions between different emotion states, and the generation of diverse
motions. To tackle these challenges, we present GMTalker, a Gaussian
mixture-based emotional talking portraits generation framework. Specifically,
we propose a Gaussian mixture-based expression generator that can construct a
continuous and disentangled latent space, achieving more flexible emotion
manipulation. Furthermore, we introduce a normalizing flow-based motion
generator pretrained on a large dataset with a wide-range motion to generate
diverse head poses, blinks, and eyeball movements. Finally, we propose a
personalized emotion-guided head generator with an emotion mapping network that
can synthesize high-fidelity and faithful emotional video portraits. Both
quantitative and qualitative experiments demonstrate our method outperforms
previous methods in image quality, photo-realism, emotion accuracy, and motion
diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://bob35buaa.github.io/GMTalker. This work has
  been submitted to the IEEE journal for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feedback-driven object detection and iterative model improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19835v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19835v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sönke Tenckhoff, Mario Koddenbrock, Erik Rodner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated object detection has become increasingly valuable across diverse
applications, yet efficient, high-quality annotation remains a persistent
challenge. In this paper, we present the development and evaluation of a
platform designed to interactively improve object detection models. The
platform allows uploading and annotating images as well as fine-tuning object
detection models. Users can then manually review and refine annotations,
further creating improved snapshots that are used for automatic object
detection on subsequent image uploads - a process we refer to as semi-automatic
annotation resulting in a significant gain in annotation efficiency.
  Whereas iterative refinement of model results to speed up annotation has
become common practice, we are the first to quantitatively evaluate its
benefits with respect to time, effort, and interaction savings. Our
experimental results show clear evidence for a significant time reduction of up
to 53% for semi-automatic compared to manual annotation. Importantly, these
efficiency gains did not compromise annotation quality, while matching or
occasionally even exceeding the accuracy of manual annotations. These findings
demonstrate the potential of our lightweight annotation platform for creating
high-quality object detection datasets and provide best practices to guide
future development of annotation platforms.
  The platform is open-source, with the frontend and backend repositories
available on GitHub. To support the understanding of our labeling process, we
have created an explanatory video demonstrating the methodology using
microscopy images of E. coli bacteria as an example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/ml-lab-htw/iterative-annotate Video:
  https://www.youtube.com/watch?v=CM9uhE8NN5E</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not Just Object, But State: Compositional Incremental Learning without
  Forgetting <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01739v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01739v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyi Zhang, Binglin Qiu, Qi Jia, Yu Liu, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most incremental learners excessively prioritize coarse classes of objects
while neglecting various kinds of states (e.g. color and material) attached to
the objects. As a result, they are limited in the ability to reason
fine-grained compositionality of state-object pairs. To remedy this limitation,
we propose a novel task called Compositional Incremental Learning
(composition-IL), enabling the model to recognize state-object compositions as
a whole in an incremental learning fashion. Since the lack of suitable
benchmarks, we re-organize two existing datasets and make them tailored for
composition-IL. Then, we propose a prompt-based Composition Incremental Learner
(CompILer), to overcome the ambiguous composition boundary problem which
challenges composition-IL largely. Specifically, we exploit multi-pool prompt
learning, which is regularized by inter-pool prompt discrepancy and intra-pool
prompt diversity. Besides, we devise object-injected state prompting by using
object prompts to guide the selection of state prompts. Furthermore, we fuse
the selected prompts by a generalized-mean strategy, to eliminate irrelevant
information learned in the prompts. Extensive experiments on two datasets
exhibit state-of-the-art performance achieved by CompILer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Event-boosted Deformable 3D Gaussians for Dynamic Scene Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Xu, Wenming Weng, Yueyi Zhang, Ruikang Xu, Zhiwei Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable 3D Gaussian Splatting (3D-GS) is limited by missing intermediate
motion information due to the low temporal resolution of RGB cameras. To
address this, we introduce the first approach combining event cameras, which
capture high-temporal-resolution, continuous motion data, with deformable 3D-GS
for dynamic scene reconstruction. We observe that threshold modeling for events
plays a crucial role in achieving high-quality reconstruction. Therefore, we
propose a GS-Threshold Joint Modeling strategy, creating a mutually reinforcing
process that greatly improves both 3D reconstruction and threshold modeling.
Moreover, we introduce a Dynamic-Static Decomposition strategy that first
identifies dynamic areas by exploiting the inability of static Gaussians to
represent motions, then applies a buffer-based soft decomposition to separate
dynamic and static areas. This strategy accelerates rendering by avoiding
unnecessary deformation in static areas, and focuses on dynamic areas to
enhance fidelity. Additionally, we contribute the first event-inclusive 4D
benchmark with synthetic and real-world dynamic scenes, on which our method
achieves state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MESA: Effective Matching Redundancy Reduction by Semantic Area
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yesheng Zhang, Shuhan Shen, Xu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MESA and DMESA as novel feature matching methods, which utilize
Segment Anything Model (SAM) to effectively mitigate matching redundancy. The
key insight of our methods is to establish implicit-semantic area matching
prior to point matching, based on advanced image understanding of SAM. Then,
informative area matches with consistent internal semantic are able to undergo
dense feature comparison, facilitating precise inside-area point matching.
Specifically, MESA adopts a sparse matching framework and first obtains
candidate areas from SAM results through a novel Area Graph (AG). Then, area
matching among the candidates is formulated as graph energy minimization and
solved by graphical models derived from AG. To address the efficiency issue of
MESA, we further propose DMESA as its dense counterpart, applying a dense
matching framework. After candidate areas are identified by AG, DMESA
establishes area matches through generating dense matching distributions. The
distributions are produced from off-the-shelf patch matching utilizing the
Gaussian Mixture Model and refined via the Expectation Maximization. With less
repetitive computation, DMESA showcases a speed improvement of nearly five
times compared to MESA, while maintaining competitive accuracy. Our methods are
extensively evaluated on five datasets encompassing indoor and outdoor scenes.
The results illustrate consistent performance improvements from our methods for
five distinct point matching baselines across all datasets. Furthermore, our
methods exhibit promise generalization and improved robustness against image
resolution variations. The code is publicly available at
https://github.com/Easonyesheng/A2PM-MESA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18pages+suppl</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-26T00:00:00Z">2025-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">56</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flying Vines: Design, Modeling, and Control of a Soft Aerial <span class="highlight-title">Robo</span>tic Arm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rianna Jitosho, Crystal E. Winston, Shengan Yang, Jinxin Li, Maxwell Ahlquist, Nicholas John Woehrle, C. Karen Liu, Allison M. Okamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerial robotic arms aim to enable inspection and environment interaction in
otherwise hard-to-reach areas from the air. However, many aerial manipulators
feature bulky or heavy robot manipulators mounted to large, high-payload aerial
vehicles. Instead, we propose an aerial robotic arm with low mass and a small
stowed configuration called a "flying vine". The flying vine consists of a
small, maneuverable quadrotor equipped with a soft, growing, inflated beam as
the arm. This soft robot arm is underactuated, and positioning of the end
effector is achieved by controlling the coupled quadrotor-vine dynamics. In
this work, we present the flying vine design and a modeling and control
framework for tracking desired end effector trajectories. The dynamic model
leverages data-driven modeling methods and introduces bilinear interpolation to
account for time-varying dynamic parameters. We use trajectory optimization to
plan quadrotor controls that produce desired end effector motions. Experimental
results on a physical prototype demonstrate that our framework enables the
flying vine to perform high-speed end effector tracking, laying a foundation
for performing dynamic maneuvers with soft aerial manipulators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to RA-L</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-<span class="highlight-title">Robo</span>t Coordination Under Physical Limitations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tohid Kargar Tasooji, Sakineh Khodadadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-robot coordination is fundamental to various applications, including
autonomous exploration, search and rescue, and cooperative transportation. This
paper presents an optimal consensus framework for multi-robot systems (MRSs)
that ensures efficient rendezvous while minimizing energy consumption and
addressing actuator constraints. A critical challenge in real-world deployments
is actuator limitations, particularly wheel velocity saturation, which can
significantly degrade control performance. To address this issue, we
incorporate Pontryagin Minimum Principle (PMP) into the control design,
facilitating constrained optimization while ensuring system stability and
feasibility. The resulting optimal control policy effectively balances
coordination efficiency and energy consumption, even in the presence of
actuation constraints. The proposed framework is validated through extensive
numerical simulations and real-world experiments conducted using a team of
Robotarium mobile robots. The experimental results confirm that our control
strategies achieve reliable and efficient coordinated rendezvous while
addressing real-world challenges such as communication delays, sensor noise,
and packet loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond <span class="highlight-title">Visual</span>s: Investigating Force Feedback in Extended Reality for
  <span class="highlight-title">Robo</span>t Data Collection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyin Li, Xinkai Jiang, Philipp Dahlinger, Gerhard Neumann, Rudolf Lioutikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work explores how force feedback affects various aspects of robot data
collection within the Extended Reality (XR) setting. Force feedback has been
proved to enhance the user experience in Extended Reality (XR) by providing
contact-rich information. However, its impact on robot data collection has not
received much attention in the robotics community. This paper addresses this
shortcoming by conducting an extensive user study on the effects of force
feedback during data collection in XR. We extended two XR-based robot control
interfaces, Kinesthetic Teaching and Motion Controllers, with haptic feedback
features. The user study is conducted using manipulation tasks ranging from
simple pick-place to complex peg assemble, requiring precise operations. The
evaluations show that force feedback enhances task performance and user
experience, particularly in tasks requiring high-precision manipulation. These
improvements vary depending on the robot control interface and task complexity.
This paper provides new insights into how different factors influence the
impact of force feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Dynamic Control of Tendon-Driven Continuum <span class="highlight-title">Robo</span>ts using Clarke
  Transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Muhmann, Reinhard M. Grassmann, Max Bartholdt, Jessica Burgner-Kahrs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a dynamic model and control framework for
tendon-driven continuum robots with multiple segments and an arbitrary number
of tendons per segment. Our approach leverages the Clarke transform, the
Euler-Lagrange formalism, and the piecewise constant curvature assumption to
formulate a dynamic model on a two-dimensional manifold embedded in the joint
space that inherently satisfies tendon constraints. We present linear
controllers that operate directly on this manifold, along with practical
methods for preventing negative tendon forces without compromising control
fidelity. We validate these approaches in simulation and on a physical
prototype with one segment and five tendons, demonstrating accurate dynamic
behavior and robust trajectory tracking under real-time conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages and 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Immersive and Wearable Thermal Rendering for Augmented Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Watkins, Ritam Ghosh, Evan Chow, Nilanjan Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In augmented reality (AR), where digital content is overlaid onto the real
world, realistic thermal feedback has been shown to enhance immersion. Yet
current thermal feedback devices, heavily influenced by the needs of virtual
reality, often hinder physical interactions and are ineffective for immersion
in AR. To bridge this gap, we have identified three design considerations
relevant for AR thermal feedback: indirect feedback to maintain dexterity,
thermal passthrough to preserve real-world temperature perception, and
spatiotemporal rendering for dynamic sensations. We then created a unique and
innovative thermal feedback device that satisfies these criteria. Human subject
experiments assessing perceptual sensitivity, object temperature matching,
spatial pattern recognition, and moving thermal stimuli demonstrated the impact
of our design, enabling realistic temperature discrimination, virtual object
perception, and enhanced immersion. These findings demonstrate that carefully
designed thermal feedback systems can bridge the sensory gap between physical
and virtual interactions, enhancing AR realism and usability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representation Improvement in Latent Space for Search-Based Testing of
  Autonomous <span class="highlight-title">Robo</span>tic Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmytro Humeniuk, Foutse Khomh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Testing autonomous robotic systems, such as self-driving cars and unmanned
aerial vehicles, is challenging due to their interaction with highly
unpredictable environments. A common practice is to first conduct
simulation-based testing, which, despite reducing real-world risks, remains
time-consuming and resource-intensive due to the vast space of possible test
scenarios. A number of search-based approaches were proposed to generate test
scenarios more efficiently. A key aspect of any search-based test generation
approach is the choice of representation used during the search process.
However, existing methods for improving test scenario representation remain
limited. We propose RILaST (Representation Improvement in Latent Space for
Search-Based Testing) approach, which enhances test representation by mapping
it to the latent space of a variational autoencoder. We evaluate RILaST on two
use cases, including autonomous drone and autonomous lane-keeping assist
system. The obtained results show that RILaST allows finding between 3 to 4.6
times more failures than baseline approaches, achieving a high level of test
diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Flower Cluster Matching Using The Unscented Transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Chu, Rashik Shrestha, Yu Gu, Jason N. Gross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring flowers over time is essential for precision robotic pollination
in agriculture. To accomplish this, a continuous spatial-temporal observation
of plant growth can be done using stationary RGB-D cameras. However, image
registration becomes a serious challenge due to changes in the visual
appearance of the plant caused by the pollination process and occlusions from
growth and camera angles. Plants flower in a manner that produces distinct
clusters on branches. This paper presents a method for matching flower clusters
using descriptors generated from RGB-D data and considers allowing for spatial
uncertainty within the cluster. The proposed approach leverages the Unscented
Transform to efficiently estimate plant descriptor uncertainty tolerances,
enabling a robust image-registration process despite temporal changes. The
Unscented Transform is used to handle the nonlinear transformations by
propagating the uncertainty of flower positions to determine the variations in
the descriptor domain. A Monte Carlo simulation is used to validate the
Unscented Transform results, confirming our method's effectiveness for flower
cluster matching. Therefore, it can facilitate improved robotics pollination in
dynamic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>*CASE2025 Under Review*</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety integrity framework for automated driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Werling, Rainer Faller, Wolfgang Betz, Daniel Straub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the comprehensive safety framework that underpinned the
development, release process, and regulatory approval of BMW's first SAE Level
3 Automated Driving System. The framework combines established qualitative and
quantitative methods from the fields of Systems Engineering, Engineering Risk
Analysis, Bayesian Data Analysis, Design of Experiments, and Statistical
Learning in a novel manner. The approach systematically minimizes the risks
associated with hardware and software faults, performance limitations, and
insufficient specifications to an acceptable level that achieves a Positive
Risk Balance. At the core of the framework is the systematic identification and
quantification of uncertainties associated with hazard scenarios and the
redundantly designed system based on designed experiments, field data, and
expert knowledge. The residual risk of the system is then estimated through
Stochastic Simulation and evaluated by Sensitivity Analysis. By integrating
these advanced analytical techniques into the V-Model, the framework fulfills,
unifies, and complements existing automotive safety standards. It therefore
provides a comprehensive, rigorous, and transparent safety assurance process
for the development and deployment of Automated Driving Systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Machine Learning and Sampling-Based Search for Multi-Goal
  Motion Planning with Dynamics <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanjie Lu, Erion Plaku
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers multi-goal motion planning in unstructured,
obstacle-rich environments where a robot is required to reach multiple regions
while avoiding collisions. The planned motions must also satisfy the
differential constraints imposed by the robot dynamics. To find solutions
efficiently, this paper leverages machine learning, Traveling Salesman Problem
(TSP), and sampling-based motion planning. The approach expands a motion tree
by adding collision-free and dynamically-feasible trajectories as branches. A
TSP solver is used to compute a tour for each node to determine the order in
which to reach the remaining goals by utilizing a cost matrix. An important
aspect of the approach is that it leverages machine learning to construct the
cost matrix by combining runtime and distance predictions to single-goal
motion-planning problems. During the motion-tree expansion, priority is given
to nodes associated with low-cost tours. Experiments with a vehicle model
operating in obstacle-rich environments demonstrate the computational
efficiency and scalability of the approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2025 International Joint Conference on Artificial
  Intelligence (IJCAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, Gianluca Corrado
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models offer a scalable and flexible paradigm for simulating
complex environments, yet current approaches fall short in addressing the
domain-specific requirements of autonomous driving - such as multi-agent
interactions, fine-grained control, and multi-camera consistency. We introduce
GAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies
these capabilities within a single generative framework. GAIA-2 supports
controllable video generation conditioned on a rich set of structured inputs:
ego-vehicle dynamics, agent configurations, environmental factors, and road
semantics. It generates high-resolution, spatiotemporally consistent
multi-camera videos across geographically diverse driving environments (UK, US,
Germany). The model integrates both structured conditioning and external latent
embeddings (e.g., from a proprietary driving model) to facilitate flexible and
semantically grounded scene synthesis. Through this integration, GAIA-2 enables
scalable simulation of both common and rare driving scenarios, advancing the
use of generative world models as a core tool in the development of autonomous
systems. Videos are available at https://wayve.ai/thinking/gaia-2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decremental Dynamics Planning for <span class="highlight-title">Robo</span>t Navigation <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanjie Lu, Tong Xu, Linji Wang, Nick Hawes, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most, if not all, robot navigation systems employ a decomposed planning
framework that includes global and local planning. To trade-off onboard
computation and plan quality, current systems have to limit all robot dynamics
considerations only within the local planner, while leveraging an extremely
simplified robot representation (e.g., a point-mass holonomic model without
dynamics) in the global level. However, such an artificial decomposition based
on either full or zero consideration of robot dynamics can lead to gaps between
the two levels, e.g., a global path based on a holonomic point-mass model may
not be realizable by a non-holonomic robot, especially in highly constrained
obstacle environments. Motivated by such a limitation, we propose a novel
paradigm, Decremental Dynamics Planning that integrates dynamic constraints
into the entire planning process, with a focus on high-fidelity dynamics
modeling at the beginning and a gradual fidelity reduction as the planning
progresses. To validate the effectiveness of this paradigm, we augment three
different planners with DDP and show overall improved planning performance. We
also develop a new DDP-based navigation system, which achieves first place in
the simulation phase of the 2025 BARN Challenge. Both simulated and physical
experiments validate DDP's hypothesized benefits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages. 2025 International Conference on Intelligent Robots and
  Systems (IROS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Effect of <span class="highlight-title">Robo</span>tic Embodiment and Empathetic Tone of LLMs
  on Empathy Elicitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liza Darwesh, Jaspreet Singh, Marin Marian, Eduard Alexa, Koen Hindriks, Kim Baraka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the elicitation of empathy toward a third party
through interaction with social agents. Participants engaged with either a
physical robot or a voice-enabled chatbot, both driven by a large language
model (LLM) programmed to exhibit either an empathetic tone or remain neutral.
The interaction is focused on a fictional character, Katie Banks, who is in a
challenging situation and in need of financial donations. The willingness to
help Katie, measured by the number of hours participants were willing to
volunteer, along with their perceptions of the agent, were assessed for 60
participants. Results indicate that neither robotic embodiment nor empathetic
tone significantly influenced participants' willingness to volunteer. While the
LLM effectively simulated human empathy, fostering genuine empathetic responses
in participants proved challenging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>*Liza Darwesh, Jaspreet Singh, Marin Marian, and Eduard Alexa
  contributed equally to this work.*</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perspective-Shifted Neuro-Symbolic World Models: A Framework for
  Socially-Aware <span class="highlight-title">Robo</span>t Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Alcedo, Pedro U. Lima, Rachid Alami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating in environments alongside humans requires agents to reason under
uncertainty and account for the beliefs and intentions of those around them.
Under a sequential decision-making framework, egocentric navigation can
naturally be represented as a Markov Decision Process (MDP). However, social
navigation additionally requires reasoning about the hidden beliefs of others,
inherently leading to a Partially Observable Markov Decision Process (POMDP),
where agents lack direct access to others' mental states. Inspired by Theory of
Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based
reinforcement learning architecture for social navigation, addressing the
challenge of belief tracking in partially observable environments; and (2) a
perspective-shift operator for belief estimation, leveraging recent work on
Influence-based Abstractions (IBA) in structured multi-agent settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoLe-VLA: Dynamic Layer-skipping <span class="highlight-title">Vision</span> Language Action Model via
  Mixture-of-Layers for Efficient <span class="highlight-title">Robo</span>t <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongyu Zhang, Menghang Dong, Yuan Zhang, Liang Heng, Xiaowei Chi, Gaole Dai, Li Du, Dan Wang, Yuan Du, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) excel in understanding complex
language and visual data, enabling generalist robotic systems to interpret
instructions and perform embodied tasks. Nevertheless, their real-world
deployment is hindered by substantial computational and storage demands. Recent
insights into the homogeneous patterns in the LLM layer have inspired
sparsification techniques to address these challenges, such as early exit and
token pruning. However, these methods often neglect the critical role of the
final layers that encode the semantic information most relevant to downstream
robotic tasks. Aligning with the recent breakthrough of the Shallow Brain
Hypothesis (SBH) in neuroscience and the mixture of experts in model
sparsification, we conceptualize each LLM layer as an expert and propose a
Mixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)
architecture for dynamic LLM layer activation. We introduce a Spatial-Temporal
Aware Router (STAR) for MoLe to selectively activate only parts of the layers
based on the robot's current state, mimicking the brain's distinct signal
pathways specialized for cognition and causal reasoning. Additionally, to
compensate for the cognitive ability of LLMs lost in MoLe, we devise a
Cognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the
understanding of task demands and improves the generation of task-relevant
action sequences by leveraging cognitive features. Extensive experiments
conducted in both RLBench simulation and real-world environments demonstrate
the superiority of MoLe-VLA in both efficiency and performance. Specifically,
MoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks
while reducing computational costs by up to x5.6 compared to standard LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTS-CBS: A New Approach for Multi-Agent Collaborative Task Sequencing
  and Path Finding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Jiang, Ruochen Li, Yibin Yang, Yihe Chen, Yuning Wang, Shaobing Xu, Jianqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses a generalization problem of Multi-Agent Pathfinding
(MAPF), called Collaborative Task Sequencing - Multi-Agent Pathfinding
(CTS-MAPF), where agents must plan collision-free paths and visit a series of
intermediate task locations in a specific order before reaching their final
destinations. To address this problem, we propose a new approach, Collaborative
Task Sequencing - Conflict-Based Search (CTS-CBS), which conducts a two-level
search. In the high level, it generates a search forest, where each tree
corresponds to a joint task sequence derived from the jTSP solution. In the low
level, CTS-CBS performs constrained single-agent path planning to generate
paths for each agent while adhering to high-level constraints. We also provide
heoretical guarantees of its completeness and optimality (or sub-optimality
with a bounded parameter). To evaluate the performance of CTS-CBS, we create
two datasets, CTS-MAPF and MG-MAPF, and conduct comprehensive experiments. The
results show that CTS-CBS adaptations for MG-MAPF outperform baseline
algorithms in terms of success rate (up to 20 times larger) and runtime (up to
100 times faster), with less than a 10% sacrifice in solution quality.
Furthermore, CTS-CBS offers flexibility by allowing users to adjust the
sub-optimality bound omega to balance between solution quality and efficiency.
Finally, practical robot tests demonstrate the algorithm's applicability in
real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Turning Circle-based Control Barrier Function for Efficient Collision
  Avoidance of Nonholonomic Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changyu Lee, Kiyong Park, Jinwhan Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new control barrier function (CBF) designed to improve
the efficiency of collision avoidance for nonholonomic vehicles. Traditional
CBFs typically rely on the shortest Euclidean distance to obstacles,
overlooking the limited heading change ability of nonholonomic vehicles. This
often leads to abrupt maneuvers and excessive speed reductions, which is not
desirable and reduces the efficiency of collision avoidance. Our approach
addresses these limitations by incorporating the distance to the turning
circle, considering the vehicle's limited maneuverability imposed by its
nonholonomic constraints. The proposed CBF is integrated with model predictive
control (MPC) to generate more efficient trajectories compared to existing
methods that rely solely on Euclidean distance-based CBFs. The effectiveness of
the proposed method is validated through numerical simulations on unicycle
vehicles and experiments with underactuated surface vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to an IEEE journal for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LGR: LLM-Guided Ranking of Frontiers for Object Goal Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitsuaki Uno, Kanji Tanaka, Daiki Iwata, Yudai Noda, Shoya Miyazaki, Kouki Terashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object Goal Navigation (OGN) is a fundamental task for robots and AI, with
key applications such as mobile robot image databases (MRID). In particular,
mapless OGN is essential in scenarios involving unknown or dynamic
environments. This study aims to enhance recent modular mapless OGN systems by
leveraging the commonsense reasoning capabilities of large language models
(LLMs). Specifically, we address the challenge of determining the visiting
order in frontier-based exploration by framing it as a frontier ranking
problem. Our approach is grounded in recent findings that, while LLMs cannot
determine the absolute value of a frontier, they excel at evaluating the
relative value between multiple frontiers viewed within a single image using
the view image as context. We dynamically manage the frontier list by adding
and removing elements, using an LLM as a ranking model. The ranking results are
represented as reciprocal rank vectors, which are ideal for multi-view,
multi-query information fusion. We validate the effectiveness of our method
through evaluations in Habitat-Sim.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures, technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Virtual Fencing Framework for Safe and Efficient Collaborative
  <span class="highlight-title">Robo</span>tics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vineela Reddy Pippera Badguna, Aliasghar Arab, Durga Avinash Kodavalla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative robots (cobots) increasingly operate alongside humans,
demanding robust real-time safeguarding. Current safety standards (e.g., ISO
10218, ANSI/RIA 15.06, ISO/TS 15066) require risk assessments but offer limited
guidance for real-time responses. We propose a virtual fencing approach that
detects and predicts human motion, ensuring safe cobot operation. Safety and
performance tradeoffs are modeled as an optimization problem and solved via
sequential quadratic programming. Experimental validation shows that our method
minimizes operational pauses while maintaining safety, providing a modular
solution for human-robot collaboration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning
  with Motion and Structure Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilong Yan, Ming Li, Haipeng Li, Shuwei Shao, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised depth estimation from monocular cameras in diverse outdoor
conditions, such as daytime, rain, and nighttime, is challenging due to the
difficulty of learning universal representations and the severe lack of labeled
real-world adverse data. Previous methods either rely on synthetic inputs and
pseudo-depth labels or directly apply daytime strategies to adverse conditions,
resulting in suboptimal results. In this paper, we present the first
synthetic-to-real robust depth estimation framework, incorporating motion and
structure priors to capture real-world knowledge effectively. In the synthetic
adaptation, we transfer motion-structure knowledge inside cost volumes for
better robust representation, using a frozen daytime model to train a depth
estimator in synthetic adverse conditions. In the innovative real adaptation,
which targets to fix synthetic-real gaps, models trained earlier identify the
weather-insensitive regions with a designed consistency-reweighting strategy to
emphasize valid pseudo-labels. We introduce a new regularization by gathering
explicit depth distributions to constrain the model when facing real-world
data. Experiments show that our method outperforms the state-of-the-art across
diverse conditions in multi-frame and single-frame evaluations. We achieve
improvements of 7.5% and 4.3% in AbsRel and RMSE on average for nuScenes and
Robotcar datasets (daytime, nighttime, rain). In zero-shot evaluation of
DrivingStereo (rain, fog), our method generalizes better than the previous
ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Learning Adaptive Dexterous <span class="highlight-title">Grasp</span>ing from Single Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangzhi Shi, Yulin Liu, Lingqi Zeng, Bo Ai, Zhengdong Hong, <span class="highlight-author">Hao Su</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can robots learn dexterous grasping skills efficiently and apply them
adaptively based on user instructions? This work tackles two key challenges:
efficient skill acquisition from limited human demonstrations and
context-driven skill selection. We introduce AdaDexGrasp, a framework that
learns a library of grasping skills from a single human demonstration per skill
and selects the most suitable one using a vision-language model (VLM). To
improve sample efficiency, we propose a trajectory following reward that guides
reinforcement learning (RL) toward states close to a human demonstration while
allowing flexibility in exploration. To learn beyond the single demonstration,
we employ curriculum learning, progressively increasing object pose variations
to enhance robustness. At deployment, a VLM retrieves the appropriate skill
based on user instructions, bridging low-level learned skills with high-level
intent. We evaluate AdaDexGrasp in both simulation and real-world settings,
showing that our approach significantly improves RL efficiency and enables
learning human-like grasp strategies across varied object configurations.
Finally, we demonstrate zero-shot transfer of our learned policies to a
real-world PSYONIC Ability Hand, with a 90% success rate across objects,
significantly outperforming the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning and Learning a Perceptual Metric for Self-Training of
  Reflective Objects in Bin-Picking with a Low-cost <span class="highlight-title">Camera</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyuan Ni, Chee Meng Chew, Marcelo H. Ang Jr., Gregory S. Chirikjian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bin-picking of metal objects using low-cost RGB-D cameras often suffers from
sparse depth information and reflective surface textures, leading to errors and
the need for manual labeling. To reduce human intervention, we propose a
two-stage framework consisting of a metric learning stage and a self-training
stage. Specifically, to automatically process data captured by a low-cost
camera (LC), we introduce a Multi-object Pose Reasoning (MoPR) algorithm that
optimizes pose hypotheses under depth, collision, and boundary constraints. To
further refine pose candidates, we adopt a Symmetry-aware Lie-group based
Bayesian Gaussian Mixture Model (SaL-BGMM), integrated with the
Expectation-Maximization (EM) algorithm, for symmetry-aware filtering.
Additionally, we propose a Weighted Ranking Information Noise Contrastive
Estimation (WR-InfoNCE) loss to enable the LC to learn a perceptual metric from
reconstructed data, supporting self-training on untrained or even unseen
objects. Experimental results show that our approach outperforms several
state-of-the-art methods on both the ROBI dataset and our newly introduced
Self-ROBI dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SARGes: Semantically Aligned Reliable Gesture Generation via Intent
  Chain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Gao, Yihua Bao, Dongdong Weng, Jiayi Zhao, Jia Li, Yan Zhou, Pengfei Wan, Di Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Co-speech gesture generation enhances human-computer interaction realism
through speech-synchronized gesture synthesis. However, generating semantically
meaningful gestures remains a challenging problem. We propose SARGes, a novel
framework that leverages large language models (LLMs) to parse speech content
and generate reliable semantic gesture labels, which subsequently guide the
synthesis of meaningful co-speech gestures.First, we constructed a
comprehensive co-speech gesture ethogram and developed an LLM-based intent
chain reasoning mechanism that systematically parses and decomposes gesture
semantics into structured inference steps following ethogram criteria,
effectively guiding LLMs to generate context-aware gesture labels.
Subsequently, we constructed an intent chain-annotated text-to-gesture label
dataset and trained a lightweight gesture label generation model, which then
guides the generation of credible and semantically coherent co-speech gestures.
Experimental results demonstrate that SARGes achieves highly
semantically-aligned gesture labeling (50.2% accuracy) with efficient
single-pass inference (0.4 seconds). The proposed method provides an
interpretable intent reasoning pathway for semantic gesture synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline <span class="highlight-title">Reinforcement Learning</span> with Discrete <span class="highlight-title">Diffusion</span> Skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        RuiXi Qiao, Jie Cheng, Xingyuan Dai, Yonglin Tian, Yisheng Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skills have been introduced to offline reinforcement learning (RL) as
temporal abstractions to tackle complex, long-horizon tasks, promoting
consistent behavior and enabling meaningful exploration. While skills in
offline RL are predominantly modeled within a continuous latent space, the
potential of discrete skill spaces remains largely underexplored. In this
paper, we propose a compact discrete skill space for offline RL tasks supported
by state-of-the-art transformer-based encoder and diffusion-based decoder.
Coupled with a high-level policy trained via offline RL techniques, our method
establishes a hierarchical RL framework where the trained diffusion decoder
plays a pivotal role. Empirical evaluations show that the proposed algorithm,
Discrete Diffusion Skill (DDS), is a powerful offline RL method. DDS performs
competitively on Locomotion and Kitchen tasks and excels on long-horizon tasks,
achieving at least a 12 percent improvement on AntMaze-v2 benchmarks compared
to existing offline RL approaches. Furthermore, DDS offers improved
interpretability, training stability, and online exploration compared to
previous skill-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRPA-MPPI: Dynamic Repulsive Potential Augmented MPPI for Reactive
  Navigation in Unstructured Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takahiro Fuke, Masafumi Endo, Kohei Honda, Genya Ishigami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reactive mobile robot navigation in unstructured environments is challenging
when robots encounter unexpected obstacles that invalidate previously planned
trajectories. Model predictive path integral control (MPPI) enables reactive
planning, but still suffers from limited prediction horizons that lead to local
minima traps near obstacles. Current solutions rely on heuristic cost design or
scenario-specific pre-training, which often limits their adaptability to new
environments. We introduce dynamic repulsive potential augmented MPPI
(DRPA-MPPI), which dynamically detects potential entrapments on the predicted
trajectories. Upon detecting local minima, DRPA-MPPI automatically switches
between standard goal-oriented optimization and a modified cost function that
generates repulsive forces away from local minima. Comprehensive testing in
simulated obstacle-rich environments confirms DRPA-MPPI's superior navigation
performance and safety compared to conventional methods with less computational
burden.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, Submitted to the 2025 IEEE International
  Conference on Automation Science and Engineering (CASE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bandwidth Allocation for Cloud-Augmented Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Schafhalter, Alexander Krentsel, Joseph E. Gonzalez, Sylvia Ratnasamy, Scott Shenker, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicle (AV) control systems increasingly rely on ML models for
tasks such as perception and planning. Current practice is to run these models
on the car's local hardware due to real-time latency constraints and
reliability concerns, which limits model size and thus accuracy. Prior work has
observed that we could augment current systems by running larger models in the
cloud, relying on faster cloud runtimes to offset the cellular network latency.
However, prior work does not account for an important practical constraint:
limited cellular bandwidth. We show that, for typical bandwidth levels,
proposed techniques for cloud-augmented AV models take too long to transfer
data, thus mostly falling back to the on-car models and resulting in no
accuracy improvement.
  In this work, we show that realizing cloud-augmented AV models requires
intelligent use of this scarce bandwidth, i.e. carefully allocating bandwidth
across tasks and providing multiple data compression and model options. We
formulate this as a resource allocation problem to maximize car utility, and
present our system \sysname which achieves an increase in average model
accuracy by up to 15 percentage points on driving scenarios from the Waymo Open
Dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Interference between Concurrent Skin Stretches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching Hei Cheng, Jonathan Eden, Denny Oetomo, Ying Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proprioception is essential for coordinating human movements and enhancing
the performance of assistive robotic devices. Skin stretch feedback, which
closely aligns with natural proprioception mechanisms, presents a promising
method for conveying proprioceptive information. To better understand the
impact of interference on skin stretch perception, we conducted a user study
with 30 participants that evaluated the effect of two simultaneous skin
stretches on user perception. We observed that when participants experience
simultaneous skin stretch stimuli, a masking effect occurs which deteriorates
perception performance in the collocated skin stretch configurations. However,
the perceived workload stays the same. These findings show that interference
can affect the perception of skin stretch such that multi-channel skin stretch
feedback designs should avoid locating modules in close proximity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pellet-based 3D Printing of Soft Thermoplastic Elastomeric Membranes for
  Soft <span class="highlight-title">Robo</span>tic Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Willemstein, Herman van der Kooij, Ali Sadeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Additive Manufacturing (AM) is a promising solution for handling the
complexity of fabricating soft robots. However, the AM of hyperelastic
materials is still challenging with limited material types. Within this work,
pellet-based 3D printing of very soft thermoplastic elastomers (TPEs) was
explored. Our results show that TPEs can have similar engineering stress and
maximum strain as Ecoflex OO-10. These TPEs were used to 3D-print airtight thin
membranes (0.2-1.2 mm), which could inflate up to a stretch of 1320\%.
Combining the membrane's large expansion and softness with the 3D printing of
hollow structures simplified the design of a bending actuator that can bend 180
degrees and reach a blocked force of 238 times its weight. In addition, by 3D
printing TPE pellets and rigid filaments, the soft membrane could grasp objects
by enveloping an object or as a sensorized sucker, which relied on the TPE's
softness to conform to the object or act as a seal. In addition, the membrane
of the sucker was utilized as a tactile sensor to detect an object before
adhesion. These results suggest the feasibility of 3D printing soft robots by
using soft TPEs and membranes as an interesting class of materials and
sensorized actuators, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A Study of Perceived Safety for Soft <span class="highlight-title">Robo</span>tics in Caregiving Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cosima du Pasquier, Jennifer Grannen, Chuer Pan, Serin L. Huber, Aliyah Smith, Monroe Kennedy, <span class="highlight-author">Shuran Song</span>, Dorsa Sadigh, Allison M. Okamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this project, we focus on human-robot interaction in caregiving scenarios
like bathing, where physical contact is inevitable and necessary for proper
task execution because force must be applied to the skin. Using finite element
analysis, we designed a 3D-printed gripper combining positive and negative
pressure for secure yet compliant handling. Preliminary tests showed it exerted
a lower, more uniform pressure profile than a standard rigid gripper. In a user
study, participants' trust in robots significantly increased after they
experienced a brief bathing demonstration performed by a robotic arm equipped
with the soft gripper. These results suggest that soft robotics can enhance
perceived safety and acceptance in intimate caregiving scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Multimodal Discrete <span class="highlight-title">Diffusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal generative models that can understand and generate across multiple
modalities are dominated by autoregressive (AR) approaches, which process
tokens sequentially from left to right, or top to bottom. These models jointly
handle images, text, video, and audio for various tasks such as image
captioning, question answering, and image generation. In this work, we explore
discrete diffusion models as a unified generative formulation in the joint text
and image domain, building upon their recent success in text generation.
Discrete diffusion models offer several advantages over AR models, including
improved control over quality versus diversity of generated samples, the
ability to perform joint multimodal inpainting (across both text and image
domains), and greater controllability in generation through guidance.
Leveraging these benefits, we present the first Unified Multimodal Discrete
Diffusion (UniDisc) model which is capable of jointly understanding and
generating text and images for a variety of downstream tasks. We compare
UniDisc to multimodal AR models, performing a scaling analysis and
demonstrating that UniDisc outperforms them in terms of both performance and
inference-time compute, enhanced controllability, editability, inpainting, and
flexible trade-off between inference time and generation quality. Code and
additional visualizations are available at https://unidisc.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://unidisc.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Deep <span class="highlight-title">Reinforcement Learning</span> in <span class="highlight-title">Robo</span>tics via Adaptive
  Gradient-Masked Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyuan Zhang, Tianyang Duan, Zheng Lin, Dong Huang, Zihan Fang, Zekai Sun, Ling Xiong, Hongbin Liang, Heming Cui, Yong Cui, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (DRL) has emerged as a promising approach for
robotic control, but its realworld deployment remains challenging due to its
vulnerability to environmental perturbations. Existing white-box adversarial
attack methods, adapted from supervised learning, fail to effectively target
DRL agents as they overlook temporal dynamics and indiscriminately perturb all
state dimensions, limiting their impact on long-term rewards. To address these
challenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR)
Attack, a white-box attack method that combines DRL with a gradient-based soft
masking mechanism to dynamically identify critical state dimensions and
optimize adversarial policies. AGMR selectively allocates perturbations to the
most impactful state features and incorporates a dynamic adjustment mechanism
to balance exploration and exploitation during training. Extensive experiments
demonstrate that AGMR outperforms state-of-the-art adversarial attack methods
in degrading the performance of the victim agent and enhances the victim
agent's robustness through adversarial defense mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anti <span class="highlight-title">Robo</span>t Speciesism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian De Freitas, Noah Castelo, Bernd Schmitt, Miklos Sarvary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid robots are a form of embodied artificial intelligence (AI) that
looks and acts more and more like humans. Powered by generative AI and advances
in robotics, humanoid robots can speak and interact with humans rather
naturally but are still easily recognizable as robots. But how will we treat
humanoids when they seem indistinguishable from humans in appearance and mind?
We find a tendency (called "anti-robot" speciesism) to deny such robots
humanlike capabilities, driven by motivations to accord members of the human
species preferential treatment. Six experiments show that robots are denied
humanlike attributes, simply because they are not biological beings and because
humans want to avoid feelings of cognitive dissonance when utilizing such
robots for unsavory tasks. Thus, people do not rationally attribute
capabilities to perfectly humanlike robots but deny them capabilities as it
suits them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In vitro 2 In vivo : Bidirectional and High-Precision Generation of In
  Vitro and In Vivo Neuronal Spike Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanori Shimono
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurons encode information in a binary manner and process complex signals.
However, predicting or generating diverse neural activity patterns remains
challenging. In vitro and in vivo studies provide distinct advantages, yet no
robust computational framework seamlessly integrates both data types. We
address this by applying the Transformer model, widely used in large-scale
language models, to neural data. To handle binary data, we introduced Dice
loss, enabling accurate cross-domain neural activity generation. Structural
analysis revealed how Dice loss enhances learning and identified key brain
regions facilitating high-precision data generation. Our findings support the
3Rs principle in animal research, particularly Replacement, and establish a
mathematical framework bridging animal experiments and human clinical studies.
This work advances data-driven neuroscience and neural activity modeling,
paving the way for more ethical and effective experimental methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAR: Teacher-Aligned Representations via Contrastive Learning for
  Quadrupedal Locomotion <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amr Mousa, Neil Karavis, Michele Caprio, Wei Pan, Richard Allmendinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadrupedal locomotion via Reinforcement Learning (RL) is commonly addressed
using the teacher-student paradigm, where a privileged teacher guides a
proprioceptive student policy. However, key challenges such as representation
misalignment between the privileged teacher and the proprioceptive-only
student, covariate shift due to behavioral cloning, and lack of deployable
adaptation lead to poor generalization in real-world scenarios. We propose
Teacher-Aligned Representations via Contrastive Learning (TAR), a framework
that leverages privileged information with self-supervised contrastive learning
to bridge this gap. By aligning representations to a privileged teacher in
simulation via contrastive objectives, our student policy learns structured
latent spaces and exhibits robust generalization to Out-of-Distribution (OOD)
scenarios, surpassing the fully privileged "Teacher". Results showed
accelerated training by 2x compared to state-of-the-art baselines to achieve
peak performance. OOD scenarios showed better generalization by 40 percent on
average compared to existing methods. Additionally, TAR transitions seamlessly
into learning during deployment without requiring privileged states, setting a
new benchmark in sample-efficient, adaptive locomotion and enabling continual
fine-tuning in real-world scenarios. Open-source code and videos are available
at https://ammousa.github.io/TARLoco/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE/RSJ International Conference
  on Intelligent Robots and Systems (IROS) 2025 for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OTTER: A <span class="highlight-title">Vision</span>-Language-Action Model with Text-Aware <span class="highlight-title">Visual</span> Feature
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03734v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03734v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam, Jitendra Malik, Ken Goldberg, Pieter Abbeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models aim to predict robotic actions based on
visual observations and language instructions. Existing approaches require
fine-tuning pre-trained visionlanguage models (VLMs) as visual and language
features are independently fed into downstream policies, degrading the
pre-trained semantic alignments. We propose OTTER, a novel VLA architecture
that leverages these existing alignments through explicit, text-aware visual
feature extraction. Instead of processing all visual features, OTTER
selectively extracts and passes only task-relevant visual features that are
semantically aligned with the language instruction to the policy transformer.
This allows OTTER to keep the pre-trained vision-language encoders frozen.
Thereby, OTTER preserves and utilizes the rich semantic understanding learned
from large-scale pre-training, enabling strong zero-shot generalization
capabilities. In simulation and real-world experiments, OTTER significantly
outperforms existing VLA models, demonstrating strong zeroshot generalization
to novel objects and environments. Video, code, checkpoints, and dataset:
https://ottervla.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> DexHandDiff: Interaction-aware <span class="highlight-title">Diffusion</span> Planning for Adaptive Dexterous
  <span class="highlight-title">Manipulation</span> <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18562v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18562v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Liang, <span class="highlight-author">Yao Mu</span>, Yixiao Wang, Tianxing Chen, Wenqi Shao, Wei Zhan, Masayoshi Tomizuka, Ping Luo, Mingyu Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dexterous manipulation with contact-rich interactions is crucial for advanced
robotics. While recent diffusion-based planning approaches show promise for
simple manipulation tasks, they often produce unrealistic ghost states (e.g.,
the object automatically moves without hand contact) or lack adaptability when
handling complex sequential interactions. In this work, we introduce
DexHandDiff, an interaction-aware diffusion planning framework for adaptive
dexterous manipulation. DexHandDiff models joint state-action dynamics through
a dual-phase diffusion process which consists of pre-interaction contact
alignment and post-contact goal-directed control, enabling goal-adaptive
generalizable dexterous manipulation. Additionally, we incorporate dynamics
model-based dual guidance and leverage large language models for automated
guidance function generation, enhancing generalizability for physical
interactions and facilitating diverse goal adaptation through language cues.
Experiments on physical interaction tasks such as door opening, pen and block
re-orientation, object relocation, and hammer striking demonstrate
DexHandDiff's effectiveness on goals outside training distributions, achieving
over twice the average success rate (59.2% vs. 29.5%) compared to existing
methods. Our framework achieves an average of 70.7% success rate on goal
adaptive dexterous tasks, highlighting its robustness and flexibility in
contact-rich manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025. Camera ready version. Previous DexDiffuser.
  Project page: https://dexdiffuser.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Michel Fortin, Olivier Gamache, William Fecteau, Effie Daum, William Larrivée-Hardy, François Pomerleau, Philippe Giguère
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Terrain awareness is an essential milestone to enable truly autonomous
off-road navigation. Accurately predicting terrain characteristics allows
optimizing a vehicle's path against potential hazards. Recent methods use deep
neural networks to predict traversability-related terrain properties in a
self-supervised manner, relying on proprioception as a training signal.
However, onboard cameras are inherently limited by their point-of-view relative
to the ground, suffering from occlusions and vanishing pixel density with
distance. This paper introduces a novel approach for self-supervised terrain
characterization using an aerial perspective from a hovering drone. We capture
terrain-aligned images while sampling the environment with a ground vehicle,
effectively training a simple predictor for vibrations, bumpiness, and energy
consumption. Our dataset includes 2.8 km of off-road data collected in forest
environment, comprising 13 484 ground-based images and 12 935 aerial images.
Our findings show that drone imagery improves terrain property prediction by
21.37 % on the whole dataset and 37.35 % in high vegetation, compared to ground
robot images. We conduct ablation studies to identify the main causes of these
performance improvements. We also demonstrate the real-world applicability of
our approach by scouting an unseen area with a drone, planning and executing an
optimized path on the ground.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, submitted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contractive Dynamical Imitation Policies for Efficient Out-of-Sample
  Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Abyaneh, Mahrokh G. Boroujeni, Hsiu-Chin Lin, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning is a data-driven approach to learning policies from expert
behavior, but it is prone to unreliable outcomes in out-of-sample (OOS)
regions. While previous research relying on stable dynamical systems guarantees
convergence to a desired state, it often overlooks transient behavior. We
propose a framework for learning policies modeled by contractive dynamical
systems, ensuring that all policy rollouts converge regardless of
perturbations, and in turn, enable efficient OOS recovery. By leveraging
recurrent equilibrium networks and coupling layers, the policy structure
guarantees contractivity for any parameter choice, which facilitates
unconstrained optimization. We also provide theoretical upper bounds for
worst-case and expected loss to rigorously establish the reliability of our
method in deployment. Empirically, we demonstrate substantial OOS performance
improvements for simulated robotic manipulation and navigation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Morphology-Control Trade-Off: Insights into Soft <span class="highlight-title">Robo</span>tic Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xie, Kai-fung Chu, Xing Wang, Fumiya Iida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft robotics holds transformative potential for enabling adaptive and
adaptable systems in dynamic environments. However, the interplay between
morphological and control complexities and their collective impact on task
performance remains poorly understood. Therefore, in this study, we investigate
these trade-offs across tasks of differing difficulty levels using four
well-used morphological complexity metrics and control complexity measured by
FLOPs. We investigate how these factors jointly influence task performance by
utilizing the evolutionary robot experiments. Results show that optimal
performance depends on the alignment between morphology and control: simpler
morphologies and lightweight controllers suffice for easier tasks, while harder
tasks demand higher complexities in both dimensions. In addition, a clear
trade-off between morphological and control complexities that achieve the same
task performance can be observed. Moreover, we also propose a sensitivity
analysis to expose the task-specific contributions of individual morphological
metrics. Our study establishes a framework for investigating the relationships
between morphology, control, and task performance, advancing the development of
task-specific robotic designs that balance computational efficiency with
adaptability. This study contributes to the practical application of soft
robotics in real-world scenarios by providing actionable insights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is planed to be submitted to a journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ManiCM: Real-time 3D <span class="highlight-title">Diffusion</span> Policy via Consistency Model for <span class="highlight-title">Robo</span>tic
  <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanxing Lu, Zifeng Gao, Tianxing Chen, Wenxun Dai, Ziwei Wang, Wenbo Ding, Yansong Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been verified to be effective in generating complex
distributions from natural images to motion trajectories. Recent
diffusion-based methods show impressive performance in 3D robotic manipulation
tasks, whereas they suffer from severe runtime inefficiency due to multiple
denoising steps, especially with high-dimensional observations. To this end, we
propose a real-time robotic manipulation model named ManiCM that imposes the
consistency constraint to the diffusion process, so that the model can generate
robot actions in only one-step inference. Specifically, we formulate a
consistent diffusion process in the robot action space conditioned on the point
cloud input, where the original action is required to be directly denoised from
any point along the ODE trajectory. To model this process, we design a
consistency distillation technique to predict the action sample directly
instead of predicting the noise within the vision community for fast
convergence in the low-dimensional action manifold. We evaluate ManiCM on 31
robotic manipulation tasks from Adroit and Metaworld, and the results
demonstrate that our approach accelerates the state-of-the-art method by 10
times in average inference speed while maintaining competitive average success
rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://manicm-fast.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in
  <span class="highlight-title">Reinforcement Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Kim, Seung-Woo Seo, Seong-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) has demonstrated strong performance in
robotic control but remains susceptible to out-of-distribution (OOD) states,
often resulting in unreliable actions and task failure. While previous methods
have focused on minimizing or preventing OOD occurrences, they largely neglect
recovery once an agent encounters such states. Although the latest research has
attempted to address this by guiding agents back to in-distribution states,
their reliance on uncertainty estimation hinders scalability in complex
environments. To overcome this limitation, we introduce Language Models for
Out-of-Distribution Recovery (LaMOuR), which enables recovery learning without
relying on uncertainty estimation. LaMOuR generates dense reward codes that
guide the agent back to a state where it can successfully perform its original
task, leveraging the capabilities of LVLMs in image description, logical
reasoning, and code generation. Experimental results show that LaMOuR
substantially enhances recovery efficiency across diverse locomotion tasks and
even generalizes effectively to complex environments, including humanoid
locomotion and mobile manipulation, where existing methods struggle. The code
and supplementary materials are available at https://lamour-rl.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing <span class="highlight-title">Robo</span>t Programming: Mixed Reality <span class="highlight-title">Gripper</span> Control <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Rettinger, Leander Hacker, Philipp Wolters, Gerhard Rigoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional robot programming methods are complex and time-consuming for
users. In recent years, alternative approaches such as mixed reality have been
explored to address these challenges and optimize robot programming. While the
findings of the mixed reality robot programming methods are convincing, most
existing methods rely on gesture interaction for robot programming. Since
controller-based interactions have proven to be more reliable, this paper
examines three controller-based programming methods within a mixed reality
scenario: 1) Classical Jogging, where the user positions the robot's end
effector using the controller's thumbsticks, 2) Direct Control, where the
controller's position and orientation directly corresponds to the end
effector's, and 3) Gripper Control, where the controller is enhanced with a
3D-printed gripper attachment to grasp and release objects. A within-subjects
study (n = 30) was conducted to compare these methods. The findings indicate
that the Gripper Control condition outperforms the others in terms of task
completion time, user experience, mental demand, and task performance, while
also being the preferred method. Therefore, it demonstrates promising potential
as an effective and efficient approach for future robot programming. Video
available at https://youtu.be/83kWr8zUFIQ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polytope Volume Monitoring Problem: Formulation and Solution via
  Parametric Linear Program Based Control Barrier Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12546v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12546v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shizhen Wu, Jinyang Dong, Xu Fang, Ning Sun, Yongchun Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the latest research on feasible space monitoring of multiple
control barrier functions (CBFs) as well as polytopic collision avoidance, this
paper studies the Polytope Volume Monitoring (PVM) problem, whose goal is to
design a control law for inputs of nonlinear systems to prevent the volume of
some state-dependent polytope from decreasing to zero. Recent studies have
explored the idea of applying Chebyshev ball method in optimization theory to
solve the case study of PVM; however, the underlying difficulties caused by
nonsmoothness have not been addressed. This paper continues the study on this
topic, where our main contribution is to establish the relationship between
nonsmooth CBF and parametric optimization theory through directional
derivatives for the first time, so as to solve PVM problems more conveniently.
In detail, inspired by Chebyshev ball approach, a parametric linear program
(PLP) based nonsmooth barrier function candidate is established for PVM, and
then, sufficient conditions for it to be a nonsmooth CBF are proposed, based on
which a quadratic program (QP) based safety filter with guaranteed feasibility
is proposed to address PVM problems. Finally, a numerical simulation example is
given to show the efficiency of the proposed safety filter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A simplified version is submitted to CDC2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied
  Instruction Following <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suyeon Shin, Sujin jeon, Junghyun Kim, Gi-Cheon Kang, Byoung-Tak Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied Instruction Following (EIF) is the task of executing natural
language instructions by navigating and interacting with objects in interactive
environments. A key challenge in EIF is compositional task planning, typically
addressed through supervised learning or few-shot in-context learning with
labeled data. To this end, we introduce the Socratic Planner, a self-QA-based
zero-shot planning method that infers an appropriate plan without any further
training. The Socratic Planner first facilitates self-questioning and answering
by the Large Language Model (LLM), which in turn helps generate a sequence of
subgoals. While executing the subgoals, an embodied agent may encounter
unexpected situations, such as unforeseen obstacles. The Socratic Planner then
adjusts plans based on dense visual feedback through a visually-grounded
re-planning mechanism. Experiments demonstrate the effectiveness of the
Socratic Planner, outperforming current state-of-the-art planning models on the
ALFRED benchmark across all metrics, particularly excelling in long-horizon
tasks that demand complex inference. We further demonstrate its real-world
applicability through deployment on a physical robot for long-horizon tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, published to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robo</span>Spatial: Teaching Spatial Understanding to 2D and 3D <span class="highlight-title">Vision</span>-Language
  Models for <span class="highlight-title">Robo</span>tics <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, Stan Birchfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial understanding is a crucial capability that enables robots to perceive
their surroundings, reason about their environment, and interact with it
meaningfully. In modern robotics, these capabilities are increasingly provided
by vision-language models. However, these models face significant challenges in
spatial reasoning tasks, as their training data are based on general-purpose
image datasets that often lack sophisticated spatial understanding. For
example, datasets frequently do not capture reference frame comprehension, yet
effective spatial reasoning requires understanding whether to reason from ego-,
world-, or object-centric perspectives. To address this issue, we introduce
RoboSpatial, a large-scale dataset for spatial understanding in robotics. It
consists of real indoor and tabletop scenes, captured as 3D scans and
egocentric images, and annotated with rich spatial information relevant to
robotics. The dataset includes 1M images, 5k 3D scans, and 3M annotated spatial
relationships, and the pairing of 2D egocentric images with 3D scans makes it
both 2D- and 3D- ready. Our experiments show that models trained with
RoboSpatial outperform baselines on downstream tasks such as spatial affordance
prediction, spatial relationship prediction, and robot manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for
  Zero-shot Object Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linqing Zhong, Chen Gao, Zihan Ding, Yue Liao, Huimin Ma, Shifeng Zhang, Xu Zhou, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find
a previously unseen object by navigating in unfamiliar environments. Such a
goal-oriented exploration heavily relies on the ability to perceive,
understand, and reason based on the spatial information of the environment.
However, current LLM-based approaches convert visual observations to language
descriptions and reason in the linguistic space, leading to the loss of spatial
information. In this paper, we introduce TopV-Nav, an MLLM-based method that
directly reasons on the top-view map with sufficient spatial information. To
fully unlock the MLLM's spatial reasoning potential in top-view perspective, we
propose the Adaptive Visual Prompt Generation (AVPG) method to adaptively
construct semantically-rich top-view map. It enables the agent to directly
utilize spatial information contained in the top-view map to conduct thorough
reasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to
dynamically zoom top-view map at preferred scales, enhancing local fine-grained
reasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to
predict and to utilize target locations, facilitating global and human-like
exploration. Experiments on MP3D and HM3D datasets demonstrate the superiority
of our TopV-Nav.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ General-purpose Clothes <span class="highlight-title">Manipulation</span> with Semantic Keypoints <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08160v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08160v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhong Deng, David Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clothes manipulation is a critical capability for household robots; yet,
existing methods are often confined to specific tasks, such as folding or
flattening, due to the complex high-dimensional geometry of deformable fabric.
This paper presents CLothes mAnipulation with Semantic keyPoints (CLASP) for
general-purpose clothes manipulation, which enables the robot to perform
diverse manipulation tasks over different types of clothes. The key idea of
CLASP is semantic keypoints -- e.g., "right shoulder", "left sleeve", etc. -- a
sparse spatial-semantic representation that is salient for both perception and
action. Semantic keypoints of clothes can be effectively extracted from depth
images and are sufficient to represent a broad range of clothes manipulation
policies. CLASP leverages semantic keypoints to bridge LLM-powered task
planning and low-level action execution in a two-level hierarchy. Extensive
simulation experiments show that CLASP outperforms baseline methods across
diverse clothes types in both seen and unseen tasks. Further, experiments with
a Kinova dual-arm system on four distinct tasks -- folding, flattening,
hanging, and placing -- confirm CLASP's performance on a real robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IEEE International Conference on Robotics and Automation
  (ICRA 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Motion Control of Multiple Mobile Manipulators for Reducing
  Interaction Wrench in Object <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhang Liu, Meng Ren, Kun Song, Gaoming Chen, Michael Yu Wang, Zhenhua Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world cooperative manipulation of objects, multiple mobile
manipulator systems may suffer from disturbances and asynchrony, leading to
excessive interaction wrenches and potentially causing object damage or
emergency stops. Existing methods often rely on torque control and dynamic
models, which are uncommon in many industrial robots and settings.
Additionally, dynamic models often neglect joint friction forces and are not
accurate. These methods are challenging to implement and validate in physical
systems. To address the problems, this paper presents a novel distributed
motion control approach aimed at reducing these unnecessary interaction
wrenches. The control law is only based on local information and joint velocity
control to enhance practical applicability. The communication delays within the
distributed architecture are considered. The stability of the control law is
rigorously proven by the Lyapunov theorem. In the simulations, the
effectiveness is shown, and the impact of communication graph connectivity and
communication delays has been studied. A comparison with other methods shows
the advantages of the proposed control law in terms of convergence speed and
robustness. Finally, the control law has been validated in physical
experiments. It does not require dynamic modeling or torque control, and thus
is more user-friendly for physical robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoAM: Foresight-Augmented Multi-Task Imitation Policy for <span class="highlight-title">Robo</span>tic
  <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19528v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19528v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Litao Liu, Wentao Wang, Yifan Han, Zhuoli Xie, Pengfei Yi, Junyan Li, Yi Qin, Wenzhao Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task imitation learning (MTIL) has shown significant potential in
robotic manipulation by enabling agents to perform various tasks using a single
policy. This simplifies the policy deployment and enhances the agent's
adaptability across different scenarios. However, key challenges remain, such
as maintaining action reliability (e.g., avoiding abnormal action sequences
that deviate from nominal task trajectories) and generalizing to unseen tasks
with a few expert demonstrations. To address these challenges, we introduce the
Foresight-Augmented Manipulation Policy (FoAM), a novel MTIL policy that
pioneers the use of multi-modal goal condition as input and introduces a
foresight augmentation in addition to the general action reconstruction. FoAM
enables the agent to reason about the visual consequences (states) of its
actions and learn more expressive embedding that captures nuanced task
variations. Extensive experiments on over 100 tasks in simulation and
real-world settings demonstrate that FoAM significantly enhances MTIL policy
performance, outperforming state-of-the-art baselines by up to 41% in success
rate. Meanwhile, we released our simulation suites, including a total of 10
scenarios and over 80 challenging tasks designed for manipulation policy
training and evaluation. See the project homepage projFoAM.github.io for
project details.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Vision</span>-based Multi-future Trajectory Prediction: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renhao Huang, Hao Xue, Maurice Pagnucco, Flora Salim, Yang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based trajectory prediction is an important task that supports safe
and intelligent behaviours in autonomous systems. Many advanced approaches have
been proposed over the years with improved spatial and temporal feature
extraction. However, human behaviour is naturally diverse and uncertain. Given
the past trajectory and surrounding environment information, an agent can have
multiple plausible trajectories in the future. To tackle this problem, an
essential task named multi-future trajectory prediction (MTP) has recently been
studied. This task aims to generate a diverse, acceptable and explainable
distribution of future predictions for each agent. In this paper, we present
the first survey for MTP with our unique taxonomies and a comprehensive
analysis of frameworks, datasets and evaluation metrics. We also compare models
on existing MTP datasets and conduct experiments on the ForkingPath dataset.
Finally, we discuss multiple future directions that can help researchers
develop novel multi-future trajectory prediction systems and other diverse
learning tasks similar to MTP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TNNLS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLIM: Scalable and Lightweight <span class="highlight-title">LiDAR</span> Mapping in Urban Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehuan Yu, Zhijian Qiao, Wenyi Liu, Huan Yin, Shaojie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR point cloud maps are extensively utilized on roads for robot navigation
due to their high consistency. However, dense point clouds face challenges of
high memory consumption and reduced maintainability for long-term operations.
In this study, we introduce SLIM, a scalable and lightweight mapping system for
long-term LiDAR mapping in urban environments. The system begins by
parameterizing structural point clouds into lines and planes. These lightweight
and structural representations meet the requirements of map merging, pose graph
optimization, and bundle adjustment, ensuring incremental management and local
consistency. For long-term operations, a map-centric nonlinear factor recovery
method is designed to sparsify poses while preserving mapping accuracy. We
validate the SLIM system with multi-session real-world LiDAR data from
classical LiDAR mapping datasets, including KITTI, NCLT, HeLiPR and M2DGR. The
experiments demonstrate its capabilities in mapping accuracy, lightweightness,
and scalability. Map re-use is also verified through map-based robot
localization. Finally, with multi-session LiDAR data, the SLIM system provides
a globally consistent map with low memory consumption (~130 KB/km on KITTI).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Robotics. Video:
  https://youtu.be/8HQnYMf_BWI Code:
  https://github.com/HKUST-Aerial-Robotics/SLIM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NuRF: Nudging the Particle Filter in Radiance Fields for <span class="highlight-title">Robo</span>t <span class="highlight-title">Visual</span>
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wugang Meng, Tianfu Wu, Huan Yin, Fumin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we localize a robot on a map only using monocular vision? This study
presents NuRF, an adaptive and nudged particle filter framework in radiance
fields for 6-DoF robot visual localization. NuRF leverages recent advancements
in radiance fields and visual place recognition. Conventional visual place
recognition meets the challenges of data sparsity and artifact-induced
inaccuracies. By utilizing radiance field-generated novel views, NuRF enhances
visual localization performance and combines coarse global localization with
the fine-grained pose tracking of a particle filter, ensuring continuous and
precise localization. Experimentally, our method converges 7 times faster than
existing Monte Carlo-based methods and achieves localization accuracy within 1
meter, offering an efficient and resilient solution for indoor visual
localization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Publication in IEEE Transactions on Cognitive and
  Developmental Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inference-Time Policy Steering through Human Interactions <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16627v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16627v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Wang, Lirui Wang, Yilun Du, Balakumar Sundaralingam, Xuning Yang, Yu-Wei Chao, Claudia Perez-D'Arpino, Dieter Fox, Julie Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative policies trained with human demonstrations can autonomously
accomplish multimodal, long-horizon tasks. However, during inference, humans
are often removed from the policy execution loop, limiting the ability to guide
a pre-trained policy towards a specific sub-goal or trajectory shape among
multiple predictions. Naive human intervention may inadvertently exacerbate
distribution shift, leading to constraint violations or execution failures. To
better align policy output with human intent without inducing
out-of-distribution errors, we propose an Inference-Time Policy Steering (ITPS)
framework that leverages human interactions to bias the generative sampling
process, rather than fine-tuning the policy on interaction data. We evaluate
ITPS across three simulated and real-world benchmarks, testing three forms of
human interaction and associated alignment distance metrics. Among six sampling
strategies, our proposed stochastic sampling with diffusion policy achieves the
best trade-off between alignment and distribution shift. Videos are available
at https://yanweiw.github.io/itps/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Physics-ML Modeling for Marine Vehicle Maneuvering Motions in the
  Presence of Environmental Disturbances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, Jian Cheng, Liang Xu, Lizhu Hao, Yan Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A hybrid physics-machine learning modeling framework is proposed for the
surface vehicles' maneuvering motions to address the modeling capability and
stability in the presence of environmental disturbances. From a deep learning
perspective, the framework is based on a variant version of residual networks
with additional feature extraction. Initially, an imperfect physical model is
derived and identified to capture the fundamental hydrodynamic characteristics
of marine vehicles. This model is then integrated with a feedforward network
through a residual block. Additionally, feature extraction from trigonometric
transformations is employed in the machine learning component to account for
the periodic influence of currents and waves. The proposed method is evaluated
using real navigational data from the 'JH7500' unmanned surface vehicle. The
results demonstrate the robust generalizability and accurate long-term
prediction capabilities of the nonlinear dynamic model in specific
environmental conditions. This approach has the potential to be extended and
applied to develop a comprehensive high-fidelity simulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The content of the manuscript will undergo significant revisions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Vehicle Driver Monitoring <span class="highlight-title">Dataset</span> from Real-World Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Sabry, Walter Morales-Alvarez, Cristina Olaverri-Monreal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From SAE Level 3 of automation onwards, drivers are allowed to engage in
activities that are not directly related to driving during their travel.
However, in level 3, a misunderstanding of the capabilities of the system might
lead drivers to engage in secondary tasks, which could impair their ability to
react to challenging traffic situations.
  Anticipating driver activity allows for early detection of risky behaviors,
to prevent accidents. To be able to predict the driver activity, a Deep
Learning network needs to be trained on a dataset. However, the use of datasets
based on simulation for training and the migration to real-world data for
prediction has proven to be suboptimal. Hence, this paper presents a real-world
driver activity dataset, openly accessible on IEEE Dataport, which encompasses
various activities that occur in autonomous driving scenarios under various
illumination and weather conditions. Results from the training process showed
that the dataset provides an excellent benchmark for implementing models for
driver activity recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Agent Inverse <span class="highlight-title">Reinforcement Learning</span> in Real World Unstructured
  Pedestrian Crowds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16439v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16439v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Chandra, Haresh Karnan, Negar Mehr, Peter Stone, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social robot navigation in crowded public spaces such as university campuses,
restaurants, grocery stores, and hospitals, is an increasingly important area
of research. One of the core strategies for achieving this goal is to
understand humans' intent--underlying psychological factors that govern their
motion--by learning their reward functions, typically via inverse reinforcement
learning (IRL). Despite significant progress in IRL, learning reward functions
of multiple agents simultaneously in dense unstructured pedestrian crowds has
remained intractable due to the nature of the tightly coupled social
interactions that occur in these scenarios \textit{e.g.} passing,
intersections, swerving, weaving, etc. In this paper, we present a new
multi-agent maximum entropy inverse reinforcement learning algorithm for real
world unstructured pedestrian crowds. Key to our approach is a simple, but
effective, mathematical trick which we name the so-called
tractability-rationality trade-off trick that achieves tractability at the cost
of a slight reduction in accuracy. We compare our approach to the classical
single-agent MaxEnt IRL as well as state-of-the-art trajectory prediction
methods on several datasets including the ETH, UCY, SCAND, JRDB, and a new
dataset, called Speedway, collected at a busy intersection on a University
campus focusing on dense, complex agent interactions. Our key findings show
that, on the dense Speedway dataset, our approach ranks 1st among top 7
baselines with >2X improvement over single-agent IRL, and is competitive with
state-of-the-art large transformer-based encoder-decoder models on sparser
datasets such as ETH/UCY (ranks 3rd among top 7 baselines).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimBEV: A Synthetic Multi-Task Multi-Sensor Driving Data Generation Tool
  and <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Goodarz Mehr, Azim Eskandarian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bird's-eye view (BEV) perception has garnered significant attention in
autonomous driving in recent years, in part because BEV representation
facilitates multi-modal sensor fusion. BEV representation enables a variety of
perception tasks including BEV segmentation, a concise view of the environment
useful for planning a vehicle's trajectory. However, this representation is not
fully supported by existing datasets, and creation of new datasets for this
purpose can be a time-consuming endeavor. To address this challenge, we
introduce SimBEV. SimBEV is a randomized synthetic data generation tool that is
extensively configurable and scalable, supports a wide array of sensors,
incorporates information from multiple sources to capture accurate BEV ground
truth, and enables a variety of perception tasks including BEV segmentation and
3D object detection. SimBEV is used to create the SimBEV dataset, a large
collection of annotated perception data from diverse driving scenarios. SimBEV
and the SimBEV dataset are open and available to the public.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqi Liu, Zihao Huang, Zhaoxi Chen, Guangcong Wang, Shoukang Hu, Liao Shen, Huiqiang Sun, Zhiguo Cao, Wei Li, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Free4D, a novel tuning-free framework for 4D scene generation from
a single image. Existing methods either focus on object-level generation,
making scene-level generation infeasible, or rely on large-scale multi-view
video datasets for expensive training, with limited generalization ability due
to the scarcity of 4D scene data. In contrast, our key insight is to distill
pre-trained foundation models for consistent 4D scene representation, which
offers promising advantages such as efficiency and generalizability. 1) To
achieve this, we first animate the input image using image-to-video diffusion
models followed by 4D geometric structure initialization. 2) To turn this
coarse structure into spatial-temporal consistent multiview videos, we design
an adaptive guidance mechanism with a point-guided denoising strategy for
spatial consistency and a novel latent replacement strategy for temporal
coherence. 3) To lift these generated observations into consistent 4D
representation, we propose a modulation-based refinement to mitigate
inconsistencies while fully leveraging the generated information. The resulting
4D representation enables real-time, controllable rendering, marking a
significant advancement in single-image-based 4D scene generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://free4d.github.io/ , Code:
  https://github.com/TQTQliu/Free4D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> FB-4D: Spatial-Temporal Coherent Dynamic 3D Content Generation with
  Feature Banks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinwei Li, Huan-ang Gao, Wenyi Li, Haohan Chi, Chenyu Liu, Chenxi Du, Yiqian Liu, Mingju Gao, Guiyu Zhang, Zongzheng Zhang, <span class="highlight-author">Li Yi</span>, Yao Yao, Jingwei Zhao, Hongyang Li, Yikai Wang, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancements in diffusion models and 3D generation techniques,
dynamic 3D content generation has become a crucial research area. However,
achieving high-fidelity 4D (dynamic 3D) generation with strong spatial-temporal
consistency remains a challenging task. Inspired by recent findings that
pretrained diffusion features capture rich correspondences, we propose FB-4D, a
novel 4D generation framework that integrates a Feature Bank mechanism to
enhance both spatial and temporal consistency in generated frames. In FB-4D, we
store features extracted from previous frames and fuse them into the process of
generating subsequent frames, ensuring consistent characteristics across both
time and multiple views. To ensure a compact representation, the Feature Bank
is updated by a proposed dynamic merging mechanism. Leveraging this Feature
Bank, we demonstrate for the first time that generating additional reference
sequences through multiple autoregressive iterations can continuously improve
generation performance. Experimental results show that FB-4D significantly
outperforms existing methods in terms of rendering quality, spatial-temporal
consistency, and robustness. It surpasses all multi-view generation tuning-free
approaches by a large margin and achieves performance on par with
training-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:https://fb-4d.c7w.tech/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Audio-<span class="highlight-title">Visual</span> Editing via Cross-Modal Delta Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan-Bo Lin, Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Chung-Ching Lin, Xiaofei Wang, Gedas Bertasius, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce zero-shot audio-video editing, a novel task that
requires transforming original audio-visual content to align with a specified
textual prompt without additional model training. To evaluate this task, we
curate a benchmark dataset, AvED-Bench, designed explicitly for zero-shot
audio-video editing. AvED-Bench includes 110 videos, each with a 10-second
duration, spanning 11 categories from VGGSound. It offers diverse prompts and
scenarios that require precise alignment between auditory and visual elements,
enabling robust evaluation. We identify limitations in existing zero-shot audio
and video editing methods, particularly in synchronization and coherence
between modalities, which often result in inconsistent outcomes. To address
these challenges, we propose AvED, a zero-shot cross-modal delta denoising
framework that leverages audio-video interactions to achieve synchronized and
coherent edits. AvED demonstrates superior results on both AvED-Bench and the
recent OAVE dataset to validate its generalization capabilities. Results are
available at https://genjib.github.io/project_page/AVED/index.html
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://genjib.github.io/project_page/AVED/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BASKET: A Large-Scale Video <span class="highlight-title">Dataset</span> for Fine-Grained Skill Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulu Pan, Ce Zhang, Gedas Bertasius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present BASKET, a large-scale basketball video dataset for fine-grained
skill estimation. BASKET contains 4,477 hours of video capturing 32,232
basketball players from all over the world. Compared to prior skill estimation
datasets, our dataset includes a massive number of skilled participants with
unprecedented diversity in terms of gender, age, skill level, geographical
location, etc. BASKET includes 20 fine-grained basketball skills, challenging
modern video recognition models to capture the intricate nuances of player
skill through in-depth video analysis. Given a long highlight video (8-10
minutes) of a particular player, the model needs to predict the skill level
(e.g., excellent, good, average, fair, poor) for each of the 20 basketball
skills. Our empirical analysis reveals that the current state-of-the-art video
models struggle with this task, significantly lagging behind the human
baseline. We believe that BASKET could be a useful resource for developing new
video models with advanced long-range, fine-grained recognition capabilities.
In addition, we hope that our dataset will be useful for domain-specific
applications such as fair basketball scouting, personalized player development,
and many others. Dataset and code are available at
https://github.com/yulupan00/BASKET.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile
  Gaussian Feature Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Zhou, Hui Ren, Yijia Weng, Shuwang Zhang, Zhen Wang, Dejia Xu, Zhiwen Fan, Suya You, Zhangyang Wang, Leonidas Guibas, Achuta Kadambi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 2D and multimodal models have achieved remarkable
success by leveraging large-scale training on extensive datasets. However,
extending these achievements to enable free-form interactions and high-level
semantic operations with complex 3D/4D scenes remains challenging. This
difficulty stems from the limited availability of large-scale, annotated 3D/4D
or multi-view datasets, which are crucial for generalizable vision and language
tasks such as open-vocabulary and prompt-based segmentation, language-guided
editing, and visual question answering (VQA). In this paper, we introduce
Feature4X, a universal framework designed to extend any functionality from 2D
vision foundation model into the 4D realm, using only monocular video input,
which is widely available from user-generated content. The "X" in Feature4X
represents its versatility, enabling any task through adaptable,
model-conditioned 4D feature field distillation. At the core of our framework
is a dynamic optimization strategy that unifies multiple model capabilities
into a single representation. Additionally, to the best of our knowledge,
Feature4X is the first method to distill and lift the features of video
foundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field
using Gaussian Splatting. Our experiments showcase novel view segment anything,
geometric and appearance scene editing, and free-form VQA across all time
steps, empowered by LLMs in feedback loops. These advancements broaden the
scope of agentic AI applications by providing a foundation for scalable,
contextually and spatiotemporally aware systems capable of immersive dynamic 4D
scene interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangled Source-Free Personalization for Facial Expression
  Recognition with Neutral Target Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masoumeh Sharafi, Emma Ollivier, Muhammad Osama Zeeshan, Soufiane Belharbi, Marco Pedersoli, Alessandro Lameiras Koerich, Simon Bacon,  Eric~Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Expression Recognition (FER) from videos is a crucial task in various
application areas, such as human-computer interaction and health monitoring
(e.g., pain, depression, fatigue, and stress). Beyond the challenges of
recognizing subtle emotional or health states, the effectiveness of deep FER
models is often hindered by the considerable variability of expressions among
subjects. Source-free domain adaptation (SFDA) methods are employed to adapt a
pre-trained source model using only unlabeled target domain data, thereby
avoiding data privacy and storage issues. Typically, SFDA methods adapt to a
target domain dataset corresponding to an entire population and assume it
includes data from all recognition classes. However, collecting such
comprehensive target data can be difficult or even impossible for FER in
healthcare applications. In many real-world scenarios, it may be feasible to
collect a short neutral control video (displaying only neutral expressions) for
target subjects before deployment. These videos can be used to adapt a model to
better handle the variability of expressions among subjects. This paper
introduces the Disentangled Source-Free Domain Adaptation (DSFDA) method to
address the SFDA challenge posed by missing target expression data. DSFDA
leverages data from a neutral target control video for end-to-end generation
and adaptation of target data with missing non-neutral data. Our method learns
to disentangle features related to expressions and identity while generating
the missing non-neutral target data, thereby enhancing model accuracy.
Additionally, our self-supervision strategy improves model adaptation by
reconstructing target images that maintain the same identity and source
expression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADS-Edit: A Multimodal Knowledge Editing <span class="highlight-title">Dataset</span> for Autonomous Driving
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Wang, Jizhan Fang, Xiang Chen, Bozhong Tian, Ziwen Xu, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Multimodal Models (LMMs) have shown promise in
Autonomous Driving Systems (ADS). However, their direct application to ADS is
hindered by challenges such as misunderstanding of traffic knowledge, complex
road conditions, and diverse states of vehicle. To address these challenges, we
propose the use of Knowledge Editing, which enables targeted modifications to a
model's behavior without the need for full retraining. Meanwhile, we introduce
ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,
which includes various real-world scenarios, multiple data types, and
comprehensive evaluation metrics. We conduct comprehensive experiments and
derive several interesting conclusions. We hope that our work will contribute
to the further advancement of knowledge editing applications in the field of
autonomous driving. Code and data are available in
https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reason-RFT: Reinforcement Fine-Tuning for <span class="highlight-title">Visual</span> Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual reasoning abilities play a crucial role in understanding complex
multimodal data, advancing both domain-specific applications and artificial
general intelligence (AGI). Existing methods improve VLM reasoning via
Chain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated
training data to enhance visual reasoning capabilities. However, this training
paradigm may lead to overfitting and cognitive rigidity, restricting the
model's ability to transfer visual reasoning skills across domains and limiting
its real-world applicability. To address these limitations, we propose
Reason-RFT, a novel reinforcement fine-tuning framework that significantly
enhances generalization capabilities in visual reasoning tasks. Reason-RFT
introduces a two-phase training framework for visual reasoning: (1) Supervised
Fine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the
reasoning potential of Vision-Language Models (VLMs), followed by (2) Group
Relative Policy Optimization (GRPO)-based reinforcement learning that generates
multiple reasoning-response pairs, significantly enhancing generalization in
visual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,
we reconstructed a comprehensive dataset spanning visual counting, structure
perception, and spatial transformation.cExperimental results demonstrate
Reasoning-RFT's three key advantages: (1) Performance Enhancement: achieving
state-of-the-art results across multiple tasks, outperforming most mainstream
open-source and proprietary models; (2) Generalization Superiority:
consistently maintaining robust performance across diverse tasks and domains,
outperforming alternative training paradigms; (3) Data Efficiency: excelling in
few-shot learning scenarios while surpassing full-dataset SFT baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniSTD: Towards Unified Spatio-Temporal Learning across Diverse
  Disciplines <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tang, Xinzhu Ma, Encheng Su, Xiufeng Song, Xiaohong Liu, Wei-Hong Li, Lei Bai, Wanli Ouyang, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional spatiotemporal models generally rely on task-specific
architectures, which limit their generalizability and scalability across
diverse tasks due to domain-specific design requirements. In this paper, we
introduce \textbf{UniSTD}, a unified Transformer-based framework for
spatiotemporal modeling, which is inspired by advances in recent foundation
models with the two-stage pretraining-then-adaption paradigm. Specifically, our
work demonstrates that task-agnostic pretraining on 2D vision and vision-text
datasets can build a generalizable model foundation for spatiotemporal
learning, followed by specialized joint training on spatiotemporal datasets to
enhance task-specific adaptability. To improve the learning capabilities across
domains, our framework employs a rank-adaptive mixture-of-expert adaptation by
using fractional interpolation to relax the discrete variables so that can be
optimized in the continuous space. Additionally, we introduce a temporal module
to incorporate temporal dynamics explicitly. We evaluate our approach on a
large-scale dataset covering 10 tasks across 4 disciplines, demonstrating that
a unified spatiotemporal model can achieve scalable, cross-task learning and
support up to 10 tasks simultaneously within one model while reducing training
costs in multi-domain applications. Code will be available at
https://github.com/1hunters/UniSTD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhysGen3D: Crafting a Miniature Interactive World from a Single Image <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Chen, Hanxiao Jiang, Shaowei Liu, Saurabh Gupta, Yunzhu Li, Hao Zhao, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Envisioning physically plausible outcomes from a single image requires a deep
understanding of the world's dynamics. To address this, we introduce PhysGen3D,
a novel framework that transforms a single image into an amodal,
camera-centric, interactive 3D scene. By combining advanced image-based
geometric and semantic understanding with physics-based simulation, PhysGen3D
creates an interactive 3D world from a static image, enabling us to "imagine"
and simulate future scenarios based on user input. At its core, PhysGen3D
estimates 3D shapes, poses, physical and lighting properties of objects,
thereby capturing essential physical attributes that drive realistic object
interactions. This framework allows users to specify precise initial
conditions, such as object speed or material properties, for enhanced control
over generated video outcomes. We evaluate PhysGen3D's performance against
closed-source state-of-the-art (SOTA) image-to-video models, including Pika,
Kling, and Gen-3, showing PhysGen3D's capacity to generate videos with
realistic physics while offering greater flexibility and fine-grained control.
Our results show that PhysGen3D achieves a unique balance of photorealism,
physical plausibility, and user-driven interactivity, opening new possibilities
for generating dynamic, physics-grounded video from an image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025, Project page: https://by-luckk.github.io/PhysGen3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MATHGLANCE: Multimodal Large Language Models Do Not Know Where to Look
  in Mathematical Diagrams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanpeng Sun, Shan Zhang, Wei Tang, Aotian Chen, Piotr Koniusz, Kai Zou, Yuan Xue, Anton van den Hengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diagrams serve as a fundamental form of visual language, representing complex
concepts and their inter-relationships through structured symbols, shapes, and
spatial arrangements. Unlike natural images, their inherently symbolic and
abstract nature poses significant challenges for Multimodal Large Language
Models (MLLMs). However, current benchmarks conflate perceptual and reasoning
tasks, making it difficult to assess whether MLLMs genuinely understand
mathematical diagrams beyond superficial pattern recognition. To address this
gap, we introduce MATHGLANCE, a benchmark specifically designed to isolate and
evaluate mathematical perception in MLLMs. MATHGLANCE comprises 1.2K images and
1.6K carefully curated questions spanning four perception tasks: shape
classification, object counting, relationship identification, and object
grounding, covering diverse domains including plane geometry, solid geometry,
and graphical representations. Our evaluation of MLLMs reveals that their
ability to understand diagrams is notably limited, particularly in fine-grained
grounding tasks. In response, we construct GeoPeP, a perception-oriented
dataset of 200K structured geometry image-text pairs explicitly annotated with
geometric primitives and precise spatial relationships. Training MLLM on GeoPeP
leads to significant gains in perceptual accuracy, which in turn substantially
improves mathematical reasoning. Our benchmark and dataset establish critical
standards for evaluating and advancing multimodal mathematical understanding,
providing valuable resources and insights to foster future MLLM research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High Quality <span class="highlight-title">Diffusion</span> Distillation on a Single GPU with Relative and
  Absolute Position Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqiang Zhang, Kenta Niwa, J. P. Lewis, Cedric Mesnage, W. Bastiaan Kleijn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce relative and absolute position matching (RAPM), a diffusion
distillation method resulting in high quality generation that can be trained
efficiently on a single GPU. Recent diffusion distillation research has
achieved excellent results for high-resolution text-to-image generation with
methods such as phased consistency models (PCM) and improved distribution
matching distillation (DMD2). However, these methods generally require many
GPUs (e.g.~8-64) and significant batchsizes (e.g.~128-2048) during training,
resulting in memory and compute requirements that are beyond the resources of
some researchers. RAPM provides effective single-GPU diffusion distillation
training with a batchsize of 1. The new method attempts to mimic the sampling
trajectories of the teacher model by matching the relative and absolute
positions. The design of relative positions is inspired by PCM. Two
discriminators are introduced accordingly in RAPM, one for matching relative
positions and the other for absolute positions. Experimental results on
StableDiffusion (SD) V1.5 and SDXL indicate that RAPM with 4 timesteps produces
comparable FID scores as the best method with 1 timestep under very limited
computational resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Detection and Music Recommendation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swetha Kambham, Hubert Jhonson, Sai Prathap Reddy Kambham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As artificial intelligence becomes more and more ingrained in daily life, we
present a novel system that uses deep learning for music recommendation and
emotion-based detection. Through the use of facial recognition and the DeepFace
framework, our method analyses human emotions in real-time and then plays music
that reflects the mood it has discovered. The system uses a webcam to take
pictures, analyses the most common facial expression, and then pulls a playlist
from local storage that corresponds to the mood it has detected. An engaging
and customised experience is ensured by allowing users to manually change the
song selection via a dropdown menu or navigation buttons. By continuously
looping over the playlist, the technology guarantees continuity. The objective
of our system is to improve emotional well-being through music therapy by
offering a responsive and automated music-selection experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SChanger: Change Detection from a Semantic Change and Spatial
  Consistency Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Zhou, Keyan Hu, Yutian Fang, Xiaoping Rui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change detection is a key task in Earth observation applications. Recently,
deep learning methods have demonstrated strong performance and widespread
application. However, change detection faces data scarcity due to the
labor-intensive process of accurately aligning remote sensing images of the
same area, which limits the performance of deep learning algorithms. To address
the data scarcity issue, we develop a fine-tuning strategy called the Semantic
Change Network (SCN). We initially pre-train the model on single-temporal
supervised tasks to acquire prior knowledge of instance feature extraction. The
model then employs a shared-weight Siamese architecture and extended Temporal
Fusion Module (TFM) to preserve this prior knowledge and is fine-tuned on
change detection tasks. The learned semantics for identifying all instances is
changed to focus on identifying only the changes. Meanwhile, we observe that
the locations of changes between the two images are spatially identical, a
concept we refer to as spatial consistency. We introduce this inductive bias
through an attention map that is generated by large-kernel convolutions and
applied to the features from both time points. This enhances the modeling of
multi-scale changes and helps capture underlying relationships in change
detection semantics. We develop a binary change detection model utilizing these
two strategies. The model is validated against state-of-the-art methods on six
datasets, surpassing all benchmark methods and achieving F1 scores of 92.87%,
86.43%, 68.95%, 97.62%, 84.58%, and 93.20% on the LEVIR-CD, LEVIR-CD+,
S2Looking, CDD, SYSU-CD, and WHU-CD datasets, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Motion Blending for Versatile Motion Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Jiang, Hongjie Li, Ziye Yuan, Zimo He, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided motion editing enables high-level semantic control and iterative
modifications beyond traditional keyframe animation. Existing methods rely on
limited pre-collected training triplets, which severely hinders their
versatility in diverse editing scenarios. We introduce MotionCutMix, an online
data augmentation technique that dynamically generates training triplets by
blending body part motions based on input text. While MotionCutMix effectively
expands the training distribution, the compositional nature introduces
increased randomness and potential body part incoordination. To model such a
rich distribution, we present MotionReFit, an auto-regressive diffusion model
with a motion coordinator. The auto-regressive architecture facilitates
learning by decomposing long sequences, while the motion coordinator mitigates
the artifacts of motion composition. Our method handles both spatial and
temporal motion edits directly from high-level human instructions, without
relying on additional specifications or Large Language Models. Through
extensive experiments, we show that MotionReFit achieves state-of-the-art
performance in text-guided motion editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A weakly-supervised deep learning model for fast localisation and
  delineation of the skeleton, internal organs, and spinal canal on Whole-Body
  <span class="highlight-title">Diffusion</span>-Weighted MRI (WB-DWI) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Candito, A. Dragan, R. Holbrey, A. Ribeiro, R. Donners, C. Messiou, N. Tunariu, D. -M. Koh, M. D. Blackledge, The Institute of Cancer Research,  London, United Kingdom, The Royal Marsden NHS Foundation Trust,  London, United Kingdom, University Hospital Basel,  Basel,  Switzerland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Apparent Diffusion Coefficient (ADC) values and Total Diffusion
Volume (TDV) from Whole-body diffusion-weighted MRI (WB-DWI) are recognized
cancer imaging biomarkers. However, manual disease delineation for ADC and TDV
measurements is unfeasible in clinical practice, demanding automation. As a
first step, we propose an algorithm to generate fast and reproducible
probability maps of the skeleton, adjacent internal organs (liver, spleen,
urinary bladder, and kidneys), and spinal canal. Methods: We developed an
automated deep-learning pipeline based on a 3D patch-based Residual U-Net
architecture that localizes and delineates these anatomical structures on
WB-DWI. The algorithm was trained using "soft-labels" (non-binary
segmentations) derived from a computationally intensive atlas-based approach.
For training and validation, we employed a multi-center WB-DWI dataset
comprising 532 scans from patients with Advanced Prostate Cancer (APC) or
Multiple Myeloma (MM), with testing on 45 patients. Results: Our
weakly-supervised deep learning model achieved an average dice
score/precision/recall of 0.66/0.6/0.73 for skeletal delineations,
0.8/0.79/0.81 for internal organs, and 0.85/0.79/0.94 for spinal canal, with
surface distances consistently below 3 mm. Relative median ADC and
log-transformed volume differences between automated and manual expert-defined
full-body delineations were below 10% and 4%, respectively. The computational
time for generating probability maps was 12x faster than the atlas-based
registration algorithm (25 s vs. 5 min). An experienced radiologist rated the
model's accuracy "good" or "excellent" on test datasets. Conclusion: Our model
offers fast and reproducible probability maps for localizing and delineating
body regions on WB-DWI, enabling ADC and TDV quantification, potentially
supporting clinicians in disease staging and treatment response assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demand Estimation with Text and Image Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Compiani, Ilya Morozov, Stephan Seiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a demand estimation method that leverages unstructured text and
image data to infer substitution patterns. Using pre-trained deep learning
models, we extract embeddings from product images and textual descriptions and
incorporate them into a random coefficients logit model. This approach enables
researchers to estimate demand even when they lack data on product attributes
or when consumers value hard-to-quantify attributes, such as visual design or
functional benefits. Using data from a choice experiment, we show that our
approach outperforms standard attribute-based models in counterfactual
predictions of consumers' second choices. We also apply it across 40 product
categories on Amazon.com and consistently find that text and image data help
identify close substitutes within each category.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saron Samuel, Dan DeGenaro, Jimena Guallar-Blasco, Kate Sanders, Oluwaseun Eisape, Arun Reddy, Alexander Martin, Andrew Yates, Eugene Yang, Cameron Carpenter, David Etter, Efsun Kayi, Matthew Wiesner, Kenton Murray, Reno Kriz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Videos inherently contain multiple modalities, including visual events, text
overlays, sounds, and speech, all of which are important for retrieval.
However, state-of-the-art multimodal language models like VAST and LanguageBind
are built on vision-language models (VLMs), and thus overly prioritize visual
signals. Retrieval benchmarks further reinforce this bias by focusing on visual
queries and neglecting other modalities. We create a search system MMMORRF that
extracts text and features from both visual and audio modalities and integrates
them with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is
both effective and efficient, demonstrating practicality in searching videos
based on users' information needs instead of visual descriptive queries. We
evaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed
for more targeted information needs, and find that it improves nDCG@20 by 81%
over leading multimodal encoders and 37% over single-modality retrieval,
demonstrating the value of integrating diverse modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast
  Ultrasound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Huang, Ao Chang, Haoran Dou, Xing Tao, Xinrui Zhou, Yan Cao, Ruobing Huang, Alejandro F Frangi, Lingyun Bao, Xin Yang, Dong Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D
automated breast ultrasound (ABUS) is crucial for clinical diagnosis and
treatment planning. Therefore, developing an automated system for nodule
segmentation can enhance user independence and expedite clinical analysis.
Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can
streamline the laborious and intricate annotation process. However, current WSS
methods face challenges in achieving precise nodule segmentation, as many of
them depend on inaccurate activation maps or inefficient pseudo-mask generation
algorithms. In this study, we introduce a novel multi-agent reinforcement
learning-based WSS framework called Flip Learning, which relies solely on 2D/3D
boxes for accurate segmentation. Specifically, multiple agents are employed to
erase the target from the box to facilitate classification tag flipping, with
the erased region serving as the predicted segmentation mask. The key
contributions of this research are as follows: (1) Adoption of a
superpixel/supervoxel-based approach to encode the standardized environment,
capturing boundary priors and expediting the learning process. (2) Introduction
of three meticulously designed rewards, comprising a classification score
reward and two intensity distribution rewards, to steer the agents' erasing
process precisely, thereby avoiding both under- and over-segmentation. (3)
Implementation of a progressive curriculum learning strategy to enable agents
to interact with the environment in a progressively challenging manner, thereby
enhancing learning efficiency. Extensively validated on the large in-house BUS
and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS
methods and foundation models, and achieves comparable performance as
fully-supervised learning algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Medical Image Analysis. 24 pages, 13 figures, 18 tabels</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLRD: Global-Local Collaborative Reason and Debate with PSL for 3D
  Open-Vocabulary Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Peng, Si Liu, Chen Gao, Yan Bai, Beipeng Mu, Xiaofei Wang, Huaxia Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of LiDAR-based 3D Open-Vocabulary Detection (3D OVD) requires the
detector to learn to detect novel objects from point clouds without
off-the-shelf training labels. Previous methods focus on the learning of
object-level representations and ignore the scene-level information, thus it is
hard to distinguish objects with similar classes. In this work, we propose a
Global-Local Collaborative Reason and Debate with PSL (GLRD) framework for the
3D OVD task, considering both local object-level information and global
scene-level information. Specifically, LLM is utilized to perform common sense
reasoning based on object-level and scene-level information, where the
detection result is refined accordingly. To further boost the LLM's ability of
precise decisions, we also design a probabilistic soft logic solver (OV-PSL) to
search for the optimal solution, and a debate scheme to confirm the class of
confusable objects. In addition, to alleviate the uneven distribution of
classes, a static balance scheme (SBC) and a dynamic balance scheme (DBC) are
designed. In addition, to reduce the influence of noise in data and training,
we further propose Reflected Pseudo Labels Generation (RPLG) and
Background-Aware Object Localization (BAOL). Extensive experiments conducted on
ScanNet and SUN RGB-D demonstrate the superiority of GLRD, where absolute
improvements in mean average precision are $+2.82\%$ on SUN RGB-D and $+3.72\%$
on ScanNet in the partial open-vocabulary setting. In the full open-vocabulary
setting, the absolute improvements in mean average precision are $+4.03\%$ on
ScanNet and $+14.11\%$ on SUN RGB-D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Machine Learning Methods for Distributed Acoustic Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaikai Shi, Qijun Zong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed acoustic sensing (DAS) technology represents an innovative
fiber-optic-based sensing methodology that enables real-time acoustic signal
monitoring through the detection of minute perturbations along optical fibers.
This sensing approach offers compelling advantages, including extensive
measurement ranges, exceptional spatial resolution, and an expansive dynamic
measurement spectrum.
  The integration of machine learning (ML) paradigms presents transformative
potential for DAS technology, encompassing critical domains such as data
augmentation, sophisticated preprocessing techniques, and advanced acoustic
event classification and recognition. By leveraging ML algorithms, DAS systems
can transition from traditional data processing methodologies to more automated
and intelligent analytical frameworks.
  The computational intelligence afforded by ML-enhanced DAS technologies
facilitates unprecedented monitoring capabilities across diverse critical
infrastructure sectors. Particularly noteworthy are the technology's
applications in transportation infrastructure, energy management systems, and
Natural disaster monitoring frameworks, where the precision of data acquisition
and the reliability of intelligent decision-making mechanisms are paramount.
  This research critically examines the comparative performance characteristics
of classical machine learning methodologies and state-of-the-art deep learning
models in the context of DAS data recognition and interpretation, offering
comprehensive insights into the evolving landscape of intelligent sensing
technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Vision</span> as LoRA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Yongjie Ye, Bingru Li, Yuxiang Nie, Jinghui Lu, Jingqun Tang, Yanjie Wang, Can Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Vision as LoRA (VoRA), a novel paradigm for transforming an LLM
into an MLLM. Unlike prevalent MLLM architectures that rely on external vision
modules for vision encoding, VoRA internalizes visual capabilities by
integrating vision-specific LoRA layers directly into the LLM. This design
allows the added parameters to be seamlessly merged into the LLM during
inference, eliminating structural complexity and minimizing computational
overhead. Moreover, inheriting the LLM's ability of handling flexible context,
VoRA can process inputs at arbitrary resolutions.
  To further strengthen VoRA's visual capabilities, we introduce a block-wise
distillation method that transfers visual priors from a pre-trained ViT into
the LoRA layers, effectively accelerating training by injecting visual
knowledge. Additionally, we apply bi-directional attention masks to better
capture the context information of an image. We successfully demonstrate that
with additional pre-training data, VoRA can perform comparably with
conventional encode-based MLLMs. All training data, codes, and model weights
will be released at https://github.com/Hon-Wong/VoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Low-Level <span class="highlight-title">Visual</span> Hallucinations Requires Self-Awareness:
  Database, Model and Training Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinan Sun, Xiongkuo Min, Zicheng Zhang, Yixuan Gao, Yuqin Cao, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multimodal large language models has resulted in
remarkable advancements in visual perception and understanding, consolidating
several tasks into a single visual question-answering framework. However, these
models are prone to hallucinations, which limit their reliability as artificial
intelligence systems. While this issue is extensively researched in natural
language processing and image captioning, there remains a lack of investigation
of hallucinations in Low-level Visual Perception and Understanding (HLPU),
especially in the context of image quality assessment tasks. We consider that
these hallucinations arise from an absence of clear self-awareness within the
models. To address this issue, we first introduce the HLPU instruction
database, the first instruction database specifically focused on hallucinations
in low-level vision tasks. This database contains approximately 200K
question-answer pairs and comprises four subsets, each covering different types
of instructions. Subsequently, we propose the Self-Awareness Failure
Elimination (SAFEQA) model, which utilizes image features, salient region
features and quality features to improve the perception and comprehension
abilities of the model in low-level vision tasks. Furthermore, we propose the
Enhancing Self-Awareness Preference Optimization (ESA-PO) framework to increase
the model's awareness of knowledge boundaries, thereby mitigating the incidence
of hallucination. Finally, we conduct comprehensive experiments on low-level
vision tasks, with the results demonstrating that our proposed method
significantly enhances self-awareness of the model in these tasks and reduces
hallucinations. Notably, our proposed method improves both accuracy and
self-awareness of the proposed model and outperforms close-source models in
terms of various evaluation metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BizGen: Advancing Article-level <span class="highlight-title">Visual</span> Text Rendering for Infographics
  Generation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Peng, Shishi Xiao, Keming Wu, Qisheng Liao, Bohan Chen, Kevin Lin, Danqing Huang, Ji Li, Yuhui Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, state-of-the-art text-to-image generation models, such as Flux and
Ideogram 2.0, have made significant progress in sentence-level visual text
rendering. In this paper, we focus on the more challenging scenarios of
article-level visual text rendering and address a novel task of generating
high-quality business content, including infographics and slides, based on user
provided article-level descriptive prompts and ultra-dense layouts. The
fundamental challenges are twofold: significantly longer context lengths and
the scarcity of high-quality business content data.
  In contrast to most previous works that focus on a limited number of
sub-regions and sentence-level prompts, ensuring precise adherence to
ultra-dense layouts with tens or even hundreds of sub-regions in business
content is far more challenging. We make two key technical contributions: (i)
the construction of scalable, high-quality business content dataset, i.e.,
Infographics-650K, equipped with ultra-dense layouts and prompts by
implementing a layer-wise retrieval-augmented infographic generation scheme;
and (ii) a layout-guided cross attention scheme, which injects tens of
region-wise prompts into a set of cropped region latent space according to the
ultra-dense layouts, and refine each sub-regions flexibly during inference
using a layout conditional CFG.
  We demonstrate the strong results of our system compared to previous SOTA
systems such as Flux and SD3 on our BizEval prompt set. Additionally, we
conduct thorough ablation experiments to verify the effectiveness of each
component. We hope our constructed Infographics-650K and BizEval can encourage
the broader community to advance the progress of business content generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025. Project Page: https://bizgen-msra.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoRad-Lung: A Radiomic-Guided Prompting Autoregressive <span class="highlight-title">Vision</span>-Language
  Model for Lung Nodule Malignancy Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadaf Khademi, Mehran Shabanpour, Reza Taleei, Anastasia Oikonomou, Arash Mohammadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lung cancer remains one of the leading causes of cancer-related mortality
worldwide. A crucial challenge for early diagnosis is differentiating uncertain
cases with similar visual characteristics and closely annotation scores. In
clinical practice, radiologists rely on quantitative, hand-crafted Radiomic
features extracted from Computed Tomography (CT) images, while recent research
has primarily focused on deep learning solutions. More recently,
Vision-Language Models (VLMs), particularly Contrastive Language-Image
Pre-Training (CLIP)-based models, have gained attention for their ability to
integrate textual knowledge into lung cancer diagnosis. While CLIP-Lung models
have shown promising results, we identified the following potential
limitations: (a) dependence on radiologists' annotated attributes, which are
inherently subjective and error-prone, (b) use of textual information only
during training, limiting direct applicability at inference, and (c)
Convolutional-based vision encoder with randomly initialized weights, which
disregards prior knowledge. To address these limitations, we introduce
AutoRad-Lung, which couples an autoregressively pre-trained VLM, with prompts
generated from hand-crafted Radiomics. AutoRad-Lung uses the vision encoder of
the Large-Scale Autoregressive Image Model (AIMv2), pre-trained using a
multi-modal autoregressive objective. Given that lung tumors are typically
small, irregularly shaped, and visually similar to healthy tissue, AutoRad-Lung
offers significant advantages over its CLIP-based counterparts by capturing
pixel-level differences. Additionally, we introduce conditional context
optimization, which dynamically generates context-specific prompts based on
input Radiomics, improving cross-modal alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARMO: Autoregressive Rigging for Multi-Category Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingze Sun, Shiwei Mao, Keyi Chen, Yurun Chen, Shunlin Lu, Jingbo Wang, Junting Dong, Ruqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large-scale generative models have significantly
improved the quality and diversity of 3D shape generation. However, most
existing methods focus primarily on generating static 3D models, overlooking
the potentially dynamic nature of certain shapes, such as humanoids, animals,
and insects. To address this gap, we focus on rigging, a fundamental task in
animation that establishes skeletal structures and skinning for 3D models. In
this paper, we introduce OmniRig, the first large-scale rigging dataset,
comprising 79,499 meshes with detailed skeleton and skinning information.
Unlike traditional benchmarks that rely on predefined standard poses (e.g.,
A-pose, T-pose), our dataset embraces diverse shape categories, styles, and
poses. Leveraging this rich dataset, we propose ARMO, a novel rigging framework
that utilizes an autoregressive model to predict both joint positions and
connectivity relationships in a unified manner. By treating the skeletal
structure as a complete graph and discretizing it into tokens, we encode the
joints using an auto-encoder to obtain a latent embedding and an autoregressive
model to predict the tokens. A mesh-conditioned latent diffusion model is used
to predict the latent embedding for conditional skeleton generation. Our method
addresses the limitations of regression-based approaches, which often suffer
from error accumulation and suboptimal connectivity estimation. Through
extensive experiments on the OmniRig dataset, our approach achieves
state-of-the-art performance in skeleton prediction, demonstrating improved
generalization across diverse object categories. The code and dataset will be
made public for academic use upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AccidentSim: Generating Physically Realistic Vehicle Collision Videos
  from Real-World Accident Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangwen Zhang, Qian Zhang, Longfei Han, Qiang Qu, Xiaoming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting real-world vehicle accident videos for autonomous driving research
is challenging due to their rarity and complexity. While existing driving video
generation methods may produce visually realistic videos, they often fail to
deliver physically realistic simulations because they lack the capability to
generate accurate post-collision trajectories. In this paper, we introduce
AccidentSim, a novel framework that generates physically realistic vehicle
collision videos by extracting and utilizing the physical clues and contextual
information available in real-world vehicle accident reports. Specifically,
AccidentSim leverages a reliable physical simulator to replicate post-collision
vehicle trajectories from the physical and contextual information in the
accident reports and to build a vehicle collision trajectory dataset. This
dataset is then used to fine-tune a language model, enabling it to respond to
user prompts and predict physically consistent post-collision trajectories
across various driving scenarios based on user descriptions. Finally, we employ
Neural Radiance Fields (NeRF) to render high-quality backgrounds, merging them
with the foreground vehicles that exhibit physically realistic trajectories to
generate vehicle collision videos. Experimental results demonstrate that the
videos produced by AccidentSim excel in both visual and physical authenticity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UWarp: A Whole Slide Image Registration Pipeline to Characterize
  Scanner-Induced Local Domain Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Schieb, Bilal Hadjadji, Daniel Tshokola Mweze, Natalia Fernanda Valderrama, Valentin Derangère, Laurent Arnould, Sylvain Ladoire, Alain Lalande, Louis-Oscar Morel, Nathan Vinçon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathology slide digitization introduces scanner-induced domain shift
that can significantly impact computational pathology models based on deep
learning methods. In the state-of-the-art, this shift is often characterized at
a broad scale (slide-level or dataset-level) but not patch-level, which limits
our comprehension of the impact of localized tissue characteristics on the
accuracy of the deep learning models. To address this challenge, we present a
domain shift analysis framework based on UWarp, a novel registration tool
designed to accurately align histological slides scanned under varying
conditions. UWarp employs a hierarchical registration approach, combining
global affine transformations with fine-grained local corrections to achieve
robust tissue patch alignment. We evaluate UWarp using two private datasets,
CypathLung and BosomShieldBreast, containing whole slide images scanned by
multiple devices. Our experiments demonstrate that UWarp outperforms existing
open-source registration methods, achieving a median target registration error
(TRE) of less than 4 pixels (<1 micrometer at 40x magnification) while
significantly reducing computational time. Additionally, we apply UWarp to
characterize scanner-induced local domain shift in the predictions of
Breast-NEOprAIdict, a deep learning model for breast cancer pathological
response prediction. We find that prediction variability is strongly correlated
with tissue density on a given patch. Our findings highlight the importance of
localized domain shift analysis and suggest that UWarp can serve as a valuable
tool for improving model robustness and domain adaptation strategies in
computational pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imitating Radiological Scrolling: A Global-Local Attention Model for 3D
  Chest CT Volumes Multi-Label Anomaly Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in the number of Computed Tomography (CT) scan
examinations has created an urgent need for automated tools, such as organ
segmentation, anomaly classification, and report generation, to assist
radiologists with their growing workload. Multi-label classification of
Three-Dimensional (3D) CT scans is a challenging task due to the volumetric
nature of the data and the variety of anomalies to be detected. Existing deep
learning methods based on Convolutional Neural Networks (CNNs) struggle to
capture long-range dependencies effectively, while Vision Transformers require
extensive pre-training, posing challenges for practical use. Additionally,
these existing methods do not explicitly model the radiologist's navigational
behavior while scrolling through CT scan slices, which requires both global
context understanding and local detail awareness. In this study, we present
CT-Scroll, a novel global-local attention model specifically designed to
emulate the scrolling behavior of radiologists during the analysis of 3D CT
scans. Our approach is evaluated on two public datasets, demonstrating its
efficacy through comprehensive experiments and an ablation study that
highlights the contribution of each model component.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, under review for MIDL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMGen: Unified Multi-modal Image Generation and Understanding in One Go 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiepeng Wang, Zhaoqing Wang, Hao Pan, Yuan Liu, Dongdong Yu, Changhu Wang, Wenping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A unified diffusion framework for multi-modal generation and understanding
has the transformative potential to achieve seamless and controllable image
diffusion and other cross-modal tasks. In this paper, we introduce MMGen, a
unified framework that integrates multiple generative tasks into a single
diffusion model. This includes: (1) multi-modal category-conditioned
generation, where multi-modal outputs are generated simultaneously through a
single inference process, given category information; (2) multi-modal visual
understanding, which accurately predicts depth, surface normals, and
segmentation maps from RGB images; and (3) multi-modal conditioned generation,
which produces corresponding RGB images based on specific modality conditions
and other aligned modalities. Our approach develops a novel diffusion
transformer that flexibly supports multi-modal output, along with a simple
modality-decoupling strategy to unify various tasks. Extensive experiments and
applications demonstrate the effectiveness and superiority of MMGen across
diverse tasks and conditions, highlighting its potential for applications that
require simultaneous generation and understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project page: https://jiepengwang.github.io/MMGen/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Flower Cluster Matching Using The Unscented Transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Chu, Rashik Shrestha, Yu Gu, Jason N. Gross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring flowers over time is essential for precision robotic pollination
in agriculture. To accomplish this, a continuous spatial-temporal observation
of plant growth can be done using stationary RGB-D cameras. However, image
registration becomes a serious challenge due to changes in the visual
appearance of the plant caused by the pollination process and occlusions from
growth and camera angles. Plants flower in a manner that produces distinct
clusters on branches. This paper presents a method for matching flower clusters
using descriptors generated from RGB-D data and considers allowing for spatial
uncertainty within the cluster. The proposed approach leverages the Unscented
Transform to efficiently estimate plant descriptor uncertainty tolerances,
enabling a robust image-registration process despite temporal changes. The
Unscented Transform is used to handle the nonlinear transformations by
propagating the uncertainty of flower positions to determine the variations in
the descriptor domain. A Monte Carlo simulation is used to validate the
Unscented Transform results, confirming our method's effectiveness for flower
cluster matching. Therefore, it can facilitate improved robotics pollination in
dynamic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>*CASE2025 Under Review*</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IAP: Improving Continual Learning of <span class="highlight-title">Vision</span>-Language Models via
  Instance-Aware Prompting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Fu, Hanbin Zhao, Jiahua Dong, Chao Zhang, Hui Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent pre-trained vision-language models (PT-VLMs) often face a Multi-Domain
Class-Incremental Learning (MCIL) scenario in practice, where several classes
and domains of multi-modal tasks are incrementally arrived. Without access to
previously learned tasks and unseen tasks, memory-constrained MCIL suffers from
forward and backward forgetting. To alleviate the above challenges,
parameter-efficient fine-tuning techniques (PEFT), such as prompt tuning, are
employed to adapt the PT-VLM to the diverse incrementally learned tasks. To
achieve effective new task adaptation, existing methods only consider the
effect of PEFT strategy selection, but neglect the influence of PEFT parameter
setting (e.g., prompting). In this paper, we tackle the challenge of optimizing
prompt designs for diverse tasks in MCIL and propose an Instance-Aware
Prompting (IAP) framework. Specifically, our Instance-Aware Gated Prompting
(IA-GP) module enhances adaptation to new tasks while mitigating forgetting by
dynamically assigning prompts across transformer layers at the instance level.
Our Instance-Aware Class-Distribution-Driven Prompting (IA-CDDP) improves the
task adaptation process by determining an accurate task-label-related
confidence score for each instance. Experimental evaluations across 11
datasets, using three performance metrics, demonstrate the effectiveness of our
proposed method. Code can be found at https://github.com/FerdinandZJU/IAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code can be found at https://github.com/FerdinandZJU/IAP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Diffusion</span> Counterfactuals for Image Regressors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung Duc Ha, Sidney Bender
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations have been successfully applied to create human
interpretable explanations for various black-box models. They are handy for
tasks in the image domain, where the quality of the explanations benefits from
recent advances in generative models. Although counterfactual explanations have
been widely applied to classification models, their application to regression
tasks remains underexplored. We present two methods to create counterfactual
explanations for image regression tasks using diffusion-based generative models
to address challenges in sparsity and quality: 1) one based on a Denoising
Diffusion Probabilistic Model that operates directly in pixel-space and 2)
another based on a Diffusion Autoencoder operating in latent space. Both
produce realistic, semantic, and smooth counterfactuals on CelebA-HQ and a
synthetic data set, providing easily interpretable insights into the
decision-making process of the regression model and reveal spurious
correlations. We find that for regression counterfactuals, changes in features
depend on the region of the predicted value. Large semantic changes are needed
for significant changes in predicted values, making it harder to find sparse
counterfactuals than with classifiers. Moreover, pixel space counterfactuals
are more sparse while latent space counterfactuals are of higher quality and
allow bigger semantic changes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 Pages, 5 Figures, Accepted at 3rd World Conference on eXplainable
  Artificial Intelligence (xAI-2025), Code and reproduction instructions
  available on GitHub, see
  https://github.com/DevinTDHa/Diffusion-Counterfactuals-for-Image-Regressors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Robustness of Cortical Morphometry in the presence of white
  matter lesions, using <span class="highlight-title">Diffusion</span> Models for Lesion Filling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinzenz Uhr, Ivan Diaz, Christian Rummel, Richard McKinley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cortical thickness measurements from magnetic resonance imaging, an important
biomarker in many neurodegenerative and neurological disorders, are derived by
many tools from an initial voxel-wise tissue segmentation. White matter (WM)
hypointensities in T1-weighted imaging, such as those arising from multiple
sclerosis or small vessel disease, are known to affect the output of brain
segmentation methods and therefore bias cortical thickness measurements. These
effects are well-documented among traditional brain segmentation tools but have
not been studied extensively in tools based on deep-learning segmentations,
which promise to be more robust. In this paper, we explore the potential of
deep learning to enhance the accuracy and efficiency of cortical thickness
measurement in the presence of WM lesions, using a high-quality lesion filling
algorithm leveraging denoising diffusion networks.
  A pseudo-3D U-Net architecture trained on the OASIS dataset to generate
synthetic healthy tissue, conditioned on binary lesion masks derived from the
MSSEG dataset, allows realistic removal of white matter lesions in multiple
sclerosis patients. By applying morphometry methods to patient images before
and after lesion filling, we analysed robustness of global and regional
cortical thickness measurements in the presence of white matter lesions.
Methods based on a deep learning-based segmentation of the brain (Fastsurfer,
DL+DiReCT, ANTsPyNet) exhibited greater robustness than those using classical
segmentation methods (Freesurfer, ANTs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TerraTorch: The Geospatial Foundation Models Toolkit <span class="chip">RSS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Gomes, Benedikt Blumenstiel, Joao Lucas de Sousa Almeida, Pedro Henrique de Oliveira, Paolo Fraccaro, Francesc Marti Escofet, Daniela Szwarcman, Naomi Simumba, Romeo Kienzler, Bianca Zadrozny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TerraTorch is a fine-tuning and benchmarking toolkit for Geospatial
Foundation Models built on PyTorch Lightning and tailored for satellite,
weather, and climate data. It integrates domain-specific data modules,
pre-defined tasks, and a modular model factory that pairs any backbone with
diverse decoder heads. These components allow researchers and practitioners to
fine-tune supported models in a no-code fashion by simply editing a training
configuration. By consolidating best practices for model development and
incorporating the automated hyperparameter optimization extension Iterate,
TerraTorch reduces the expertise and time required to fine-tune or benchmark
models on new Earth Observation use cases. Furthermore, TerraTorch directly
integrates with GEO-Bench, allowing for systematic and reproducible
benchmarking of Geospatial Foundation Models. TerraTorch is open sourced under
Apache 2.0, available at https://github.com/IBM/terratorch, and can be
installed via pip install terratorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IGARSS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Intermediate States: Explaining <span class="highlight-title">Visual</span> Redundancy through
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingchen Yang, Bowen Cao, Anran Zhang, Weibo Gu, Winston Hu, Guang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Langue Models (MLLMs) often process thousands of visual
tokens, which consume a significant portion of the context window and impose a
substantial computational burden. Prior work has empirically explored visual
token pruning methods based on MLLMs' intermediate states (e.g., attention
scores). However, they have limitations in precisely defining visual redundancy
due to their inability to capture the influence of visual tokens on MLLMs'
visual understanding (i.e., the predicted probabilities for textual token
candidates). To address this issue, we manipulate the visual input and
investigate variations in the textual output from both token-centric and
context-centric perspectives, achieving intuitive and comprehensive analysis.
Experimental results reveal that visual tokens with low ViT-[cls] association
and low text-to-image attention scores can contain recognizable information and
significantly contribute to images' overall information. To develop a more
reliable method for identifying and pruning redundant visual tokens, we
integrate these two perspectives and introduce a context-independent condition
to identify redundant prototypes from training images, which probes the
redundancy of each visual token during inference. Extensive experiments on
single-image, multi-image and video comprehension tasks demonstrate the
effectiveness of our method, notably achieving 90% to 110% of the performance
while pruning 80% to 90% of visual tokens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TD-BFR: Truncated <span class="highlight-title">Diffusion</span> Model for Efficient Blind Face Restoration <span class="chip">ICME 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziying Zhang, Xiang Gao, Zhixin Wang, Qiang hu, Xiaoyun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based methodologies have shown significant potential in blind face
restoration (BFR), leveraging their robust generative capabilities. However,
they are often criticized for two significant problems: 1) slow training and
inference speed, and 2) inadequate recovery of fine-grained facial details. To
address these problems, we propose a novel Truncated Diffusion model for
efficient Blind Face Restoration (TD-BFR), a three-stage paradigm tailored for
the progressive resolution of degraded images. Specifically, TD-BFR utilizes an
innovative truncated sampling method, starting from low-quality (LQ) images at
low resolution to enhance sampling speed, and then introduces an adaptive
degradation removal module to handle unknown degradations and connect the
generation processes across different resolutions. Additionally, we further
adapt the priors of pre-trained diffusion models to recover rich facial
details. Our method efficiently restores high-quality images in a
coarse-to-fine manner and experimental results demonstrate that TD-BFR is, on
average, \textbf{4.75$\times$} faster than current state-of-the-art
diffusion-based BFR methods while maintaining competitive quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, Gianluca Corrado
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models offer a scalable and flexible paradigm for simulating
complex environments, yet current approaches fall short in addressing the
domain-specific requirements of autonomous driving - such as multi-agent
interactions, fine-grained control, and multi-camera consistency. We introduce
GAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies
these capabilities within a single generative framework. GAIA-2 supports
controllable video generation conditioned on a rich set of structured inputs:
ego-vehicle dynamics, agent configurations, environmental factors, and road
semantics. It generates high-resolution, spatiotemporally consistent
multi-camera videos across geographically diverse driving environments (UK, US,
Germany). The model integrates both structured conditioning and external latent
embeddings (e.g., from a proprietary driving model) to facilitate flexible and
semantically grounded scene synthesis. Through this integration, GAIA-2 enables
scalable simulation of both common and rare driving scenarios, advancing the
use of generative world models as a core tool in the development of autonomous
systems. Videos are available at https://wayve.ai/thinking/gaia-2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D
  Generation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinnan Chen, Lingting Zhu, Zeyu Hu, Shengju Qian, Yugang Chen, Xin Wang, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in auto-regressive transformers have revolutionized
generative modeling across different domains, from language processing to
visual generation, demonstrating remarkable capabilities. However, applying
these advances to 3D generation presents three key challenges: the unordered
nature of 3D data conflicts with sequential next-token prediction paradigm,
conventional vector quantization approaches incur substantial compression loss
when applied to 3D meshes, and the lack of efficient scaling strategies for
higher resolution latent prediction. To address these challenges, we introduce
MAR-3D, which integrates a pyramid variational autoencoder with a cascaded
masked auto-regressive transformer (Cascaded MAR) for progressive latent
upscaling in the continuous space. Our architecture employs random masking
during training and auto-regressive denoising in random order during inference,
naturally accommodating the unordered property of 3D latent tokens.
Additionally, we propose a cascaded training strategy with condition
augmentation that enables efficiently up-scale the latent token resolution with
fast convergence. Extensive experiments demonstrate that MAR-3D not only
achieves superior performance and generalization capabilities compared to
existing methods but also exhibits enhanced scaling capabilities compared to
joint distribution modeling approaches (e.g., diffusion transformers).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Aceepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Object Detection: A Comprehensive <span class="highlight-title">Survey</span> on Challenges, Techniques
  and Real-World Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahya Nikouei, Bita Baroutian, Shahabedin Nabavi, Fateme Taraghi, Atefe Aghaei, Ayoob Sajedi, Mohsen Ebrahimi Moghaddam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small object detection (SOD) is a critical yet challenging task in computer
vision, with applications like spanning surveillance, autonomous systems,
medical imaging, and remote sensing. Unlike larger objects, small objects
contain limited spatial and contextual information, making accurate detection
difficult. Challenges such as low resolution, occlusion, background
interference, and class imbalance further complicate the problem. This survey
provides a comprehensive review of recent advancements in SOD using deep
learning, focusing on articles published in Q1 journals during 2024-2025. We
analyzed challenges, state-of-the-art techniques, datasets, evaluation metrics,
and real-world applications. Recent advancements in deep learning have
introduced innovative solutions, including multi-scale feature extraction,
Super-Resolution (SR) techniques, attention mechanisms, and transformer-based
architectures. Additionally, improvements in data augmentation, synthetic data
generation, and transfer learning have addressed data scarcity and domain
adaptation issues. Furthermore, emerging trends such as lightweight neural
networks, knowledge distillation (KD), and self-supervised learning offer
promising directions for improving detection efficiency, particularly in
resource-constrained environments like Unmanned Aerial Vehicles (UAV)-based
surveillance and edge computing. We also review widely used datasets, along
with standard evaluation metrics such as mean Average Precision (mAP) and
size-specific AP scores. The survey highlights real-world applications,
including traffic monitoring, maritime surveillance, industrial defect
detection, and precision agriculture. Finally, we discuss open research
challenges and future directions, emphasizing the need for robust domain
adaptation techniques, better feature fusion strategies, and real-time
performance optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Vision</span>-Amplified Semantic Entropy for Hallucination Detection in Medical
  <span class="highlight-title">Visual</span> Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehui Liao, Shishuai Hu, Ke Zou, Huazhu Fu, Liangli Zhen, Yong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated significant
potential in medical Visual Question Answering (VQA). Yet, they remain prone to
hallucinations-incorrect responses that contradict input images, posing
substantial risks in clinical decision-making. Detecting these hallucinations
is essential for establishing trust in MLLMs among clinicians and patients,
thereby enabling their real-world adoption. Current hallucination detection
methods, especially semantic entropy (SE), have demonstrated promising
hallucination detection capacity for LLMs. However, adapting SE to medical
MLLMs by incorporating visual perturbations presents a dilemma. Weak
perturbations preserve image content and ensure clinical validity, but may be
overlooked by medical MLLMs, which tend to over rely on language priors. In
contrast, strong perturbations can distort essential diagnostic features,
compromising clinical interpretation. To address this issue, we propose Vision
Amplified Semantic Entropy (VASE), which incorporates weak image
transformations and amplifies the impact of visual input, to improve
hallucination detection in medical VQA. We first estimate the semantic
predictive distribution under weak visual transformations to preserve clinical
validity, and then amplify visual influence by contrasting this distribution
with that derived from a distorted image. The entropy of the resulting
distribution is estimated as VASE. Experiments on two medical open-ended VQA
datasets demonstrate that VASE consistently outperforms existing hallucination
detection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLLM-Selector: Necessity and Diversity-driven High-Value Data Selection
  for Enhanced <span class="highlight-title">Visual</span> Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Jiayi Ji, Jie Lou, Debing Zhang, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual instruction tuning (VIT) has emerged as a crucial technique for
enabling multi-modal large language models (MLLMs) to follow user instructions
adeptly. Yet, a significant gap persists in understanding the attributes of
high-quality instruction tuning data and frameworks for its automated
selection. To address this, we introduce MLLM-Selector, an automated approach
that identifies valuable data for VIT by weighing necessity and diversity. Our
process starts by randomly sampling a subset from the VIT data pool to
fine-tune a pretrained model, thus creating a seed model with an initial
ability to follow instructions. Then, leveraging the seed model, we calculate
necessity scores for each sample in the VIT data pool to identify samples
pivotal for enhancing model performance. Our findings underscore the importance
of mixing necessity and diversity in data choice, leading to the creation of
MLLM-Selector, our methodology that fuses necessity scoring with strategic
sampling for superior data refinement. Empirical results indicate that within
identical experimental conditions, MLLM-Selector surpasses LLaVA-1.5 in some
benchmarks with less than 1% of the data and consistently exceeds performance
across all validated benchmarks when using less than 50%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient and General-Purpose Few-Shot Misclassification
  Detection for <span class="highlight-title">Vision</span>-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanhu Zeng, Zhen Cheng, Fei Zhu, Xu-Yao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable prediction by classifiers is crucial for their deployment in high
security and dynamically changing situations. However, modern neural networks
often exhibit overconfidence for misclassified predictions, highlighting the
need for confidence estimation to detect errors. Despite the achievements
obtained by existing methods on small-scale datasets, they all require training
from scratch and there are no efficient and effective misclassification
detection (MisD) methods, hindering practical application towards large-scale
and ever-changing datasets. In this paper, we pave the way to exploit vision
language model (VLM) leveraging text information to establish an efficient and
general-purpose misclassification detection framework. By harnessing the power
of VLM, we construct FSMisD, a Few-Shot prompt learning framework for MisD to
refrain from training from scratch and therefore improve tuning efficiency. To
enhance misclassification detection ability, we use adaptive pseudo sample
generation and a novel negative loss to mitigate the issue of overconfidence by
pushing category prompts away from pseudo features. We conduct comprehensive
experiments with prompt learning methods and validate the generalization
ability across various datasets with domain shift. Significant and consistent
improvement demonstrates the effectiveness, efficiency and generalizability of
our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VPO: Aligning Text-to-Video Generation Models with Prompt Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiale Cheng, Ruiliang Lyu, Xiaotao Gu, Xiao Liu, Jiazheng Xu, Yida Lu, Jiayan Teng, Zhuoyi Yang, Yuxiao Dong, Jie Tang, Hongning Wang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video generation models have achieved remarkable progress in text-to-video
tasks. These models are typically trained on text-video pairs with highly
detailed and carefully crafted descriptions, while real-world user inputs
during inference are often concise, vague, or poorly structured. This gap makes
prompt optimization crucial for generating high-quality videos. Current methods
often rely on large language models (LLMs) to refine prompts through in-context
learning, but suffer from several limitations: they may distort user intent,
omit critical details, or introduce safety risks. Moreover, they optimize
prompts without considering the impact on the final video quality, which can
lead to suboptimal results. To address these issues, we introduce VPO, a
principled framework that optimizes prompts based on three core principles:
harmlessness, accuracy, and helpfulness. The generated prompts faithfully
preserve user intents and, more importantly, enhance the safety and quality of
generated videos. To achieve this, VPO employs a two-stage optimization
approach. First, we construct and refine a supervised fine-tuning (SFT) dataset
based on principles of safety and alignment. Second, we introduce both
text-level and video-level feedback to further optimize the SFT model with
preference learning. Our extensive experiments demonstrate that VPO
significantly improves safety, alignment, and video quality compared to
baseline methods. Moreover, VPO shows strong generalization across video
generation models. Furthermore, we demonstrate that VPO could outperform and be
combined with RLHF methods on video generation models, underscoring the
effectiveness of VPO in aligning video generation models. Our code and data are
publicly available at https://github.com/thu-coai/VPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning Guided Latent <span class="highlight-title">Diffusion</span> Model for Image-to-Image
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Si, Bo Wang, Zhao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diffusion model has demonstrated superior performance in synthesizing
diverse and high-quality images for text-guided image translation. However,
there remains room for improvement in both the formulation of text prompts and
the preservation of reference image content. First, variations in target text
prompts can significantly influence the quality of the generated images, and it
is often challenging for users to craft an optimal prompt that fully captures
the content of the input image. Second, while existing models can introduce
desired modifications to specific regions of the reference image, they
frequently induce unintended alterations in areas that should remain unchanged.
To address these challenges, we propose pix2pix-zeroCon, a zero-shot
diffusion-based method that eliminates the need for additional training by
leveraging patch-wise contrastive loss. Specifically, we automatically
determine the editing direction in the text embedding space based on the
reference image and target prompts. Furthermore, to ensure precise content and
structural preservation in the edited image, we introduce cross-attention
guiding loss and patch-wise contrastive loss between the generated and original
image embeddings within a pre-trained diffusion model. Notably, our approach
requires no additional training and operates directly on a pre-trained
text-to-image diffusion model. Extensive experiments demonstrate that our
method surpasses existing models in image-to-image translation, achieving
enhanced fidelity and controllability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dissecting and Mitigating <span class="highlight-title">Diffusion</span> Bias via Mechanistic
  Interpretability <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingdong Shi, Changming Li, Yifan Wang, Yongxiang Zhao, Anqi Pang, Sibei Yang, Jingyi Yu, Kan Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated impressive capabilities in synthesizing
diverse content. However, despite their high-quality outputs, these models
often perpetuate social biases, including those related to gender and race.
These biases can potentially contribute to harmful real-world consequences,
reinforcing stereotypes and exacerbating inequalities in various social
contexts. While existing research on diffusion bias mitigation has
predominantly focused on guiding content generation, it often neglects the
intrinsic mechanisms within diffusion models that causally drive biased
outputs. In this paper, we investigate the internal processes of diffusion
models, identifying specific decision-making mechanisms, termed bias features,
embedded within the model architecture. By directly manipulating these
features, our method precisely isolates and adjusts the elements responsible
for bias generation, permitting granular control over the bias levels in the
generated content. Through experiments on both unconditional and conditional
diffusion models across various social bias attributes, we demonstrate our
method's efficacy in managing generation distribution while preserving image
quality. We also dissect the discovered model mechanism, revealing different
intrinsic features controlling fine-grained aspects of generation, boosting
further research on mechanistic interpretability of diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025; Project Page:
  https://foundation-model-research.github.io/difflens</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Trial to Triumph: Advancing Long Video Understanding via <span class="highlight-title">Visual</span>
  Context Sample Scaling and Self-reward Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Suo, Fan Ma, Linchao Zhu, Tianyi Wang, Fengyun Rao, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large language models (MLLMs) show remarkable ability in video
understanding. Nevertheless, understanding long videos remains challenging as
the models can only process a finite number of frames in a single inference,
potentially omitting crucial visual information. To address the challenge, we
propose generating multiple predictions through visual context sampling,
followed by a scoring mechanism to select the final prediction. Specifically,
we devise a bin-wise sampling strategy that enables MLLMs to generate diverse
answers based on various combinations of keyframes, thereby enriching the
visual context. To determine the final prediction from the sampled answers, we
employ a self-reward by linearly combining three scores: (1) a frequency score
indicating the prevalence of each option, (2) a marginal confidence score
reflecting the inter-intra sample certainty of MLLM predictions, and (3) a
reasoning score for different question types, including clue-guided answering
for global questions and temporal self-refocusing for local questions. The
frequency score ensures robustness through majority correctness, the
confidence-aligned score reflects prediction certainty, and the typed-reasoning
score addresses cases with sparse key visual information using tailored
strategies. Experiments show that this approach covers the correct answer for a
high percentage of long video questions, on seven datasets show that our method
improves the performance of three MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lipschitz Constant Meets Condition Number: Learning Robust and Compact
  Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangqi Feng, Shing-Ho J. Lin, Baoyuan Gao, Xian Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has revealed that high compression of Deep Neural Networks
(DNNs), e.g., massive pruning of the weight matrix of a DNN, leads to a severe
drop in accuracy and susceptibility to adversarial attacks. Integration of
network pruning into an adversarial training framework has been proposed to
promote adversarial robustness. It has been observed that a highly pruned
weight matrix tends to be ill-conditioned, i.e., increasing the condition
number of the weight matrix. This phenomenon aggravates the vulnerability of a
DNN to input noise. Although a highly pruned weight matrix is considered to be
able to lower the upper bound of the local Lipschitz constant to tolerate large
distortion, the ill-conditionedness of such a weight matrix results in a
non-robust DNN model. To overcome this challenge, this work develops novel
joint constraints to adjust the weight distribution of networks, namely, the
Transformed Sparse Constraint joint with Condition Number Constraint (TSCNC),
which copes with smoothing distribution and differentiable constraint functions
to reduce condition number and thus avoid the ill-conditionedness of weight
matrices. Furthermore, our theoretical analyses unveil the relevance between
the condition number and the local Lipschitz constant of the weight matrix,
namely, the sharply increasing condition number becomes the dominant factor
that restricts the robustness of over-sparsified models. Extensive experiments
are conducted on several public datasets, and the results show that the
proposed constraints significantly improve the robustness of a DNN with high
pruning rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention Xception UNet (AXUNet): A Novel Combination of CNN and
  Self-Attention for Brain Tumor Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farzan Moodi, Fereshteh Khodadadi Shoushtari, Gelareh Valizadeh, Dornaz Mazinani, Hanieh Mobarak Salari, Hamidreza Saligheh Rad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of glioma brain tumors is crucial for diagnosis and
treatment planning. Deep learning techniques offer promising solutions, but
optimal model architectures remain under investigation. We used the BraTS 2021
dataset, selecting T1 with contrast enhancement (T1CE), T2, and
Fluid-Attenuated Inversion Recovery (FLAIR) sequences for model development.
The proposed Attention Xception UNet (AXUNet) architecture integrates an
Xception backbone with dot-product self-attention modules, inspired by
state-of-the-art (SOTA) large language models such as Google Bard and OpenAI
ChatGPT, within a UNet-shaped model. We compared AXUNet with SOTA models.
Comparative evaluation on the test set demonstrated improved results over
baseline models. Inception-UNet and Xception-UNet achieved mean Dice scores of
90.88 and 93.24, respectively. Attention ResUNet (AResUNet) attained a mean
Dice score of 92.80, with the highest score of 84.92 for enhancing tumor (ET)
among all models. Attention Gate UNet (AGUNet) yielded a mean Dice score of
90.38. AXUNet outperformed all models with a mean Dice score of 93.73. It
demonstrated superior Dice scores across whole tumor (WT) and tumor core (TC)
regions, achieving 92.59 for WT, 86.81 for TC, and 84.89 for ET. The
integration of the Xception backbone and dot-product self-attention mechanisms
in AXUNet showcases enhanced performance in capturing spatial and contextual
information. The findings underscore the potential utility of AXUNet in
facilitating precise tumor delineation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Siformer: Feature-isolated Transformer for Efficient Skeleton-based Sign
  Language Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muxin Pu, Mei Kuan Lim, Chun Yong Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign language recognition (SLR) refers to interpreting sign language glosses
from given videos automatically. This research area presents a complex
challenge in computer vision because of the rapid and intricate movements
inherent in sign languages, which encompass hand gestures, body postures, and
even facial expressions. Recently, skeleton-based action recognition has
attracted increasing attention due to its ability to handle variations in
subjects and backgrounds independently. However, current skeleton-based SLR
methods exhibit three limitations: 1) they often neglect the importance of
realistic hand poses, where most studies train SLR models on non-realistic
skeletal representations; 2) they tend to assume complete data availability in
both training or inference phases, and capture intricate relationships among
different body parts collectively; 3) these methods treat all sign glosses
uniformly, failing to account for differences in complexity levels regarding
skeletal representations. To enhance the realism of hand skeletal
representations, we present a kinematic hand pose rectification method for
enforcing constraints. Mitigating the impact of missing data, we propose a
feature-isolated mechanism to focus on capturing local spatial-temporal
context. This method captures the context concurrently and independently from
individual features, thus enhancing the robustness of the SLR model.
Additionally, to adapt to varying complexity levels of sign glosses, we develop
an input-adaptive inference approach to optimise computational efficiency and
accuracy. Experimental results demonstrate the effectiveness of our approach,
as evidenced by achieving a new state-of-the-art (SOTA) performance on WLASL100
and LSA64. For WLASL100, we achieve a top-1 accuracy of 86.50\%, marking a
relative improvement of 2.39% over the previous SOTA. For LSA64, we achieve a
top-1 accuracy of 99.84%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, ACM Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Beam <span class="highlight-title">Diffusion</span> Models for Decoding Image Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme Fernandes, Vasco Ramos, Regev Cohen, Idan Szpektor, João Magalhães
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While diffusion models excel at generating high-quality images from text
prompts, they struggle with visual consistency in image sequences. Existing
methods generate each image independently, leading to disjointed narratives - a
challenge further exacerbated in non-linear storytelling, where scenes must
connect beyond adjacent frames. We introduce a novel beam search strategy for
latent space exploration, enabling conditional generation of full image
sequences with beam search decoding. Unlike prior approaches that use fixed
latent priors, our method dynamically searches for an optimal sequence of
latent representations, ensuring coherent visual transitions. To address beam
search's quadratic complexity, we integrate a cross-attention mechanism that
efficiently scores search paths and enables pruning, prioritizing alignment
with both textual prompts and visual context. Human evaluations confirm that
our approach outperforms baseline methods, producing full sequences with
superior coherence, visual continuity, and textual alignment. By bridging
advances in search optimization and latent space refinement, this work sets a
new standard for structured image sequence generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Facial Expression Recognition <span class="highlight-title">Dataset</span>s for Deep Learning: A
  Benchmark Study with Novel Similarity Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        F. Xavier Gaya-Morey, Cristina Manresa-Yee, Célia Martinie, Jose M. Buades-Rubio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the key characteristics and suitability of widely
used Facial Expression Recognition (FER) datasets for training deep learning
models. In the field of affective computing, FER is essential for interpreting
human emotions, yet the performance of FER systems is highly contingent on the
quality and diversity of the underlying datasets. To address this issue, we
compiled and analyzed 24 FER datasets, including those targeting specific age
groups such as children, adults, and the elderly, and processed them through a
comprehensive normalization pipeline. In addition, we enriched the datasets
with automatic annotations for age and gender, enabling a more nuanced
evaluation of their demographic properties. To further assess dataset efficacy,
we introduce three novel metricsLocal, Global, and Paired Similarity, which
quantitatively measure dataset difficulty, generalization capability, and
cross-dataset transferability. Benchmark experiments using state-of-the-art
neural networks reveal that large-scale, automatically collected datasets
(e.g., AffectNet, FER2013) tend to generalize better, despite issues with
labeling noise and demographic biases, whereas controlled datasets offer higher
annotation quality but limited variability. Our findings provide actionable
recommendations for dataset selection and design, advancing the development of
more robust, fair, and effective FER systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cherry Yield Forecast: Harvest Prediction for Individual Sweet Cherry
  Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Gilson, Peter Pietrzyk, Chiara Paglia, Annika Killer, Fabian Keil, Lukas Meyer, Dominikus Kittemann, Patrick Noack, Oliver Scholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is part of a publication series from the For5G project that has
the goal of creating digital twins of sweet cherry trees. At the beginning a
brief overview of the revious work in this project is provided. Afterwards the
focus shifts to a crucial problem in the fruit farming domain: the difficulty
of making reliable yield predictions early in the season. Following three Satin
sweet cherry trees along the year 2023 enabled the collection of accurate
ground truth data about the development of cherries from dormancy until
harvest. The methodology used to collect this data is presented, along with its
valuation and visualization. The predictive power of counting objects at all
relevant vegetative stages of the fruit development cycle in cherry trees with
regards to yield predictions is investigated. It is found that all investigated
fruit states are suitable for yield predictions based on linear regression.
Conceptionally, there is a trade-off between earliness and external events with
the potential to invalidate the prediction. Considering this, two optimal
timepoints are suggested that are opening cluster stage before the start of the
flowering and the early fruit stage right after the second fruit drop. However,
both timepoints are challenging to solve with automated procedures based on
image data. Counting developing cherries based on images is exceptionally
difficult due to the small fruit size and their tendency to be occluded by
leaves. It was not possible to obtain satisfying results relying on a
state-of-the-art fruit-counting method. Counting the elements within a bursting
bud is also challenging, even when using high resolution cameras. It is
concluded that accurate yield prediction for sweet cherry trees is possible
when objects are manually counted and that automated features extraction with
similar accuracy remains an open problem yet to be solved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ITA-MDT: Image-Timestep-Adaptive Masked <span class="highlight-title">Diffusion</span> Transformer Framework
  for Image-Based Virtual Try-On <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Woo Hong, Tri Ton, Trung X. Pham, Gwanhyeong Koo, Sunjae Yoon, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces ITA-MDT, the Image-Timestep-Adaptive Masked Diffusion
Transformer Framework for Image-Based Virtual Try-On (IVTON), designed to
overcome the limitations of previous approaches by leveraging the Masked
Diffusion Transformer (MDT) for improved handling of both global garment
context and fine-grained details. The IVTON task involves seamlessly
superimposing a garment from one image onto a person in another, creating a
realistic depiction of the person wearing the specified garment. Unlike
conventional diffusion-based virtual try-on models that depend on large
pre-trained U-Net architectures, ITA-MDT leverages a lightweight, scalable
transformer-based denoising diffusion model with a mask latent modeling scheme,
achieving competitive results while reducing computational overhead. A key
component of ITA-MDT is the Image-Timestep Adaptive Feature Aggregator (ITAFA),
a dynamic feature aggregator that combines all of the features from the image
encoder into a unified feature of the same size, guided by diffusion timestep
and garment image complexity. This enables adaptive weighting of features,
allowing the model to emphasize either global information or fine-grained
details based on the requirements of the denoising stage. Additionally, the
Salient Region Extractor (SRE) module is presented to identify complex region
of the garment to provide high-resolution local information to the denoising
model as an additional condition alongside the global information of the full
garment image. This targeted conditioning strategy enhances detail preservation
of fine details in highly salient garment regions, optimizing computational
resources by avoiding unnecessarily processing entire garment image.
Comparative evaluations confirms that ITA-MDT improves efficiency while
maintaining strong performance, reaching state-of-the-art results in several
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025, Project Page: https://jiwoohong93.github.io/ita-mdt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RSRWKV: A Linear-Complexity 2D Attention Mechanism for Efficient Remote
  Sensing <span class="highlight-title">Vision</span> Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunshan Li, Rong Wang, Xiaofei Yang, Dianhui Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution remote sensing analysis faces challenges in global context
modeling due to scene complexity and scale diversity. While CNNs excel at local
feature extraction via parameter sharing, their fixed receptive fields
fundamentally restrict long-range dependency modeling. Vision Transformers
(ViTs) effectively capture global semantic relationships through self-attention
mechanisms but suffer from quadratic computational complexity relative to image
resolution, creating critical efficiency bottlenecks for high-resolution
imagery. The RWKV model's linear-complexity sequence modeling achieves
breakthroughs in NLP but exhibits anisotropic limitations in vision tasks due
to its 1D scanning mechanism. To address these challenges, we propose RSRWKV,
featuring a novel 2D-WKV scanning mechanism that bridges sequential processing
and 2D spatial reasoning while maintaining linear complexity. This enables
isotropic context aggregation across multiple directions. The MVC-Shift module
enhances multi-scale receptive field coverage, while the ECA module strengthens
cross-channel feature interaction and semantic saliency modeling. Experimental
results demonstrate RSRWKV's superior performance over CNN and Transformer
baselines in classification, detection, and segmentation tasks on NWPU
RESISC45, VHR-10.v2, and GLH-Water datasets, offering a scalable solution for
high-resolution remote sensing analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pluggable Style Representation Learning for Multi-Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongda Liu, Longguang Wang, Weijun Guan, Ye Zhang, Yulan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the high diversity of image styles, the scalability to various styles
plays a critical role in real-world applications. To accommodate a large amount
of styles, previous multi-style transfer approaches rely on enlarging the model
size while arbitrary-style transfer methods utilize heavy backbones. However,
the additional computational cost introduced by more model parameters hinders
these methods to be deployed on resource-limited devices. To address this
challenge, in this paper, we develop a style transfer framework by decoupling
the style modeling and transferring. Specifically, for style modeling, we
propose a style representation learning scheme to encode the style information
into a compact representation. Then, for style transferring, we develop a
style-aware multi-style transfer network (SaMST) to adapt to diverse styles
using pluggable style representations. In this way, our framework is able to
accommodate diverse image styles in the learned style representations without
introducing additional overhead during inference, thereby maintaining
efficiency. Experiments show that our style representation can extract accurate
style information. Moreover, qualitative and quantitative results demonstrate
that our method achieves state-of-the-art performance in terms of both accuracy
and efficiency. The codes are available in
https://github.com/The-Learning-And-Vision-Atelier-LAVA/SaMST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 13 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-ReS: Self-Reflection in Large <span class="highlight-title">Vision</span>-Language Models for Long Video
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Pereira, Vasco Lopes, David Semedo, Joao Neves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) demonstrate remarkable performance in
short-video tasks such as video question answering, but struggle in long-video
understanding. The linear frame sampling strategy, conventionally used by
LVLMs, fails to account for the non-linear distribution of key events in video
data, often introducing redundant or irrelevant information in longer contexts
while risking the omission of critical events in shorter ones. To address this,
we propose SelfReS, a non-linear spatiotemporal self-reflective sampling method
that dynamically selects key video fragments based on user prompts. Unlike
prior approaches, SelfReS leverages the inherently sparse attention maps of
LVLMs to define reflection tokens, enabling relevance-aware token selection
without requiring additional training or external modules. Experiments
demonstrate that SelfReS can be seamlessly integrated into strong base LVLMs,
improving long-video task accuracy and achieving up to 46% faster inference
speed within the same GPU memory budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic
  Activation Sparsity <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Ma, Jiaqi Tang, Bin Guo, Fan Dang, Sicong Liu, Zhui Zhu, Lei Wu, Cheng Fang, Ying-Cong Chen, Zhiwen Yu, Yunhao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the growing integration of deep models into mobile terminals, the
accuracy of these models declines significantly due to various deployment
interferences. Test-time adaptation (TTA) has emerged to improve the
performance of deep models by adapting them to unlabeled target data online.
Yet, the significant memory cost, particularly in resource-constrained
terminals, impedes the effective deployment of most backward-propagation-based
TTA methods. To tackle memory constraints, we introduce SURGEON, a method that
substantially reduces memory cost while preserving comparable accuracy
improvements during fully test-time adaptation (FTTA) without relying on
specific network architectures or modifications to the original training
procedure. Specifically, we propose a novel dynamic activation sparsity
strategy that directly prunes activations at layer-specific dynamic ratios
during adaptation, allowing for flexible control of learning ability and memory
cost in a data-sensitive manner. Among this, two metrics, Gradient Importance
and Layer Activation Memory, are considered to determine the layer-wise pruning
ratios, reflecting accuracy contribution and memory efficiency, respectively.
Experimentally, our method surpasses the baselines by not only reducing memory
usage but also achieving superior accuracy, delivering SOTA performance across
diverse datasets, architectures, and tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistency Trajectory Matching for One-Step Generative Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyi You, Mingyang Zhang, Leheng Zhang, Kexuan Shi, Xingyu Zhou, Shuhang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current diffusion-based super-resolution (SR) approaches achieve commendable
performance at the cost of high inference overhead. Therefore, distillation
techniques are utilized to accelerate the multi-step teacher model into
one-step student model. Nevertheless, these methods significantly raise
training costs and constrain the performance of the student model by the
teacher model. To overcome these tough challenges, we propose Consistency
Trajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy
that is able to generate photo-realistic SR results in one step. Concretely, we
first formulate a Probability Flow Ordinary Differential Equation (PF-ODE)
trajectory to establish a deterministic mapping from low-resolution (LR) images
with noise to high-resolution (HR) images. Then we apply the Consistency
Training (CT) strategy to directly learn the mapping in one step, eliminating
the necessity of pre-trained diffusion model. To further enhance the
performance and better leverage the ground-truth during the training process,
we aim to align the distribution of SR results more closely with that of the
natural images. To this end, we propose to minimize the discrepancy between
their respective PF-ODE trajectories from the LR image distribution by our
meticulously designed Distribution Trajectory Matching (DTM) loss, resulting in
improved realism of our recovered HR images. Comprehensive experimental results
demonstrate that the proposed methods can attain comparable or even superior
capabilities on both synthetic and real datasets while maintaining minimal
inference latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoGEM: Training-free Action Grounding in Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Vogel, Walid Bousselham, Anna Kukleva, Nina Shvetsova, Hilde Kuehne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language foundation models have shown impressive capabilities across
various zero-shot tasks, including training-free localization and grounding,
primarily focusing on localizing objects in images. However, leveraging those
capabilities to localize actions and events in videos is challenging, as
actions have less physical outline and are usually described by higher-level
concepts. In this work, we propose VideoGEM, the first training-free spatial
action grounding method based on pretrained image- and video-language
backbones. Namely, we adapt the self-self attention formulation of GEM to
spatial activity grounding. We observe that high-level semantic concepts, such
as actions, usually emerge in the higher layers of the image- and
video-language models. We, therefore, propose a layer weighting in the
self-attention path to prioritize higher layers. Additionally, we introduce a
dynamic weighting method to automatically tune layer weights to capture each
layer`s relevance to a specific prompt. Finally, we introduce a prompt
decomposition, processing action, verb, and object prompts separately,
resulting in a better spatial localization of actions. We evaluate the proposed
approach on three image- and video-language backbones, CLIP, OpenCLIP, and
ViCLIP, and on four video grounding datasets, V-HICO, DALY,
YouCook-Interactions, and GroundingYouTube, showing that the proposed
training-free approach is able to outperform current trained state-of-the-art
approaches for spatial video grounding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Focused Transformer for Single Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Long, Xingyu Zhou, Leheng Zhang, Shuhang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based methods have achieved remarkable results in image
super-resolution tasks because they can capture non-local dependencies in
low-quality input images. However, this feature-intensive modeling approach is
computationally expensive because it calculates the similarities between
numerous features that are irrelevant to the query features when obtaining
attention weights. These unnecessary similarity calculations not only degrade
the reconstruction performance but also introduce significant computational
overhead. How to accurately identify the features that are important to the
current query features and avoid similarity calculations between irrelevant
features remains an urgent problem. To address this issue, we propose a novel
and effective Progressive Focused Transformer (PFT) that links all isolated
attention maps in the network through Progressive Focused Attention (PFA) to
focus attention on the most important tokens. PFA not only enables the network
to capture more critical similar features, but also significantly reduces the
computational cost of the overall network by filtering out irrelevant features
before calculating similarities. Extensive experiments demonstrate the
effectiveness of the proposed method, achieving state-of-the-art performance on
various single image super-resolution benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Euclidean Distance to Convex Polyhedra and Application to Class
  Representation in Spectral Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Bottenmuller, Florent Magaud, Arnaud Demortière, Etienne Decencière, Petr Dokladal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the aim of estimating the abundance map from observations only, linear
unmixing approaches are not always suitable to spectral images, especially when
the number of bands is too small or when the spectra of the observed data are
too correlated. To address this issue in the general case, we present a novel
approach which provides an adapted spatial density function based on any
arbitrary linear classifier. A robust mathematical formulation for computing
the Euclidean distance to polyhedral sets is presented, along with an efficient
algorithm that provides the exact minimum-norm point in a polyhedron. An
empirical evaluation on the widely-used Samson hyperspectral dataset
demonstrates that the proposed method surpasses state-of-the-art approaches in
reconstructing abundance maps. Furthermore, its application to spectral images
of a Lithium-ion battery, incompatible with linear unmixing models, validates
the method's generality and effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Pyramid Network for Efficient Multimodal Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ai, Kunyi Wang, Zezhou Wang, Hao Lu, Jin Tian, Yaxin Luo, Peng Xing, Jen-Yuan Huang, Huaxia Li, Gen luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated impressive
performance in various vision-language (VL) tasks, but their expensive
computations still limit the real-world application. To address this issue,
recent efforts aim to compress the visual features to save the computational
costs of MLLMs. However, direct visual compression methods, e.g. efficient
projectors, inevitably destroy the visual semantics in MLLM, especially in
difficult samples. To overcome this shortcoming, we propose a novel dynamic
pyramid network (DPN) for efficient MLLMs. Specifically, DPN formulates MLLM as
a hierarchical structure where visual features are gradually compressed with
increasing depth. In this case, even with a high compression ratio,
fine-grained visual information can still be perceived in shallow layers. To
maximize the benefit of DPN, we further propose an innovative Dynamic Pooling
Experts (DPE) that can dynamically choose the optimal visual compression rate
according to input features. With this design, harder samples will be assigned
larger computations, thus preserving the model performance. To validate our
approach, we conduct extensive experiments on two popular MLLMs and ten
benchmarks. Experimental results show that DPN can save up to 56% average FLOPs
on LLaVA while further achieving +0.74% performance gains. Besides, the
generalization ability of DPN is also validated on the existing high-resolution
MLLM called LLaVA-HR. Our source codes are anonymously released at
https://github.com/aihao2000/DPN-LLaVA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recovering Dynamic 3D Sketches from Videos <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeah Lee, Changwoon Choi, Young Min Kim, Jaesik Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding 3D motion from videos presents inherent challenges due to the
diverse types of movement, ranging from rigid and deformable objects to
articulated structures. To overcome this, we propose Liv3Stroke, a novel
approach for abstracting objects in motion with deformable 3D strokes. The
detailed movements of an object may be represented by unstructured motion
vectors or a set of motion primitives using a pre-defined articulation from a
template model. Just as a free-hand sketch can intuitively visualize scenes or
intentions with a sparse set of lines, we utilize a set of parametric 3D curves
to capture a set of spatially smooth motion elements for general objects with
unknown structures. We first extract noisy, 3D point cloud motion guidance from
video frames using semantic features, and our approach deforms a set of curves
to abstract essential motion features as a set of explicit 3D representations.
Such abstraction enables an understanding of prominent components of motions
while maintaining robustness to environmental factors. Our approach allows
direct analysis of 3D object movements from video, tackling the uncertainty
that typically occurs when translating real-world motion into recorded footage.
The project page is accessible via: https://jaeah.me/liv3stroke_web}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EditCLIP: Representation Learning for Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Wang, Aleksandar Cvejic, Abdelrahman Eldesokey, Peter Wonka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EditCLIP, a novel representation-learning approach for image
editing. Our method learns a unified representation of edits by jointly
encoding an input image and its edited counterpart, effectively capturing their
transformation. To evaluate its effectiveness, we employ EditCLIP to solve two
tasks: exemplar-based image editing and automated edit evaluation. In
exemplar-based image editing, we replace text-based instructions in
InstructPix2Pix with EditCLIP embeddings computed from a reference exemplar
image pair. Experiments demonstrate that our approach outperforms
state-of-the-art methods while being more efficient and versatile. For
automated evaluation, EditCLIP assesses image edits by measuring the similarity
between the EditCLIP embedding of a given image pair and either a textual
editing instruction or the EditCLIP embedding of another reference image pair.
Experiments show that EditCLIP aligns more closely with human judgments than
existing CLIP-based metrics, providing a reliable measure of edit quality and
structural preservation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://qianwangx.github.io/EditCLIP/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Driven MRI Spine Pathology Detection: A Comprehensive Deep Learning
  Approach for Automated Diagnosis in Diverse Clinical Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bargava Subramanian, Naveen Kumarasami, Praveen Shastry, Raghotham Sripadraj, Kalyan Sivasailam, Anandakumar D, Abinaya Ramachandran, Sudhir MP, Gunakutti G, Kishore Prasath Venkatesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Study Design This study presents the development of an autonomous AI system
for MRI spine pathology detection, trained on a dataset of 2 million MRI spine
scans sourced from diverse healthcare facilities across India. The AI system
integrates advanced architectures, including Vision Transformers, U-Net with
cross-attention, MedSAM, and Cascade R-CNN, enabling comprehensive
classification, segmentation, and detection of 43 distinct spinal pathologies.
The dataset is balanced across age groups, genders, and scanner manufacturers
to ensure robustness and adaptability. Subgroup analyses were conducted to
validate the model's performance across different patient demographics, imaging
conditions, and equipment types.
  Performance The AI system achieved up to 97.9 percent multi-pathology
detection, demonstrating consistent performance across age, gender, and
manufacturer subgroups. The normal vs. abnormal classification achieved 98.0
percent accuracy, and the system was deployed across 13 major healthcare
enterprises in India, encompassing diagnostic centers, large hospitals, and
government facilities. During deployment, it processed approximately 100,000
plus MRI spine scans, leading to reduced reporting times and increased
diagnostic efficiency by automating the identification of common spinal
conditions.
  Conclusion The AI system's high precision and recall validate its capability
as a reliable tool for autonomous normal/abnormal classification, pathology
segmentation, and detection. Its scalability and adaptability address critical
diagnostic gaps, optimize radiology workflows, and improve patient care across
varied healthcare environments in India.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages , 3 figurea</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikeDerain: Unveiling Clear Videos from Rainy Sequences Using Color
  Spike Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Liang, Xian Zhong, Wenxuan Liu, Yajing Zheng, Wenxin Huang, Zhaofei Yu, Tiejun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Restoring clear frames from rainy videos presents a significant challenge due
to the rapid motion of rain streaks. Traditional frame-based visual sensors,
which capture scene content synchronously, struggle to capture the fast-moving
details of rain accurately. In recent years, neuromorphic sensors have
introduced a new paradigm for dynamic scene perception, offering microsecond
temporal resolution and high dynamic range. However, existing multimodal
methods that fuse event streams with RGB images face difficulties in handling
the complex spatiotemporal interference of raindrops in real scenes, primarily
due to hardware synchronization errors and computational redundancy. In this
paper, we propose a Color Spike Stream Deraining Network (SpikeDerain), capable
of reconstructing spike streams of dynamic scenes and accurately removing rain
streaks. To address the challenges of data scarcity in real continuous rainfall
scenes, we design a physically interpretable rain streak synthesis model that
generates parameterized continuous rain patterns based on arbitrary background
images. Experimental results demonstrate that the network, trained with this
synthetic data, remains highly robust even under extreme rainfall conditions.
These findings highlight the effectiveness and robustness of our method across
varying rainfall levels and datasets, setting new standards for video deraining
tasks. The code will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wan: Open and Advanced Large-Scale Video Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         WanTeam,  :, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, Ziyu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report presents Wan, a comprehensive and open suite of video foundation
models designed to push the boundaries of video generation. Built upon the
mainstream diffusion transformer paradigm, Wan achieves significant
advancements in generative capabilities through a series of innovations,
including our novel VAE, scalable pre-training strategies, large-scale data
curation, and automated evaluation metrics. These contributions collectively
enhance the model's performance and versatility. Specifically, Wan is
characterized by four key features: Leading Performance: The 14B model of Wan,
trained on a vast dataset comprising billions of images and videos,
demonstrates the scaling laws of video generation with respect to both data and
model size. It consistently outperforms the existing open-source models as well
as state-of-the-art commercial solutions across multiple internal and external
benchmarks, demonstrating a clear and significant performance superiority.
Comprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B
parameters, for efficiency and effectiveness respectively. It also covers
multiple downstream applications, including image-to-video, instruction-guided
video editing, and personal video generation, encompassing up to eight tasks.
Consumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource
efficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range
of consumer-grade GPUs. Openness: We open-source the entire series of Wan,
including source code and all models, with the goal of fostering the growth of
the video generation community. This openness seeks to significantly expand the
creative possibilities of video production in the industry and provide academia
with high-quality video foundation models. All the code and models are
available at https://github.com/Wan-Video/Wan2.1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, 33 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Heterogeneous Adversarial Transferability via Feature
  Permutation Attacks <span class="chip">PAKDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wu, Tie Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks in black-box settings are highly practical, with
transfer-based attacks being the most effective at generating adversarial
examples (AEs) that transfer from surrogate models to unseen target models.
However, their performance significantly degrades when transferring across
heterogeneous architectures -- such as CNNs, MLPs, and Vision Transformers
(ViTs) -- due to fundamental architectural differences. To address this, we
propose Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method
that enhances adversarial transferability across diverse architectures. FPA
introduces a novel feature permutation (FP) operation, which rearranges pixel
values in selected feature maps to simulate long-range dependencies,
effectively making CNNs behave more like ViTs and MLPs. This enhances feature
diversity and improves transferability both across heterogeneous architectures
and within homogeneous CNNs. Extensive evaluations on 14 state-of-the-art
architectures show that FPA achieves maximum absolute gains in attack success
rates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming
existing black-box attacks. Additionally, FPA is highly generalizable and can
seamlessly integrate with other transfer-based attacks to further boost their
performance. Our findings establish FPA as a robust, efficient, and
computationally lightweight strategy for enhancing adversarial transferability
across heterogeneous architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PAKDD 2025. Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruction-Oriented Preference Alignment for Enhancing Multi-Modal
  Comprehension Capability of MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitian Wang, Yue Liao, Kang Rong, Fengyun Rao, Yibo Yang, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference alignment has emerged as an effective strategy to enhance the
performance of Multimodal Large Language Models (MLLMs) following supervised
fine-tuning. While existing preference alignment methods predominantly target
hallucination factors, they overlook the factors essential for multi-modal
comprehension capabilities, often narrowing their improvements on hallucination
mitigation. To bridge this gap, we propose Instruction-oriented Preference
Alignment (IPA), a scalable framework designed to automatically construct
alignment preferences grounded in instruction fulfillment efficacy. Our method
involves an automated preference construction coupled with a dedicated
verification process that identifies instruction-oriented factors, avoiding
significant variability in response representations. Additionally, IPA
incorporates a progressive preference collection pipeline, further recalling
challenging samples through model self-evolution and reference-guided
refinement. Experiments conducted on Qwen2VL-7B demonstrate IPA's effectiveness
across multiple benchmarks, including hallucination evaluation, visual question
answering, and text understanding tasks, highlighting its capability to enhance
general comprehension.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceptually Accurate 3D Talking Head Generation: New Definitions,
  Speech-Mesh Representation, and Evaluation Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lee Chae-Yeon, Oh Hyun-Bin, Han EunGi, Kim Sung-Bin, Suekyeong Nam, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in speech-driven 3D talking head generation have made
significant progress in lip synchronization. However, existing models still
struggle to capture the perceptual alignment between varying speech
characteristics and corresponding lip movements. In this work, we claim that
three criteria -- Temporal Synchronization, Lip Readability, and Expressiveness
-- are crucial for achieving perceptually accurate lip movements. Motivated by
our hypothesis that a desirable representation space exists to meet these three
criteria, we introduce a speech-mesh synchronized representation that captures
intricate correspondences between speech signals and 3D face meshes. We found
that our learned representation exhibits desirable characteristics, and we plug
it into existing models as a perceptual loss to better align lip movements to
the given speech. In addition, we utilize this representation as a perceptual
metric and introduce two other physically grounded lip synchronization metrics
to assess how well the generated 3D talking heads align with these three
criteria. Experiments show that training 3D talking head generation models with
our perceptual loss significantly improve all three aspects of perceptually
accurate lip synchronization. Codes and datasets are available at
https://perceptual-3d-talking-head.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Convolutional Neural Networks for Improved Detection of Intracranial
  bleeding in CT Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bargava Subramanian, Naveen Kumarasami, Praveen Shastry, Kalyan Sivasailam, Anandakumar D, Elakkiya R, Harsha KG, Rithanya V, Harini T, Afshin Hussain, Kishore Prasath Venkatesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Intracranial bleeding (IB) is a life-threatening condition caused
by traumatic brain injuries, including epidural, subdural, subarachnoid, and
intraparenchymal hemorrhages. Rapid and accurate detection is crucial to
prevent severe complications. Traditional imaging can be slow and prone to
variability, especially in high-pressure scenarios. Artificial Intelligence
(AI) provides a solution by quickly analyzing medical images, identifying
subtle hemorrhages, and flagging urgent cases. By enhancing diagnostic speed
and accuracy, AI improves workflows and patient care. This article explores
AI's role in transforming IB detection in emergency settings.
  Methods: A U-shaped 3D Convolutional Neural Network (CNN) automates IB
detection and classification in volumetric CT scans. Advanced preprocessing,
including CLAHE and intensity normalization, enhances image quality. The
architecture preserves spatial and contextual details for precise segmentation.
A dataset of 2,912 annotated CT scans was used for training and evaluation.
  Results: The model achieved high performance across major bleed types, with
precision, recall, and accuracy exceeding 90 percent in most cases 96 percent
precision for epidural hemorrhages and 94 percent accuracy for subarachnoid
hemorrhages. Its ability to classify and localize hemorrhages highlights its
clinical reliability.
  Conclusion: This U-shaped 3D CNN offers a scalable solution for automating IB
detection, reducing diagnostic delays, and improving emergency care outcomes.
Future work will expand dataset diversity, optimize real-time processing, and
integrate multimodal data for enhanced clinical applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attribute-formed Class-specific Concept Space: Endowing Language
  Bottleneck Model with Better Interpretability and Scalability <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyang Zhang, Qianli Luo, Guowu Yang, Wenjing Yang, Weide Liu, Guosheng Lin, Fengmao Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Bottleneck Models (LBMs) are proposed to achieve interpretable image
recognition by classifying images based on textual concept bottlenecks.
However, current LBMs simply list all concepts together as the bottleneck
layer, leading to the spurious cue inference problem and cannot generalized to
unseen classes. To address these limitations, we propose the Attribute-formed
Language Bottleneck Model (ALBM). ALBM organizes concepts in the
attribute-formed class-specific space, where concepts are descriptions of
specific attributes for specific classes. In this way, ALBM can avoid the
spurious cue inference problem by classifying solely based on the essential
concepts of each class. In addition, the cross-class unified attribute set also
ensures that the concept spaces of different classes have strong correlations,
as a result, the learned concept classifier can be easily generalized to unseen
classes. Moreover, to further improve interpretability, we propose Visual
Attribute Prompt Learning (VAPL) to extract visual features on fine-grained
attributes. Furthermore, to avoid labor-intensive concept annotation, we
propose the Description, Summary, and Supplement (DSS) strategy to
automatically generate high-quality concept sets with a complete and precise
attribute. Extensive experiments on 9 widely used few-shot benchmarks
demonstrate the interpretability, transferability, and performance of our
approach. The code and collected concept sets are available at
https://github.com/tiggers23/ALBM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traversing Distortion-Perception Tradeoff using a Single Score-Based
  Generative Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Wang, Suzhi Bi, Ying-Jun Angela Zhang, Xiaojun Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distortion-perception (DP) tradeoff reveals a fundamental conflict
between distortion metrics (e.g., MSE and PSNR) and perceptual quality. Recent
research has increasingly concentrated on evaluating denoising algorithms
within the DP framework. However, existing algorithms either prioritize
perceptual quality by sacrificing acceptable distortion, or focus on minimizing
MSE for faithful restoration. When the goal shifts or noisy measurements vary,
adapting to different points on the DP plane needs retraining or even
re-designing the model. Inspired by recent advances in solving inverse problems
using score-based generative models, we explore the potential of flexibly and
optimally traversing DP tradeoffs using a single pre-trained score-based model.
Specifically, we introduce a variance-scaled reverse diffusion process and
theoretically characterize the marginal distribution. We then prove that the
proposed sample process is an optimal solution to the DP tradeoff for
conditional Gaussian distribution. Experimental results on two-dimensional and
image datasets illustrate that a single score network can effectively and
flexibly traverse the DP tradeoff for general denoising problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE/CVF Conference on Computer Vision and Pattern
  Recognition 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Aware Weakly Supervised Image <span class="highlight-title">Manipulation</span> Localization with SAM
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Wang, Changtao Miao, Dianmo Sheng, Tao Gong, Qi Chu, Bin Liu, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malicious image manipulation poses societal risks, increasing the importance
of effective image manipulation detection methods. Recent approaches in image
manipulation detection have largely been driven by fully supervised approaches,
which require labor-intensive pixel-level annotations. Thus, it is essential to
explore weakly supervised image manipulation localization methods that only
require image-level binary labels for training. However, existing weakly
supervised image manipulation methods overlook the importance of edge
information for accurate localization, leading to suboptimal localization
performance. To address this, we propose a Context-Aware Boundary Localization
(CABL) module to aggregate boundary features and learn context-inconsistency
for localizing manipulated areas. Furthermore, by leveraging Class Activation
Mapping (CAM) and Segment Anything Model (SAM), we introduce the CAM-Guided SAM
Refinement (CGSR) module to generate more accurate manipulation localization
maps. By integrating two modules, we present a novel weakly supervised
framework based on a dual-branch Transformer-CNN architecture. Our method
achieves outstanding localization performance across multiple datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at
  Intermediate Resolution with Structure-Aware Multimodal U-Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenwei Zhang, Anne Condon, Khanh Dao Duc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at
intermediate resolution (4-8 {\AA}) is crucial in protein structure
determination. Recent advances in deep learning have led to the development of
automated approaches for enhancing experimental cryo-EM density maps. Yet,
these methods are not optimized for intermediate-resolution maps and rely on
map density features alone. To address this, we propose CryoSAMU, a novel
method designed to enhance 3D cryo-EM density maps of protein structures using
structure-aware multimodal U-Nets and trained on curated
intermediate-resolution density maps. We comprehensively evaluate CryoSAMU
across various metrics and demonstrate its competitive performance compared to
state-of-the-art methods. Notably, CryoSAMU achieves significantly faster
processing speed, showing promise for future practical applications. Our code
is available at https://github.com/chenwei-zhang/CryoSAMU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4
  supplementary tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RelTriple: Learning Plausible Indoor Layouts by Integrating Relationship
  Triples into the <span class="highlight-title">Diffusion</span> Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaifan Sun, Bingchen Yang, Peter Wonka, Jun Xiao, Haiyong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generation of indoor furniture layouts has significant applications in
augmented reality, smart homes, and architectural design. Successful furniture
arrangement requires proper physical relationships (e.g., collision avoidance)
and spacing relationships between furniture and their functional zones to be
respected. However, manually defined relationships are almost always incomplete
and can produce unrealistic layouts. This work instead extracts spacing
relationships automatically based on a hierarchical analysis and adopts the
Delaunay Triangulation to produce important triple relationships. Compared to
pairwise relationship modeling, triple relationships account for interactions
and space utilization among multiple objects. To this end, we introduce
RelTriple, a novel approach that enhances furniture distribution by learning
spacing relationships between objects and regions. We formulate triple
relationships as object-to-object (O2O) losses and object-to-region (O2R)
losses and integrate them directly into the training process of generative
diffusion. Our approach consistently improves over existing state-of-the-art
methods in visual results evaluation metrics on unconditional layout
generation, floorplan-conditioned layout generation, and scene rearrangement,
achieving at least 12% on the introduced spatial relationship metric and
superior spatial coherence and practical usability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InsViE-1M: Effective Instruction-based Video Editing with Elaborate
  <span class="highlight-title">Dataset</span> Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Wu, Liyi Chen, Ruibin Li, Shihao Wang, Chenxi Xie, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-based video editing allows effective and interactive editing of
videos using only instructions without extra inputs such as masks or
attributes. However, collecting high-quality training triplets (source video,
edited video, instruction) is a challenging task. Existing datasets mostly
consist of low-resolution, short duration, and limited amount of source videos
with unsatisfactory editing quality, limiting the performance of trained
editing models. In this work, we present a high-quality Instruction-based Video
Editing dataset with 1M triplets, namely InsViE-1M. We first curate
high-resolution and high-quality source videos and images, then design an
effective editing-filtering pipeline to construct high-quality editing triplets
for model training. For a source video, we generate multiple edited samples of
its first frame with different intensities of classifier-free guidance, which
are automatically filtered by GPT-4o with carefully crafted guidelines. The
edited first frame is propagated to subsequent frames to produce the edited
video, followed by another round of filtering for frame quality and motion
evaluation. We also generate and filter a variety of video editing triplets
from high-quality images. With the InsViE-1M dataset, we propose a multi-stage
learning strategy to train our InsViE model, progressively enhancing its
instruction following and editing ability. Extensive experiments demonstrate
the advantages of our InsViE-1M dataset and the trained model over
state-of-the-art works. Codes are available at InsViE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Parameter-Efficient Tuning with Token Redundancy Reduction <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwonyoung Kim, Jungin Park, Jin Kim, Hyeongjun Kwon, Kwanghoon Sohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient tuning (PET) aims to transfer pre-trained foundation
models to downstream tasks by learning a small number of parameters. Compared
to traditional fine-tuning, which updates the entire model, PET significantly
reduces storage and transfer costs for each task regardless of exponentially
increasing pre-trained model capacity. However, most PET methods inherit the
inference latency of their large backbone models and often introduce additional
computational overhead due to additional modules (e.g. adapters), limiting
their practicality for compute-intensive applications. In this paper, we
propose Faster Parameter-Efficient Tuning (FPET), a novel approach that
enhances inference speed and training efficiency while maintaining high storage
efficiency. Specifically, we introduce a plug-and-play token redundancy
reduction module delicately designed for PET. This module refines tokens from
the self-attention layer using an adapter to learn the accurate similarity
between tokens and cuts off the tokens through a fully-differentiable token
merging strategy, which uses a straight-through estimator for optimal token
reduction. Experimental results prove that our FPET achieves faster inference
and higher memory efficiency than the pre-trained backbone while keeping
competitive performance on par with state-of-the-art PET methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025 Camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViLBench: A Suite for <span class="highlight-title">Vision</span>-Language Process Reward Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoqin Tu, Weitao Feng, Hardy Chen, Hui Liu, Xianfeng Tang, Cihang Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process-supervised reward models serve as a fine-grained function that
provides detailed step-wise feedback to model responses, facilitating effective
selection of reasoning trajectories for complex tasks. Despite its advantages,
evaluation on PRMs remains less explored, especially in the multimodal domain.
To address this gap, this paper first benchmarks current vision large language
models (VLLMs) as two types of reward models: output reward models (ORMs) and
process reward models (PRMs) on multiple vision-language benchmarks, which
reveal that neither ORM nor PRM consistently outperforms across all tasks, and
superior VLLMs do not necessarily yield better rewarding performance. To
further advance evaluation, we introduce ViLBench, a vision-language benchmark
designed to require intensive process reward signals. Notably, OpenAI's GPT-4o
with Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the
benchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a
promising pathway towards bridging the gap between general VLLMs and reward
models -- by collecting 73.6K vision-language process reward data using an
enhanced tree-search algorithm, our 3B model is able to achieve an average
improvement of 3.3% over standard CoT and up to 2.5% compared to its untrained
counterpart on ViLBench by selecting OpenAI o1's generations. We release the
implementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,
and data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EGVD: Event-Guided Video <span class="highlight-title">Diffusion</span> Model for Physically Realistic
  Large-Motion Frame Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziran Zhang, Xiaohui Li, Yihao Liu, Yujin Wang, Yueting Chen, Tianfan Xue, Shi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video frame interpolation (VFI) in scenarios with large motion remains
challenging due to motion ambiguity between frames. While event cameras can
capture high temporal resolution motion information, existing event-based VFI
methods struggle with limited training data and complex motion patterns. In
this paper, we introduce Event-Guided Video Diffusion Model (EGVD), a novel
framework that leverages the powerful priors of pre-trained stable video
diffusion models alongside the precise temporal information from event cameras.
Our approach features a Multi-modal Motion Condition Generator (MMCG) that
effectively integrates RGB frames and event signals to guide the diffusion
process, producing physically realistic intermediate frames. We employ a
selective fine-tuning strategy that preserves spatial modeling capabilities
while efficiently incorporating event-guided temporal information. We
incorporate input-output normalization techniques inspired by recent advances
in diffusion modeling to enhance training stability across varying noise
levels. To improve generalization, we construct a comprehensive dataset
combining both real and simulated event data across diverse scenarios.
Extensive experiments on both real and simulated datasets demonstrate that EGVD
significantly outperforms existing methods in handling large motion and
challenging lighting conditions, achieving substantial improvements in
perceptual quality metrics (27.4% better LPIPS on Prophesee and 24.1% on BSRGB)
while maintaining competitive fidelity measures. Code and datasets available
at: https://github.com/OpenImagingLab/EGVD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mamba-3D as Masked Autoencoders for Accurate and Data-Efficient Analysis
  of Medical Ultrasound Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaheng Zhou, Yanfeng Zhou, Wei Fang, Yuxing Tang, Le Lu, Ge Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound videos are an important form of clinical imaging data, and deep
learning-based automated analysis can improve diagnostic accuracy and clinical
efficiency. However, the scarcity of labeled data and the inherent challenges
of video analysis have impeded the advancement of related methods. In this
work, we introduce E-ViM$^3$, a data-efficient Vision Mamba network that
preserves the 3D structure of video data, enhancing long-range dependencies and
inductive biases to better model space-time correlations. With our design of
Enclosure Global Tokens (EGT), the model captures and aggregates global
features more effectively than competing methods. To further improve data
efficiency, we employ masked video modeling for self-supervised pre-training,
with the proposed Spatial-Temporal Chained (STC) masking strategy designed to
adapt to various video scenarios. Experiments demonstrate that E-ViM$^3$
performs as the state-of-the-art in two high-level semantic analysis tasks
across four datasets of varying sizes: EchoNet-Dynamic, CAMUS, MICCAI-BUV, and
WHBUS. Furthermore, our model achieves competitive performance with limited
labels, highlighting its potential impact on real-world clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LogicQA: Logical Anomaly Detection with <span class="highlight-title">Vision</span> Language Model Generated
  Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejin Kwon, Daeun Moon, Youngje Oh, Hyunsoo Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly Detection (AD) focuses on detecting samples that differ from the
standard pattern, making it a vital tool in process control. Logical anomalies
may appear visually normal yet violate predefined constraints on object
presence, arrangement, or quantity, depending on reasoning and explainability.
We introduce LogicQA, a framework that enhances AD by providing industrial
operators with explanations for logical anomalies. LogicQA compiles
automatically generated questions into a checklist and collects responses to
identify violations of logical constraints. LogicQA is training-free,
annotation-free, and operates in a few-shot setting. We achieve
state-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO
AD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the
explanations of anomalies. Also, our approach has shown outstanding performance
on semiconductor SEM corporate data, further validating its effectiveness in
industrial applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhysAnimator: Physics-Guided Generative Cartoon Animation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Xie, Yiwei Zhao, Ying Jiang, Chenfanfu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating hand-drawn animation sequences is labor-intensive and demands
professional expertise. We introduce PhysAnimator, a novel approach for
generating physically plausible meanwhile anime-stylized animation from static
anime illustrations. Our method seamlessly integrates physics-based simulations
with data-driven generative models to produce dynamic and visually compelling
animations. To capture the fluidity and exaggeration characteristic of anime,
we perform image-space deformable body simulations on extracted mesh
geometries. We enhance artistic control by introducing customizable energy
strokes and incorporating rigging point support, enabling the creation of
tailored animation effects such as wind interactions. Finally, we extract and
warp sketches from the simulation sequence, generating a texture-agnostic
representation, and employ a sketch-guided video diffusion model to synthesize
high-quality animation frames. The resulting animations exhibit temporal
consistency and visual plausibility, demonstrating the effectiveness of our
method in creating dynamic anime-style animations. See our project page for
more demos: https://xpandora.github.io/PhysAnimator/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OTTER: A <span class="highlight-title">Vision</span>-Language-Action Model with Text-Aware <span class="highlight-title">Visual</span> Feature
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03734v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03734v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam, Jitendra Malik, Ken Goldberg, Pieter Abbeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models aim to predict robotic actions based on
visual observations and language instructions. Existing approaches require
fine-tuning pre-trained visionlanguage models (VLMs) as visual and language
features are independently fed into downstream policies, degrading the
pre-trained semantic alignments. We propose OTTER, a novel VLA architecture
that leverages these existing alignments through explicit, text-aware visual
feature extraction. Instead of processing all visual features, OTTER
selectively extracts and passes only task-relevant visual features that are
semantically aligned with the language instruction to the policy transformer.
This allows OTTER to keep the pre-trained vision-language encoders frozen.
Thereby, OTTER preserves and utilizes the rich semantic understanding learned
from large-scale pre-training, enabling strong zero-shot generalization
capabilities. In simulation and real-world experiments, OTTER significantly
outperforms existing VLA models, demonstrating strong zeroshot generalization
to novel objects and environments. Video, code, checkpoints, and dataset:
https://ottervla.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R-LiViT: A <span class="highlight-title">LiDAR</span>-<span class="highlight-title">Visual</span>-Thermal <span class="highlight-title">Dataset</span> Enabling Vulnerable Road User
  Focused Roadside Perception <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Mirlach, Lei Wan, Andreas Wiedholz, Hannan Ejaz Keen, Andreas Eich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, the integration of roadside perception systems is
essential for overcoming occlusion challenges and enhancing the safety of
Vulnerable Road Users (VRUs). While LiDAR and visual (RGB) sensors are commonly
used, thermal imaging remains underrepresented in datasets, despite its
acknowledged advantages for VRU detection in extreme lighting conditions. In
this paper, we present R-LiViT, the first dataset to combine LiDAR, RGB, and
thermal imaging from a roadside perspective, with a strong focus on VRUs.
R-LiViT captures three intersections during both day and night, ensuring a
diverse dataset. It includes 10,000 LiDAR frames and 2,400 temporally and
spatially aligned RGB and thermal images across over 150 traffic scenarios,
with 6 and 8 annotated classes respectively, providing a comprehensive resource
for tasks such as object detection and tracking. The dataset and the code for
reproducing our evaluation results are made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, submitted to ICCV2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> DexHandDiff: Interaction-aware <span class="highlight-title">Diffusion</span> Planning for Adaptive Dexterous
  <span class="highlight-title">Manipulation</span> <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18562v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18562v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Liang, <span class="highlight-author">Yao Mu</span>, Yixiao Wang, Tianxing Chen, Wenqi Shao, Wei Zhan, Masayoshi Tomizuka, Ping Luo, Mingyu Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dexterous manipulation with contact-rich interactions is crucial for advanced
robotics. While recent diffusion-based planning approaches show promise for
simple manipulation tasks, they often produce unrealistic ghost states (e.g.,
the object automatically moves without hand contact) or lack adaptability when
handling complex sequential interactions. In this work, we introduce
DexHandDiff, an interaction-aware diffusion planning framework for adaptive
dexterous manipulation. DexHandDiff models joint state-action dynamics through
a dual-phase diffusion process which consists of pre-interaction contact
alignment and post-contact goal-directed control, enabling goal-adaptive
generalizable dexterous manipulation. Additionally, we incorporate dynamics
model-based dual guidance and leverage large language models for automated
guidance function generation, enhancing generalizability for physical
interactions and facilitating diverse goal adaptation through language cues.
Experiments on physical interaction tasks such as door opening, pen and block
re-orientation, object relocation, and hammer striking demonstrate
DexHandDiff's effectiveness on goals outside training distributions, achieving
over twice the average success rate (59.2% vs. 29.5%) compared to existing
methods. Our framework achieves an average of 70.7% success rate on goal
adaptive dexterous tasks, highlighting its robustness and flexibility in
contact-rich manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025. Camera ready version. Previous DexDiffuser.
  Project page: https://dexdiffuser.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Networking Systems for Video Anomaly Detection: A Tutorial and <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10347v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10347v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liu, Yang Liu, Jieyu Lin, Jielin Li, Liang Cao, Peng Sun, Bo Hu, Liang Song, Azzedine Boukerche, Victor C. M. Leung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing utilization of surveillance cameras in smart cities, coupled
with the surge of online video applications, has heightened concerns regarding
public security and privacy protection, which propelled automated Video Anomaly
Detection (VAD) into a fundamental research task within the Artificial
Intelligence (AI) community. With the advancements in deep learning and edge
computing, VAD has made significant progress and advances synergized with
emerging applications in smart cities and video internet, which has moved
beyond the conventional research scope of algorithm engineering to deployable
Networking Systems for VAD (NSVAD), a practical hotspot for intersection
exploration in the AI, IoVT, and computing fields. In this article, we
delineate the foundational assumptions, learning frameworks, and applicable
scenarios of various deep learning-driven VAD routes, offering an exhaustive
tutorial for novices in NSVAD. In addition, this article elucidates core
concepts by reviewing recent advances and typical solutions and aggregating
available research resources accessible at https://github.com/fdjingliu/NSVAD.
Lastly, this article projects future development trends and discusses how the
integration of AI and computing technologies can address existing research
challenges and promote open opportunities, serving as an insightful guide for
prospective researchers and engineers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised to ACM Computing Surveys, under review, for more information
  and supplementary material, please see https://github.com/fdjingliu/NSVAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Augmentation in Earth Observation: A <span class="highlight-title">Diffusion</span> Model Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06218v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06218v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago Sousa, Benoît Ries, Nicolas Guelfi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality Earth Observation (EO) imagery is essential for accurate
analysis and informed decision making across sectors. However, data scarcity
caused by atmospheric conditions, seasonal variations, and limited geographical
coverage hinders the effective application of Artificial Intelligence (AI) in
EO. Traditional data augmentation techniques, which rely on basic parameterized
image transformations, often fail to introduce sufficient diversity across key
semantic axes. These axes include natural changes such as snow and floods,
human impacts like urbanization and roads, and disasters such as wildfires and
storms, which limits the accuracy of AI models in EO applications. To address
this, we propose a four-stage data augmentation approach that integrates
diffusion models to enhance semantic diversity. Our method employs meta-prompts
for instruction generation, vision-language models for rich captioning,
EO-specific diffusion model fine-tuning, and iterative data augmentation.
Extensive experiments using four augmentation techniques demonstrate that our
approach consistently outperforms established methods, generating semantically
diverse EO images and improving AI model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harmony: A Joint Self-Supervised and Weakly-Supervised Framework for
  Learning General Purpose <span class="highlight-title">Visual</span> Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Baharoon, Jonathan Klein, Dominik L. Michels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language contrastive learning frameworks like CLIP enable learning
representations from natural language supervision, and provide strong zero-shot
classification capabilities. However, due to the nature of the supervisory
signal in these paradigms, they lack the ability to learn localized features,
leading to degraded performance on dense prediction tasks like segmentation and
detection. On the other hand, self-supervised learning methods have shown the
ability to learn granular representations, complementing the high-level
features in vision-language training. In this work, we present Harmony, a
framework that combines vision-language training with discriminative and
generative self-supervision to learn visual features that can be generalized
across different vision downstream tasks. Our framework is specifically
designed to work on web-scraped data by not relying on negative examples and
addressing the one-to-one correspondence issue using soft CLIP targets
generated by an EMA model. We comprehensively evaluate Harmony across various
vision downstream tasks and find that it significantly outperforms the baseline
CLIP and the previously leading joint self and weakly-supervised methods,
MaskCLIP and SLIP. Specifically, when comparing against these methods, Harmony
shows superior performance in fine-tuning and zero-shot classification on
ImageNet-1k, semantic segmentation on ADE20K, and both object detection and
instance segmentation on MS-COCO, when pre-training a ViT-B on CC3M. We also
show that Harmony outperforms other self-supervised learning methods like iBOT
and MAE across all tasks evaluated. Our code is publicly at
https://github.com/MohammedSB/Harmony}{https://github.com/MohammedSB/Harmony
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Scalable Foundation Model for Multi-modal and Hyperspectral
  Geospatial Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12843v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12843v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Si, Yuxuan Wan, Minh Do, Deepak Vasisht, Han Zhao, Hendrik F. Hamann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geospatial raster data, such as that collected by satellite-based imaging
systems at different times and spectral bands, hold immense potential for
enabling a wide range of high-impact applications. This potential stems from
the rich information that is spatially and temporally contextualized across
multiple channels and sensing modalities. Recent work has adapted existing
self-supervised learning approaches for such geospatial data. However, they
fall short of scalable model architectures, leading to inflexibility and
computational inefficiencies when faced with an increasing number of channels
and modalities. To address these limitations, we introduce Low-rank Efficient
Spatial-Spectral Vision Transformer with three key innovations: i) the LESS
Attention Block that approximates high-dimensional spatial-spectral attention
through Kronecker's product of the low-dimensional spatial and spectral
attention components; ii) the Continuous Positional-Channel Embedding Layer
that preserves both the continuity and physical characteristics of each
spatial-spectral patch; and iii) the Perception Field Mask that exploits local
spatial dependencies by constraining attention to neighboring patches. To
evaluate the proposed innovations, we construct GFM-Bench, which serves as a
comprehensive benchmark for such geospatial raster data. We pretrain LESS ViT
using a Hyperspectral Masked Autoencoder framework with integrated positional
and channel masking strategies. Experimental results demonstrate that our
proposed method achieves competitive performance against state-of-the-art
multi-modal geospatial foundation models while outperforming them on
cross-satellite generalization tasks with higher computational efficiency. The
flexibility and extensibility of our framework make it a promising direction
for future geospatial data analysis tasks that involve a wide range of
modalities and channels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COSMOS: Cross-Modality Self-Distillation for <span class="highlight-title">Vision</span> Language
  Pre-training <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghwan Kim, Rui Xiao, Mariana-Iuliana Georgescu, Stephan Alaniz, Zeynep Akata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) trained with contrastive loss have achieved
significant advancements in various vision and language tasks. However, the
global nature of the contrastive loss makes VLMs focus predominantly on
foreground objects, neglecting other crucial information in the image, which
limits their effectiveness in downstream tasks. To address these challenges, we
propose COSMOS: CrOSs-MOdality Self-distillation for vision-language
pre-training that integrates a novel text-cropping strategy and cross-attention
module into a self-supervised learning framework. We create global and local
views of images and texts (i.e., multi-modal augmentations), which are
essential for self-distillation in VLMs. We further introduce a cross-attention
module, enabling COSMOS to learn comprehensive cross-modal representations
optimized via a cross-modality self-distillation loss. COSMOS consistently
outperforms previous strong baselines on various zero-shot downstream tasks,
including retrieval, classification, and semantic segmentation. Additionally,
it surpasses CLIP-based models trained on larger datasets in visual perception
and contextual understanding tasks. Code is available at
https://github.com/ExplainableML/cosmos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The mathematics of adversarial attacks in AI -- Why deep learning is
  unstable despite the existence of stable neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.06098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.06098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Bastounis, Anders C Hansen, Verner Vlačić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unprecedented success of deep learning (DL) makes it unchallenged when it
comes to classification problems. However, it is well established that the
current DL methodology produces universally unstable neural networks (NNs). The
instability problem has caused an enormous research effort -- with a vast
literature on so-called adversarial attacks -- yet there has been no solution
to the problem. Our paper addresses why there has been no solution to the
problem, as we prove the following mathematical paradox: any training procedure
based on training neural networks for classification problems with a fixed
architecture will yield neural networks that are either inaccurate or unstable
(if accurate) -- despite the provable existence of both accurate and stable
neural networks for the same classification problems. The key is that the
stable and accurate neural networks must have variable dimensions depending on
the input, in particular, variable dimensions is a necessary condition for
stability.
  Our result points towards the paradox that accurate and stable neural
networks exist, however, modern algorithms do not compute them. This yields the
question: if the existence of neural networks with desirable properties can be
proven, can one also find algorithms that compute them? There are cases in
mathematics where provable existence implies computability, but will this be
the case for neural networks? The contrary is true, as we demonstrate how
neural networks can provably exist as approximate minimisers to standard
optimisation problems with standard cost functions, however, no randomised
algorithm can compute them with probability better than 1/2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 1 figure. Revised to make minor changes to notation and
  references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MC-LLaVA: Multi-Concept Personalized <span class="highlight-title">Vision</span>-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11706v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11706v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, Shanghang Zhang, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current vision-language models (VLMs) show exceptional abilities across
diverse tasks, such as visual question answering. To enhance user experience,
recent studies investigate VLM personalization to understand user-provided
concepts. However, they mainly focus on single-concept personalization,
neglecting the existence and interplay of multiple concepts, which limits
real-world applicability. This paper proposes the first multi-concept
personalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a
multi-concept instruction tuning strategy, effectively integrating multiple
concepts in a single training step. To reduce the costs related to joint
training, we propose a personalized textual prompt that uses visual token
information to initialize concept tokens. Additionally, we introduce a
personalized visual prompt during inference, aggregating location confidence
maps for enhanced recognition and grounding capabilities. To advance
multi-concept personalization research, we further contribute a high-quality
instruction tuning dataset. We carefully collect images with multiple
characters and objects from movies and manually generate question-answer
samples for multi-concept scenarios, featuring superior diversity.
Comprehensive qualitative and quantitative experiments demonstrate that
MC-LLaVA can achieve impressive multi-concept personalized responses, paving
the way for VLMs to become better user-specific assistants. The code and
dataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intuitive Axial Augmentation Using Polar-Sine-Based Piecewise Distortion
  for Medical Slice-Wise Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqin Zhang, Qingkui Chen, Chen Huang, Zhengjie Zhang, Meiling Chen, Zhibing Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most data-driven models for medical image analysis rely on universal
augmentations to improve accuracy. Experimental evidence has confirmed their
effectiveness, but the unclear mechanism underlying them poses a barrier to the
widespread acceptance and trust in such methods within the medical community.
We revisit and acknowledge the unique characteristics of medical images apart
from traditional digital images, and consequently, proposed a medical-specific
augmentation algorithm that is more elastic and aligns well with radiology scan
procedure. The method performs piecewise affine with sinusoidal distorted ray
according to radius on polar coordinates, thus simulating uncertain postures of
human lying flat on the scanning table. Our method could generate human
visceral distribution without affecting the fundamental relative position on
axial plane. Two non-adaptive algorithms, namely Meta-based Scan Table Removal
and Similarity-Guided Parameter Search, are introduced to bolster robustness of
our augmentation method. In contrast to other methodologies, our method is
highlighted for its intuitive design and ease of understanding for medical
professionals, thereby enhancing its applicability in clinical scenarios.
Experiments show our method improves accuracy with two modality across multiple
famous segmentation frameworks without requiring more data samples. Our preview
code is available in: https://github.com/MGAMZ/PSBPD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at Smart Health</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel
  Reconstruction from Sparse-View Dynamic DSA Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhentao Liu, Ruyi Zha, Huangxuan Zhao, Hongdong Li, Zhiming Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing 3D vessel structures from sparse-view dynamic digital
subtraction angiography (DSA) images enables accurate medical assessment while
reducing radiation exposure. Existing methods often produce suboptimal results
or require excessive computation time. In this work, we propose 4D radiative
Gaussian splatting (4DRGS) to achieve high-quality reconstruction efficiently.
In detail, we represent the vessels with 4D radiative Gaussian kernels. Each
kernel has time-invariant geometry parameters, including position, rotation,
and scale, to model static vessel structures. The time-dependent central
attenuation of each kernel is predicted from a compact neural network to
capture the temporal varying response of contrast agent flow. We splat these
Gaussian kernels to synthesize DSA images via X-ray rasterization and optimize
the model with real captured ones. The final 3D vessel volume is voxelized from
the well-trained kernels. Moreover, we introduce accumulated attenuation
pruning and bounded scaling activation to improve reconstruction quality.
Extensive experiments on real-world patient data demonstrate that 4DRGS
achieves impressive results in 5 minutes training, which is 32x faster than the
state-of-the-art method. This underscores the potential of 4DRGS for real-world
clinics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IPMI 2025 Oral; Zhentao Liu and Ruyi Zha made equal contributions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Wang, Marcus Klasson, Matias Turkulainen, Shuz<span class="highlight-author">he Wang</span>, Juho Kannala, Arno Solin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian splatting enables fast novel view synthesis in static 3D
environments. However, reconstructing real-world environments remains
challenging as distractors or occluders break the multi-view consistency
assumption required for accurate 3D reconstruction. Most existing methods rely
on external semantic information from pre-trained models, introducing
additional computational overhead as pre-processing steps or during
optimization. In this work, we propose a novel method, DeSplat, that directly
separates distractors and static scene elements purely based on volume
rendering of Gaussian primitives. We initialize Gaussians within each camera
view for reconstructing the view-specific distractors to separately model the
static 3D scene and distractors in the alpha compositing stages. DeSplat yields
an explicit scene separation of static elements and distractors, achieving
comparable results to prior distractor-free approaches without sacrificing
rendering speed. We demonstrate DeSplat's effectiveness on three benchmark data
sets for distractor-free novel view synthesis. See the project website at
https://aaltoml.github.io/desplat/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Black-Box Forgery Attacks on Semantic Watermarks for <span class="highlight-title">Diffusion</span> Models <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Müller, Denis Lukovnikov, Jonas Thietke, Asja Fischer, Erwin Quiring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating watermarking into the generation process of latent diffusion
models (LDMs) simplifies detection and attribution of generated content.
Semantic watermarks, such as Tree-Rings and Gaussian Shading, represent a novel
class of watermarking techniques that are easy to implement and highly robust
against various perturbations. However, our work demonstrates a fundamental
security vulnerability of semantic watermarks. We show that attackers can
leverage unrelated models, even with different latent spaces and architectures
(UNet vs DiT), to perform powerful and realistic forgery attacks. Specifically,
we design two watermark forgery attacks. The first imprints a targeted
watermark into real images by manipulating the latent representation of an
arbitrary image in an unrelated LDM to get closer to the latent representation
of a watermarked image. We also show that this technique can be used for
watermark removal. The second attack generates new images with the target
watermark by inverting a watermarked image and re-generating it with an
arbitrary prompt. Both attacks just need a single reference image with the
target watermark. Overall, our findings question the applicability of semantic
watermarks by revealing that attackers can easily forge or remove these
watermarks under realistic conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 22 figures, 8 tables, to be published in The IEEE/CVF
  Conference on Computer Vision and Pattern Recognition 2025 (CVPR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing Vecset <span class="highlight-title">Diffusion</span> Model for Fast Shape Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Haolin Liu, Fuyun Wang, Huiwen Shi, Xianghui Yang, Qingxiang Lin, Jingwei Huang, Yuhong Liu, Jie Jiang, Chunchao Guo, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D shape generation has greatly flourished through the development of
so-called "native" 3D diffusion, particularly through the Vecset Diffusion
Model (VDM). While recent advancements have shown promising results in
generating high-resolution 3D shapes, VDM still struggles with high-speed
generation. Challenges exist because of difficulties not only in accelerating
diffusion sampling but also VAE decoding in VDM, areas under-explored in
previous works. To address these challenges, we present FlashVDM, a systematic
framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables
flexible diffusion sampling with as few as 5 inference steps and comparable
quality, which is made possible by stabilizing consistency distillation with
our newly introduced Progressive Flow Distillation. For VAE, we introduce a
lightning vecset decoder equipped with Adaptive KV Selection, Hierarchical
Volume Decoding, and Efficient Network Design. By exploiting the locality of
the vecset and the sparsity of shape surface in the volume, our decoder
drastically lowers FLOPs, minimizing the overall decoding overhead. We apply
FlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic
evaluation, we show that our model significantly outperforms existing fast 3D
generation methods, achieving comparable performance to the state-of-the-art
while reducing inference time by over 45x for reconstruction and 32x for
generation. Code and models are available at
https://github.com/Tencent/FlashVDM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparison of marker-less 2D image-based methods for infant pose
  estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04980v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04980v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Jahn, Sarah Flügge, Dajie Zhang, Luise Poustka, Sven Bölte, Florentin Wörgötter, Peter B Marschik, Tomas Kulvicius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study we compare the performance of available generic- and
infant-pose estimators for a video-based automated general movement assessment
(GMA), and the choice of viewing angle for optimal recordings, i.e.,
conventional diagonal view used in GMA vs. top-down view. We used 4500
annotated video-frames from 75 recordings of infant spontaneous motor functions
from 4 to 26 weeks. To determine which pose estimation method and camera angle
yield the best pose estimation accuracy on infants in a GMA related setting,
the distance to human annotations and the percentage of correct key-points
(PCK) were computed and compared. The results show that the best performing
generic model trained on adults, ViTPose, also performs best on infants. We see
no improvement from using infant-pose estimators over the generic pose
estimators on our infant dataset. However, when retraining a generic model on
our data, there is a significant improvement in pose estimation accuracy. The
pose estimation accuracy obtained from the top-down view is significantly
better than that obtained from the diagonal view, especially for the detection
of the hip key-points. The results also indicate limited generalization
capabilities of infant-pose estimators to other infant datasets, which hints
that one should be careful when choosing infant pose estimators and using them
on infant datasets which they were not trained on. While the standard GMA
method uses a diagonal view for assessment, pose estimation accuracy
significantly improves using a top-down view. This suggests that a top-down
view should be included in recording setups for automated GMA research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youyu Chen, Junjun Jiang, Kui Jiang, Xiao Tang, Zhihao Li, Xianming Liu, Yinyu Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian
primitives, where the rendering resolution and the primitive number, concluded
as the optimization complexity, dominate the time cost in primitive
optimization. In this paper, we propose DashGaussian, a scheduling scheme over
the optimization complexity of 3DGS that strips redundant complexity to
accelerate 3DGS optimization. Specifically, we formulate 3DGS optimization as
progressively fitting 3DGS to higher levels of frequency components in the
training views, and propose a dynamic rendering resolution scheme that largely
reduces the optimization complexity based on this formulation. Besides, we
argue that a specific rendering resolution should cooperate with a proper
primitive number for a better balance between computing redundancy and fitting
quality, where we schedule the growth of the primitives to synchronize with the
rendering resolution. Extensive experiments show that our method accelerates
the optimization of various 3DGS backbones by 45.7% on average while preserving
the rendering quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2025. Project page: https://dashgaussian.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Multimodal Driving Scenes via Next-Scene Prediction <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanhao Wu, Haoyang Zhang, Tianwei Lin, Lichao Huang, Shujie Luo, Rui Wu, Congpei Qiu, Wei Ke, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models in Autonomous Driving (AD) enable diverse scene creation,
yet existing methods fall short by only capturing a limited range of
modalities, restricting the capability of generating controllable scenes for
comprehensive evaluation of AD systems. In this paper, we introduce a
multimodal generation framework that incorporates four major data modalities,
including a novel addition of map modality. With tokenized modalities, our
scene sequence generation framework autoregressively predicts each scene while
managing computational demands through a two-stage approach. The Temporal
AutoRegressive (TAR) component captures inter-frame dynamics for each modality
while the Ordered AutoRegressive (OAR) component aligns modalities within each
scene by sequentially predicting tokens in a fixed order. To maintain coherence
between map and ego-action modalities, we introduce the Action-aware Map
Alignment (AMA) module, which applies a transformation based on the ego-action
to maintain coherence between these modalities. Our framework effectively
generates complex, realistic driving scenes over extended sequences, ensuring
multimodal consistency and offering fine-grained control over scene elements.
Project page: https://yanhaowu.github.io/UMGen/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18227v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18227v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Zhong, Zihong Luo, Chengzhi Liu, Feilong Tang, Zelin Peng, Ming Hu, Yingzhen Hu, Jionglong Su, Zongyuan Ge, Imran Razzak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;
however, its accuracy and robustness significantly decrease when applied to
medical image segmentation. Existing methods address this issue through
modality fusion, integrating textual and image information to provide more
detailed priors. In this study, we argue that the granularity of text and the
domain gap affect the accuracy of the priors. Furthermore, the discrepancy
between high-level abstract semantics and pixel-level boundary details in
images can introduce noise into the fusion process. To address this, we propose
Prior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner
to leverage specialized medical knowledge for better modality alignment. The
core of our method lies in efficiently addressing the domain gap with
fine-grained text from a medical LLM. Meanwhile, it also enhances the priors'
quality after modality alignment, ensuring more accurate segmentation. In
addition, our decoder enhances the model's expressive capabilities through
multi-level feature fusion and iterative mask optimizer operations, supporting
unprompted learning. We also propose a unified pipeline that effectively
supplies high-quality semantic information to SAM. Extensive experiments on the
Synapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art
performance. Our code is released at https://github.com/logan-0623/PG-SAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scale-Equivariant Imaging: Self-Supervised Learning for Image
  Super-Resolution and Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11232v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11232v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérémy Scanvic, Mike Davies, Patrice Abry, Julián Tachella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised methods have recently proved to be nearly as effective as
supervised ones in various imaging inverse problems, paving the way for
learning-based approaches in scientific and medical imaging applications where
ground truth data is hard or expensive to obtain. These methods critically rely
on invariance to translations and/or rotations of the image distribution to
learn from incomplete measurement data alone. However, existing approaches fail
to obtain competitive performances in the problems of image super-resolution
and deblurring, which play a key role in most imaging systems. In this work, we
show that invariance to roto-translations is insufficient to learn from
measurements that only contain low-frequency information. Instead, we propose
scale-equivariant imaging, a new self-supervised approach that leverages the
fact that many image distributions are approximately scale-invariant, enabling
the recovery of high-frequency information lost in the measurement process. We
demonstrate throughout a series of experiments on real datasets that the
proposed method outperforms other self-supervised approaches, and obtains
performances on par with fully supervised learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP in Medical Imaging: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07353v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07353v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Zhao, Yuxiao Liu, Han Wu, Mei Wang, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Zhiming Cui, Qian Wang, Dinggang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP), a simple yet effective
pre-training paradigm, successfully introduces text supervision to vision
models. It has shown promising results across various tasks due to its
generalizability and interpretability. The use of CLIP has recently gained
increasing interest in the medical imaging domain, serving as a pre-training
paradigm for image-text alignment, or a critical component in diverse clinical
tasks. With the aim of facilitating a deeper understanding of this promising
direction, this survey offers an in-depth exploration of the CLIP within the
domain of medical imaging, regarding both refined CLIP pre-training and
CLIP-driven applications. In this paper, we (1) first start with a brief
introduction to the fundamentals of CLIP methodology; (2) then investigate the
adaptation of CLIP pre-training in the medical imaging domain, focusing on how
to optimize CLIP given characteristics of medical images and reports; (3)
further explore practical utilization of CLIP pre-trained models in various
tasks, including classification, dense prediction, and cross-modal tasks; and
(4) finally discuss existing limitations of CLIP in the context of medical
imaging, and propose forward-looking directions to address the demands of
medical imaging domain. Studies featuring technical and practical value are
both investigated. We expect this survey will provide researchers with a
holistic understanding of the CLIP paradigm and its potential implications. The
project page of this survey can also be found on
https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page available at
  https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TechCoach: Towards Technical-Point-Aware Descriptive Action Coaching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan-Ming Li, An-Lan Wang, Kun-Yu Lin, Yu-Ming Tang, Ling-An Zeng, Jian-Fang Hu, Wei-Shi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To guide a learner in mastering action skills, it is crucial for a coach to
1) reason through the learner's action execution and technical points
(TechPoints), and 2) provide detailed, comprehensible feedback on what is done
well and what can be improved. However, existing score-based action assessment
methods are still far from reaching this practical scenario. To bridge this
gap, we investigate a new task termed Descriptive Action Coaching (DescCoach)
which requires the model to provide detailed commentary on what is done well
and what can be improved beyond a simple quality score for action execution. To
this end, we first build a new dataset named EE4D-DescCoach. Through an
automatic annotation pipeline, our dataset goes beyond the existing action
assessment datasets by providing detailed TechPoint-level commentary.
Furthermore, we propose TechCoach, a new framework that explicitly incorporates
TechPoint-level reasoning into the DescCoach process. The central to our method
lies in the Context-aware TechPoint Reasoner, which enables TechCoach to learn
TechPoint-related quality representation by querying visual context under the
supervision of TechPoint-level coaching commentary. By leveraging the visual
context and the TechPoint-related quality representation, a unified
TechPoint-aware Action Assessor is then employed to provide the overall
coaching commentary together with the quality score. Combining all of these, we
establish a new benchmark for DescCoach and evaluate the effectiveness of our
method through extensive experiments. The data and code will be made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perception of <span class="highlight-title">Visual</span> Content: Differences Between Humans and Foundation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18968v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18968v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nardiena A. Pratama, Shaoyang Fan, Gianluca Demartini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-annotated content is often used to train machine learning (ML) models.
However, recently, language and multi-modal foundational models have been used
to replace and scale-up human annotator's efforts. This study compares
human-generated and ML-generated annotations of images representing diverse
socio-economic contexts. We aim to understand differences in perception and
identify potential biases in content interpretation. Our dataset comprises
images of people from various geographical regions and income levels, covering
various daily activities and home environments. We compare human and
ML-generated annotations semantically and evaluate their impact on predictive
models. Our results show highest similarity between ML captions and human
labels from a low-level perspective, i.e., types of words that appear and
sentence structures, but all three annotations are alike in how similar or
dissimilar they perceive images across different regions. Additionally, ML
Captions resulted in best overall region classification performance, while ML
Objects and ML Captions performed best overall for income regression. The
varying performance of annotation sets highlights the notion that all
annotations are important, and that human-generated annotations are yet to be
replaceable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 5 tables; updated version for a
  Revise-and-Resubmit at ICWSM 2025. This version includes a larger and more
  diverse dataset, leading to updated results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cutting Voxel Projector a New Approach to Construct 3D Cone Beam CT
  Operator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.09841v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.09841v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vojtěch Kulvait, Julian Moosmann, Georg Rose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel class of projectors for 3D cone beam tomographic
reconstruction. Analytical formulas are derived to compute the relationship
between the volume of a voxel projected onto a detector pixel and its
contribution to the line integral of attenuation recorded by that pixel. Based
on these formulas, we construct a near-exact projector and backprojector,
particularly suited for algebraic reconstruction techniques and hierarchical
reconstruction approaches with nonuniform voxel grids. Unlike traditional
projectors, which assume a uniform grid with fixed voxel sizes, our method
enables local refinement of voxels, allowing for adaptive grid resolution and
improved reconstruction quality in regions of interest. We have implemented
this cutting voxel projector along with a relaxed, speed-optimized version and
compared them to two established projectors: a ray-tracing projector based on
Siddon's algorithm and a TT footprint projector. Our results demonstrate that
the cutting voxel projector achieves higher accuracy than the TT projector,
especially for large cone beam angles. Furthermore, the relaxed version of the
cutting voxel projector offers a significant speed advantage, while maintaining
comparable accuracy. In contrast, Siddon's algorithm, tuned to achieve the same
accuracy, is considerably slower than the cutting voxel projector. All
algorithms are implemented in a GPU optimized open-source framework for
algebraic reconstruction. GitHub repository of the project
https://github.com/kulvait/KCT_cbct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Event-driven 3D Reconstruction: Development under Different
  Categories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanzhi Xu, Haoxian Zhou, Haodong Chen, Vera Chung, Qiang Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras have gained increasing attention for 3D reconstruction due to
their high temporal resolution, low latency, and high dynamic range. They
capture per-pixel brightness changes asynchronously, allowing accurate
reconstruction under fast motion and challenging lighting conditions. In this
survey, we provide a comprehensive review of event-driven 3D reconstruction
methods, including stereo, monocular, and multimodal systems. We further
categorize recent developments based on geometric, learning-based, and hybrid
approaches. Emerging trends, such as neural radiance fields and 3D Gaussian
splatting with event data, are also covered. The related works are structured
chronologically to illustrate the innovations and progression within the field.
To support future research, we also highlight key research gaps and future
research directions in dataset, experiment, evaluation, event representation,
etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, 6 tables, submitted to an anonymous conference
  under double-blind review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14754v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14754v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Franchi, Nikhil Garg, Wendy Ju, Emma Pierson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Street scene datasets, collected from Street View or dashboard cameras, offer
a promising means of detecting urban objects and incidents like street
flooding. However, a major challenge in using these datasets is their lack of
reliable labels: there are myriad types of incidents, many types occur rarely,
and ground-truth measures of where incidents occur are lacking. Here, we
propose BayFlood, a two-stage approach which circumvents this difficulty.
First, we perform zero-shot classification of where incidents occur using a
pretrained vision-language model (VLM). Second, we fit a spatial Bayesian model
on the VLM classifications. The zero-shot approach avoids the need to annotate
large training sets, and the Bayesian model provides frequent desiderata in
urban settings - principled measures of uncertainty, smoothing across
locations, and incorporation of external data like stormwater accumulation
zones. We comprehensively validate this two-stage approach, showing that VLMs
provide strong zero-shot signal for floods across multiple cities and time
periods, the Bayesian model improves out-of-sample prediction relative to
baseline methods, and our inferred flood risk correlates with known external
predictors of risk. Having validated our approach, we show it can be used to
improve urban flood detection: our analysis reveals 113,738 people who are at
high risk of flooding overlooked by current methods, identifies demographic
biases in existing methods, and suggests locations for new flood sensors. More
broadly, our results showcase how Bayesian modeling of zero-shot LM annotations
represents a promising paradigm because it avoids the need to collect large
labeled datasets and leverages the power of foundation models while providing
the expressiveness and uncertainty quantification of Bayesian models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fantastic Copyrighted Beasts and How (Not) to Generate Them 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14526v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14526v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luxi He, Yangsibo Huang, Weijia Shi, Tinghao Xie, Haotian Liu, Yue Wang, Luke Zettlemoyer, Chiyuan Zhang, Danqi Chen, Peter Henderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show that image and video generation models can be prompted to
reproduce copyrighted content from their training data, raising serious legal
concerns about copyright infringement. Copyrighted characters (e.g., Mario,
Batman) present a significant challenge: at least one lawsuit has already
awarded damages based on the generation of such characters. Consequently,
commercial services like DALL-E have started deploying interventions. However,
little research has systematically examined these problems: (1) Can users
easily prompt models to generate copyrighted characters, even if it is
unintentional?; (2) How effective are the existing mitigation strategies? To
address these questions, we introduce a novel evaluation framework with metrics
that assess both the generated image's similarity to copyrighted characters and
its consistency with user intent, grounded in a set of popular copyrighted
characters from diverse studios and regions. We show that state-of-the-art
image and video generation models can still generate characters even if
characters' names are not explicitly mentioned, sometimes with only two generic
keywords (e.g., prompting with "videogame, plumber" consistently generates
Nintendo's Mario character). We also introduce semi-automatic techniques to
identify such keywords or descriptions that trigger character generation. Using
this framework, we evaluate mitigation strategies, including prompt rewriting
and new approaches we propose. Our findings reveal that common methods, such as
DALL-E's prompt rewriting, are insufficient alone and require supplementary
strategies like negative prompting. Our work provides empirical grounding for
discussions on copyright mitigation strategies and offers actionable insights
for model deployers implementing these safeguards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards End-to-End Neuromorphic Voxel-based 3D Object Reconstruction
  Without Physical Priors <span class="chip">ICME</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanzhi Xu, Langyi Chen, Haodong Chen, Vera Chung, Qiang Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic cameras, also known as event cameras, are asynchronous
brightness-change sensors that can capture extremely fast motion without
suffering from motion blur, making them particularly promising for 3D
reconstruction in extreme environments. However, existing research on 3D
reconstruction using monocular neuromorphic cameras is limited, and most of the
methods rely on estimating physical priors and employ complex multi-step
pipelines. In this work, we propose an end-to-end method for dense voxel 3D
reconstruction using neuromorphic cameras that eliminates the need to estimate
physical priors. Our method incorporates a novel event representation to
enhance edge features, enabling the proposed feature-enhancement model to learn
more effectively. Additionally, we introduced Optimal Binarization Threshold
Selection Principle as a guideline for future related work, using the optimal
reconstruction results achieved with threshold optimization as the benchmark.
Our method achieves a 54.6% improvement in reconstruction accuracy compared to
the baseline method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 15 figures, 5 tables, accepted by IEEE International
  Conference on Multimedia & Expo (ICME) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Real-World Test-Time Adaptation: Tri-Net Self-Training with
  Balanced Normalization <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyi Su, Xun Xu, Kui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-Time Adaptation aims to adapt source domain model to testing data at
inference stage with success demonstrated in adapting to unseen corruptions.
However, these attempts may fail under more challenging real-world scenarios.
Existing works mainly consider real-world test-time adaptation under non-i.i.d.
data stream and continual domain shift. In this work, we first complement the
existing real-world TTA protocol with a globally class imbalanced testing set.
We demonstrate that combining all settings together poses new challenges to
existing methods. We argue the failure of state-of-the-art methods is first
caused by indiscriminately adapting normalization layers to imbalanced testing
data. To remedy this shortcoming, we propose a balanced batchnorm layer to swap
out the regular batchnorm at inference stage. The new batchnorm layer is
capable of adapting without biasing towards majority classes. We are further
inspired by the success of self-training (ST) in learning from unlabeled data
and adapt ST for test-time adaptation. However, ST alone is prone to over
adaption which is responsible for the poor performance under continual domain
shift. Hence, we propose to improve self-training under continual domain shift
by regularizing model updates with an anchored loss. The final TTA model,
termed as TRIBE, is built upon a tri-net architecture with balanced batchnorm
layers. We evaluate TRIBE on four datasets representing real-world TTA
settings. TRIBE consistently achieves the state-of-the-art performance across
multiple evaluation protocols. The code is available at
https://github.com/Gorilla-Lab-SCUT/TRIBE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024. 19 pages, 7 figures and 22 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inference-Time Scaling for Flow Models via Stochastic Generation and
  Rollover Budget Forcing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaihoon Kim, Taehoon Yoon, Jisung Hwang, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an inference-time scaling approach for pretrained flow models.
Recently, inference-time scaling has gained significant attention in LLMs and
diffusion models, improving sample quality or better aligning outputs with user
preferences by leveraging additional computation. For diffusion models,
particle sampling has allowed more efficient scaling due to the stochasticity
at intermediate denoising steps. On the contrary, while flow models have gained
popularity as an alternative to diffusion models--offering faster generation
and high-quality outputs in state-of-the-art image and video generative
models--efficient inference-time scaling methods used for diffusion models
cannot be directly applied due to their deterministic generative process. To
enable efficient inference-time scaling for flow models, we propose three key
ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)
Interpolant conversion, broadening the search space and enhancing sample
diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of
computational resources across timesteps to maximize budget utilization. Our
experiments show that SDE-based generation, particularly variance-preserving
(VP) interpolant-based generation, improves the performance of particle
sampling methods for inference-time scaling in flow models. Additionally, we
demonstrate that RBF with VP-SDE achieves the best performance, outperforming
all previous inference-time scaling approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://flow-inference-time-scaling.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning <span class="highlight-title">Visual</span> Contrastive learning models via Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08923v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08923v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirabbas Afzali, Borna Khodabandeh, Ali Rasekh, Mahyar JafariNodeh, Sepehr kazemi, Simon Gottschalk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning models have demonstrated impressive abilities to capture
semantic similarities by aligning representations in the embedding space.
However, their performance can be limited by the quality of the training data
and its inherent biases. While Preference Optimization (PO) methods such as
Reinforcement Learning from Human Feedback (RLHF) and Direct Preference
Optimization (DPO) have been applied to align generative models with human
preferences, their use in contrastive learning has yet to be explored. This
paper introduces a novel method for training contrastive learning models using
different PO methods to break down complex concepts. Our method systematically
aligns model behavior with desired preferences, enhancing performance on the
targeted task. In particular, we focus on enhancing model robustness against
typographic attacks and inductive biases, commonly seen in contrastive
vision-language models like CLIP. Our experiments demonstrate that models
trained using PO outperform standard contrastive learning techniques while
retaining their ability to handle adversarial challenges and maintain accuracy
on other downstream tasks. This makes our method well-suited for tasks
requiring fairness, robustness, and alignment with specific preferences. We
evaluate our method for tackling typographic attacks on images and explore its
ability to disentangle gender concepts and mitigate gender bias, showcasing the
versatility of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor
  Scene Generation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05874v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05874v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifei Yang, Keyang Lu, Chao Zhang, Jiaxing Qi, Hanqi Jiang, Ruifei Ma, Shenglin Yin, Yifan Xu, Mingzhe Xing, Zhen Xiao, Jieyi Long, Guangyao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable 3D scene generation has extensive applications in virtual
reality and interior design, where the generated scenes should exhibit high
levels of realism and controllability in terms of geometry. Scene graphs
provide a suitable data representation that facilitates these applications.
However, current graph-based methods for scene generation are constrained to
text-based inputs and exhibit insufficient adaptability to flexible user
inputs, hindering the ability to precisely control object geometry. To address
this issue, we propose MMGDreamer, a dual-branch diffusion model for scene
generation that incorporates a novel Mixed-Modality Graph, visual enhancement
module, and relation predictor. The mixed-modality graph allows object nodes to
integrate textual and visual modalities, with optional relationships between
nodes. It enhances adaptability to flexible user inputs and enables meticulous
control over the geometry of objects in the generated scenes. The visual
enhancement module enriches the visual fidelity of text-only nodes by
constructing visual representations using text embeddings. Furthermore, our
relation predictor leverages node representations to infer absent relationships
between nodes, resulting in more coherent scene layouts. Extensive experimental
results demonstrate that MMGDreamer exhibits superior control of object
geometry, achieving state-of-the-art scene generation performance. Project
page: https://yangzhifeio.github.io/project/MMGDreamer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025 Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking the Hidden Potential of CLIP in Generalizable Deepfake
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrii Yermakov, Jan Cech, Jiri Matas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the challenge of detecting partially manipulated facial
deepfakes, which involve subtle alterations to specific facial features while
retaining the overall context, posing a greater detection difficulty than fully
synthetic faces. We leverage the Contrastive Language-Image Pre-training (CLIP)
model, specifically its ViT-L/14 visual encoder, to develop a generalizable
detection method that performs robustly across diverse datasets and unknown
forgery techniques with minimal modifications to the original model. The
proposed approach utilizes parameter-efficient fine-tuning (PEFT) techniques,
such as LN-tuning, to adjust a small subset of the model's parameters,
preserving CLIP's pre-trained knowledge and reducing overfitting. A tailored
preprocessing pipeline optimizes the method for facial images, while
regularization strategies, including L2 normalization and metric learning on a
hyperspherical manifold, enhance generalization. Trained on the FaceForensics++
dataset and evaluated in a cross-dataset fashion on Celeb-DF-v2, DFDC, FFIW,
and others, the proposed method achieves competitive detection accuracy
comparable to or outperforming much more complex state-of-the-art techniques.
This work highlights the efficacy of CLIP's visual encoder in facial deepfake
detection and establishes a simple, powerful baseline for future research,
advancing the field of generalizable deepfake detection. The code is available
at: https://github.com/yermandy/deepfake-detection
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and
  Generalizable Point Cloud Analysis <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Sun, Qiuhong Ke, Ming Cheng, Yongcai Wang, Deying Li, Chenhui Gou, Jianfei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a general solution to enable point cloud recognition
models to handle distribution shifts at test time. Unlike prior methods, which
rely heavily on training data (often inaccessible during online inference) and
are limited to recognizing a fixed set of point cloud classes predefined during
training, we explore a more practical and challenging scenario: adapting the
model solely based on online test data to recognize both previously seen
classes and novel, unseen classes at test time. To this end, we develop
\textbf{Point-Cache}, a hierarchical cache model that captures essential clues
of online test samples, particularly focusing on the global structure of point
clouds and their local-part details. Point-Cache, which serves as a rich 3D
knowledge base, is dynamically managed to prioritize the inclusion of
high-quality samples. Designed as a plug-and-play module, our method can be
flexibly integrated into large multimodal 3D models to support open-vocabulary
point cloud recognition. Notably, our solution operates with efficiency
comparable to zero-shot inference, as it is entirely training-free. Point-Cache
demonstrates substantial gains across 8 challenging benchmarks and 4
representative large 3D models, highlighting its effectiveness. Code is
available at https://github.com/auniquesun/Point-Cache.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARVEL-40M+: Multi-Level <span class="highlight-title">Visual</span> Elaboration for High-Fidelity Text-to-3D
  Content Creation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sankalp Sinha, Mohammad Sadil Khan, Muhammad Usama, Shino Sam, Didier Stricker, Sk Aziz Ali, Muhammad Zeshan Afzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating high-fidelity 3D content from text prompts remains a significant
challenge in computer vision due to the limited size, diversity, and annotation
depth of the existing datasets. To address this, we introduce MARVEL-40M+, an
extensive dataset with 40 million text annotations for over 8.9 million 3D
assets aggregated from seven major 3D datasets. Our contribution is a novel
multi-stage annotation pipeline that integrates open-source pretrained
multi-view VLMs and LLMs to automatically produce multi-level descriptions,
ranging from detailed (150-200 words) to concise semantic tags (10-20 words).
This structure supports both fine-grained 3D reconstruction and rapid
prototyping. Furthermore, we incorporate human metadata from source datasets
into our annotation pipeline to add domain-specific information in our
annotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D,
a two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our
annotations and use a pretrained image-to-3D network to generate 3D textured
meshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly
outperforms existing datasets in annotation quality and linguistic diversity,
achieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators. Project
page is available at https://sankalpsinha-cmos.github.io/MARVEL/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuseTalk: Real-Time High-Fidelity Video Dubbing via Spatio-Temporal
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10122v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10122v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Zhang, Zhizhou Zhong, Minhao Liu, Zhaokang Chen, Bin Wu, Yubin Zeng, Chao Zhan, Yingjie He, Junxin Huang, Wenjiang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time video dubbing that preserves identity consistency while achieving
accurate lip synchronization remains a critical challenge. Existing approaches
face a trilemma: diffusion-based methods achieve high visual fidelity but
suffer from prohibitive computational costs, while GAN-based solutions
sacrifice lip-sync accuracy or dental details for real-time performance. We
present MuseTalk, a novel two-stage training framework that resolves this
trade-off through latent space optimization and spatio-temporal data sampling
strategy. Our key innovations include: (1) During the Facial Abstract
Pretraining stage, we propose Informative Frame Sampling to temporally align
reference-source pose pairs, eliminating redundant feature interference while
preserving identity cues. (2) In the Lip-Sync Adversarial Finetuning stage, we
employ Dynamic Margin Sampling to spatially select the most suitable
lip-movement-promoting regions, balancing audio-visual synchronization and
dental clarity. (3) MuseTalk establishes an effective audio-visual feature
fusion framework in the latent space, delivering 30 FPS output at 256*256
resolution on an NVIDIA V100 GPU. Extensive experiments demonstrate that
MuseTalk outperforms state-of-the-art methods in visual fidelity while
achieving comparable lip-sync accuracy. %The codes and models will be made
publicly available upon acceptance. The code is made available at
\href{https://github.com/TMElyralab/MuseTalk}{https://github.com/TMElyralab/MuseTalk}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PHT-CAD: Efficient CAD Parametric Primitive Analysis with Progressive
  Hierarchical Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18147v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18147v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Niu, Yuwen Chen, Haiyang Yu, Zhuofan Chen, Xianghui Que, Bin Li, Xiangyang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing,
yet 2D Parametric Primitive Analysis (PPA) remains underexplored due to two key
challenges: structural constraint reasoning and advanced semantic
understanding. To tackle these challenges, we first propose an Efficient Hybrid
Parametrization (EHP) for better representing 2D engineering drawings. EHP
contains four types of atomic component i.e., point, line, circle, and arc).
Additionally, we propose PHT-CAD, a novel 2D PPA framework that harnesses the
modality alignment and reasoning capabilities of Vision-Language Models (VLMs)
for precise engineering drawing analysis. In PHT-CAD, we introduce four
dedicated regression heads to predict corresponding atomic components. To train
PHT-CAD, a three-stage training paradigm Progressive Hierarchical Tuning (PHT)
is proposed to progressively enhance PHT-CAD's capability to perceive
individual primitives, infer structural constraints, and align annotation
layers with their corresponding geometric representations. Considering that
existing datasets lack complete annotation layers and real-world engineering
drawings, we introduce ParaCAD, the first large-scale benchmark that explicitly
integrates both the geometric and annotation layers. ParaCAD comprises over 10
million annotated drawings for training and 3,000 real-world industrial
drawings with complex topological structures and physical constraints for test.
Extensive experiments demonstrate the effectiveness of PHT-CAD and highlight
the practical significance of ParaCAD in advancing 2D PPA research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEIM: DETR with Improved Matching for Fast Convergence <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04234v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04234v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihua Huang, Zhichao Lu, Xiaodong Cun, Yongjun Yu, Xiao Zhou, Xi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DEIM, an innovative and efficient training framework designed to
accelerate convergence in real-time object detection with Transformer-based
architectures (DETR). To mitigate the sparse supervision inherent in one-to-one
(O2O) matching in DETR models, DEIM employs a Dense O2O matching strategy. This
approach increases the number of positive samples per image by incorporating
additional targets, using standard data augmentation techniques. While Dense
O2O matching speeds up convergence, it also introduces numerous low-quality
matches that could affect performance. To address this, we propose the
Matchability-Aware Loss (MAL), a novel loss function that optimizes matches
across various quality levels, enhancing the effectiveness of Dense O2O.
Extensive experiments on the COCO dataset validate the efficacy of DEIM. When
integrated with RT-DETR and D-FINE, it consistently boosts performance while
reducing training time by 50%. Notably, paired with RT-DETRv2, DEIM achieves
53.2% AP in a single day of training on an NVIDIA 4090 GPU. Additionally,
DEIM-trained real-time models outperform leading real-time object detectors,
with DEIM-D-FINE-L and DEIM-D-FINE-X achieving 54.7% and 56.5% AP at 124 and 78
FPS on an NVIDIA T4 GPU, respectively, without the need for additional data. We
believe DEIM sets a new baseline for advancements in real-time object
detection. Our code and pre-trained models are available at
https://github.com/ShihuaHuang95/DEIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupling Fine Detail and Global Geometry for Compressed Depth Map
  Super-Resolution <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03239v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03239v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Zheng, Wencheng Han, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering high-quality depth maps from compressed sources has gained
significant attention due to the limitations of consumer-grade depth cameras
and the bandwidth restrictions during data transmission. However, current
methods still suffer from two challenges. First, bit-depth compression produces
a uniform depth representation in regions with subtle variations, hindering the
recovery of detailed information. Second, densely distributed random noise
reduces the accuracy of estimating the global geometric structure of the scene.
To address these challenges, we propose a novel framework, termed
geometry-decoupled network (GDNet), for compressed depth map super-resolution
that decouples the high-quality depth map reconstruction process by handling
global and detailed geometric features separately. To be specific, we propose
the fine geometry detail encoder (FGDE), which is designed to aggregate fine
geometry details in high-resolution low-level image features while
simultaneously enriching them with complementary information from
low-resolution context-level image features. In addition, we develop the global
geometry encoder (GGE) that aims at suppressing noise and extracting global
geometric information effectively via constructing compact feature
representation in a low-rank space. We conduct experiments on multiple
benchmark datasets, demonstrating that our GDNet significantly outperforms
current methods in terms of geometric consistency and detail recovery. In the
ECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st
place award. Our codes are available at: https://github.com/Ian0926/GDNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025 & The 1st place award for the ECCV 2024 AIM
  Compressed Depth Upsampling Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NLPrompt: Noise-Label Prompt Learning for <span class="highlight-title">Vision</span>-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bikang Pan, Qun Li, Xiaoying Tang, Wei Huang, Zhen Fang, Feng Liu, Jingya Wang, Jingyi Yu, Ye Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of vision-language foundation models, such as CLIP, has
revolutionized image-text representation, enabling a broad range of
applications via prompt learning. Despite its promise, real-world datasets
often contain noisy labels that can degrade prompt learning performance. In
this paper, we demonstrate that using mean absolute error (MAE) loss in prompt
learning, named PromptMAE, significantly enhances robustness against noisy
labels while maintaining high accuracy. Though MAE is straightforward and
recognized for its robustness, it is rarely used in noisy-label learning due to
its slow convergence and poor performance outside prompt learning scenarios. To
elucidate the robustness of PromptMAE, we leverage feature learning theory to
show that MAE can suppress the influence of noisy samples, thereby improving
the signal-to-noise ratio and enhancing overall robustness. Additionally, we
introduce PromptOT, a prompt-based optimal transport data purification method
to enhance the robustness further. PromptOT employs text features in
vision-language models as prototypes to construct an optimal transportation
matrix. This matrix effectively partitions datasets into clean and noisy
subsets, allowing for the application of cross-entropy loss to the clean subset
and MAE loss to the noisy subset. Our Noise-Label Prompt Learning method, named
NLPrompt, offers a simple and efficient approach that leverages the expressive
representations and precise alignment capabilities of vision-language models
for robust prompt learning. We validate NLPrompt through extensive experiments
across various noise settings, demonstrating significant performance
improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MozzaVID: Mozzarella Volumetric Image <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pawel Tomasz Pieta, Peter Winkel Rasmussen, Anders Bjorholm Dahl, Jeppe Revall Frisvad, Siavash Arjomand Bigdeli, Carsten Gundlach, Anders Nymark Christensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influenced by the complexity of volumetric imaging, there is a shortage of
established datasets useful for benchmarking volumetric deep-learning models.
As a consequence, new and existing models are not easily comparable, limiting
the development of architectures optimized specifically for volumetric data. To
counteract this trend, we introduce MozzaVID - a large, clean, and versatile
volumetric classification dataset. Our dataset contains X-ray computed
tomography (CT) images of mozzarella microstructure and enables the
classification of 25 cheese types and 149 cheese samples. We provide data in
three different resolutions, resulting in three dataset instances containing
from 591 to 37,824 images. While being general-purpose, the dataset also
facilitates investigating mozzarella structure properties. The structure of
food directly affects its functional properties and thus its consumption
experience. Understanding food structure helps tune the production and
mimicking it enables sustainable alternatives to animal-derived food products.
The complex and disordered nature of food structures brings a unique challenge,
where a choice of appropriate imaging method, scale, and sample size is not
trivial. With this dataset we aim to address these complexities, contributing
to more robust structural analysis models. The dataset can be downloaded from:
https://archive.compute.dtu.dk/files/public/projects/MozzaVID/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ONER: Online Experience Replay for Incremental Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03907v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03907v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Jin, Jiahui Zhu, Guodong Wang, Shiwei Li, Jinjin Zhang, Xinyue Liu, Qingjie Liu, Yunhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental anomaly detection aims to sequentially identify defects in
industrial product lines but suffers from catastrophic forgetting, primarily
due to knowledge overwriting during parameter updates and feature conflicts
between tasks. In this work, We propose ONER (ONline Experience Replay), an
end-to-end framework that addresses these issues by synergistically integrating
two types of experience: (1) decomposed prompts, which dynamically generate
image-conditioned prompts from reusable modules to retain prior knowledge thus
prevent knowledge overwriting, and (2) semantic prototypes, which enforce
separability in latent feature spaces at pixel and image levels to mitigate
cross-task feature conflicts. Extensive experiments demonstrate the superiority
of ONER, achieving state-of-the-art performance with +4.4% Pixel AUROC and
+28.3% Pixel AUPR improvements on the MVTec AD dataset over prior methods.
Remarkably, ONER achieves this with only 0.019M parameters and 5 training
epochs per task, confirming its efficiency and stability for real-world
industrial deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiTCtrl: Exploring Attention Control in Multi-Modal <span class="highlight-title">Diffusion</span>
  Transformer for Tuning-Free Multi-Prompt Longer Video Generation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sora-like video generation models have achieved remarkable progress with a
Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current
video generation models predominantly focus on single-prompt, struggling to
generate coherent scenes with multiple sequential prompts that better reflect
real-world dynamic scenarios. While some pioneering works have explored
multi-prompt video generation, they face significant challenges including
strict training data requirements, weak prompt following, and unnatural
transitions. To address these problems, we propose DiTCtrl, a training-free
multi-prompt video generation method under MM-DiT architectures for the first
time. Our key idea is to take the multi-prompt video generation task as
temporal video editing with smooth transitions. To achieve this goal, we first
analyze MM-DiT's attention mechanism, finding that the 3D full attention
behaves similarly to that of the cross/self-attention blocks in the UNet-like
diffusion models, enabling mask-guided precise semantic control across
different prompts with attention sharing for multi-prompt video generation.
Based on our careful design, the video generated by DiTCtrl achieves smooth
transitions and consistent object motion given multiple sequential prompts
without additional training. Besides, we also present MPVBench, a new benchmark
specially designed for multi-prompt video generation to evaluate the
performance of multi-prompt generation. Extensive experiments demonstrate that
our method achieves state-of-the-art performance without additional training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025; 21 pages, 23 figures, Project page:
  https://onevfall.github.io/project_page/ditctrl ; GitHub repository:
  https://github.com/TencentARC/DiTCtrl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Oasis: One Image is All You Need for Multimodal Instruction Data
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08741v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08741v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Letian Zhang, Quan Cui, Bingchen Zhao, Cheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of multi-modal large language models (MLLMs) has been largely
attributed to the large-scale training data. However, the training data of many
MLLMs is unavailable due to privacy concerns. The expensive and labor-intensive
process of collecting multi-modal data further exacerbates the problem. Is it
possible to synthesize multi-modal training data automatically without
compromising diversity and quality? In this paper, we propose a new method,
Oasis, to synthesize high-quality multi-modal data with only images. Oasis
breaks through traditional methods by prompting only images to the MLLMs, thus
extending the data diversity by a large margin. Our method features a delicate
quality control method which ensures the data quality. We collected over 500k
data and conducted incremental experiments on LLaVA-NeXT. Extensive experiments
demonstrate that our method can significantly improve the performance of MLLMs.
The image-based synthesis also allows us to focus on the specific-domain
ability of MLLMs. Code and dataset are publicly available at
https://github.com/Letian2003/MM_INF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Referring Video Object Segmentation via Language-aligned Track Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongchan Kim, Woojeong Jin, Sangbeom Lim, Heeji Yoon, Hyunwook Choi, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring video object segmentation (RVOS) requires tracking and segmenting
an object throughout a video according to a given natural language expression,
demanding both complex motion understanding and the alignment of visual
representations with language descriptions. Given these challenges, the
recently proposed Segment Anything Model 2 (SAM2) emerges as a potential
candidate due to its ability to generate coherent segmentation mask tracks
across video frames, and provide an inherent spatio-temporal objectness in its
object token representations. In this paper, we introduce SOLA (Selection by
Object Language Alignment), a novel framework that leverages SAM2 object tokens
as compact video-level object representations, which are aligned with language
features through a lightweight track selection module. To effectively
facilitate this alignment, we propose an IoU-based pseudo-labeling strategy,
which bridges the modality gap between SAM2 representations with language
features. Extensive experiments show that SOLA achieves state-of-the-art
performance on the MeViS dataset and demonstrate that SOLA offers an effective
solution for RVOS. Our project page is available at:
https://cvlab-kaist.github.io/SOLA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is available at https://cvlab-kaist.github.io/SOLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified
  3D Perception <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07746v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07746v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Wolters, Johannes Gilg, Torben Teepe, Fabian Herzog, Anouar Laouichi, Martin Hofmann, Gerhard Rigoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-cost, vision-centric 3D perception systems for autonomous driving have
made significant progress in recent years, narrowing the gap to expensive
LiDAR-based methods. The primary challenge in becoming a fully reliable
alternative lies in robust depth prediction capabilities, as camera-based
systems struggle with long detection ranges and adverse lighting and weather
conditions. In this work, we introduce HyDRa, a novel camera-radar fusion
architecture for diverse 3D perception tasks. Building upon the principles of
dense BEV (Bird's Eye View)-based architectures, HyDRa introduces a hybrid
fusion approach to combine the strengths of complementary camera and radar
features in two distinct representation spaces. Our Height Association
Transformer module leverages radar features already in the perspective view to
produce more robust and accurate depth predictions. In the BEV, we refine the
initial sparse representation by a Radar-weighted Depth Consistency. HyDRa
achieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and
58.4 AMOTA (+1.5) on the public nuScenes dataset. Moreover, our new
semantically rich and spatially accurate BEV features can be directly converted
into a powerful occupancy representation, beating all previous camera-based
methods on the Occ3D benchmark by an impressive 3.7 mIoU. Code and models are
available at https://github.com/phi-wol/hydra.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARFlow: Human Action-Reaction Flow Matching with Physical Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Jiang, Jingya Wang, Haotao Lu, Kaiyang Ji, Baoxiong Jia, Siyuan Huang, Ye Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human action-reaction synthesis, a fundamental challenge in modeling causal
human interactions, plays a critical role in applications ranging from virtual
reality to social robotics. While diffusion-based models have demonstrated
promising performance, they exhibit two key limitations for interaction
synthesis: reliance on complex noise-to-reaction generators with intricate
conditional mechanisms, and frequent physical violations in generated motions.
To address these issues, we propose Action-Reaction Flow Matching (ARFlow), a
novel framework that establishes direct action-to-reaction mappings,
eliminating the need for complex conditional mechanisms. Our approach
introduces two key innovations: an x1-prediction method that directly outputs
human motions instead of velocity fields, enabling explicit constraint
enforcement; and a training-free, gradient-based physical guidance mechanism
that effectively prevents body penetration artifacts during sampling. Extensive
experiments on NTU120 and Chi3D datasets demonstrate that ARFlow not only
outperforms existing methods in terms of Fr\'echet Inception Distance and
motion diversity but also significantly reduces body collisions, as measured by
our new Intersection Volume and Intersection Frequency metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://arflow2025.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CogVideoX: Text-to-Video <span class="highlight-title">Diffusion</span> Models with An Expert Transformer <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CogVideoX, a large-scale text-to-video generation model based on
diffusion transformer, which can generate 10-second continuous videos aligned
with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360
pixels. Previous video generation models often had limited movement and short
durations, and is difficult to generate videos with coherent narratives based
on text. We propose several designs to address these issues. First, we propose
a 3D Variational Autoencoder (VAE) to compress videos along both spatial and
temporal dimensions, to improve both compression rate and video fidelity.
Second, to improve the text-video alignment, we propose an expert transformer
with the expert adaptive LayerNorm to facilitate the deep fusion between the
two modalities. Third, by employing a progressive training and multi-resolution
frame pack technique, CogVideoX is adept at producing coherent, long-duration,
different shape videos characterized by significant motions. In addition, we
develop an effective text-video data processing pipeline that includes various
data preprocessing strategies and a video captioning method, greatly
contributing to the generation quality and semantic alignment. Results show
that CogVideoX demonstrates state-of-the-art performance across both multiple
machine metrics and human evaluations. The model weight of both 3D Causal VAE,
Video caption model and CogVideoX are publicly available at
https://github.com/THUDM/CogVideo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TopoBDA: Towards Bezier Deformable Attention for Road Topology
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18951v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18951v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammet Esat Kalfaoglu, Halil Ibrahim Ozturk, Ozsel Kilinc, Alptekin Temizel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding road topology is crucial for autonomous driving. This paper
introduces TopoBDA (Topology with Bezier Deformable Attention), a novel
approach that enhances road topology comprehension by leveraging Bezier
Deformable Attention (BDA). TopoBDA processes multi-camera 360-degree imagery
to generate Bird's Eye View (BEV) features, which are refined through a
transformer decoder employing BDA. BDA utilizes Bezier control points to drive
the deformable attention mechanism, improving the detection and representation
of elongated and thin polyline structures, such as lane centerlines.
Additionally, TopoBDA integrates two auxiliary components: an instance mask
formulation loss and a one-to-many set prediction loss strategy, to further
refine centerline detection and enhance road topology understanding.
Experimental evaluations on the OpenLane-V2 dataset demonstrate that TopoBDA
outperforms existing methods, achieving state-of-the-art results in centerline
detection and topology reasoning. TopoBDA also achieves the best results on the
OpenLane-V1 dataset in 3D lane detection. Further experiments on integrating
multi-modal data -- such as LiDAR, radar, and SDMap -- show that multimodal
inputs can further enhance performance in road topology understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HumanDiT: Pose-Guided <span class="highlight-title">Diffusion</span> Transformer for Long-form Human Motion
  Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04847v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04847v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijun Gan, Yi Ren, Chen Zhang, Zhenhui Ye, Pan Xie, Xiang Yin, Zehuan Yuan, Bingyue Peng, Jianke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion video generation has advanced significantly, while existing
methods still struggle with accurately rendering detailed body parts like hands
and faces, especially in long sequences and intricate motions. Current
approaches also rely on fixed resolution and struggle to maintain visual
consistency. To address these limitations, we propose HumanDiT, a pose-guided
Diffusion Transformer (DiT)-based framework trained on a large and wild dataset
containing 14,000 hours of high-quality video to produce high-fidelity videos
with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,
supports numerous video resolutions and variable sequence lengths, facilitating
learning for long-sequence video generation; (ii) we introduce a prefix-latent
reference strategy to maintain personalized characteristics across extended
sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to
generate subsequent pose sequences, facilitating video continuation from static
images or existing videos. It also utilizes a Pose Adapter to enable pose
transfer with given sequences. Extensive experiments demonstrate its superior
performance in generating long-form, pose-accurate videos across diverse
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://agnjason.github.io/HumanDiT-page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied
  Instruction Following <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suyeon Shin, Sujin jeon, Junghyun Kim, Gi-Cheon Kang, Byoung-Tak Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied Instruction Following (EIF) is the task of executing natural
language instructions by navigating and interacting with objects in interactive
environments. A key challenge in EIF is compositional task planning, typically
addressed through supervised learning or few-shot in-context learning with
labeled data. To this end, we introduce the Socratic Planner, a self-QA-based
zero-shot planning method that infers an appropriate plan without any further
training. The Socratic Planner first facilitates self-questioning and answering
by the Large Language Model (LLM), which in turn helps generate a sequence of
subgoals. While executing the subgoals, an embodied agent may encounter
unexpected situations, such as unforeseen obstacles. The Socratic Planner then
adjusts plans based on dense visual feedback through a visually-grounded
re-planning mechanism. Experiments demonstrate the effectiveness of the
Socratic Planner, outperforming current state-of-the-art planning models on the
ALFRED benchmark across all metrics, particularly excelling in long-horizon
tasks that demand complex inference. We further demonstrate its real-world
applicability through deployment on a physical robot for long-horizon tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, published to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Framework to Rule Them All: Unifying RL-Based and RL-Free Methods in
  RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we primarily examine a variety of RL-based and RL-free
methods designed to address Reinforcement Learning from Human Feedback (RLHF)
and Large Reasoning Models (LRMs). We begin with a concise overview of the
typical steps involved in RLHF and LRMs. Next, we reinterpret several RL-based
and RL-free algorithms through the perspective of neural structured bandit
prediction, providing a clear conceptual framework that uncovers a deeper
connection between these seemingly distinct approaches. Following this, we
briefly review some core principles of reinforcement learning, drawing
attention to an often-overlooked aspect in existing RLHF studies. This leads to
a detailed derivation of the standard RLHF objective within a full RL context,
demonstrating its equivalence to neural structured bandit prediction. Finally,
by reinvestigating the principles behind Proximal Policy Optimization (PPO), we
pinpoint areas needing adjustment, which culminates in the introduction of the
Generalized Reinforce Optimization (GRO) framework, seamlessly integrating
RL-based and RL-free methods in RLHF. We look forward to the community's
efforts to empirically validate GRO and invite constructive feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMRL: Multi-Modal Representation Learning for <span class="highlight-title">Vision</span>-Language Models <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuncheng Guo, Xiaodong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pre-trained Vision-Language Models (VLMs) have become essential
for transfer learning across diverse tasks. However, adapting these models with
limited few-shot data often leads to overfitting, diminishing their performance
on new tasks. To tackle this issue, we propose a novel Multi-Modal
Representation Learning (MMRL) framework that introduces a shared, learnable,
and modality-agnostic representation space. MMRL projects the space tokens to
text and image representation tokens, facilitating more effective multi-modal
interactions. Unlike previous approaches that solely optimize class token
features, MMRL integrates representation tokens at higher layers of the
encoders--where dataset-specific features are more prominent--while preserving
generalized knowledge in the lower layers. During training, both representation
and class features are optimized, with trainable projection layer applied to
the representation tokens, whereas the class token projection layer remains
frozen to retain pre-trained knowledge. Furthermore, a regularization term is
introduced to align the class features and text features with the zero-shot
features from the frozen VLM, thereby safeguarding the model's generalization
capacity. For inference, a decoupling strategy is employed, wherein both
representation and class features are utilized for base classes, while only the
class features, which retain more generalized knowledge, are used for new
tasks. Extensive experiments across 15 datasets demonstrate that MMRL
outperforms state-of-the-art methods, achieving a balanced trade-off between
task-specific adaptation and generalization. Code is available at
https://github.com/yunncheng/MMRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robo</span>Spatial: Teaching Spatial Understanding to 2D and 3D <span class="highlight-title">Vision</span>-Language
  Models for <span class="highlight-title">Robo</span>tics <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, Stan Birchfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial understanding is a crucial capability that enables robots to perceive
their surroundings, reason about their environment, and interact with it
meaningfully. In modern robotics, these capabilities are increasingly provided
by vision-language models. However, these models face significant challenges in
spatial reasoning tasks, as their training data are based on general-purpose
image datasets that often lack sophisticated spatial understanding. For
example, datasets frequently do not capture reference frame comprehension, yet
effective spatial reasoning requires understanding whether to reason from ego-,
world-, or object-centric perspectives. To address this issue, we introduce
RoboSpatial, a large-scale dataset for spatial understanding in robotics. It
consists of real indoor and tabletop scenes, captured as 3D scans and
egocentric images, and annotated with rich spatial information relevant to
robotics. The dataset includes 1M images, 5k 3D scans, and 3M annotated spatial
relationships, and the pairing of 2D egocentric images with 3D scans makes it
both 2D- and 3D- ready. Our experiments show that models trained with
RoboSpatial outperform baselines on downstream tasks such as spatial affordance
prediction, spatial relationship prediction, and robot manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for
  Zero-shot Object Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linqing Zhong, Chen Gao, Zihan Ding, Yue Liao, Huimin Ma, Shifeng Zhang, Xu Zhou, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find
a previously unseen object by navigating in unfamiliar environments. Such a
goal-oriented exploration heavily relies on the ability to perceive,
understand, and reason based on the spatial information of the environment.
However, current LLM-based approaches convert visual observations to language
descriptions and reason in the linguistic space, leading to the loss of spatial
information. In this paper, we introduce TopV-Nav, an MLLM-based method that
directly reasons on the top-view map with sufficient spatial information. To
fully unlock the MLLM's spatial reasoning potential in top-view perspective, we
propose the Adaptive Visual Prompt Generation (AVPG) method to adaptively
construct semantically-rich top-view map. It enables the agent to directly
utilize spatial information contained in the top-view map to conduct thorough
reasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to
dynamically zoom top-view map at preferred scales, enhancing local fine-grained
reasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to
predict and to utilize target locations, facilitating global and human-like
exploration. Experiments on MP3D and HM3D datasets demonstrate the superiority
of our TopV-Nav.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Grained Domain Generalization with Feature Structuralization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09166v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09166v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Yu, Dongyue Chen, Qilong Wang, Qinghua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-grained domain generalization (FGDG) is a more challenging task than
traditional DG tasks due to its small inter-class variations and relatively
large intra-class disparities. When domain distribution changes, the
vulnerability of subtle features leads to a severe deterioration in model
performance. Nevertheless, humans inherently demonstrate the capacity for
generalizing to out-of-distribution data, leveraging structured
multi-granularity knowledge that emerges from discerning the commonality and
specificity within categories. Likewise, we propose a Feature Structuralized
Domain Generalization (FSDG) model, wherein features experience
structuralization into common, specific, and confounding segments, harmoniously
aligned with their relevant semantic concepts, to elevate performance in FGDG.
Specifically, feature structuralization (FS) is accomplished through joint
optimization of five constraints: a decorrelation function applied to
disentangled segments, three constraints ensuring common feature consistency
and specific feature distinctiveness, and a prediction calibration term. By
imposing these stipulations, FSDG is prompted to disentangle and align features
based on multi-granularity knowledge, facilitating robust subtle distinctions
among categories. Extensive experimentation on three benchmarks consistently
validates the superiority of FSDG over state-of-the-art counterparts, with an
average improvement of 6.2% in FGDG performance. Beyond that, the
explainability analysis on explicit concept matching intensity between the
shared concepts among categories and the model channels, along with experiments
on various mainstream model architectures, substantiates the validity of FS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OverLoCK: An <span class="highlight-title">Overview</span>-first-Look-Closely-next ConvNet with
  Context-Mixing Dynamic Kernels <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Lou, Yizhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Top-down attention plays a crucial role in the human vision system, wherein
the brain initially obtains a rough overview of a scene to discover salient
cues (i.e., overview first), followed by a more careful finer-grained
examination (i.e., look closely next). However, modern ConvNets remain confined
to a pyramid structure that successively downsamples the feature map for
receptive field expansion, neglecting this crucial biomimetic principle. We
present OverLoCK, the first pure ConvNet backbone architecture that explicitly
incorporates a top-down attention mechanism. Unlike pyramid backbone networks,
our design features a branched architecture with three synergistic
sub-networks: 1) a Base-Net that encodes low/mid-level features; 2) a
lightweight Overview-Net that generates dynamic top-down attention through
coarse global context modeling (i.e., overview first); and 3) a robust
Focus-Net that performs finer-grained perception guided by top-down attention
(i.e., look closely next). To fully unleash the power of top-down attention, we
further propose a novel context-mixing dynamic convolution (ContMix) that
effectively models long-range dependencies while preserving inherent local
inductive biases even when the input resolution increases, addressing critical
limitations in existing convolutions. Our OverLoCK exhibits a notable
performance improvement over existing methods. For instance, OverLoCK-T
achieves a Top-1 accuracy of 84.2%, significantly surpassing ConvNeXt-B while
using only around one-third of the FLOPs/parameters. On object detection, our
OverLoCK-S clearly surpasses MogaNet-B by 1% in AP^b. On semantic segmentation,
our OverLoCK-T remarkably improves UniRepLKNet-T by 1.7% in mIoU. Code is
publicly available at https://rb.gy/wit4jh.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via
  Frequency-Decoupled Alignment and Degradation-Robust Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pihai Sun, Junjun Jiang, Yuanqi Yao, Youyu Chen, Wenbo Zhao, Kui Jiang, Xianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-event joint depth estimation methods leverage complementary modalities
for robust perception, yet face challenges in generalizability stemming from
two factors: 1) limited annotated image-event-depth datasets causing
insufficient cross-modal supervision, and 2) inherent frequency mismatches
between static images and dynamic event streams with distinct spatiotemporal
patterns, leading to ineffective feature fusion. To address this dual
challenge, we propose Frequency-decoupled Unified Self-supervised Encoder
(FUSE) with two synergistic components: The Parameter-efficient Self-supervised
Transfer (PST) establishes cross-modal knowledge transfer through latent space
alignment with image foundation models, effectively mitigating data scarcity by
enabling joint encoding without depth ground truth. Complementing this, we
propose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple
high-frequency edge features from low-frequency structural components,
resolving modality-specific frequency mismatches through physics-aware fusion.
This combined approach enables FUSE to construct a universal image-event
encoder that only requires lightweight decoder adaptation for target datasets.
Extensive experiments demonstrate state-of-the-art performance with 14% and
24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework
exhibits remarkable zero-shot adaptability to challenging scenarios including
extreme lighting and motion blur, significantly advancing real-world deployment
capabilities. The source code for our method is publicly available at:
https://github.com/sunpihai-up/FUSE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAWN: Dynamic Frame Avatar with Non-autoregressive <span class="highlight-title">Diffusion</span> Framework
  for Talking Head Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13726v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13726v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanbo Cheng, Limin Lin, Chenyu Liu, Pengcheng Xia, Pengfei Hu, Jiefeng Ma, Jun Du, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking head generation intends to produce vivid and realistic talking head
videos from a single portrait and speech audio clip. Although significant
progress has been made in diffusion-based talking head generation, almost all
methods rely on autoregressive strategies, which suffer from limited context
utilization beyond the current generation step, error accumulation, and slower
generation speed. To address these challenges, we present DAWN (Dynamic frame
Avatar With Non-autoregressive diffusion), a framework that enables all-at-once
generation of dynamic-length video sequences. Specifically, it consists of two
main components: (1) audio-driven holistic facial dynamics generation in the
latent motion space, and (2) audio-driven head pose and blink generation.
Extensive experiments demonstrate that our method generates authentic and vivid
videos with precise lip motions, and natural pose/blink movements.
Additionally, with a high generation speed, DAWN possesses strong extrapolation
capabilities, ensuring the stable production of high-quality long videos. These
results highlight the considerable promise and potential impact of DAWN in the
field of talking head video generation. Furthermore, we hope that DAWN sparks
further exploration of non-autoregressive approaches in diffusion models. Our
code will be publicly available at https://github.com/Hanbo-Cheng/DAWN-pytorch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stealthy Backdoor Attack in Self-Supervised Learning <span class="highlight-title">Vision</span> Encoders for
  Large <span class="highlight-title">Vision</span> Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18290v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18290v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyi Liu, Huan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) vision encoders learn high-quality image
representations and thus have become a vital part of developing vision modality
of large vision language models (LVLMs). Due to the high cost of training such
encoders, pre-trained encoders are widely shared and deployed into many LVLMs,
which are security-critical or bear societal significance. Under this practical
scenario, we reveal a new backdoor threat that significant visual
hallucinations can be induced into these LVLMs by merely compromising vision
encoders. Because of the sharing and reuse of these encoders, many downstream
LVLMs may inherit backdoor behaviors from encoders, leading to widespread
backdoors. In this work, we propose BadVision, the first method to exploit this
vulnerability in SSL vision encoders for LVLMs with novel trigger optimization
and backdoor learning techniques. We evaluate BadVision on two types of SSL
encoders and LVLMs across eight benchmarks. We show that BadVision effectively
drives the LVLMs to attacker-chosen hallucination with over 99% attack success
rate, causing a 77.6% relative visual understanding error while maintaining the
stealthiness. SoTA backdoor detection methods cannot detect our attack
effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hi-ALPS -- An Experimental Robustness Quantification of Six <span class="highlight-title">LiDAR</span>-based
  Object Detection Systems for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Arzberger, Ramin Tavakoli Kolagari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Light Detection and Ranging (LiDAR) is an essential sensor technology for
autonomous driving as it can capture high-resolution 3D data. As 3D object
detection systems (OD) can interpret such point cloud data, they play a key
role in the driving decisions of autonomous vehicles. Consequently, such 3D OD
must be robust against all types of perturbations and must therefore be
extensively tested. One approach is the use of adversarial examples, which are
small, sometimes sophisticated perturbations in the input data that change,
i.e., falsify, the prediction of the OD. These perturbations are carefully
designed based on the weaknesses of the OD. The robustness of the OD cannot be
quantified with adversarial examples in general, because if the OD is
vulnerable to a given attack, it is unclear whether this is due to the
robustness of the OD or whether the attack algorithm produces particularly
strong adversarial examples. The contribution of this work is Hi-ALPS --
Hierarchical Adversarial-example-based LiDAR Perturbation Level System, where
higher robustness of the OD is required to withstand the perturbations as the
perturbation levels increase. In doing so, the Hi-ALPS levels successively
implement a heuristic followed by established adversarial example approaches.
In a series of comprehensive experiments using Hi-ALPS, we quantify the
robustness of six state-of-the-art 3D OD under different types of
perturbations. The results of the experiments show that none of the OD is
robust against all Hi-ALPS levels; an important factor for the ranking is that
human observers can still correctly recognize the perturbed objects, as the
respective perturbations are small. To increase the robustness of the OD, we
discuss the applicability of state-of-the-art countermeasures. In addition, we
derive further suggestions for countermeasures based on our experimental
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on
  Boundary-Adaptive Gaussian Splitting <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Zhang, Junjun Jiang, Youyu Chen, Kui Jiang, Xianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate object segmentation is crucial for high-quality scene understanding
in the 3D vision domain. However, 3D segmentation based on 3D Gaussian
Splatting (3DGS) struggles with accurately delineating object boundaries, as
Gaussian primitives often span across object edges due to their inherent volume
and the lack of semantic guidance during training. In order to tackle these
challenges, we introduce Clear Object Boundaries for 3DGS Segmentation
(COB-GS), which aims to improve segmentation accuracy by clearly delineating
blurry boundaries of interwoven Gaussian primitives within the scene. Unlike
existing approaches that remove ambiguous Gaussians and sacrifice visual
quality, COB-GS, as a 3DGS refinement method, jointly optimizes semantic and
visual information, allowing the two different levels to cooperate with each
other effectively. Specifically, for the semantic guidance, we introduce a
boundary-adaptive Gaussian splitting technique that leverages semantic gradient
statistics to identify and split ambiguous Gaussians, aligning them closely
with object boundaries. For the visual optimization, we rectify the degraded
suboptimal texture of the 3DGS scene, particularly along the refined boundary
structures. Experimental results show that COB-GS substantially improves
segmentation accuracy and robustness against inaccurate masks from pre-trained
model, yielding clear boundaries while preserving high visual quality. Code is
available at https://github.com/ZestfulJX/COB-GS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In the Blink of an Eye: Instant Game Map Editing using a Generative-AI
  Smart Brush 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vitaly Gnatyuk, Valeriia Koriukina, Ilya Levoshevich, Pavel Nurminskiy, Guenter Wallner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With video games steadily increasing in complexity, automated generation of
game content has found widespread interest. However, the task of 3D gaming map
art creation remains underexplored to date due to its unique complexity and
domain-specific challenges. While recent works have addressed related topics
such as retro-style level generation and procedural terrain creation, these
works primarily focus on simpler data distributions. To the best of our
knowledge, we are the first to demonstrate the application of modern AI
techniques for high-resolution texture manipulation in complex, highly detailed
AAA 3D game environments. We introduce a novel Smart Brush for map editing,
designed to assist artists in seamlessly modifying selected areas of a game map
with minimal effort. By leveraging generative adversarial networks and
diffusion models we propose two variants of the brush that enable efficient and
context-aware generation. Our hybrid workflow aims to enhance both artistic
flexibility and production efficiency, enabling the refinement of environments
without manually reworking every detail, thus helping to bridge the gap between
automation and creative control in game development. A comparative evaluation
of our two methods with adapted versions of several state-of-the art models
shows that our GAN-based brush produces the sharpest and most detailed outputs
while preserving image context while the evaluated state-of-the-art models tend
towards blurrier results and exhibit difficulties in maintaining contextual
consistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VesselSAM: Leveraging SAM for Aortic Vessel Segmentation with LoRA and
  Atrous Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18185v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18185v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adnan Iltaf, Rayan Merghani Ahmed, Zhenxi Zhang, Bin Li, Shoujun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation is crucial for clinical diagnosis and treatment
planning, especially when dealing with complex anatomical structures such as
vessels. However, accurately segmenting vessels remains challenging due to
their small size, intricate edge structures, and susceptibility to artifacts
and imaging noise. In this work, we propose VesselSAM, an enhanced version of
the Segment Anything Model (SAM), specifically tailored for aortic vessel
segmentation. VesselSAM incorporates AtrousLoRA, a novel module integrating
Atrous Attention and Low-Rank Adaptation (LoRA), to enhance segmentation
performance. Atrous Attention enables the model to capture multi-scale
contextual information, preserving both fine-grained local details and broader
global context. Additionally, LoRA facilitates efficient fine-tuning of the
frozen SAM image encoder, reducing the number of trainable parameters and
thereby enhancing computational efficiency. We evaluate VesselSAM using two
challenging datasets: the Aortic Vessel Tree (AVT) dataset and the Type-B
Aortic Dissection (TBAD) dataset. VesselSAM achieves state-of-the-art
performance, attaining DSC scores of 93.50\%, 93.25\%, 93.02\%, and 93.26\%
across multi-center datasets. Our results demonstrate that VesselSAM delivers
high segmentation accuracy while significantly reducing computational overhead
compared to existing large-scale models. This development paves the way for
enhanced AI-based aortic vessel segmentation in clinical environments. The code
and models will be released at https://github.com/Adnan-CAS/AtrousLora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Large <span class="highlight-title">Vision</span>-Language Models via Directed Scene Graph for
  Comprehensive Image Captioning <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08614v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08614v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Lu, Wei Wu, Kecheng Zheng, Shuailei Ma, Biao Gong, Jiawei Liu, Wei Zhai, Yang Cao, Yujun Shen, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating detailed captions comprehending text-rich visual content in images
has received growing attention for Large Vision-Language Models (LVLMs).
However, few studies have developed benchmarks specifically tailored for
detailed captions to measure their accuracy and comprehensiveness. In this
paper, we introduce a detailed caption benchmark, termed as CompreCap, to
evaluate the visual context from a directed scene graph view. Concretely, we
first manually segment the image into semantically meaningful regions (i.e.,
semantic segmentation mask) according to common-object vocabulary, while also
distinguishing attributes of objects within all those regions. Then directional
relation labels of these objects are annotated to compose a directed scene
graph that can well encode rich compositional information of the image. Based
on our directed scene graph, we develop a pipeline to assess the generated
detailed captions from LVLMs on multiple levels, including the object-level
coverage, the accuracy of attribute descriptions, the score of key
relationships, etc. Experimental results on the CompreCap dataset confirm that
our evaluation method aligns closely with human evaluation scores across LVLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2025. Code and Dataset:
  https://github.com/LuFan31/CompreCap</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Vision</span>-based Multi-future Trajectory Prediction: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renhao Huang, Hao Xue, Maurice Pagnucco, Flora Salim, Yang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based trajectory prediction is an important task that supports safe
and intelligent behaviours in autonomous systems. Many advanced approaches have
been proposed over the years with improved spatial and temporal feature
extraction. However, human behaviour is naturally diverse and uncertain. Given
the past trajectory and surrounding environment information, an agent can have
multiple plausible trajectories in the future. To tackle this problem, an
essential task named multi-future trajectory prediction (MTP) has recently been
studied. This task aims to generate a diverse, acceptable and explainable
distribution of future predictions for each agent. In this paper, we present
the first survey for MTP with our unique taxonomies and a comprehensive
analysis of frameworks, datasets and evaluation metrics. We also compare models
on existing MTP datasets and conduct experiments on the ForkingPath dataset.
Finally, we discuss multiple future directions that can help researchers
develop novel multi-future trajectory prediction systems and other diverse
learning tasks similar to MTP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TNNLS 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-25T00:00:00Z">2025-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">51</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Post-Training Preference Alignment for Multi-Agent Motion
  Generation Models Using Implicit Feedback from Pre-training Demonstrations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Tian, Kratarth Goel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in LLMs have revolutionized motion generation models in
embodied applications. While LLM-type auto-regressive motion generation models
benefit from training scalability, there remains a discrepancy between their
token prediction objectives and human preferences. As a result, models
pre-trained solely with token-prediction objectives often generate behaviors
that deviate from what humans would prefer, making post-training preference
alignment crucial for producing human-preferred motions. Unfortunately,
post-training alignment requires extensive preference rankings of motions
generated by the pre-trained model, which are costly to annotate, especially in
multi-agent settings. Recently, there has been growing interest in leveraging
pre-training demonstrations to scalably generate preference data for
post-training alignment. However, these methods often adopt an adversarial
assumption, treating all pre-trained model-generated samples as unpreferred
examples. This adversarial approach overlooks the valuable signal provided by
preference rankings among the model's own generations, ultimately reducing
alignment effectiveness and potentially leading to misaligned behaviors. In
this work, instead of treating all generated samples as equally bad, we
leverage implicit preferences encoded in pre-training demonstrations to
construct preference rankings among the pre-trained model's generations,
offering more nuanced preference alignment guidance with zero human cost. We
apply our approach to large-scale traffic simulation and demonstrate its
effectiveness in improving the realism of pre-trained model's generated
behaviors, making a lightweight 1M motion generation model comparable to SOTA
large imitation-based models by relying solely on implicit feedback from
pre-training demonstrations, without additional post-training human preference
annotations or high computational costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extendable Long-Horizon Planning via Hierarchical Multiscale <span class="highlight-title">Diffusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Chen, Hany Hamed, Doojin Baek, Taegu Kang, Yoshua Bengio, Sungjin Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles a novel problem, extendable long-horizon planning-enabling
agents to plan trajectories longer than those in training data without
compounding errors. To tackle this, we propose the Hierarchical Multiscale
Diffuser (HM-Diffuser) and Progressive Trajectory Extension (PTE), an
augmentation method that iteratively generates longer trajectories by stitching
shorter ones. HM-Diffuser trains on these extended trajectories using a
hierarchical structure, efficiently handling tasks across multiple temporal
scales. Additionally, we introduce Adaptive Plan Pondering and the Recursive
HM-Diffuser, which consolidate hierarchical layers into a single model to
process temporal scales recursively. Experimental results demonstrate the
effectiveness of our approach, advancing diffusion-based planners for scalable
long-horizon planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Scene-Level Signed Directional Distance Function with
  Ellipsoidal Priors and Neural Residuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhirui Dai, Hojoon Shin, Yulun Tian, Ki Myung Brian Lee, Nikolay Atanasov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense geometric environment representations are critical for autonomous
mobile robot navigation and exploration. Recent work shows that implicit
continuous representations of occupancy, signed distance, or radiance learned
using neural networks offer advantages in reconstruction fidelity, efficiency,
and differentiability over explicit discrete representations based on meshes,
point clouds, and voxels. In this work, we explore a directional formulation of
signed distance, called signed directional distance function (SDDF). Unlike
signed distance function (SDF) and similar to neural radiance fields (NeRF),
SDDF has a position and viewing direction as input. Like SDF and unlike NeRF,
SDDF directly provides distance to the observed surface along the direction,
rather than integrating along the view ray, allowing efficient view synthesis.
To learn and predict scene-level SDDF efficiently, we develop a differentiable
hybrid representation that combines explicit ellipsoid priors and implicit
neural residuals. This approach allows the model to effectively handle large
distance discontinuities around obstacle boundaries while preserving the
ability for dense high-fidelity prediction. We show that SDDF is competitive
with the state-of-the-art neural implicit scene models in terms of
reconstruction accuracy and rendering efficiency, while allowing differentiable
view prediction for robot trajectory optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gemini <span class="highlight-title">Robo</span>tics: Bringing AI into the Physical World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, Steven Bohez, Konstantinos Bousmalis, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Oscar Chang, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, David D'Ambrosio, Sudeep Dasari, Todor Davchev, Coline Devin, Norman Di Palo, Tianli Ding, Adil Dostmohamed, Danny Driess, Yilun Du, Debidatta Dwibedi, Michael Elabd, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Leonard Hasenclever, Nicolas Heess, Brandon Hernaez, Alexander Herzog, R. Alex Hofer, Jan Humplik, Atil Iscen, Mithun George Jacob, Deepali Jain, Ryan Julian, Dmitry Kalashnikov, M. Emre Karagozler, Stefani Karp, Chase Kew, Jerad Kirkland, Sean Kirmani, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Jacky Liang, Yixin Lin, Sharath Maddineni, Anirudha Majumdar, Assaf Hurwitz Michaely, Robert Moreno, Michael Neunert, Francesco Nori, Carolina Parada, Emilio Parisotto, Peter Pastor, Acorn Pooley, Kanishka Rao, Krista Reymann, Dorsa Sadigh, Stefano Saliceti, Pannag Sanketi, Pierre Sermanet, Dhruv Shah, Mohit Sharma, Kathryn Shea, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Jost Tobias Springenberg, Rachel Sterneck, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Vincent Vanhoucke, Jake Varley, Grace Vesom, Giulia Vezzani, Oriol Vinyals, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Yuxiang Yang, Rui Yao, Sergey Yaroshenko, Wenhao Yu, Wentao Yuan, Jingwei Zhang, Tingnan Zhang, Allan Zhou, Yuxiang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large multimodal models have led to the emergence of
remarkable generalist capabilities in digital domains, yet their translation to
physical agents such as robots remains a significant challenge. This report
introduces a new family of AI models purposefully designed for robotics and
built upon the foundation of Gemini 2.0. We present Gemini Robotics, an
advanced Vision-Language-Action (VLA) generalist model capable of directly
controlling robots. Gemini Robotics executes smooth and reactive movements to
tackle a wide range of complex manipulation tasks while also being robust to
variations in object types and positions, handling unseen environments as well
as following diverse, open vocabulary instructions. We show that with
additional fine-tuning, Gemini Robotics can be specialized to new capabilities
including solving long-horizon, highly dexterous tasks, learning new
short-horizon tasks from as few as 100 demonstrations and adapting to
completely novel robot embodiments. This is made possible because Gemini
Robotics builds on top of the Gemini Robotics-ER model, the second model we
introduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends
Gemini's multimodal reasoning capabilities into the physical world, with
enhanced spatial and temporal understanding. This enables capabilities relevant
to robotics including object detection, pointing, trajectory and grasp
prediction, as well as multi-view correspondence and 3D bounding box
predictions. We show how this novel combination can support a variety of
robotics applications. We also discuss and address important safety
considerations related to this new class of robotics foundation models. The
Gemini Robotics family marks a substantial step towards developing
general-purpose robots that realizes AI's potential in the physical world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperdimensional Uncertainty Quantification for Multimodal Uncertainty
  Fusion in Autonomous Vehicles Perception <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Chen, Junyao Wang, Trier Mortlock, Pramod Khargonekar, Mohammad Abdullah Al Faruque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty Quantification (UQ) is crucial for ensuring the reliability of
machine learning models deployed in real-world autonomous systems. However,
existing approaches typically quantify task-level output prediction uncertainty
without considering epistemic uncertainty at the multimodal feature fusion
level, leading to sub-optimal outcomes. Additionally, popular uncertainty
quantification methods, e.g., Bayesian approximations, remain challenging to
deploy in practice due to high computational costs in training and inference.
In this paper, we propose HyperDUM, a novel deterministic uncertainty method
(DUM) that efficiently quantifies feature-level epistemic uncertainty by
leveraging hyperdimensional computing. Our method captures the channel and
spatial uncertainties through channel and patch -wise projection and bundling
techniques respectively. Multimodal sensor features are then adaptively
weighted to mitigate uncertainty propagation and improve feature fusion. Our
evaluations show that HyperDUM on average outperforms the state-of-the-art
(SOTA) algorithms by up to 2.01%/1.27% in 3D Object Detection and up to 1.29%
improvement over baselines in semantic segmentation tasks under various types
of uncertainties. Notably, HyperDUM requires 2.36x less Floating Point
Operations and up to 38.30x less parameters than SOTA methods, providing an
efficient solution for real-world autonomous systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Magnetically and Electrically Powered Metallo-Dielectric Janus
  Micro<span class="highlight-title">robo</span>ts: Enhanced Motion Control and Operation Beyond Planar Limits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ido Rachbuch, Sinwook Park, Yuval Katz, Touvia Miloh, Gilad Yossifon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces the integration of hybrid magnetic and electric
actuation mechanisms to achieve advanced motion capabilities for Janus particle
(JP) microrobots. We demonstrate enhanced in-plane motion control through
versatile control strategies and present the concepts of interplanar
transitions and 2.5-dimensional (2.5D) trajectories, enabled by magnetic
levitation and electrostatic trapping. These innovations expand the mobility of
JPs into 3D space, allowing dynamic operation beyond the limitations of
traditional surface-bound motion. Key functionalities include obstacle
crossing, transitions to elevated surfaces, and discrete surface patterning
enabling highly localized interventions. Using this set of tools, we also
showcase the controlled out-of-plane transport of both synthetic and biological
cargo. Together, these advancements lay the groundwork for novel
microrobot-related applications in microfluidic systems and biomedical
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EventFly: Event <span class="highlight-title">Camera</span> Perception from Ground to the Sky <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Kong, Dongyue Lu, Xiang Xu, Lai Xing Ng, Wei Tsang Ooi, Benoit R. Cottereau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-platform adaptation in event-based dense perception is crucial for
deploying event cameras across diverse settings, such as vehicles, drones, and
quadrupeds, each with unique motion dynamics, viewpoints, and class
distributions. In this work, we introduce EventFly, a framework for robust
cross-platform adaptation in event camera perception. Our approach comprises
three key components: i) Event Activation Prior (EAP), which identifies
high-activation regions in the target domain to minimize prediction entropy,
fostering confident, domain-adaptive predictions; ii) EventBlend, a data-mixing
strategy that integrates source and target event voxel grids based on
EAP-driven similarity and density maps, enhancing feature alignment; and iii)
EventMatch, a dual-discriminator technique that aligns features from source,
target, and blended domains for better domain-invariant learning. To
holistically assess cross-platform adaptation abilities, we introduce EXPo, a
large-scale benchmark with diverse samples across vehicle, drone, and quadruped
platforms. Extensive experiments validate our effectiveness, demonstrating
substantial gains over popular adaptation methods. We hope this work can pave
the way for more adaptive, high-performing event perception across diverse and
complex environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025; 30 pages, 8 figures, 16 tables; Project Page at
  https://event-fly.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuperFlow++: Enhanced Spatiotemporal Consistency for Cross-Modal Data
  Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Xu, Lingdong Kong, Hui Shuai, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu, Qingshan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR representation learning has emerged as a promising approach to reducing
reliance on costly and labor-intensive human annotations. While existing
methods primarily focus on spatial alignment between LiDAR and camera sensors,
they often overlook the temporal dynamics critical for capturing motion and
scene continuity in driving scenarios. To address this limitation, we propose
SuperFlow++, a novel framework that integrates spatiotemporal cues in both
pretraining and downstream tasks using consecutive LiDAR-camera pairs.
SuperFlow++ introduces four key components: (1) a view consistency alignment
module to unify semantic information across camera views, (2) a dense-to-sparse
consistency regularization mechanism to enhance feature robustness across
varying point cloud densities, (3) a flow-based contrastive learning approach
that models temporal relationships for improved scene understanding, and (4) a
temporal voting strategy that propagates semantic information across LiDAR
scans to improve prediction consistency. Extensive evaluations on 11
heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms
state-of-the-art methods across diverse tasks and driving conditions.
Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover
emergent properties that provide deeper insights into developing scalable 3D
foundation models. With strong generalizability and computational efficiency,
SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based
perception in autonomous driving. The code is publicly available at
https://github.com/Xiangxu-0103/SuperFlow
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 15 pages, 6 figures, 10 tables; Code at
  https://github.com/Xiangxu-0103/SuperFlow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visuo-Tactile Object Pose Estimation for a Multi-Finger <span class="highlight-title">Robo</span>t Hand with
  Low-Resolution In-Hand Tactile Sensing <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Mack, Felix Grüninger, Benjamin A. Richardson, Regine Lendway, Katherine J. Kuchenbecker, Joerg Stueckler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D pose estimation of grasped objects is an important prerequisite
for robots to perform assembly or in-hand manipulation tasks, but object
occlusion by the robot's own hand greatly increases the difficulty of this
perceptual task. Here, we propose that combining visual information and
proprioception with binary, low-resolution tactile contact measurements from
across the interior surface of an articulated robotic hand can mitigate this
issue. The visuo-tactile object-pose-estimation problem is formulated
probabilistically in a factor graph. The pose of the object is optimized to
align with the three kinds of measurements using a robust cost function to
reduce the influence of visual or tactile outlier readings. The advantages of
the proposed approach are first demonstrated in simulation: a custom 15-DoF
robot hand with one binary tactile sensor per link grasps 17 YCB objects while
observed by an RGB-D camera. This low-resolution in-hand tactile sensing
significantly improves object-pose estimates under high occlusion and also high
visual noise. We also show these benefits through grasping tests with a
preliminary real version of our tactile hand, obtaining reasonable
visuo-tactile estimates of object pose at approximately 13.3 Hz on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the IEEE International Conference on
  Robotics and Automation (ICRA), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Agent Framework Integrating Large Language Models and Generative
  AI for Accelerated Metamaterial Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Tian, Martin Taylor Sobczak, Dhanush Patil, Jixin Hou, Lin Pang, Arunachalam Ramanathan, Libin Yang, Xianyan Chen, Yuval Golan, Hongyue Sun, Kenan Song, Xianqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metamaterials, renowned for their exceptional mechanical, electromagnetic,
and thermal properties, hold transformative potential across diverse
applications, yet their design remains constrained by labor-intensive
trial-and-error methods and limited data interoperability. Here, we introduce
CrossMatAgent--a novel multi-agent framework that synergistically integrates
large language models with state-of-the-art generative AI to revolutionize
metamaterial design. By orchestrating a hierarchical team of agents--each
specializing in tasks such as pattern analysis, architectural synthesis, prompt
engineering, and supervisory feedback--our system leverages the multimodal
reasoning of GPT-4o alongside the generative precision of DALL-E 3 and a
fine-tuned Stable Diffusion XL model. This integrated approach automates data
augmentation, enhances design fidelity, and produces simulation- and 3D
printing-ready metamaterial patterns. Comprehensive evaluations, including
CLIP-based alignment, SHAP interpretability analyses, and mechanical
simulations under varied load conditions, demonstrate the framework's ability
to generate diverse, reproducible, and application-ready designs. CrossMatAgent
thus establishes a scalable, AI-driven paradigm that bridges the gap between
conceptual innovation and practical realization, paving the way for accelerated
metamaterial development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenLex3D: A New Evaluation Benchmark for Open-Vocabulary 3D Scene
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christina Kassab, Sacha Morin, Martin Büchner, Matías Mattamala, Kumaraditya Gupta, Abhinav Valada, Liam Paull, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D scene understanding has been transformed by open-vocabulary language
models that enable interaction via natural language. However, the evaluation of
these representations is limited to closed-set semantics that do not capture
the richness of language. This work presents OpenLex3D, a dedicated benchmark
to evaluate 3D open-vocabulary scene representations. OpenLex3D provides
entirely new label annotations for 23 scenes from Replica, ScanNet++, and HM3D,
which capture real-world linguistic variability by introducing synonymical
object categories and additional nuanced descriptions. By introducing an
open-set 3D semantic segmentation task and an object retrieval task, we provide
insights on feature precision, segmentation, and downstream capabilities. We
evaluate various existing 3D open-vocabulary methods on OpenLex3D, showcasing
failure cases, and avenues for improvement. The benchmark is publicly available
at: https://openlex3d.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dita: Scaling <span class="highlight-title">Diffusion</span> Transformer for Generalist
  <span class="highlight-title">Vision</span>-Language-Action Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, Yuntao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent vision-language-action models trained on diverse robot datasets
exhibit promising generalization capabilities with limited in-domain data,
their reliance on compact action heads to predict discretized or continuous
actions constrains adaptability to heterogeneous action spaces. We present
Dita, a scalable framework that leverages Transformer architectures to directly
denoise continuous action sequences through a unified multimodal diffusion
process. Departing from prior methods that condition denoising on fused
embeddings via shallow networks, Dita employs in-context conditioning --
enabling fine-grained alignment between denoised actions and raw visual tokens
from historical observations. This design explicitly models action deltas and
environmental nuances. By scaling the diffusion action denoiser alongside the
Transformer's scalability, Dita effectively integrates cross-embodiment
datasets across diverse camera perspectives, observation scenes, tasks, and
action spaces. Such synergy enhances robustness against various variances and
facilitates the successful execution of long-horizon tasks. Evaluations across
extensive benchmarks demonstrate state-of-the-art or comparative performance in
simulation. Notably, Dita achieves robust real-world adaptation to
environmental variances and complex long-horizon tasks through 10-shot
finetuning, using only third-person camera inputs. The architecture establishes
a versatile, lightweight and open-source baseline for generalist robot policy
learning. Project Page: https://robodita.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; https://robodita.github.io;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-SD: Semi-Supervised Metric Depth Estimation via Surrounding <span class="highlight-title">Camera</span>s
  for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusen Xie, Zhengmin Huang, Shaojie Shen, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Semi-SD, a novel metric depth estimation
framework tailored for surrounding cameras equipment in autonomous driving. In
this work, the input data consists of adjacent surrounding frames and camera
parameters. We propose a unified spatial-temporal-semantic fusion module to
construct the visual fused features. Cross-attention components for surrounding
cameras and adjacent frames are utilized to focus on metric scale information
refinement and temporal feature matching. Building on this, we propose a pose
estimation framework using surrounding cameras, their corresponding estimated
depths, and extrinsic parameters, which effectively address the scale ambiguity
in multi-camera setups. Moreover, semantic world model and monocular depth
estimation world model are integrated to supervised the depth estimation, which
improve the quality of depth estimation. We evaluate our algorithm on DDAD and
nuScenes datasets, and the results demonstrate that our method achieves
state-of-the-art performance in terms of surrounding camera based depth
estimation quality. The source code will be available on
https://github.com/xieyuser/Semi-SD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Cognitive States for Adaptive Scaffolding of Understanding in
  Explanatory Tasks in HRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Groß, Birte Richter, Bjarne Thomzik, Britta Wrede
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how scaffolding strategies influence human understanding in
human-robot interaction is important for developing effective assistive
systems. This empirical study investigates linguistic scaffolding strategies
based on negation as an important means that de-biases the user from potential
errors but increases processing costs and hesitations as a means to ameliorate
processing costs. In an adaptive strategy, the user state with respect to the
current state of understanding and processing capacity was estimated via a
scoring scheme based on task performance, prior scaffolding strategy, and
current eye gaze behavior. In the study, the adaptive strategy of providing
negations and hesitations was compared with a non-adaptive strategy of
providing only affirmations. The adaptive scaffolding strategy was generated
using the computational model SHIFT. Our findings indicate that using adaptive
scaffolding strategies with SHIFT tends to (1) increased processing costs, as
reflected in longer reaction times, but (2) improved task understanding,
evidenced by a lower error rate of almost 23%. We assessed the efficiency of
SHIFT's selected scaffolding strategies across different cognitive states,
finding that in three out of five states, the error rate was lower compared to
the baseline condition. We discuss how these results align with the assumptions
of the SHIFT model and highlight areas for refinement. Moreover, we demonstrate
how scaffolding strategies, such as negation and hesitation, contribute to more
effective human-robot explanatory dialogues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk-Aware <span class="highlight-title">Reinforcement Learning</span> for Autonomous Driving: Improving
  Safety When Driving through Intersection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Leng, Ran Yu, Wei Han, Lu Xiong, Zhuoren Li, Hailong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying reinforcement learning to autonomous driving has garnered widespread
attention. However, classical reinforcement learning methods optimize policies
by maximizing expected rewards but lack sufficient safety considerations, often
putting agents in hazardous situations. This paper proposes a risk-aware
reinforcement learning approach for autonomous driving to improve the safety
performance when crossing the intersection. Safe critics are constructed to
evaluate driving risk and work in conjunction with the reward critic to update
the actor. Based on this, a Lagrangian relaxation method and cyclic gradient
iteration are combined to project actions into a feasible safe region.
Furthermore, a Multi-hop and Multi-layer perception (MLP) mixed Attention
Mechanism (MMAM) is incorporated into the actor-critic network, enabling the
policy to adapt to dynamic traffic and overcome permutation sensitivity
challenges. This allows the policy to focus more effectively on surrounding
potential risks while enhancing the identification of passing opportunities.
Simulation tests are conducted on different tasks at unsignalized
intersections. The results show that the proposed approach effectively reduces
collision rates and improves crossing efficiency in comparison to baseline
algorithms. Additionally, our ablation experiments demonstrate the benefits of
incorporating risk-awareness and MMAM into RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-aware Joint Orchestration of 5G and <span class="highlight-title">Robo</span>ts: Experimental Testbed
  and Field Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Groshev, Lanfranco Zanzi, Carmen Delgado, Xi Li, Antonio de la Oliva, Xavier Costa-Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  5G mobile networks introduce a new dimension for connecting and operating
mobile robots in outdoor environments, leveraging cloud-native and offloading
features of 5G networks to enable fully flexible and collaborative cloud robot
operations. However, the limited battery life of robots remains a significant
obstacle to their effective adoption in real-world exploration scenarios. This
paper explores, via field experiments, the potential energy-saving gains of
OROS, a joint orchestration of 5G and Robot Operating System (ROS) that
coordinates multiple 5G-connected robots both in terms of navigation and
sensing, as well as optimizes their cloud-native service resource utilization
while minimizing total resource and energy consumption on the robots based on
real-time feedback. We designed, implemented and evaluated our proposed OROS in
an experimental testbed composed of commercial off-the-shelf robots and a local
5G infrastructure deployed on a campus. The experimental results demonstrated
that OROS significantly outperforms state-of-the-art approaches in terms of
energy savings by offloading demanding computational tasks to the 5G edge
infrastructure and dynamic energy management of on-board sensors (e.g.,
switching them off when they are not needed). This strategy achieves
approximately 15% energy savings on the robots, thereby extending battery life,
which in turn allows for longer operating times and better resource
utilization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 15 figures, journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZodiAq: An Isotropic Flagella-Inspired Soft Underwater Drone for Safe
  Marine Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anup Teejo Mathew, Daniel Feliu-Talegon, Yusuf Abdullahi Adamu, Ikhlas Ben Hmida, Costanza Armanini, Cesare Stefanini, Lakmal Seneviratne, Federico Renda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inherent challenges of robotic underwater exploration, such as
hydrodynamic effects, the complexity of dynamic coupling, and the necessity for
sensitive interaction with marine life, call for the adoption of soft robotic
approaches in marine exploration. To address this, we present a novel
prototype, ZodiAq, a soft underwater drone inspired by prokaryotic bacterial
flagella. ZodiAq's unique dodecahedral structure, equipped with 12
flagella-like arms, ensures design redundancy and compliance, ideal for
navigating complex underwater terrains. The prototype features a central unit
based on a Raspberry Pi, connected to a sensory system for inertial, depth, and
vision detection, and an acoustic modem for communication. Combined with the
implemented control law, it renders ZodiAq an intelligent system. This paper
details the design and fabrication process of ZodiAq, highlighting design
choices and prototype capabilities. Based on the strain-based modeling of
Cosserat rods, we have developed a digital twin of the prototype within a
simulation toolbox to ease analysis and control. To optimize its operation in
dynamic aquatic conditions, a simplified model-based controller has been
developed and implemented, facilitating intelligent and adaptive movement in
the hydrodynamic environment. Extensive experimental demonstrations highlight
the drone's potential, showcasing its design redundancy, embodied intelligence,
crawling gait, and practical applications in diverse underwater settings. This
research contributes significantly to the field of underwater soft robotics,
offering a promising new avenue for safe, efficient, and environmentally
conscious underwater exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, including disclaimer page, pre-peer-review version of the
  manuscript, and supplementary material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DataPlatter: Boosting <span class="highlight-title">Robo</span>tic <span class="highlight-title">Manipulation</span> Generalization with Minimal
  Costly Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liming Zheng, Feng Yan, Fanfan Liu, Chengjian Feng, Yufeng Zhong, Yiyang Huang, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing adoption of Vision-Language-Action (VLA) models in embodied AI
intensifies the demand for diverse manipulation demonstrations. However, high
costs associated with data collection often result in insufficient data
coverage across all scenarios, which limits the performance of the models. It
is observed that the spatial reasoning phase (SRP) in large workspace dominates
the failure cases. Fortunately, this data can be collected with low cost,
underscoring the potential of leveraging inexpensive data to improve model
performance. In this paper, we introduce the DataPlatter method, a framework
that decouples training trajectories into distinct task stages and leverages
abundant easily collectible SRP data to enhance VLA model's generalization.
Through analysis we demonstrate that sub-task-specific training with additional
SRP data with proper proportion can act as a performance catalyst for robot
manipulation, maximizing the utilization of costly physical interaction phase
(PIP) data. Experiments show that through introducing large proportion of
cost-effective SRP trajectories into a limited set of PIP data, we can achieve
a maximum improvement of 41\% on success rate in zero-shot scenes, while with
the ability to transfer manipulation skill to novel targets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Robo</span>Flamingo-Plus: Fusion of Depth and RGB Perception with
  <span class="highlight-title">Vision</span>-Language Models for Enhanced <span class="highlight-title">Robo</span>tic <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robotic technologies advancing towards more complex multimodal
interactions and manipulation tasks, the integration of advanced
Vision-Language Models (VLMs) has become a key driver in the field. Despite
progress with current methods, challenges persist in fusing depth and RGB
information within 3D environments and executing tasks guided by linguistic
instructions. In response to these challenges, we have enhanced the existing
RoboFlamingo framework by introducing RoboFlamingo-Plus, which incorporates
depth data into VLMs to significantly improve robotic manipulation performance.
Our research achieves a nuanced fusion of RGB and depth information by
integrating a pre-trained Vision Transformer (ViT) with a resampling technique,
closely aligning this combined data with linguistic cues for superior
multimodal understanding. The novelty of RoboFlamingo-Plus lies in its
adaptation of inputs for depth data processing, leveraging a pre-trained
resampler for depth feature extraction, and employing cross-attention
mechanisms for optimal feature integration. These improvements allow
RoboFlamingo-Plus to not only deeply understand 3D environments but also easily
perform complex, language-guided tasks in challenging settings. Experimental
results show that RoboFlamingo-Plus boosts robotic manipulation by 10-20% over
current methods, marking a significant advancement. Codes and model weights are
public at RoboFlamingo-Plus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-LINS: a Multi-Map <span class="highlight-title">LiDAR</span>-Inertial System for Over-Degenerate
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Ma, Jie Xu, Shenghai Yuan, Tian Zhi, Wenlu Yu, Jun Zhou, Lihua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLAM plays a crucial role in automation tasks, such as warehouse logistics,
healthcare robotics, and restaurant delivery. These scenes come with various
challenges, including navigating around crowds of people, dealing with flying
plastic bags that can temporarily blind sensors, and addressing reduced LiDAR
density caused by cooking smoke. Such scenarios can result in over-degeneracy,
causing the map to drift. To address this issue, this paper presents a
multi-map LiDAR-inertial system (MM-LINS) for the first time. The front-end
employs an iterated error state Kalman filter for state estimation and
introduces a reliable evaluation strategy for degeneracy detection. If
over-degeneracy is detected, the active map will be stored into sleeping maps.
Subsequently, the system continuously attempts to construct new maps using a
dynamic initialization method to ensure successful initialization upon leaving
the over-degeneracy. Regarding the back-end, the Scan Context descriptor is
utilized to detect inter-map similarity. Upon successful recognition of a
sleeping map that shares a common region with the active map, the overlapping
trajectory region is utilized to constrain the positional transformation near
the edge of the prior map. In response to this, a constraint-enhanced map
fusion strategy is proposed to achieve high-precision positional and mapping
results. Experiments have been conducted separately on both public datasets
that exhibited over-degenerate conditions and in real-world environments. These
tests demonstrated the effectiveness of MM-LINS in over-degeneracy environment.
Our codes are open-sourced on Github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Intelligent Vehicles</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Body Discovery of Embodied AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Sun, Pengfei Tian, Xiaozhu Hu, Xiaoyu Zhao, Huiying Li, Zhenliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the pursuit of realizing artificial general intelligence (AGI), the
importance of embodied artificial intelligence (AI) becomes increasingly
apparent. Following this trend, research integrating robots with AGI has become
prominent. As various kinds of embodiments have been designed, adaptability to
diverse embodiments will become important to AGI. We introduce a new challenge,
termed "Body Discovery of Embodied AI", focusing on tasks of recognizing
embodiments and summarizing neural signal functionality. The challenge
encompasses the precise definition of an AI body and the intricate task of
identifying embodiments in dynamic environments, where conventional approaches
often prove inadequate. To address these challenges, we apply causal inference
method and evaluate it by developing a simulator tailored for testing
algorithms with virtual environments. Finally, we validate the efficacy of our
algorithms through empirical testing, demonstrating their robust performance in
various scenarios based on virtual environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ G-Dex<span class="highlight-title">Grasp</span>: Generalizable Dexterous <span class="highlight-title">Grasp</span>ing Synthesis Via Part-Aware
  Prior Retrieval and Prior-Assisted Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juntao Jian, Xiuping Liu, Zixuan Chen, Manyi Li, Jian Liu, Ruizhen Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in dexterous grasping synthesis have demonstrated significant
progress in producing reasonable and plausible grasps for many task purposes.
But it remains challenging to generalize to unseen object categories and
diverse task instructions. In this paper, we propose G-DexGrasp, a
retrieval-augmented generation approach that can produce high-quality dexterous
hand configurations for unseen object categories and language-based task
instructions. The key is to retrieve generalizable grasping priors, including
the fine-grained contact part and the affordance-related distribution of
relevant grasping instances, for the following synthesis pipeline.
Specifically, the fine-grained contact part and affordance act as generalizable
guidance to infer reasonable grasping configurations for unseen objects with a
generative model, while the relevant grasping distribution plays as
regularization to guarantee the plausibility of synthesized grasps during the
subsequent refinement optimization. Our comparison experiments validate the
effectiveness of our key designs for generalization and demonstrate the
remarkable performance against the existing approaches. Project page:
https://g-dexgrasp.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quality-focused Active Adversarial Policy for Safe <span class="highlight-title">Grasp</span>ing in
  Human-<span class="highlight-title">Robo</span>t Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghao Li, Razvan Beuran, Nak Young Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-guided robot grasping methods based on Deep Neural Networks (DNNs)
have achieved remarkable success in handling unknown objects, attributable to
their powerful generalizability. However, these methods with this
generalizability tend to recognize the human hand and its adjacent objects as
graspable targets, compromising safety during Human-Robot Interaction (HRI). In
this work, we propose the Quality-focused Active Adversarial Policy (QFAAP) to
solve this problem. Specifically, the first part is the Adversarial Quality
Patch (AQP), wherein we design the adversarial quality patch loss and leverage
the grasp dataset to optimize a patch with high quality scores. Next, we
construct the Projected Quality Gradient Descent (PQGD) and integrate it with
the AQP, which contains only the hand region within each real-time frame,
endowing the AQP with fast adaptability to the human hand shape. Through AQP
and PQGD, the hand can be actively adversarial with the surrounding objects,
lowering their quality scores. Therefore, further setting the quality score of
the hand to zero will reduce the grasping priority of both the hand and its
adjacent objects, enabling the robot to grasp other objects away from the hand
without emergency stops. We conduct extensive experiments on the benchmark
datasets and a cobot, showing the effectiveness of QFAAP. Our code and demo
videos are available here: https://github.com/clee-jaist/QFAAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MATT-GS: Masked Attention-based 3DGS for <span class="highlight-title">Robo</span>t Perception and Object
  Detection <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jee Won Lee, Hansol Lim, SooYeun Yang, Jongseong Brad Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel masked attention-based 3D Gaussian Splatting
(3DGS) approach to enhance robotic perception and object detection in
industrial and smart factory environments. U2-Net is employed for background
removal to isolate target objects from raw images, thereby minimizing clutter
and ensuring that the model processes only relevant data. Additionally, a Sobel
filter-based attention mechanism is integrated into the 3DGS framework to
enhance fine details - capturing critical features such as screws, wires, and
intricate textures essential for high-precision tasks. We validate our approach
using quantitative metrics, including L1 loss, SSIM, PSNR, comparing the
performance of the background-removed and attention-incorporated 3DGS model
against the ground truth images and the original 3DGS training baseline. The
results demonstrate significant improves in visual fidelity and detail
preservation, highlighting the effectiveness of our method in enhancing robotic
vision for object recognition and manipulation in complex industrial settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the 2025 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS) for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Uncertainty Unification: A Case Study for Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoting Peng, Haonan Chen, Katherine Driggs-Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning human preferences is essential for human-robot interaction, as it
enables robots to adapt their behaviors to align with human expectations and
goals. However, the inherent uncertainties in both human behavior and robotic
systems make preference learning a challenging task. While probabilistic
robotics algorithms offer uncertainty quantification, the integration of human
preference uncertainty remains underexplored. To bridge this gap, we introduce
uncertainty unification and propose a novel framework, uncertainty-unified
preference learning (UUPL), which enhances Gaussian Process (GP)-based
preference learning by unifying human and robot uncertainties. Specifically,
UUPL includes a human preference uncertainty model that improves GP posterior
mean estimation, and an uncertainty-weighted Gaussian Mixture Model (GMM) that
enhances GP predictive variance accuracy. Additionally, we design a
user-specific calibration process to align uncertainty representations across
users, ensuring consistency and reliability in the model performance.
Comprehensive experiments and user studies demonstrate that UUPL achieves
state-of-the-art performance in both prediction accuracy and user rating. An
ablation study further validates the effectiveness of human uncertainty model
and uncertainty-weighted GMM of UUPL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Observation Adaptation via Annealed Importance Resampling for Partially
  Observable Markov Decision Processes <span class="chip">ICAPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunuo Zhang, Baiting Luo, Ayan Mukhopadhyay, Abhishek Dubey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partially observable Markov decision processes (POMDPs) are a general
mathematical model for sequential decision-making in stochastic environments
under state uncertainty. POMDPs are often solved \textit{online}, which enables
the algorithm to adapt to new information in real time. Online solvers
typically use bootstrap particle filters based on importance resampling for
updating the belief distribution. Since directly sampling from the ideal state
distribution given the latest observation and previous state is infeasible,
particle filters approximate the posterior belief distribution by propagating
states and adjusting weights through prediction and resampling steps. However,
in practice, the importance resampling technique often leads to particle
degeneracy and sample impoverishment when the state transition model poorly
aligns with the posterior belief distribution, especially when the received
observation is highly informative. We propose an approach that constructs a
sequence of bridge distributions between the state-transition and optimal
distributions through iterative Monte Carlo steps, better accommodating noisy
observations in online POMDP solvers. Our algorithm demonstrates significantly
superior performance compared to state-of-the-art methods when evaluated across
multiple challenging POMDP domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as Oral Presentation to ICAPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Underwater Vehicle With Orientation Adjustable Thrusters: Design
  and Adaptive Tracking Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Wang, Shihan Kong, Zhanhua Xin, Kaiwei Zhu, Dongyue Li, Junzhi Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous underwater vehicles (AUVs) are essential for marine exploration
and research. However, conventional designs often struggle with limited
maneuverability in complex, dynamic underwater environments. This paper
introduces an innovative orientation-adjustable thruster AUV (OATAUV), equipped
with a redundant vector thruster configuration that enables full
six-degree-of-freedom (6-DOF) motion and composite maneuvers. To overcome
challenges associated with uncertain model parameters and environmental
disturbances, a novel feedforward adaptive model predictive controller (FFAMPC)
is proposed to ensure robust trajectory tracking, which integrates real-time
state feedback with adaptive parameter updates. Extensive experiments,
including closed-loop tracking and composite motion tests in a laboratory pool,
validate the enhanced performance of the OAT-AUV. The results demonstrate that
the OAT-AUV's redundant vector thruster configuration enables 23.8% cost
reduction relative to common vehicles, while the FF-AMPC controller achieves
68.6% trajectory tracking improvement compared to PID controllers. Uniquely,
the system executes composite helical/spiral trajectories unattainable by
similar vehicles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cube<span class="highlight-title">Robo</span>t: Grounding Language in Rubik's Cube <span class="highlight-title">Manipulation</span> via
  <span class="highlight-title">Vision</span>-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feiyang Wang, Xiaomin Yu, Wangyu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proving Rubik's Cube theorems at the high level represents a notable
milestone in human-level spatial imagination and logic thinking and reasoning.
Traditional Rubik's Cube robots, relying on complex vision systems and fixed
algorithms, often struggle to adapt to complex and dynamic scenarios. To
overcome this limitation, we introduce CubeRobot, a novel vision-language model
(VLM) tailored for solving 3x3 Rubik's Cubes, empowering embodied agents with
multimodal understanding and execution capabilities. We used the CubeCoT image
dataset, which contains multiple-level tasks (43 subtasks in total) that humans
are unable to handle, encompassing various cube states. We incorporate a
dual-loop VisionCoT architecture and Memory Stream, a paradigm for extracting
task-related features from VLM-generated planning queries, thus enabling
CubeRobot to independent planning, decision-making, reflection and separate
management of high- and low-level Rubik's Cube tasks. Furthermore, in low-level
Rubik's Cube restoration tasks, CubeRobot achieved a high accuracy rate of
100%, similar to 100% in medium-level tasks, and achieved an accuracy rate of
80% in high-level tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoinFT: A Coin-Sized, Capacitive 6-Axis Force Torque Sensor for <span class="highlight-title">Robo</span>tic
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojung Choi, Jun En Low, Tae Myung Huh, Gabriela A. Uribe, Seongheon Hong, Kenneth A. W. Hoffman, Julia Di, Tony G. Chen, Andrew A. Stanley, Mark R. Cutkosky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CoinFT, a capacitive 6-axis force/torque (F/T) sensor that is
compact, light, low-cost, and robust with an average mean-squared error of
0.11N for force and 0.84mNm for moment when the input ranges from 0~10N and
0~4N in normal and shear directions, respectively. CoinFT is a stack of two
rigid PCBs with comb-shaped electrodes connected by an array of silicone rubber
pillars. The microcontroller interrogates the electrodes in different subsets
in order to enhance sensitivity for measuring 6-axis F/T. The combination of
desirable features of CoinFT enables various contact-rich robot interactions at
a scale, across different embodiment domains including drones, robot
end-effectors, and wearable haptic devices. We demonstrate the utility of
CoinFT on drones by performing an attitude-based force control to perform tasks
that require careful contact force modulation. The design, fabrication, and
firmware of CoinFT are open-sourced at
https://hojung-choi.github.io/coinft.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Multi-Object <span class="highlight-title">Grasp</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianze Chen, Ricardo Frumento, Giulia Pagnanelli, Gianmarco Cei, Villa Keth, Shahadding Gafarov, Jian Gong, Zihe Ye, Marco Baracca, Salvatore D'Avella, Matteo Bianchi, Yu Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we describe a multi-object grasping benchmark to evaluate the
grasping and manipulation capabilities of robotic systems in both pile and
surface scenarios. The benchmark introduces three robot multi-object grasping
benchmarking protocols designed to challenge different aspects of robotic
manipulation. These protocols are: 1) the Only-Pick-Once protocol, which
assesses the robot's ability to efficiently pick multiple objects in a single
attempt; 2) the Accurate pick-trnsferring protocol, which evaluates the robot's
capacity to selectively grasp and transport a specific number of objects from a
cluttered environment; and 3) the Pick-transferring-all protocol, which
challenges the robot to clear an entire scene by sequentially grasping and
transferring all available objects. These protocols are intended to be adopted
by the broader robotics research community, providing a standardized method to
assess and compare robotic systems' performance in multi-object grasping tasks.
We establish baselines for these protocols using standard planning and
perception algorithms on a Barrett hand, Robotiq parallel jar gripper, and the
Pisa/IIT Softhand-2, which is a soft underactuated robotic hand. We discuss the
results in relation to human performance in similar tasks we well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper contains 11 pages and 5 figures. This paper is under
  review of a robotics journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aether: Geometric-Aware Unified World Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Tong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of geometric reconstruction and generative modeling remains a
critical challenge in developing AI systems capable of human-like spatial
reasoning. This paper proposes Aether, a unified framework that enables
geometry-aware reasoning in world models by jointly optimizing three core
capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video
prediction, and (3) goal-conditioned visual planning. Through task-interleaved
feature learning, Aether achieves synergistic knowledge sharing across
reconstruction, prediction, and planning objectives. Building upon video
generation models, our framework demonstrates unprecedented synthetic-to-real
generalization despite never observing real-world data during training.
Furthermore, our approach achieves zero-shot generalization in both action
following and reconstruction tasks, thanks to its intrinsic geometric modeling.
Remarkably, even without real-world data, its reconstruction performance is
comparable with or even better than that of domain-specific models.
Additionally, Aether employs camera trajectories as geometry-informed action
spaces, enabling effective action-conditioned prediction and visual planning.
We hope our work inspires the community to explore new frontiers in
physically-reasonable world modeling and its applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://aether-world.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Any6D: Model-free 6D Pose Estimation of Novel Objects <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeyeop Lee, Bowen Wen, Minjun Kang, Gyuree Kang, In So Kweon, Kuk-Jin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Any6D, a model-free framework for 6D object pose estimation that
requires only a single RGB-D anchor image to estimate both the 6D pose and size
of unknown objects in novel scenes. Unlike existing methods that rely on
textured 3D models or multiple viewpoints, Any6D leverages a joint object
alignment process to enhance 2D-3D alignment and metric scale estimation for
improved pose accuracy. Our approach integrates a render-and-compare strategy
to generate and refine pose hypotheses, enabling robust performance in
scenarios with occlusions, non-overlapping views, diverse lighting conditions,
and large cross-environment variations. We evaluate our method on five
challenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,
demonstrating its effectiveness in significantly outperforming state-of-the-art
methods for novel object pose estimation. Project page:
https://taeyeop.com/any6d
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025, Project Page: https://taeyeop.com/any6d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning framework for action prediction reveals multi-timescale
  locomotor control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16340v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16340v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Chen Wang, Antoine De Comite, Alexandra Voloshina, Monica Daley, Nidhi Seethapathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling movement in real-world tasks is a fundamental goal for motor
control, biomechanics, and rehabilitation engineering. However, widely used
data-driven models of essential tasks like locomotion make simplifying
assumptions such as linear and fixed timescale mappings between past inputs and
future actions, which do not generalize to real-world contexts. Here, we
develop a deep learning-based framework for action prediction with
architecture-dependent trial embeddings, outperforming traditional models
across contexts (walking and running, treadmill and overground, varying
terrains) and input modalities (multiple body states, gaze). We find that
neural network architectures with flexible input history-dependence like GRU
and Transformer perform best overall. By quantifying the model's predictions
relative to an autoregressive baseline, we identify context- and
modality-dependent timescales. These analyses reveal that there is greater
reliance on fast-timescale predictions in complex terrain, gaze predicts future
foot placement before body states, and the full-body state predictions precede
those by center-of-mass-relevant states. This deep learning framework for
action prediction provides quantifiable insights into the control of real-world
locomotion and can be extended to other actions, contexts, and populations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AO-<span class="highlight-title">Grasp</span>: <span class="highlight-title">Articulate</span>d Object <span class="highlight-title">Grasp</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15928v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15928v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlota Parés Morlans, Claire Chen, Yijia Weng, Michelle Yi, Yuying Huang, Nick Heppert, Linqi Zhou, Leonidas Guibas, Jeannette Bohg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AO-Grasp, a grasp proposal method that generates 6 DoF grasps
that enable robots to interact with articulated objects, such as opening and
closing cabinets and appliances. AO-Grasp consists of two main contributions:
the AO-Grasp Model and the AO-Grasp Dataset. Given a segmented partial point
cloud of a single articulated object, the AO-Grasp Model predicts the best
grasp points on the object with an Actionable Grasp Point Predictor. Then, it
finds corresponding grasp orientations for each of these points, resulting in
stable and actionable grasp proposals. We train the AO-Grasp Model on our new
AO-Grasp Dataset, which contains 78K actionable parallel-jaw grasps on
synthetic articulated objects. In simulation, AO-Grasp achieves a 45.0 % grasp
success rate, whereas the highest performing baseline achieves a 35.0% success
rate. Additionally, we evaluate AO-Grasp on 120 real-world scenes of objects
with varied geometries, articulation axes, and joint states, where AO-Grasp
produces successful grasps on 67.5% of scenes, while the baseline only produces
successful grasps on 33.3% of scenes. To the best of our knowledge, AO-Grasp is
the first method for generating 6 DoF grasps on articulated objects directly
from partial point clouds without requiring part detection or hand-designed
grasp heuristics. Project website: https://stanford-iprl-lab.github.io/ao-grasp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://stanford-iprl-lab.github.io/ao-grasp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TwoStep: Multi-agent Task Planning using Classical Planners and Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Bai, Ishika Singh, David Traum, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical planning formulations like the Planning Domain Definition Language
(PDDL) admit action sequences guaranteed to achieve a goal state given an
initial state if any are possible. However, reasoning problems defined in PDDL
do not capture temporal aspects of action taking, such as concurrent actions
between two agents when there are no conflicting conditions, without
significant modification and definition to existing PDDL domains. A human
expert aware of such constraints can decompose a goal into subgoals, each
reachable through single agent planning, to take advantage of simultaneous
actions. In contrast to classical planning, large language models (LLMs)
directly used for inferring plan steps rarely guarantee execution success, but
are capable of leveraging commonsense reasoning to assemble action sequences.
We combine the strengths of both classical planning and LLMs by approximating
human intuitions for multi-agent planning goal decomposition. We demonstrate
that LLM-based goal decomposition leads to faster planning times than solving
multi-agent PDDL problems directly while simultaneously achieving fewer plan
execution steps than a single agent plan alone, as well as most multiagent
plans, while guaranteeing execution success. Additionally, we find that
LLM-based approximations of subgoals result in similar multi-agent execution
lengths to those specified by human experts. Website and resources at
https://glamor-usc.github.io/twostep
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reactive Collision Avoidance for Safe Agile Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Saviolo, Niko Picello, Jeffrey Mao, Rishabh Verma, Giuseppe Loianno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reactive collision avoidance is essential for agile robots navigating complex
and dynamic environments, enabling real-time obstacle response. However, this
task is inherently challenging because it requires a tight integration of
perception, planning, and control, which traditional methods often handle
separately, resulting in compounded errors and delays. This paper introduces a
novel approach that unifies these tasks into a single reactive framework using
solely onboard sensing and computing. Our method combines nonlinear model
predictive control with adaptive control barrier functions, directly linking
perception-driven constraints to real-time planning and control. Constraints
are determined by using a neural network to refine noisy RGB-D data, enhancing
depth accuracy, and selecting points with the minimum time-to-collision to
prioritize the most immediate threats. To maintain a balance between safety and
agility, a heuristic dynamically adjusts the optimization process, preventing
overconstraints in real time. Extensive experiments with an agile quadrotor
demonstrate effective collision avoidance across diverse indoor and outdoor
environments, without requiring environment-specific tuning or explicit
mapping.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotically-Optimal Multi-Query Path Planning for a Polygonal <span class="highlight-title">Robo</span>t <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03920v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03920v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duo Zhang, Zihe Ye, Jingjin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortest-path roadmaps, also known as reduced visibility graphs, provides a
highly efficient multi-query method for computing optimal paths in
two-dimensional environments. Combined with Minkowski sum computations,
shortest-path roadmaps can compute optimal paths for a translating robot in 2D.
In this study, we explore the intuitive idea of stacking up a set of reduced
visibility graphs at different orientations for a polygonal holonomic robot to
support the fast computation of near-optimal paths, allowing simultaneous 2D
translation and rotation. The resulting algorithm, rotation-stacked visibility
graph (RVG), is shown to be resolution-complete and asymptotically optimal.
Extensive computational experiments show RVG significantly outperforms
state-of-the-art single- and multi-query sampling-based methods on both
computation time and solution optimality fronts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perception of Emotions in Human and <span class="highlight-title">Robo</span>t Faces: Is the Eye Region
  Enough? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14337v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14337v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmaya Mishra, Gabriel Skantze, Peter Hagoort, Rinus Verdonschot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increased interest in developing next-gen social robots has raised
questions about the factors affecting the perception of robot emotions. This
study investigates the impact of robot appearances (humanlike, mechanical) and
face regions (full-face, eye-region) on human perception of robot emotions. A
between-subjects user study (N = 305) was conducted where participants were
asked to identify the emotions being displayed in videos of robot faces, as
well as a human baseline. Our findings reveal three important insights for
effective social robot face design in Human-Robot Interaction (HRI): Firstly,
robots equipped with a back-projected, fully animated face - regardless of
whether they are more human-like or more mechanical-looking - demonstrate a
capacity for emotional expression comparable to that of humans. Secondly, the
recognition accuracy of emotional expressions in both humans and robots
declines when only the eye region is visible. Lastly, within the constraint of
only the eye region being visible, robots with more human-like features
significantly enhance emotion recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 16th International Conference on
  Social Robotics, Odense, Denmark (ICSR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autoregressive Action Sequence Learning for <span class="highlight-title">Robo</span>tic <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03132v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03132v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhang, Yuhan Liu, Haonan Chang, Liam Schramm, Abdeslam Boularias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing a universal policy architecture that performs well across diverse
robots and task configurations remains a key challenge. In this work, we
address this by representing robot actions as sequential data and generating
actions through autoregressive sequence modeling. Existing autoregressive
architectures generate end-effector waypoints sequentially as word tokens in
language modeling, which are limited to low-frequency control tasks. Unlike
language, robot actions are heterogeneous and often include continuous values
-- such as joint positions, 2D pixel coordinates, and end-effector poses --
which are not easily suited for language-based modeling. Based on this insight,
we introduce a straightforward enhancement: we extend causal transformers'
single-token prediction to support predicting a variable number of tokens in a
single step through our Chunking Causal Transformer (CCT). This enhancement
enables robust performance across diverse tasks of various control frequencies,
greater efficiency by having fewer autoregression steps, and lead to a hybrid
action sequence design by mixing different types of actions and using a
different chunk size for each action type. Based on CCT, we propose the
Autoregressive Policy (ARP) architecture, which solves manipulation tasks by
generating hybrid action sequences. We evaluate ARP across diverse robotic
manipulation environments, including Push-T, ALOHA, and RLBench, and show that
ARP, as a universal architecture, matches or outperforms the
environment-specific state-of-the-art in all tested benchmarks, while being
more efficient in computation and parameter sizes. Videos of our real robot
demonstrations, all source code and the pretrained models of ARP can be found
at http://github.com/mlzxy/arp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>(RA-L 2025) Add a new figure to explain why chunking autoregression
  works. Put back the previous in-depth discussion for arxiv release</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joey Wilson, Marcelino Almeida, Sachit Mahajan, Martin Labrie, Maani Ghaffari, Omid Ghasemalizadeh, Min Sun, Cheng-Hao Kuo, Arnab Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel algorithm for quantifying uncertainty and
information gained within 3D Gaussian Splatting (3D-GS) through P-Optimality.
While 3D-GS has proven to be a useful world model with high-quality
rasterizations, it does not natively quantify uncertainty or information,
posing a challenge for real-world applications such as 3D-GS SLAM. We propose
to quantify information gain in 3D-GS by reformulating the problem through the
lens of optimal experimental design, which is a classical solution widely used
in literature. By restructuring information quantification of 3D-GS through
optimal experimental design, we arrive at multiple solutions, of which
T-Optimality and D-Optimality perform the best quantitatively and qualitatively
as measured on two popular datasets. Additionally, we propose a block diagonal
covariance approximation which provides a measure of correlation at the expense
of a greater computation cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UAVs Meet LLMs: <span class="highlight-title">Overview</span>s and Perspectives Toward Agentic Low-Altitude
  Mobility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonglin Tian, Fei Lin, Yiduo Li, Tengchao Zhang, Qiyao Zhang, Xuan Fu, Jun Huang, Xingyuan Dai, Yutong Wang, Chunwei Tian, Bai Li, Yisheng Lv, Levente Kovács, Fei-Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has
introduced transformative advancements across various domains, like
transportation, logistics, and agriculture. Leveraging flexible perspectives
and rapid maneuverability, UAVs extend traditional systems' perception and
action capabilities, garnering widespread attention from academia and industry.
However, current UAV operations primarily depend on human control, with only
limited autonomy in simple scenarios, and lack the intelligence and
adaptability needed for more complex environments and tasks. The emergence of
large language models (LLMs) demonstrates remarkable problem-solving and
generalization capabilities, offering a promising pathway for advancing UAV
intelligence. This paper explores the integration of LLMs and UAVs, beginning
with an overview of UAV systems' fundamental components and functionalities,
followed by an overview of the state-of-the-art in LLM technology.
Subsequently, it systematically highlights the multimodal data resources
available for UAVs, which provide critical support for training and evaluation.
Furthermore, it categorizes and analyzes key tasks and application scenarios
where UAVs and LLMs converge. Finally, a reference roadmap towards agentic UAVs
is proposed, aiming to enable UAVs to achieve agentic intelligence through
autonomous perception, memory, reasoning, and tool utilization. Related
resources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Performance-Based Design Optimization Framework for Soft
  <span class="highlight-title">Gripper</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Rahimi Nohooji, Holger Voos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a hierarchical, performance-based framework for the
design optimization of multi-fingered soft grippers. To address the need for
systematically defined performance indices, the framework structures the
optimization process into three integrated layers: Task Space, Motion Space,
and Design Space. In the Task Space, performance indices are defined as core
objectives, while the Motion Space interprets these into specific movement
primitives. Finally, the Design Space applies parametric and topological
optimization techniques to refine the geometry and material distribution of the
system, achieving a balanced design across key performance metrics. The
framework's layered structure enhances SG design, ensuring balanced performance
and scalability for complex tasks and contributing to broader advancements in
soft robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, 1 Algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Helvipad: A Real-World <span class="highlight-title">Dataset</span> for Omnidirectional Stereo Depth
  Estimation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Zayene, Jannik Endres, Albias Havolli, Charles Corbière, Salim Cherkaoui, Alexandre Kontouli, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite progress in stereo depth estimation, omnidirectional imaging remains
underexplored, mainly due to the lack of appropriate data. We introduce
Helvipad, a real-world dataset for omnidirectional stereo depth estimation,
featuring 40K video frames from video sequences across diverse environments,
including crowded indoor and outdoor scenes with various lighting conditions.
Collected using two 360{\deg} cameras in a top-bottom setup and a LiDAR sensor,
the dataset includes accurate depth and disparity labels by projecting 3D point
clouds onto equirectangular images. Additionally, we provide an augmented
training set with an increased label density by using depth completion. We
benchmark leading stereo depth estimation models for both standard and
omnidirectional images. The results show that while recent stereo methods
perform decently, a challenge persists in accurately estimating depth in
omnidirectional imaging. To address this, we introduce necessary adaptations to
stereo models, leading to improved performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025. Project page:
  https://vita-epfl.github.io/Helvipad</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitation Learning with Limited Actions via <span class="highlight-title">Diffusion</span> Planners and Deep
  Koopman Controllers <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianxin Bi, Kelvin Lim, Kaiqi Chen, Yifei Huang, Harold Soh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion-based robot policies have demonstrated
significant potential in imitating multi-modal behaviors. However, these
approaches typically require large quantities of demonstration data paired with
corresponding robot action labels, creating a substantial data collection
burden. In this work, we propose a plan-then-control framework aimed at
improving the action-data efficiency of inverse dynamics controllers by
leveraging observational demonstration data. Specifically, we adopt a Deep
Koopman Operator framework to model the dynamical system and utilize
observation-only trajectories to learn a latent action representation. This
latent representation can then be effectively mapped to real high-dimensional
continuous actions using a linear action decoder, requiring minimal
action-labeled data. Through experiments on simulated robot manipulation tasks
and a real robot experiment with multi-modal expert demonstrations, we
demonstrate that our approach significantly enhances action-data efficiency and
achieves high task success rates with limited action data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Conference on Robotics and Automation
  (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On-Device Self-Supervised Learning of Low-Latency Monocular Depth from
  Only Events <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Hagenaars, Yilun Wu, Federico Paredes-Vallés, Stein Stroobants, Guido de Croon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras provide low-latency perception for only milliwatts of power.
This makes them highly suitable for resource-restricted, agile robots such as
small flying drones. Self-supervised learning based on contrast maximization
holds great potential for event-based robot vision, as it foregoes the need for
high-frequency ground truth and allows for online learning in the robot's
operational environment. However, online, on-board learning raises the major
challenge of achieving sufficient computational efficiency for real-time
learning, while maintaining competitive visual perception performance. In this
work, we improve the time and memory efficiency of the contrast maximization
pipeline, making on-device learning of low-latency monocular depth possible. We
demonstrate that online learning on board a small drone yields more accurate
depth estimates and more successful obstacle avoidance behavior compared to
only pre-training. Benchmarking experiments show that the proposed pipeline is
not only efficient, but also achieves state-of-the-art depth estimation
performance among self-supervised approaches. Our work taps into the unused
potential of online, on-device robot learning, promising smaller reality gaps
and better performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BimArt: A Unified Approach for the Synthesis of 3D Bimanual Interaction
  with <span class="highlight-title">Articulate</span>d Objects <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanyue Zhang, Rishabh Dabral, Vladislav Golyanik, Vasileios Choutas, Eduardo Alvarado, Thabo Beeler, Marc Habermann, Christian Theobalt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present BimArt, a novel generative approach for synthesizing 3D bimanual
hand interactions with articulated objects. Unlike prior works, we do not rely
on a reference grasp, a coarse hand trajectory, or separate modes for grasping
and articulating. To achieve this, we first generate distance-based contact
maps conditioned on the object trajectory with an articulation-aware feature
representation, revealing rich bimanual patterns for manipulation. The learned
contact prior is then used to guide our hand motion generator, producing
diverse and realistic bimanual motions for object movement and articulation.
Our work offers key insights into feature representation and contact prior for
articulated objects, demonstrating their effectiveness in taming the complex,
high-dimensional space of bimanual hand-object interactions. Through
comprehensive quantitative experiments, we demonstrate a clear step towards
simplified and high-quality hand-object animations that surpass the state of
the art in motion quality and diversity. Project page:
https://vcai.mpi-inf.mpg.de/projects/bimart/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robo</span>Matrix: A Skill-centric Hierarchical Framework for Scalable <span class="highlight-title">Robo</span>t
  Task Planning and Execution in Open-World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00171v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00171v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixin Mao, Weiheng Zhong, Zhou Jiang, Dong Fang, Zhongyue Zhang, Zihan Lan, Haosheng Li, Fan Jia, Tiancai Wang, Haoqiang Fan, Osamu Yoshie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing robot policies predominantly adopt the task-centric approach,
requiring end-to-end task data collection. This results in limited
generalization to new tasks and difficulties in pinpointing errors within
long-horizon, multi-stage tasks. To address this, we propose RoboMatrix, a
skill-centric hierarchical framework designed for scalable robot task planning
and execution in open-world environments. RoboMatrix extracts general
meta-skills from diverse complex tasks, enabling the completion of unseen tasks
through skill composition. Its architecture consists of a high-level scheduling
layer that utilizes large language models (LLMs) for task decomposition, an
intermediate skill layer housing meta-skill models, and a low-level hardware
layer for robot control. A key innovation of our work is the introduction of
the first unified vision-language-action (VLA) model capable of seamlessly
integrating both movement and manipulation within one model. This is achieved
by combining vision and language prompts to generate discrete actions.
Experimental results demonstrate that RoboMatrix achieves a 50% higher success
rate than task-centric baselines when applied to unseen objects, scenes, and
tasks. To advance open-world robotics research, we will open-source code,
hardware designs, model weights, and datasets at
https://github.com/WayneMao/RoboMatrix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MCVO: A Generic <span class="highlight-title">Visual</span> Odometry for Arbitrarily Arranged Multi-<span class="highlight-title">Camera</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huai Yu, Junhao Wang, Yao He, Wen Yang, Gui-Song Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Making multi-camera visual SLAM systems easier to set up and more robust to
the environment is attractive for vision robots. Existing monocular and
binocular vision SLAM systems have narrow sensing Field-of-View (FoV),
resulting in degenerated accuracy and limited robustness in textureless
environments. Thus multi-camera SLAM systems are gaining attention because they
can provide redundancy with much wider FoV. However, the usual arbitrary
placement and orientation of multiple cameras make the pose scale estimation
and system updating challenging. To address these problems, we propose a robust
visual odometry system for rigidly-bundled arbitrarily-arranged multi-cameras,
namely MCVO, which can achieve metric-scale state estimation with high
flexibility in the cameras' arrangement. Specifically, we first design a
learning-based feature tracking framework to shift the pressure of CPU
processing of multiple video streams to GPU. Then we initialize the odometry
system with the metric-scale poses under the rigid constraints between moving
cameras. Finally, we fuse the features of the multi-cameras in the back-end to
achieve robust pose estimation and online scale optimization. Additionally,
multi-camera features help improve the loop detection for pose graph
optimization. Experiments on KITTI-360 and MultiCamData datasets validate its
robustness over arbitrarily arranged cameras. Compared with other stereo and
multi-camera visual SLAM systems, our method obtains higher pose accuracy with
better generalization ability. Our codes and online demos are available at
https://github.com/JunhaoWang615/MCVO
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> <span class="highlight-title">Robo</span>Brain: A Unified Brain Model for <span class="highlight-title">Robo</span>tic <span class="highlight-title">Manipulation</span> from Abstract
  to Concrete 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, <span class="highlight-author">Yao Mu</span>, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Multimodal Large Language Models (MLLMs) have shown
remarkable capabilities across various multimodal contexts. However, their
application in robotic scenarios, particularly for long-horizon manipulation
tasks, reveals significant limitations. These limitations arise from the
current MLLMs lacking three essential robotic brain capabilities: Planning
Capability, which involves decomposing complex manipulation instructions into
manageable sub-tasks; Affordance Perception, the ability to recognize and
interpret the affordances of interactive objects; and Trajectory Prediction,
the foresight to anticipate the complete manipulation trajectory necessary for
successful execution. To enhance the robotic brain's core capabilities from
abstract to concrete, we introduce ShareRobot, a high-quality heterogeneous
dataset that labels multi-dimensional information such as task planning, object
affordance, and end-effector trajectory. ShareRobot's diversity and accuracy
have been meticulously refined by three human annotators. Building on this
dataset, we developed RoboBrain, an MLLM-based model that combines robotic and
general multi-modal data, utilizes a multi-stage training strategy, and
incorporates long videos and high-resolution images to improve its robotic
manipulation capabilities. Extensive experiments demonstrate that RoboBrain
achieves state-of-the-art performance across various robotic tasks,
highlighting its potential to advance robotic brain capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoURDF: Unsupervised <span class="highlight-title">Robo</span>t Modeling from Point Cloud Frames Using
  Cluster Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiong Lin, Lechen Zhang, Kwansoo Lee, Jialong Ning, Judah Goldfeder, Hod Lipson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot description models are essential for simulation and control, yet their
creation often requires significant manual effort. To streamline this modeling
process, we introduce AutoURDF, an unsupervised approach for constructing
description files for unseen robots from point cloud frames. Our method
leverages a cluster-based point cloud registration model that tracks the 6-DoF
transformations of point clusters. Through analyzing cluster movements, we
hierarchically address the following challenges: (1) moving part segmentation,
(2) body topology inference, and (3) joint parameter estimation. The complete
pipeline produces robot description files that are fully compatible with
existing simulators. We validate our method across a variety of robots, using
both synthetic and real-world scan data. Results indicate that our approach
outperforms previous methods in registration and body topology estimation
accuracy, offering a scalable solution for automated robot modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> SyncDiff: Synchronized Motion <span class="highlight-title">Diffusion</span> for Multi-Body Human-Object
  Interaction Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20104v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20104v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkun He, Yun Liu, Ruitao Liu, <span class="highlight-author">Li Yi</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing realistic human-object interaction motions is a critical problem
in VR/AR and human animation. Unlike the commonly studied scenarios involving a
single human or hand interacting with one object, we address a more generic
multi-body setting with arbitrary numbers of humans, hands, and objects. This
complexity introduces significant challenges in synchronizing motions due to
the high correlations and mutual influences among bodies. To address these
challenges, we introduce SyncDiff, a novel method for multi-body interaction
synthesis using a synchronized motion diffusion strategy. SyncDiff employs a
single diffusion model to capture the joint distribution of multi-body motions.
To enhance motion fidelity, we propose a frequency-domain motion decomposition
scheme. Additionally, we introduce a new set of alignment scores to emphasize
the synchronization of different body motions. SyncDiff jointly optimizes both
data sample likelihood and alignment likelihood through an explicit
synchronization strategy. Extensive experiments across four datasets with
various multi-body configurations demonstrate the superiority of SyncDiff over
existing state-of-the-art motion synthesis methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aether: Geometric-Aware Unified World Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Tong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of geometric reconstruction and generative modeling remains a
critical challenge in developing AI systems capable of human-like spatial
reasoning. This paper proposes Aether, a unified framework that enables
geometry-aware reasoning in world models by jointly optimizing three core
capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video
prediction, and (3) goal-conditioned visual planning. Through task-interleaved
feature learning, Aether achieves synergistic knowledge sharing across
reconstruction, prediction, and planning objectives. Building upon video
generation models, our framework demonstrates unprecedented synthetic-to-real
generalization despite never observing real-world data during training.
Furthermore, our approach achieves zero-shot generalization in both action
following and reconstruction tasks, thanks to its intrinsic geometric modeling.
Remarkably, even without real-world data, its reconstruction performance is
comparable with or even better than that of domain-specific models.
Additionally, Aether employs camera trajectories as geometry-informed action
spaces, enabling effective action-conditioned prediction and visual planning.
We hope our work inspires the community to explore new frontiers in
physically-reasonable world modeling and its applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://aether-world.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HunyuanPortrait: Implicit Condition Control for Enhanced Portrait
  Animation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zunnan Xu, Zhentao Yu, Zixiang Zhou, Jun Zhou, Xiaoyu Jin, Fa-Ting Hong, Xiaozhong Ji, Junwei Zhu, Chengfei Cai, Shiyu Tang, Qin Lin, Xiu Li, Qinglin Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HunyuanPortrait, a diffusion-based condition control method that
employs implicit representations for highly controllable and lifelike portrait
animation. Given a single portrait image as an appearance reference and video
clips as driving templates, HunyuanPortrait can animate the character in the
reference image by the facial expression and head pose of the driving videos.
In our framework, we utilize pre-trained encoders to achieve the decoupling of
portrait motion information and identity in videos. To do so, implicit
representation is adopted to encode motion information and is employed as
control signals in the animation phase. By leveraging the power of stable video
diffusion as the main building block, we carefully design adapter layers to
inject control signals into the denoising unet through attention mechanisms.
These bring spatial richness of details and temporal consistency.
HunyuanPortrait also exhibits strong generalization performance, which can
effectively disentangle appearance and motion under different image styles. Our
framework outperforms existing methods, demonstrating superior temporal
consistency and controllability. Our project is available at
https://kkakkkka.github.io/HunyuanPortrait.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MC-LLaVA: Multi-Concept Personalized <span class="highlight-title">Vision</span>-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, Shanghang Zhang, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current vision-language models (VLMs) show exceptional abilities across
diverse tasks, such as visual question answering. To enhance user experience,
recent studies investigate VLM personalization to understand user-provided
concepts. However, they mainly focus on single-concept personalization,
neglecting the existence and interplay of multiple concepts, which limits
real-world applicability. This paper proposes the first multi-concept
personalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a
multi-concept instruction tuning strategy, effectively integrating multiple
concepts in a single training step. To reduce the costs related to joint
training, we propose a personalized textual prompt that uses visual token
information to initialize concept tokens. Additionally, we introduce a
personalized visual prompt during inference, aggregating location confidence
maps for enhanced recognition and grounding capabilities. To advance
multi-concept personalization research, we further contribute a high-quality
instruction tuning dataset. We carefully collect images with multiple
characters and objects from movies and manually generate question-answer
samples for multi-concept scenarios, featuring superior diversity.
Comprehensive qualitative and quantitative experiments demonstrate that
MC-LLaVA can achieve impressive multi-concept personalized responses, paving
the way for VLMs to become better user-specific assistants. The code and
dataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I sincerely apologize for any inconvenience caused. We actually
  uploaded this paper to arXiv in November 2024, as arXiv:2411.11706. During
  this update, we did not consider the replacement operation of arXiv, which
  led to duplicate submissions. We have made modifications at the original
  address arXiv:2411.11706</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to segment anatomy and lesions from disparately labeled sources
  in brain MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meva Himmetoglu, Ilja Ciernik, Ender Konukoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting healthy tissue structures alongside lesions in brain Magnetic
Resonance Images (MRI) remains a challenge for today's algorithms due to
lesion-caused disruption of the anatomy and lack of jointly labeled training
datasets, where both healthy tissues and lesions are labeled on the same
images. In this paper, we propose a method that is robust to lesion-caused
disruptions and can be trained from disparately labeled training sets, i.e.,
without requiring jointly labeled samples, to automatically segment both. In
contrast to prior work, we decouple healthy tissue and lesion segmentation in
two paths to leverage multi-sequence acquisitions and merge information with an
attention mechanism. During inference, an image-specific adaptation reduces
adverse influences of lesion regions on healthy tissue predictions. During
training, the adaptation is taken into account through meta-learning and
co-training is used to learn from disparately labeled training images. Our
model shows an improved performance on several anatomical structures and
lesions on a publicly available brain glioblastoma dataset compared to the
state-of-the-art segmentation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lightweight Embedded FPGA Deployment of Learned Image Compression with
  Knowledge Distillation and Hybrid Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04832v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04832v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alaa Mazouz, Sumanta Chaudhuri, Marco Cagnanzzo, Mihai Mitrea, Enzo Tartaglione, Attilio Fiandrotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learnable Image Compression (LIC) has shown the potential to outperform
standardized video codecs in RD efficiency, prompting the research for
hardware-friendly implementations. Most existing LIC hardware implementations
prioritize latency to RD-efficiency and through an extensive exploration of the
hardware design space. We present a novel design paradigm where the burden of
tuning the design for a specific hardware platform is shifted towards model
dimensioning and without compromising on RD-efficiency. First, we design a
framework for distilling a leaner student LIC model from a reference teacher:
by tuning a single model hyperparameters, we can meet the constraints of
different hardware platforms without a complex hardware design exploration.
Second, we propose a hardware-friendly implementation of the Generalized
Divisive Normalization - GDN activation that preserves RD efficiency even post
parameter quantization. Third, we design a pipelined FPGA configuration which
takes full advantage of available FPGA resources by leveraging parallel
processing and optimizing resource allocation. Our experiments with a state of
the art LIC model show that we outperform all existing FPGA implementations
while performing very close to the original model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1. Submitted to IEEE Transactions on Circuits and Systems for Video
  Technology in March 2025. 2. Corrected numerous mistakes from previous
  versions in results, citations and metrics numbers in figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequency Dynamic Convolution for Dense Image Prediction <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linwei Chen, Lin Gu, Liang Li, Chenggang Yan, Ying Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Dynamic Convolution (DY-Conv) has shown promising performance by
enabling adaptive weight selection through multiple parallel weights combined
with an attention mechanism, the frequency response of these weights tends to
exhibit high similarity, resulting in high parameter costs but limited
adaptability. In this work, we introduce Frequency Dynamic Convolution
(FDConv), a novel approach that mitigates these limitations by learning a fixed
parameter budget in the Fourier domain. FDConv divides this budget into
frequency-based groups with disjoint Fourier indices, enabling the construction
of frequency-diverse weights without increasing the parameter cost. To further
enhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency
Band Modulation (FBM). KSM dynamically adjusts the frequency response of each
filter at the spatial level, while FBM decomposes weights into distinct
frequency bands in the frequency domain and modulates them dynamically based on
local content. Extensive experiments on object detection, segmentation, and
classification validate the effectiveness of FDConv. We demonstrate that when
applied to ResNet-50, FDConv achieves superior performance with a modest
increase of +3.6M parameters, outperforming previous methods that require
substantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).
Moreover, FDConv seamlessly integrates into a variety of architectures,
including ConvNeXt, Swin-Transformer, offering a flexible and efficient
solution for modern vision tasks. The code is made publicly available at
https://github.com/Linwei-Chen/FDConv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re-HOLD: Video Hand Object Interaction Reenactment via adaptive
  Layout-instructed <span class="highlight-title">Diffusion</span> Model <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16942v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16942v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingying Fan, Quanwei Yang, Kaisiyuan Wang, Hang Zhou, Yingying Li, Haocheng Feng, Errui Ding, Yu Wu, Jingdong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current digital human studies focusing on lip-syncing and body movement are
no longer sufficient to meet the growing industrial demand, while human video
generation techniques that support interacting with real-world environments
(e.g., objects) have not been well investigated. Despite human hand synthesis
already being an intricate problem, generating objects in contact with hands
and their interactions presents an even more challenging task, especially when
the objects exhibit obvious variations in size and shape. To tackle these
issues, we present a novel video Reenactment framework focusing on Human-Object
Interaction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD).
Our key insight is to employ specialized layout representation for hands and
objects, respectively. Such representations enable effective disentanglement of
hand modeling and object adaptation to diverse motion sequences. To further
improve the generation quality of HOI, we design an interactive textural
enhancement module for both hands and objects by introducing two independent
memory banks. We also propose a layout adjustment strategy for the cross-object
reenactment scenario to adaptively adjust unreasonable layouts caused by
diverse object sizes during inference. Comprehensive qualitative and
quantitative evaluations demonstrate that our proposed framework significantly
outperforms existing methods. Project page: https://fyycs.github.io/Re-HOLD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IncEventGS: Pose-Free Gaussian Splatting from a Single Event <span class="highlight-title">Camera</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08107v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08107v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Huang, Chengrui Dong, Xuanhua Chen, Peidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for
novel view synthesis have achieved remarkable progress with frame-based camera
(e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel
type of bio-inspired visual sensor, i.e. event camera, has demonstrated
advantages in high temporal resolution, high dynamic range, low power
consumption and low latency. Due to its unique asynchronous and irregular data
capturing process, limited work has been proposed to apply neural
representation or 3D Gaussian splatting for an event camera. In this work, we
present IncEventGS, an incremental 3D Gaussian Splatting reconstruction
algorithm with a single event camera. To recover the 3D scene representation
incrementally, we exploit the tracking and mapping paradigm of conventional
SLAM pipelines for IncEventGS. Given the incoming event stream, the tracker
firstly estimates an initial camera motion based on prior reconstructed 3D-GS
scene representation. The mapper then jointly refines both the 3D scene
representation and camera motion based on the previously estimated motion
trajectory from the tracker. The experimental results demonstrate that
IncEventGS delivers superior performance compared to prior NeRF-based methods
and other related baselines, even we do not have the ground-truth camera poses.
Furthermore, our method can also deliver better performance compared to
state-of-the-art event visual odometry methods in terms of camera motion
estimation. Code is publicly available at:
https://github.com/wu-cvgl/IncEventGS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Page: https://github.com/wu-cvgl/IncEventGS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Any6D: Model-free 6D Pose Estimation of Novel Objects <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeyeop Lee, Bowen Wen, Minjun Kang, Gyuree Kang, In So Kweon, Kuk-Jin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Any6D, a model-free framework for 6D object pose estimation that
requires only a single RGB-D anchor image to estimate both the 6D pose and size
of unknown objects in novel scenes. Unlike existing methods that rely on
textured 3D models or multiple viewpoints, Any6D leverages a joint object
alignment process to enhance 2D-3D alignment and metric scale estimation for
improved pose accuracy. Our approach integrates a render-and-compare strategy
to generate and refine pose hypotheses, enabling robust performance in
scenarios with occlusions, non-overlapping views, diverse lighting conditions,
and large cross-environment variations. We evaluate our method on five
challenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,
demonstrating its effectiveness in significantly outperforming state-of-the-art
methods for novel object pose estimation. Project page:
https://taeyeop.com/any6d
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025, Project Page: https://taeyeop.com/any6d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Calibration enhanced Parameter Synthesis for CLIP-based
  Class-incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncen Guo, Xiaoguang Zhu, Lianlong Sun, Liangyu Teng, Di Li, Yang Liu, Liang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental Learning (CIL) enables models to continuously learn new
class knowledge while memorizing previous classes, facilitating their
adaptation and evolution in dynamic environments. Traditional CIL methods are
mainly based on visual features, which limits their ability to handle complex
scenarios. In contrast, Vision-Language Models (VLMs) show promising potential
to promote CIL by integrating pretrained knowledge with textual features.
However, previous methods make it difficult to overcome catastrophic forgetting
while preserving the generalization capabilities of VLMs. To tackle these
challenges, we propose Feature Calibration enhanced Parameter Synthesis (FCPS)
in this paper. Specifically, our FCPS employs a specific parameter adjustment
mechanism to iteratively refine the proportion of original visual features
participating in the final class determination, ensuring the model's
foundational generalization capabilities. Meanwhile, parameter integration
across different tasks achieves a balance between learning new class knowledge
and retaining old knowledge. Experimental results on popular benchmarks (e.g.,
CIFAR100 and ImageNet100) validate the superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superpixel Tokenization for <span class="highlight-title">Vision</span> Transformers: Preserving Semantic
  Integrity in <span class="highlight-title">Visual</span> Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04680v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04680v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaihyun Lew, Soohyuk Jang, Jaehoon Lee, Seungryong Yoo, Eunji Kim, Saehyung Lee, Jisoo Mok, Siwon Kim, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers, a groundbreaking architecture proposed for Natural Language
Processing (NLP), have also achieved remarkable success in Computer Vision. A
cornerstone of their success lies in the attention mechanism, which models
relationships among tokens. While the tokenization process in NLP inherently
ensures that a single token does not contain multiple semantics, the
tokenization of Vision Transformer (ViT) utilizes tokens from uniformly
partitioned square image patches, which may result in an arbitrary mixing of
visual concepts in a token. In this work, we propose to substitute the
grid-based tokenization in ViT with superpixel tokenization, which employs
superpixels to generate a token that encapsulates a sole visual concept.
Unfortunately, the diverse shapes, sizes, and locations of superpixels make
integrating superpixels into ViT tokenization rather challenging. Our
tokenization pipeline, comprised of pre-aggregate extraction and
superpixel-aware aggregation, overcomes the challenges that arise in superpixel
tokenization. Extensive experiments demonstrate that our approach, which
exhibits strong compatibility with existing frameworks, enhances the accuracy
and robustness of ViT on various downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/jangsoohyuk/SuiT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12836v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12836v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park, Eunbyung Park, Sangpil Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D
reconstruction and novel view synthesis, leading to its widespread commercial
use. Consequently, copyright protection via watermarking has become critical.
However, because 3DGS relies on millions of Gaussians, which require gigabytes
of storage, efficient transfer and storage require compression. Existing 3DGS
watermarking methods are vulnerable to quantization-based compression, often
resulting in the loss of the embedded watermark. To address this challenge, we
propose a novel watermarking method that ensures watermark robustness after
model compression while maintaining high rendering quality. In detail, we
incorporate a quantization distortion layer that simulates compression during
training, preserving the watermark under quantization-based compression. Also,
we propose a learnable watermark embedding feature that embeds the watermark
into the anchor feature, ensuring structural consistency and seamless
integration into the 3D scene. Furthermore, we present a frequency-aware anchor
growing mechanism to enhance image quality in high-frequency regions by
effectively identifying Guassians within these regions. Experimental results
confirm that our method preserves the watermark and maintains superior image
quality under high compression, validating it as a promising approach for a
secure 3DGS model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMD-Hummingbird: Towards an Efficient Text-to-Video Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18559v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18559v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Isobe, He Cui, Dong Zhou, Mengmeng Ge, Dong Li, Emad Barsoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Video (T2V) generation has attracted significant attention for its
ability to synthesize realistic videos from textual descriptions. However,
existing models struggle to balance computational efficiency and high visual
quality, particularly on resource-limited devices, e.g.,iGPUs and mobile
phones. Most prior work prioritizes visual fidelity while overlooking the need
for smaller, more efficient models suitable for real-world deployment. To
address this challenge, we propose a lightweight T2V framework, termed
Hummingbird, which prunes existing models and enhances visual quality through
visual feedback learning. Our approach reduces the size of the U-Net from 1.4
billion to 0.7 billion parameters, significantly improving efficiency while
preserving high-quality video generation. Additionally, we introduce a novel
data processing pipeline that leverages Large Language Models (LLMs) and Video
Quality Assessment (VQA) models to enhance the quality of both text prompts and
video data. To support user-driven training and style customization, we
publicly release the full training code, including data processing and model
training. Extensive experiments show that our method achieves a 31X speedup
compared to state-of-the-art models such as VideoCrafter2, while also attaining
the highest overall score on VBench. Moreover, our method supports the
generation of videos with up to 26 frames, addressing the limitations of
existing U-Net-based methods in long video generation. Notably, the entire
training process requires only four GPUs, yet delivers performance competitive
with existing leading methods. Hummingbird presents a practical and efficient
solution for T2V generation, combining high performance, scalability, and
flexibility for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage:
  https://www.amd.com/en/developer/resources/technical-articles/amd-hummingbird-0-9b-text-to-video-diffusion-model-with-4-step-inferencing.html|
  GitHub: https://github.com/AMD-AIG-AIMA/AMD-Hummingbird-T2V</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIM2PC: Aerial Image to 3D Building Point Cloud Reconstruction <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soulaimene Turki, Daniel Panangian, Houda Chaabouni-Chouayakh, Ksenia Bittner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional urban reconstruction of buildings from single-view images
has attracted significant attention over the past two decades. However, recent
methods primarily focus on rooftops from aerial images, often overlooking
essential geometrical details. Additionally, there is a notable lack of
datasets containing complete 3D point clouds for entire buildings, along with
challenges in obtaining reliable camera pose information for aerial images.
This paper addresses these challenges by presenting a novel methodology, AIM2PC
, which utilizes our generated dataset that includes complete 3D point clouds
and determined camera poses. Our approach takes features from a single aerial
image as input and concatenates them with essential additional conditions, such
as binary masks and Sobel edge maps, to enable more edge-aware reconstruction.
By incorporating a point cloud diffusion model based on Centered denoising
Diffusion Probabilistic Models (CDPM), we project these concatenated features
onto the partially denoised point cloud using our camera poses at each
diffusion step. The proposed method is able to reconstruct the complete 3D
building point cloud, including wall information and demonstrates superior
performance compared to existing baseline techniques. To allow further
comparisons with our methodology the dataset has been made available at
https://github.com/Soulaimene/AIM2PCDataset
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISPRS Geospatial Week 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhang, Weihong Pan, Chong Bao, Xiyu Zhang, Xiaojun Xiang, Hanqing Jiang, Hujun Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans perceive and comprehend their surroundings through information
spanning multiple frequencies. In immersive scenes, people naturally scan their
environment to grasp its overall structure while examining fine details of
objects that capture their attention. However, current NeRF frameworks
primarily focus on modeling either high-frequency local views or the broad
structure of scenes with low-frequency information, which is limited to
balancing both. We introduce FA-NeRF, a novel frequency-aware framework for
view synthesis that simultaneously captures the overall scene structure and
high-definition details within a single NeRF model. To achieve this, we propose
a 3D frequency quantification method that analyzes the scene's frequency
distribution, enabling frequency-aware rendering. Our framework incorporates a
frequency grid for fast convergence and querying, a frequency-aware feature
re-weighting strategy to balance features across different frequency contents.
Extensive experiments show that our method significantly outperforms existing
approaches in modeling entire scenes while preserving fine details. Project
page: https://coscatter.github.io/LookCloser/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Project page: https://coscatter.github.io/LookCloser</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-24T00:00:00Z">2025-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">55</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aether: Geometric-Aware Unified World Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Tong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of geometric reconstruction and generative modeling remains a
critical challenge in developing AI systems capable of human-like spatial
reasoning. This paper proposes Aether, a unified framework that enables
geometry-aware reasoning in world models by jointly optimizing three core
capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video
prediction, and (3) goal-conditioned visual planning. Through task-interleaved
feature learning, Aether achieves synergistic knowledge sharing across
reconstruction, prediction, and planning objectives. Building upon video
generation models, our framework demonstrates unprecedented synthetic-to-real
generalization despite never observing real-world data during training.
Furthermore, our approach achieves zero-shot generalization in both action
following and reconstruction tasks, thanks to its intrinsic geometric modeling.
Remarkably, even without real-world data, its reconstruction performance far
exceeds that of domain-specific models. Additionally, Aether leverages a
geometry-informed action space to seamlessly translate predictions into
actions, enabling effective autonomous trajectory planning. We hope our work
inspires the community to explore new frontiers in physically-reasonable world
modeling and its applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://aether-world.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaWorld: Learning Adaptable World Models with Latent Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models aim to learn action-controlled prediction models and have proven
essential for the development of intelligent agents. However, most existing
world models rely heavily on substantial action-labeled data and costly
training, making it challenging to adapt to novel environments with
heterogeneous actions through limited interactions. This limitation can hinder
their applicability across broader domains. To overcome this challenge, we
propose AdaWorld, an innovative world model learning approach that enables
efficient adaptation. The key idea is to incorporate action information during
the pretraining of world models. This is achieved by extracting latent actions
from videos in a self-supervised manner, capturing the most critical
transitions between frames. We then develop an autoregressive world model that
conditions on these latent actions. This learning paradigm enables highly
adaptable world models, facilitating efficient transfer and learning of new
actions even with limited interactions and finetuning. Our comprehensive
experiments across multiple environments demonstrate that AdaWorld achieves
superior performance in both simulation quality and visual planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://adaptable-world-model.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Generation of Sub-goals for Lifelong Learning in <span class="highlight-title">Robo</span>ts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuel Fallas Hernández, Sergio Martínez Alonso, Alejandro Romero, Jose A. Becerra Permuy, Richard J. Duro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the challenges of open-ended learning in robots is the need to
autonomously discover goals and learn skills to achieve them. However, when in
lifelong learning settings, it is always desirable to generate sub-goals with
their associated skills, without relying on explicit reward, as steppingstones
to a goal. This allows sub-goals and skills to be reused to facilitate
achieving other goals. This work proposes a two-pronged approach for sub-goal
generation to address this challenge: a top-down approach, where sub-goals are
hierarchically derived from general goals using intrinsic motivations to
discover them, and a bottom-up approach, where sub-goal chains emerge from
making latent relationships between goals and perceptual classes that were
previously learned in different domains explicit. These methods help the robot
to autonomously generate and chain sub-goals as a way to achieve more general
goals. Additionally, they create more abstract representations of goals,
helping to reduce sub-goal duplication and make the learning of skills more
efficient. Implemented within an existing cognitive architecture for lifelong
open-ended learning and tested with a real robot, our approach enhances the
robot's ability to discover and achieve goals, generate sub-goals in an
efficient manner, generalize learned skills, and operate in dynamic and unknown
environments without explicit intermediate rewards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online 3D Scene Reconstruction Using Neural Object Priors <span class="chip">3DV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Chabal, Shizhe Chen, Jean Ponce, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of reconstructing a scene online at the
level of objects given an RGB-D video sequence. While current object-aware
neural implicit representations hold promise, they are limited in online
reconstruction efficiency and shape completion. Our main contributions to
alleviate the above limitations are twofold. First, we propose a feature grid
interpolation mechanism to continuously update grid-based object-centric neural
implicit representations as new object parts are revealed. Second, we construct
an object library with previously mapped objects in advance and leverage the
corresponding shape priors to initialize geometric object models in new videos,
subsequently completing them with novel views as well as synthesized past views
to avoid losing original object details. Extensive experiments on synthetic
environments from the Replica dataset, real-world ScanNet sequences and videos
captured in our laboratory demonstrate that our approach outperforms
state-of-the-art neural implicit models for this task in terms of
reconstruction accuracy and completeness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3DV 2025. Project page:
  https://www.di.ens.fr/willow/research/online-scene-reconstruction/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Bootstrapped Model Predictive Control <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Wang, Hanwei Guo, Siz<span class="highlight-author">he Wang</span>, Long Qian, Xuguang Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Predictive Control (MPC) has been demonstrated to be effective in
continuous control tasks. When a world model and a value function are
available, planning a sequence of actions ahead of time leads to a better
policy. Existing methods typically obtain the value function and the
corresponding policy in a model-free manner. However, we find that such an
approach struggles with complex tasks, resulting in poor policy learning and
inaccurate value estimation. To address this problem, we leverage the strengths
of MPC itself. In this work, we introduce Bootstrapped Model Predictive Control
(BMPC), a novel algorithm that performs policy learning in a bootstrapped
manner. BMPC learns a network policy by imitating an MPC expert, and in turn,
uses this policy to guide the MPC process. Combined with model-based
TD-learning, our policy learning yields better value estimation and further
boosts the efficiency of MPC. We also introduce a lazy reanalyze mechanism,
which enables computationally efficient imitation learning. Our method achieves
superior performance over prior works on diverse continuous control tasks. In
particular, on challenging high-dimensional locomotion tasks, BMPC
significantly improves data efficiency while also enhancing asymptotic
performance and training stability, with comparable training time and smaller
network sizes. Code is available at https://github.com/wertyuilife2/bmpc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Multi-<span class="highlight-title">Robo</span>t Coordination through Locality-Based Factorized
  Multi-Agent Actor-Critic Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chak Lam Shek, Amrit Singh Bedi, Anjon Basak, Ellen Novoseller, Nick Waytowich, Priya Narayanan, Dinesh Manocha, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a novel cooperative multi-agent reinforcement
learning method called \textbf{Loc}ality based \textbf{Fac}torized
\textbf{M}ulti-Agent \textbf{A}ctor-\textbf{C}ritic (Loc-FACMAC). Existing
state-of-the-art algorithms, such as FACMAC, rely on global reward information,
which may not accurately reflect the quality of individual robots' actions in
decentralized systems. We integrate the concept of locality into critic
learning, where strongly related robots form partitions during training. Robots
within the same partition have a greater impact on each other, leading to more
precise policy evaluation. Additionally, we construct a dependency graph to
capture the relationships between robots, facilitating the partitioning
process. This approach mitigates the curse of dimensionality and prevents
robots from using irrelevant information. Our method improves existing
algorithms by focusing on local rewards and leveraging partition-based learning
to enhance training efficiency and performance. We evaluate the performance of
Loc-FACMAC in three environments: Hallway, Multi-cartpole, and
Bounded-Cooperative-Navigation. We explore the impact of partition sizes on the
performance and compare the result with baseline MARL algorithms such as LOMAQ,
FACMAC, and QMIX. The experiments reveal that, if the locality structure is
defined properly, Loc-FACMAC outperforms these baseline algorithms up to 108\%,
indicating that exploiting the locality structure in the actor-critic framework
improves the MARL performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlphaSpace: Enabling <span class="highlight-title">Robo</span>tic Actions through Semantic Tokenization and
  Symbolic Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Dao, Dinh Bach Vu, Bui Quang Huy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents AlphaSpace, a novel methodology designed to enhance the
spatial reasoning capabilities of large language models (LLMs) for 3D Cartesian
space navigation. AlphaSpace employs a semantics-based tokenization strategy,
encoding height information through specialized semantic tokens, and integrates
primarily symbolic synthetic reasoning data. This approach enables LLMs to
accurately manipulate objects by positioning them at specific [x, y, z]
coordinates. Experimental results demonstrate that AlphaSpace significantly
outperforms existing models on manipulation subtasks, achieving a total
accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5
Sonnet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Tube-based Control Strategy for <span class="highlight-title">Vision</span>-guided Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Der-Hau Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A robust control strategy for autonomous vehicles can improve system
stability, enhance riding comfort, and prevent driving accidents. This paper
presents a novel interpolation tube-based constrained iterative linear
quadratic regulator (itube-CILQR) algorithm for autonomous
computer-vision-based vehicle lane-keeping. The goal of the algorithm is to
enhance robustness during high-speed cornering on tight turns. The advantages
of itube-CILQR over the standard tube-approach include reduced system
conservatism and increased computational speed. Numerical and vision-based
experiments were conducted to examine the feasibility of the proposed
algorithm. The proposed itube-CILQR algorithm is better suited to vehicle
lane-keeping than variational CILQR-based methods and model predictive control
(MPC) approaches using a classical interior-point solver. Specifically, in
evaluation experiments, itube-CILQR achieved an average runtime of 3.16 ms to
generate a control signal to guide a self-driving vehicle; itube-MPC typically
required a 4.67-times longer computation time to complete the same task.
Moreover, the influence of conservatism on system behavior was investigated by
exploring the interpolation variable trajectories derived from the proposed
itube-CILQR algorithm during lane-keeping maneuvers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Robo</span>Engine: Plug-and-Play <span class="highlight-title">Robo</span>t Data Augmentation with Semantic <span class="highlight-title">Robo</span>t
  Segmentation and Background Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengbo Yuan, Suraj Joshi, Shaoting Zhu, Hang Su, Hang Zhao, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual augmentation has become a crucial technique for enhancing the visual
robustness of imitation learning. However, existing methods are often limited
by prerequisites such as camera calibration or the need for controlled
environments (e.g., green screen setups). In this work, we introduce
RoboEngine, the first plug-and-play visual robot data augmentation toolkit. For
the first time, users can effortlessly generate physics- and task-aware robot
scenes with just a few lines of code. To achieve this, we present a novel robot
scene segmentation dataset, a generalizable high-quality robot segmentation
model, and a fine-tuned background generation model, which together form the
core components of the out-of-the-box toolkit. Using RoboEngine, we demonstrate
the ability to generalize robot manipulation tasks across six entirely new
scenes, based solely on demonstrations collected from a single scene, achieving
a more than 200% performance improvement compared to the no-augmentation
baseline. All datasets, model weights, and the toolkit will be publicly
released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://roboengine.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Continual Adaptation of Pretrained <span class="highlight-title">Robo</span>tic Policy with Online
  Meta-Learned Adapters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Zhu, Endong Sun, Guanhe Huang, Oya Celiktutan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual adaptation is essential for general autonomous agents. For example,
a household robot pretrained with a repertoire of skills must still adapt to
unseen tasks specific to each household. Motivated by this, building upon
parameter-efficient fine-tuning in language models, prior works have explored
lightweight adapters to adapt pretrained policies, which can preserve learned
features from the pretraining phase and demonstrate good adaptation
performances. However, these approaches treat task learning separately,
limiting knowledge transfer between tasks. In this paper, we propose Online
Meta-Learned adapters (OMLA). Instead of applying adapters directly, OMLA can
facilitate knowledge transfer from previously learned tasks to current learning
tasks through a novel meta-learning objective. Extensive experiments in both
simulated and real-world environments demonstrate that OMLA can lead to better
adaptation performances compared to the baseline methods. The project link:
https://ricky-zhu.github.io/OMLA/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project link: https://ricky-zhu.github.io/OMLA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Any6D: Model-free 6D Pose Estimation of Novel Objects <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeyeop Lee, Bowen Wen, Minjun Kang, Gyuree Kang, In So Kweon, Kuk-Jin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Any6D, a model-free framework for 6D object pose estimation that
requires only a single RGB-D anchor image to estimate both the 6D pose and size
of unknown objects in novel scenes. Unlike existing methods that rely on
textured 3D models or multiple viewpoints, Any6D leverages a joint object
alignment process to enhance 2D-3D alignment and metric scale estimation for
improved pose accuracy. Our approach integrates a render-and-compare strategy
to generate and refine pose hypotheses, enabling robust performance in
scenarios with occlusions, non-overlapping views, diverse lighting conditions,
and large cross-environment variations. We evaluate our method on five
challenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,
demonstrating its effectiveness in significantly outperforming state-of-the-art
methods for novel object pose estimation. Project page:
https://taeyeop.com/any6d
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025, Project Page: https://taeyeop.com/any6d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FF-SRL: High Performance GPU-Based Surgical Simulation For <span class="highlight-title">Robo</span>t
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Dall'Alba, Michał Nasket, Sabina Kaminska, Przemysław Korzeniowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic surgery is a rapidly developing field that can greatly benefit from
the automation of surgical tasks. However, training techniques such as
Reinforcement Learning (RL) require a high number of task repetitions, which
are generally unsafe and impractical to perform on real surgical systems. This
stresses the need for simulated surgical environments, which are not only
realistic, but also computationally efficient and scalable. We introduce FF-SRL
(Fast and Flexible Surgical Reinforcement Learning), a high-performance
learning environment for robotic surgery. In FF-SRL both physics simulation and
RL policy training reside entirely on a single GPU. This avoids typical
bottlenecks associated with data transfer between the CPU and GPU, leading to
accelerated learning rates. Our results show that FF-SRL reduces the training
time of a complex tissue manipulation task by an order of magnitude, down to a
couple of minutes, compared to a common CPU/GPU simulator. Such speed-up may
facilitate the experimentation with RL techniques and contribute to the
development of new generation of surgical systems. To this end, we make our
code publicly available to the community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-agent coordination for data gathering with periodic requests and
  deliveries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaroslav Marchukov, Luis Montano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this demo work we develop a method to plan and coordinate a multi-agent
team to gather information on demand. The data is periodically requested by a
static Operation Center (OC) from changeable goals locations. The mission of
the team is to reach these locations, taking measurements and delivering the
data to the OC. Due to the limited communication range as well as signal
attenuation because of the obstacles, the agents must travel to the OC, to
upload the data. The agents can play two roles: ones as workers gathering data,
the others as collectors traveling invariant paths for collecting the data of
the workers to re-transmit it to the OC. The refreshing time of the delivered
information depends on the number of available agents as well as of the
scenario. The proposed algorithm finds out the best balance between the number
of collectors-workers and the partition of the scenario into working areas in
the planning phase, which provides the minimum refreshing time and will be the
one executed by the agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication-aware planning for <span class="highlight-title">robo</span>t teams deployment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaroslav Marchukov, Luis Montano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the present work we address the problem of deploying a team of robots in a
scenario where some locations of interest must be reached. Thus, a planning for
a deployment is required, before sending the robots. The obstacles, the limited
communication range, and the need of communicating to a base station, constrain
the connectivity of the team and the deployment planning. We propose a method
consisting of three algorithms: a distributed path planner to obtain
communication-aware trajectories; a deployment planner providing dual-use of
the robots, visiting primary goals and performing connectivity tasks; and a
clustering algorithm to allocate the tasks to robots, and obtain the best goal
visit order for the mission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parental Guidance: Efficient Lifelong Learning through Evolutionary
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Octi Zhang, Quanquan Peng, Rosario Scalise, Bryon Boots
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing robotic agents that can perform well in diverse environments while
showing a variety of behaviors is a key challenge in AI and robotics.
Traditional reinforcement learning (RL) methods often create agents that
specialize in narrow tasks, limiting their adaptability and diversity. To
overcome this, we propose a preliminary, evolution-inspired framework that
includes a reproduction module, similar to natural species reproduction,
balancing diversity and specialization. By integrating RL, imitation learning
(IL), and a coevolutionary agent-terrain curriculum, our system evolves agents
continuously through complex tasks. This approach promotes adaptability,
inheritance of useful traits, and continual learning. Agents not only refine
inherited skills but also surpass their predecessors. Our initial experiments
show that this method improves exploration efficiency and supports open-ended
learning, offering a scalable solution where sparse reward coupled with diverse
terrain environments induces a multi-task setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, CoRL 2024 Workshop MAPoDeL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ P3Nav: A Unified Framework for Embodied Navigation Integrating
  Perception, Planning, and Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufeng Zhong, Chengjian Feng, Feng Yan, Fanfan Liu, Liming Zheng, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In language-guided visual navigation, agents locate target objects in unseen
environments using natural language instructions. For reliable navigation in
unfamiliar scenes, agents must possess strong perception, planning, and
prediction capabilities. Additionally, when agents revisit previously explored
areas during long-term navigation, they may retain irrelevant and redundant
historical perceptions, leading to suboptimal results. In this work, we
introduce \textbf{P3Nav}, a unified framework that integrates
\textbf{P}erception, \textbf{P}lanning, and \textbf{P}rediction capabilities
through \textbf{Multitask Collaboration} on navigation and embodied question
answering (EQA) tasks, thereby enhancing navigation performance. Furthermore,
P3Nav employs an \textbf{Adaptive 3D-aware History Sampling} strategy to
effectively and efficiently utilize historical observations. By leveraging the
large language models (LLM), P3Nav comprehends diverse commands and complex
visual scenes, resulting in appropriate navigation actions. P3Nav achieves a
75\% success rate in object goal navigation on the
$\mathrm{CHORES}$-$\mathbb{S}$ benchmark, setting a new state-of-the-art
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Forces Exerted by Shoulder and Elbow Fabric-based Pneumatic
  Actuators for Pediatric Exosuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrnoosh Ayazi, Ipsita Sahin, Caio Mucchiani, Elena Kokkoni, Konstantinos Karydis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enhance pediatric exosuit design, it is crucial to assess the
actuator-generated forces. This work evaluates the contact forces exerted by
soft fabric-based pneumatic actuators in an upper extremity pediatric exosuit.
Two actuators were examined: a single-cell bidirectional actuator for shoulder
abduction/adduction and a bellow-type actuator for elbow extension/flexion.
Experiments assessed the impact of actuator anchoring points and the adjacent
joint's angle on exerted forces and actuated joint range of motion (ROM). These
were measured via load cells and encoders integrated into a custom infant-scale
engineered apparatus with two degrees of freedom (two revolute joints). For the
shoulder actuator, results show that anchoring it further from the shoulder
joint center while the elbow is flexed at $90^\circ$ yields the highest ROM
while minimizing the peak force exerted on the body. For the elbow actuator,
anchoring it symmetrically while the shoulder joint is at $0^\circ$ optimizes
actuator performance. These findings contribute a key step toward co-optimizing
the considered exosuit design for functionality and wearability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Innovative Automated Stretch Elastic Waistband Sewing Machine for
  Garment Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prof Dr Ray Wai Man Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is applied research for the development of the Automated Stretch
Elastic Waistband Sewing Machine represents a significant advancement in
garment manufacturing, addressing the industry's need for increased efficiency,
precision, and adaptability. This machine integrates innovative features such
as a sensor-based automatic waistband expansion system, synchronized sewing
speed and rolling wheel speed, and a differential feed top-loading mechanism.
These enhancements streamline the sewing process, reduce manual intervention,
and ensure consistent product quality. The machine's design incorporates both
3-wheel and 2-wheel rolling systems, each optimized for different elastic band
dimensions and elongation factors. The 3-wheel rolling system accommodates a
larger maximum boundary, while the 2-wheel rolling system offers a tighter
operational range, providing flexibility to meet diverse manufacturing
requirements. The Automated Stretch Elastic Waistband Sewing Machine has a
design that controls the pulling apart force so as not to break the elastic
waistband. It sets a new standard for quality and innovation, empowering
manufacturers to meet the demands of a competitive market with precision and
ease.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Reinforcement Learning</span> for Adaptive Planner Parameter Tuning: A
  Perspective on Hierarchical Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Wangtao, Wei Yufei, Xu Jiadong, Jia Wenhao, Li Liang, Xiong Rong, Wang Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic parameter tuning methods for planning algorithms, which integrate
pipeline approaches with learning-based techniques, are regarded as promising
due to their stability and capability to handle highly constrained
environments. While existing parameter tuning methods have demonstrated
considerable success, further performance improvements require a more
structured approach. In this paper, we propose a hierarchical architecture for
reinforcement learning-based parameter tuning. The architecture introduces a
hierarchical structure with low-frequency parameter tuning, mid-frequency
planning, and high-frequency control, enabling concurrent enhancement of both
upper-layer parameter tuning and lower-layer control through iterative
training. Experimental evaluations in both simulated and real-world
environments show that our method surpasses existing parameter tuning
approaches. Furthermore, our approach achieves first place in the Benchmark for
Autonomous Robot Navigation (BARN) Challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Embedding Adaptation for Human Preference Alignment in <span class="highlight-title">Diffusion</span>
  Planners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Zheng Terence Ng, Jianda Chen, Yuan Xu, Tianwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the challenge of personalizing trajectories generated in
automated decision-making systems by introducing a resource-efficient approach
that enables rapid adaptation to individual users' preferences. Our method
leverages a pretrained conditional diffusion model with Preference Latent
Embeddings (PLE), trained on a large, reward-free offline dataset. The PLE
serves as a compact representation for capturing specific user preferences. By
adapting the pretrained model using our proposed preference inversion method,
which directly optimizes the learnable PLE, we achieve superior alignment with
human preferences compared to existing solutions like Reinforcement Learning
from Human Feedback (RLHF) and Low-Rank Adaptation (LoRA). To better reflect
practical applications, we create a benchmark experiment using real human
preferences on diverse, high-reward trajectories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Vision</span>-Guided Loco-<span class="highlight-title">Manipulation</span> with a Snake <span class="highlight-title">Robo</span>t 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Salagame, Sasank Potluri, Keshav Bharadwaj Vaidyanathan, Kruthika Gangaraju, Eric Sihite, Milad Ramezani, Alireza Ramezani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the development and integration of a vision-guided
loco-manipulation pipeline for Northeastern University's snake robot, COBRA.
The system leverages a YOLOv8-based object detection model and depth data from
an onboard stereo camera to estimate the 6-DOF pose of target objects in real
time. We introduce a framework for autonomous detection and control, enabling
closed-loop loco-manipulation for transporting objects to specified goal
locations. Additionally, we demonstrate open-loop experiments in which COBRA
successfully performs real-time object detection and loco-manipulation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NMPC-based Unified Posture <span class="highlight-title">Manipulation</span> and Thrust Vectoring for Fault
  Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Salagame, Shashwat Pandya, Ioannis Mandralis, Eric Sihite, Alireza Ramezani, Morteza Gharib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-rotors face significant risks, as actuator failures at high altitudes
can easily result in a crash and the robot's destruction. Therefore, rapid
fault recovery in the event of an actuator failure is necessary for the
fault-tolerant and safe operation of unmanned aerial robots. In this work, we
present a fault recovery approach based on the unification of posture
manipulation and thrust vectoring. The key contributions of this work are: 1)
Derivation of two flight dynamics models (high-fidelity and reduced-order) that
capture posture control and thrust vectoring. 2) Design of a controller based
on Nonlinear Model Predictive Control (NMPC) and demonstration of fault
recovery in simulation using a high-fidelity model of the Multi-Modal Mobility
Morphobot (M4) in Simscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ground Penetrating Radar-Assisted Multimodal <span class="highlight-title">Robo</span>t Odometry Using
  Subsurface Feature Matrix 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haifeng Li, Jiajun Guo, Xuanxin Fan, Dezhen Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localization of robots using subsurface features observed by
ground-penetrating radar (GPR) enhances and adds robustness to common sensor
modalities, as subsurface features are less affected by weather, seasons, and
surface changes. We introduce an innovative multimodal odometry approach using
inputs from GPR, an inertial measurement unit (IMU), and a wheel encoder. To
efficiently address GPR signal noise, we introduce an advanced feature
representation called the subsurface feature matrix (SFM). The SFM leverages
frequency domain data and identifies peaks within radar scans. Additionally, we
propose a novel feature matching method that estimates GPR displacement by
aligning SFMs. The integrations from these three input sources are consolidated
using a factor graph approach to achieve multimodal robot odometry. Our method
has been developed and evaluated with the CMU-GPR public dataset, demonstrating
improvements in accuracy and robustness with real-time performance in robotic
odometry tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Orientation Field for OSM-Guided Autonomous Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Huang, Wei Gao, Zhiyuan Zhang, Maani Ghaffari, Dezhen Song, Cheng-Zhong Xu, Hui Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenStreetMap (OSM) has gained popularity recently in autonomous navigation
due to its public accessibility, lower maintenance costs, and broader
geographical coverage. However, existing methods often struggle with noisy OSM
data and incomplete sensor observations, leading to inaccuracies in trajectory
planning. These challenges are particularly evident in complex driving
scenarios, such as at intersections or facing occlusions. To address these
challenges, we propose a robust and explainable two-stage framework to learn an
Orientation Field (OrField) for robot navigation by integrating LiDAR scans and
OSM routes. In the first stage, we introduce the novel representation, OrField,
which can provide orientations for each grid on the map, reasoning jointly from
noisy LiDAR scans and OSM routes. To generate a robust OrField, we train a deep
neural network by encoding a versatile initial OrField and output an optimized
OrField. Based on OrField, we propose two trajectory planners for OSM-guided
robot navigation, called Field-RRT* and Field-Bezier, respectively, in the
second stage by improving the Rapidly Exploring Random Tree (RRT) algorithm and
Bezier curve to estimate the trajectories. Thanks to the robustness of OrField
which captures both global and local information, Field-RRT* and Field-Bezier
can generate accurate and reliable trajectories even in challenging conditions.
We validate our approach through experiments on the SemanticKITTI dataset and
our own campus dataset. The results demonstrate the effectiveness of our
method, achieving superior performance in complex and noisy conditions. Our
code for network training and real-world deployment is available at
https://github.com/IMRL/OriField.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures, and 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GI-SLAM: Gaussian-Inertial SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xulang Liu, Ning Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has recently emerged as a powerful
representation of geometry and appearance for dense Simultaneous Localization
and Mapping (SLAM). Through rapid, differentiable rasterization of 3D
Gaussians, many 3DGS SLAM methods achieve near real-time rendering and
accelerated training. However, these methods largely overlook inertial data,
witch is a critical piece of information collected from the inertial
measurement unit (IMU). In this paper, we present GI-SLAM, a novel
gaussian-inertial SLAM system which consists of an IMU-enhanced camera tracking
module and a realistic 3D Gaussian-based scene representation for mapping. Our
method introduces an IMU loss that seamlessly integrates into the deep learning
framework underpinning 3D Gaussian Splatting SLAM, effectively enhancing the
accuracy, robustness and efficiency of camera tracking. Moreover, our SLAM
system supports a wide range of sensor configurations, including monocular,
stereo, and RGBD cameras, both with and without IMU integration. Our method
achieves competitive performance compared with existing state-of-the-art
real-time methods on the EuRoC and TUM-RGBD datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Modified Feedback Strategies in LQ Games under Control
  Imperfections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdis Rabbani, Navid Mojahed, Shima Nazari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Game-theoretic approaches and Nash equilibrium have been widely applied
across various engineering domains. However, practical challenges such as
disturbances, delays, and actuator limitations can hinder the precise execution
of Nash equilibrium strategies. This work explores the impact of such
implementation imperfections on game trajectories and players' costs within the
context of a two-player linear quadratic (LQ) nonzero-sum game. Specifically,
we analyze how small deviations by one player affect the state and cost
function of the other player. To address these deviations, we propose an
adjusted control policy that not only mitigates adverse effects optimally but
can also exploit the deviations to enhance performance. Rigorous mathematical
analysis and proofs are presented, demonstrating through a representative
example that the proposed policy modification achieves up to $61\%$ improvement
compared to the unadjusted feedback policy and up to $0.59\%$ compared to the
feedback Nash strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, Preprint version of a paper submitted to L-CSS
  and CDC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contact-based <span class="highlight-title">Grasp</span> Control and Inverse Kinematics for a Five-fingered
  <span class="highlight-title">Robo</span>tic Hand 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robinson Umeike
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an implementation and analysis of a five-fingered robotic
grasping system that combines contact-based control with inverse kinematics
solutions. Using the PyBullet simulation environment and the DexHand v2 model,
we demonstrate a comprehensive approach to achieving stable grasps through
contact point optimization with force closure validation. Our method achieves
movement efficiency ratings between 0.966-0.996 for non-thumb fingers and 0.879
for the thumb, while maintaining positional accuracy within 0.0267-0.0283m for
non-thumb digits and 0.0519m for the thumb. The system demonstrates rapid
position stabilization at 240Hz simulation frequency and maintains stable
contact configurations throughout the grasp execution. Experimental results
validate the effectiveness of our approach, while also identifying areas for
future enhancement in thumb opposition movements and horizontal plane control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, 5 Figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dom, cars don't fly! -- Or do they? In-Air Vehicle Maneuver for
  High-Speed Off-Road Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuj Pokhrel, Aniket Datar, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When pushing the speed limit for aggressive off-road navigation on uneven
terrain, it is inevitable that vehicles may become airborne from time to time.
During time-sensitive tasks, being able to fly over challenging terrain can
also save time, instead of cautiously circumventing or slowly negotiating
through. However, most off-road autonomy systems operate under the assumption
that the vehicles are always on the ground and therefore limit operational
speed. In this paper, we present a novel approach for in-air vehicle maneuver
during high-speed off-road navigation. Based on a hybrid forward kinodynamic
model using both physics principles and machine learning, our fixed-horizon,
sampling-based motion planner ensures accurate vehicle landing poses and their
derivatives within a short airborne time window using vehicle throttle and
steering commands. We test our approach in extensive in-air experiments both
indoors and outdoors, compare it against an error-driven control method, and
demonstrate that precise and timely in-air vehicle maneuver is possible through
existing ground vehicle controls.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooperative Control of Multi-Quadrotors for Transporting Cable-Suspended
  Payloads: Obstacle-Aware Planning and Event-Based Nonlinear Model Predictive
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tohid Kargar Tasooji, Sakineh Khodadadi, Guangjun Liu, Richard Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel methodology for the cooperative control of
multiple quadrotors transporting cablesuspended payloads, emphasizing
obstacle-aware planning and event-based Nonlinear Model Predictive Control
(NMPC). Our approach integrates trajectory planning with real-time control
through a combination of the A* algorithm for global path planning and NMPC for
local control, enhancing trajectory adaptability and obstacle avoidance. We
propose an advanced event-triggered control system that updates based on events
identified through dynamically generated environmental maps. These maps are
constructed using a dual-camera setup, which includes multi-camera systems for
static obstacle detection and event cameras for high-resolution, low-latency
detection of dynamic obstacles. This design is crucial for addressing
fast-moving and transient obstacles that conventional cameras may overlook,
particularly in environments with rapid motion and variable lighting
conditions. When new obstacles are detected, the A* algorithm recalculates
waypoints based on the updated map, ensuring safe and efficient navigation.
This real-time obstacle detection and map updating integration allows the
system to adaptively respond to environmental changes, markedly improving
safety and navigation efficiency. The system employs SLAM and object detection
techniques utilizing data from multi-cameras, event cameras, and IMUs for
accurate localization and comprehensive environmental mapping. The NMPC
framework adeptly manages the complex dynamics of multiple quadrotors and
suspended payloads, incorporating safety constraints to maintain dynamic
feasibility and stability. Extensive simulations validate the proposed
approach, demonstrating significant enhancements in energy efficiency,
computational resource management, and responsiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evolutionary Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianren Wang, Yifan Su, Abhinav Gupta, Deepak Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite its extreme sample inefficiency, on-policy reinforcement learning has
become a fundamental tool in real-world applications. With recent advances in
GPU-driven simulation, the ability to collect vast amounts of data for RL
training has scaled exponentially. However, studies show that current on-policy
methods, such as PPO, fail to fully leverage the benefits of parallelized
environments, leading to performance saturation beyond a certain scale. In
contrast, Evolutionary Algorithms (EAs) excel at increasing diversity through
randomization, making them a natural complement to RL. However, existing EvoRL
methods have struggled to gain widespread adoption due to their extreme sample
inefficiency. To address these challenges, we introduce Evolutionary Policy
Optimization (EPO), a novel policy gradient algorithm that combines the
strengths of EA and policy gradients. We show that EPO significantly improves
performance across diverse and challenging environments, demonstrating superior
scalability with parallelized simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website at https://sites.google.com/view/epo-rl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Double Oracle Algorithm for Game-Theoretic <span class="highlight-title">Robo</span>t Allocation on Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian An, Lifeng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of game-theoretic robot allocation where two players
strategically allocate robots to compete for multiple sites of interest. Robots
possess offensive or defensive capabilities to interfere and weaken their
opponents to take over a competing site. This problem belongs to the
conventional Colonel Blotto Game. Considering the robots' heterogeneous
capabilities and environmental factors, we generalize the conventional Blotto
game by incorporating heterogeneous robot types and graph constraints that
capture the robot transitions between sites. Then we employ the Double Oracle
Algorithm (DOA) to solve for the Nash equilibrium of the generalized Blotto
game. Particularly, for cyclic-dominance-heterogeneous (CDH) robots that
inhibit each other, we define a new transformation rule between any two robot
types. Building on the transformation, we design a novel utility function to
measure the game's outcome quantitatively. Moreover, we rigorously prove the
correctness of the designed utility function. Finally, we conduct extensive
simulations to demonstrate the effectiveness of DOA on computing Nash
equilibrium for homogeneous, linear heterogeneous, and CDH robot allocation on
graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Informed Multi-Agent <span class="highlight-title">Reinforcement Learning</span> for Distributed
  Multi-<span class="highlight-title">Robo</span>t Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00212v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00212v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo Sebastian, Thai Duong, Nikolay Atanasov, Eduardo Montijano, Carlos Sagues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The networked nature of multi-robot systems presents challenges in the
context of multi-agent reinforcement learning. Centralized control policies do
not scale with increasing numbers of robots, whereas independent control
policies do not exploit the information provided by other robots, exhibiting
poor performance in cooperative-competitive tasks. In this work we propose a
physics-informed reinforcement learning approach able to learn distributed
multi-robot control policies that are both scalable and make use of all the
available information to each robot. Our approach has three key
characteristics. First, it imposes a port-Hamiltonian structure on the policy
representation, respecting energy conservation properties of physical robot
systems and the networked nature of robot team interactions. Second, it uses
self-attention to ensure a sparse policy representation able to handle
time-varying information at each robot from the interaction graph. Third, we
present a soft actor-critic reinforcement learning algorithm parameterized by
our self-attention port-Hamiltonian control policy, which accounts for the
correlation among robots during training while overcoming the need of value
function factorization. Extensive simulations in different multi-robot
scenarios demonstrate the success of the proposed approach, surpassing previous
multi-robot reinforcement learning solutions in scalability, while achieving
similar or superior performance (with averaged cumulative reward up to x2
greater than the state-of-the-art with robot teams x6 larger than the number of
robots at training time). We also validate our approach on multiple real robots
in the Georgia Tech Robotarium under imperfect communication, demonstrating
zero-shot sim-to-real transfer and scalability across number of robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is under review at IEEE T-RO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Kalib: Easy Hand-Eye Calibration with Reference Point Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tutian Tang, Minghao Liu, Wenqiang Xu, <span class="highlight-author">Cewu Lu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand-eye calibration aims to estimate the transformation between a camera and
a robot. Traditional methods rely on fiducial markers, which require
considerable manual effort and precise setup. Recent advances in deep learning
have introduced markerless techniques but come with more prerequisites, such as
retraining networks for each robot, and accessing accurate mesh models for data
generation. In this paper, we propose Kalib, an automatic and easy-to-setup
hand-eye calibration method that leverages the generalizability of visual
foundation models to overcome these challenges. It features only two basic
prerequisites, the robot's kinematic chain and a predefined reference point on
the robot. During calibration, the reference point is tracked in the camera
space. Its corresponding 3D coordinates in the robot coordinate can be inferred
by forward kinematics. Then, a PnP solver directly estimates the transformation
between the camera and the robot without training new networks or accessing
mesh models. Evaluations in simulated and real-world benchmarks show that Kalib
achieves good accuracy with a lower manual workload compared with recent
baseline methods. We also demonstrate its application in multiple real-world
settings with various robot arms and grippers. Kalib's user-friendly design and
minimal setup requirements make it a possible solution for continuous operation
in unstructured environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code, data, and supplementary materials are available at
  https://sites.google.com/view/hand-eye-kalib</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ETAP: Event-based Tracking of Any Point 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Friedhelm Hamann, Daniel Gehrig, Filbert Febryanto, Kostas Daniilidis, Guillermo Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking any point (TAP) recently shifted the motion estimation paradigm from
focusing on individual salient points with local templates to tracking
arbitrary points with global image contexts. However, while research has mostly
focused on driving the accuracy of models in nominal settings, addressing
scenarios with difficult lighting conditions and high-speed motions remains out
of reach due to the limitations of the sensor. This work addresses this
challenge with the first event camera-based TAP method. It leverages the high
temporal resolution and high dynamic range of event cameras for robust
high-speed tracking, and the global contexts in TAP methods to handle
asynchronous and sparse event measurements. We further extend the TAP framework
to handle event feature variations induced by motion -- thereby addressing an
open challenge in purely event-based tracking -- with a novel feature-alignment
loss which ensures the learning of motion-robust features. Our method is
trained with data from a new data generation pipeline and systematically
ablated across all design decisions. Our method shows strong cross-dataset
generalization and performs 136% better on the average Jaccard metric than the
baselines. Moreover, on an established feature tracking benchmark, it achieves
a 20% improvement over the previous best event-only method and even surpasses
the previous best events-and-frames method by 4.1%. Our code is available at
https://github.com/tub-rip/ETAP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 15 figures, 8 tables. Project page:
  https://github.com/tub-rip/ETAP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CarPlanner: Consistent Auto-regressive Trajectory Planning for
  Large-scale <span class="highlight-title">Reinforcement Learning</span> in Autonomous Driving <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19908v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19908v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongkun Zhang, Jiaming Liang, Ke Guo, Sha Lu, Qi Wang, Rong Xiong, Zhenwei Miao, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory planning is vital for autonomous driving, ensuring safe and
efficient navigation in complex environments. While recent learning-based
methods, particularly reinforcement learning (RL), have shown promise in
specific scenarios, RL planners struggle with training inefficiencies and
managing large-scale, real-world driving scenarios. In this paper, we introduce
\textbf{CarPlanner}, a \textbf{C}onsistent \textbf{a}uto-\textbf{r}egressive
\textbf{Planner} that uses RL to generate multi-modal trajectories. The
auto-regressive structure enables efficient large-scale RL training, while the
incorporation of consistency ensures stable policy learning by maintaining
coherent temporal consistency across time steps. Moreover, CarPlanner employs a
generation-selection framework with an expert-guided reward function and an
invariant-view module, simplifying RL training and enhancing policy
performance. Extensive analysis demonstrates that our proposed RL framework
effectively addresses the challenges of training efficiency and performance
enhancement, positioning CarPlanner as a promising solution for trajectory
planning in autonomous driving. To the best of our knowledge, we are the first
to demonstrate that the RL-based planner can surpass both IL- and rule-based
state-of-the-arts (SOTAs) on the challenging large-scale real-world dataset
nuPlan. Our proposed CarPlanner surpasses RL-, IL-, and rule-based SOTA
approaches within this demanding dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Believing is Seeing: Unobserved Object Detection using Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05869v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05869v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhransu S. Bhattacharjee, Dylan Campbell, Rahul Shome
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can objects that are not visible in an image -- but are in the vicinity of
the camera -- be detected? This study introduces the novel tasks of 2D, 2.5D
and 3D unobserved object detection for predicting the location of nearby
objects that are occluded or lie outside the image frame. We adapt several
state-of-the-art pre-trained generative models to address this task, including
2D and 3D diffusion models and vision-language models, and show that they can
be used to infer the presence of objects that are not directly observed. To
benchmark this task, we propose a suite of metrics that capture different
aspects of performance. Our empirical evaluation on indoor scenes from the
RealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the
use of generative models for the unobserved object detection task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/CVF Computer Vision and Pattern Recognition 2025; 22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evolution 6.0: Evolving <span class="highlight-title">Robo</span>tic Capabilities Through Generative Design <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17034v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17034v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Haris Khan, Artyom Myshlyaev, Artem Lykov, Miguel Altamirano Cabrera, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new concept, Evolution 6.0, which represents the evolution of
robotics driven by Generative AI. When a robot lacks the necessary tools to
accomplish a task requested by a human, it autonomously designs the required
instruments and learns how to use them to achieve the goal. Evolution 6.0 is an
autonomous robotic system powered by Vision-Language Models (VLMs),
Vision-Language Action (VLA) models, and Text-to-3D generative models for tool
design and task execution. The system comprises two key modules: the Tool
Generation Module, which fabricates task-specific tools from visual and textual
data, and the Action Generation Module, which converts natural language
instructions into robotic actions. It integrates QwenVLM for environmental
understanding, OpenVLA for task execution, and Llama-Mesh for 3D tool
generation. Evaluation results demonstrate a 90% success rate for tool
generation with a 10-second inference time, and action generation achieving
83.5% in physical and visual generalization, 70% in motion generalization, and
37% in semantic generalization. Future improvements will focus on bimanual
manipulation, expanded task capabilities, and enhanced environmental
interpretation to improve real-world adaptability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving <span class="highlight-title">robo</span>t navigation in crowded environments using intrinsic
  rewards <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Martinez-Baselga, Luis Riazuelo, Luis Montano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous navigation in crowded environments is an open problem with many
applications, essential for the coexistence of robots and humans in the smart
cities of the future. In recent years, deep reinforcement learning approaches
have proven to outperform model-based algorithms. Nevertheless, even though the
results provided are promising, the works are not able to take advantage of the
capabilities that their models offer. They usually get trapped in local optima
in the training process, that prevent them from learning the optimal policy.
They are not able to visit and interact with every possible state
appropriately, such as with the states near the goal or near the dynamic
obstacles. In this work, we propose using intrinsic rewards to balance between
exploration and exploitation and explore depending on the uncertainty of the
states instead of on the time the agent has been trained, encouraging the agent
to get more curious about unknown states. We explain the benefits of the
approach and compare it with other exploration algorithms that may be used for
crowd navigation. Many simulation experiments are performed modifying several
algorithms of the state-of-the-art, showing that the use of intrinsic rewards
makes the robot learn faster and reach higher rewards and success rates (fewer
collisions) in shorter navigation times, outperforming the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted in 2023 IEEE International Conference on Robotics and
  Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AVOCADO: Adaptive Optimal Collision Avoidance driven by Opinion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Martinez-Baselga, Eduardo Sebastián, Eduardo Montijano, Luis Riazuelo, Carlos Sagüés, Luis Montano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AVOCADO (AdaptiVe Optimal Collision Avoidance Driven by Opinion),
a novel navigation approach to address holonomic robot collision avoidance when
the robot does not know how cooperative the other agents in the environment
are. AVOCADO departs from a Velocity Obstacle's (VO) formulation akin to the
Optimal Reciprocal Collision Avoidance method. However, instead of assuming
reciprocity, it poses an adaptive control problem to adapt to the cooperation
level of other robots and agents in real time. This is achieved through a novel
nonlinear opinion dynamics design that relies solely on sensor observations. As
a by-product, we leverage tools from the opinion dynamics formulation to
naturally avoid the deadlocks in geometrically symmetric scenarios that
typically suffer VO-based planners. Extensive numerical simulations show that
AVOCADO surpasses existing motion planners in mixed cooperative/non-cooperative
navigation environments in terms of success rate, time to goal and
computational time. In addition, we conduct multiple real experiments that
verify that AVOCADO is able to avoid collisions in environments crowded with
other robots and humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is published at IEEE Transactions on Robotics under DOI
  10.1109/TRO.2025.3552350</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Humanoid Policy ~ Human Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13441v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13441v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ri-Zhao Qiu, Shiqi Yang, Xuxin Cheng, Chaitanya Chawla, Jialong Li, Tairan He, Ge Yan, David J. Yoon, Ryan Hoque, Lars Paulsen, Ge Yang, Jian Zhang, Sha Yi, Guanya Shi, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training manipulation policies for humanoid robots with diverse data enhances
their robustness and generalization across tasks and platforms. However,
learning solely from robot demonstrations is labor-intensive, requiring
expensive tele-operated data collection which is difficult to scale. This paper
investigates a more scalable data source, egocentric human demonstrations, to
serve as cross-embodiment training data for robot learning. We mitigate the
embodiment gap between humanoids and humans from both the data and modeling
perspectives. We collect an egocentric task-oriented dataset (PH2D) that is
directly aligned with humanoid manipulation demonstrations. We then train a
human-humanoid behavior policy, which we term Human Action Transformer (HAT).
The state-action space of HAT is unified for both humans and humanoid robots
and can be differentiably retargeted to robot actions. Co-trained with
smaller-scale robot data, HAT directly models humanoid robots and humans as
different embodiments without additional supervision. We show that human data
improves both generalization and robustness of HAT with significantly better
data collection efficiency. Code and data: https://human-as-robot.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data: https://human-as-robot.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emergency-Brake Simplex: Toward A Verifiably Safe Control-CPS
  Architecture for Abrupt Runtime Reachability Constraint Changes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01831v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01831v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henghua Shen, Qixin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a system's constraints change abruptly, the system's reachability safety
does no longer sustain. Thus, the system can reach a forbidden/dangerous value.
Conventional remedy practically involves online controller redesign (OCR) to
re-establish the reachability's compliance with the new constraints, which,
however, is usually too slow. There is a need for an online strategy capable of
managing runtime changes in reachability constraints. However, to the best of
the authors' knowledge, this topic has not been addressed in the existing
literature. In this paper, we propose a fast fault tolerance strategy to
recover the system's reachability safety in runtime. Instead of redesigning the
system's controller, we propose to change the system's reference state to
modify the system's reachability to comply with the new constraints. We frame
the reference state search as an optimization problem and employ the
Karush-Kuhn-Tucker (KKT) method as well as the Interior Point Method (IPM)
based Newton's method (as a fallback for the KKT method) for fast solution
derivation. The optimization also allows more future fault tolerance. Numerical
simulations demonstrate that our method outperforms the conventional OCR method
in terms of computational efficiency and success rate. Specifically, the
results show that the proposed method finds a solution $10^{2}$ (with the IPM
based Newton's method) $\sim 10^{4}$ (with the KKT method) times faster than
the OCR method. Additionally, the improvement rate of the success rate of our
method over the OCR method is $40.81\%$ without considering the deadline of run
time. The success rate remains at $49.44\%$ for the proposed method, while it
becomes $0\%$ for the OCR method when a deadline of $1.5 \; seconds$ is
imposed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uni-Gaussians: Unifying <span class="highlight-title">Camera</span> and <span class="highlight-title">Lidar</span> Simulation with Gaussians for
  Dynamic Driving Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08317v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08317v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Yuan, Yuechuan Pu, Hongcheng Luo, Fengtian Lang, Cheng Chi, Teng Li, Yingying Shen, Haiyang Sun, Bing Wang, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the safety of autonomous vehicles necessitates comprehensive
simulation of multi-sensor data, encompassing inputs from both cameras and
LiDAR sensors, across various dynamic driving scenarios. Neural rendering
techniques, which utilize collected raw sensor data to simulate these dynamic
environments, have emerged as a leading methodology. While NeRF-based
approaches can uniformly represent scenes for rendering data from both camera
and LiDAR, they are hindered by slow rendering speeds due to dense sampling.
Conversely, Gaussian Splatting-based methods employ Gaussian primitives for
scene representation and achieve rapid rendering through rasterization.
However, these rasterization-based techniques struggle to accurately model
non-linear optical sensors. This limitation restricts their applicability to
sensors beyond pinhole cameras. To address these challenges and enable unified
representation of dynamic driving scenarios using Gaussian primitives, this
study proposes a novel hybrid approach. Our method utilizes rasterization for
rendering image data while employing Gaussian ray-tracing for LiDAR data
rendering. Experimental results on public datasets demonstrate that our
approach outperforms current state-of-the-art methods. This work presents a
unified and efficient solution for realistic simulation of camera and LiDAR
data in autonomous driving scenarios using Gaussian primitives, offering
significant advancements in both rendering quality and computational
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in
  <span class="highlight-title">Reinforcement Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Kim, Seung-Woo Seo, Seong-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) has demonstrated strong performance in
robotic control but remains susceptible to out-of-distribution (OOD) states,
often resulting in unreliable actions and task failure. While previous methods
have focused on minimizing or preventing OOD occurrences, they largely neglect
recovery once an agent encounters such states. Although the latest research has
attempted to address this by guiding agents back to in-distribution states,
their reliance on uncertainty estimation hinders scalability in complex
environments. To overcome this limitation, we introduce Language Models for
Out-of-Distribution Recovery (LaMOuR), which enables recovery learning without
relying on uncertainty estimation. LaMOuR generates dense reward codes that
guide the agent back to a state where it can successfully perform its original
task, leveraging the capabilities of LVLMs in image description, logical
reasoning, and code generation. Experimental results show that LaMOuR
substantially enhances recovery efficiency across diverse locomotion tasks and
even generalizes effectively to complex environments, including humanoid
locomotion and mobile manipulation, where existing methods struggle. The code
and supplementary materials are available at https://lamour-rl.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff-DAgger: Uncertainty Estimation with <span class="highlight-title">Diffusion</span> Policy for <span class="highlight-title">Robo</span>tic
  <span class="highlight-title">Manipulation</span> <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14868v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14868v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sung-Wook Lee, Xuhui Kang, Yen-Ling Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, diffusion policy has shown impressive results in handling
multi-modal tasks in robotic manipulation. However, it has fundamental
limitations in out-of-distribution failures that persist due to compounding
errors and its limited capability to extrapolate. One way to address these
limitations is robot-gated DAgger, an interactive imitation learning with a
robot query system to actively seek expert help during policy rollout. While
robot-gated DAgger has high potential for learning at scale, existing methods
like Ensemble-DAgger struggle with highly expressive policies: They often
misinterpret policy disagreements as uncertainty at multi-modal decision
points. To address this problem, we introduce Diff-DAgger, an efficient
robot-gated DAgger algorithm that leverages the training objective of diffusion
policy. We evaluate Diff-DAgger across different robot tasks including
stacking, pushing, and plugging, and show that Diff-DAgger improves the task
failure prediction by 39.0%, the task completion rate by 20.6%, and reduces the
wall-clock time by a factor of 7.8. We hope that this work opens up a path for
efficiently incorporating expressive yet data-hungry policies into interactive
robot learning settings. The project website is available at:
https://diffdagger.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: diffdagger.github.io 8 pages, 6 figures, accepted by
  International Conference on Robotics and Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Control Strategies for Pursuit-Evasion Under Occlusion Using Visibility
  and Safety Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minnan Zhou, Mustafa Shaikh, Vatsalya Chaubey, Patrick Haggerty, Shumon Koga, Dimitra Panagou, Nikolay Atanasov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a control strategy for pursuit-evasion problems in
environments with occlusions. We address the challenge of a mobile pursuer
keeping a mobile evader within its field of view (FoV) despite line-of-sight
obstructions. The signed distance function (SDF) of the FoV is used to
formulate visibility as a control barrier function (CBF) constraint on the
pursuer's control inputs. Similarly, obstacle avoidance is formulated as a CBF
constraint based on the SDF of the obstacle set. While the visibility and
safety CBFs are Lipschitz continuous, they are not differentiable everywhere,
necessitating the use of generalized gradients. To achieve non-myopic pursuit,
we generate reference control trajectories leading to evader visibility using a
sampling-based kinodynamic planner. The pursuer then tracks this reference via
convex optimization under the CBF constraints. We validate our approach in
CARLA simulations and real-world robot experiments, demonstrating successful
visibility maintenance using only onboard sensing, even under severe occlusions
and dynamic evader movements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Diffusion</span>Drive: Truncated <span class="highlight-title">Diffusion</span> Model for End-to-End Autonomous
  Driving <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the diffusion model has emerged as a powerful generative technique
for robotic policy learning, capable of modeling multi-mode action
distributions. Leveraging its capability for end-to-end autonomous driving is a
promising direction. However, the numerous denoising steps in the robotic
diffusion policy and the more dynamic, open-world nature of traffic scenes pose
substantial challenges for generating diverse driving actions at a real-time
speed. To address these challenges, we propose a novel truncated diffusion
policy that incorporates prior multi-mode anchors and truncates the diffusion
schedule, enabling the model to learn denoising from anchored Gaussian
distribution to the multi-mode driving action distribution. Additionally, we
design an efficient cascade diffusion decoder for enhanced interaction with
conditional scene context. The proposed model, DiffusionDrive, demonstrates
10$\times$ reduction in denoising steps compared to vanilla diffusion policy,
delivering superior diversity and quality in just 2 steps. On the
planning-oriented NAVSIM dataset, with the aligned ResNet-34 backbone,
DiffusionDrive achieves 88.1 PDMS without bells and whistles, setting a new
record, while running at a real-time speed of 45 FPS on an NVIDIA 4090.
Qualitative results on challenging scenarios further confirm that
DiffusionDrive can robustly generate diverse plausible driving actions. Code
and model will be available at https://github.com/hustvl/DiffusionDrive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025. Code & demo & model are available at
  https://github.com/hustvl/DiffusionDrive</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applications of Spiking Neural Networks in <span class="highlight-title">Visual</span> Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13186v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13186v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somayeh Hussaini, Michael Milford, Tobias Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for
their largely-unrealized potential energy efficiency and low latency
particularly when implemented on neuromorphic hardware. Our paper highlights
three advancements for SNNs in Visual Place Recognition (VPR). Firstly, we
propose Modular SNNs, where each SNN represents a set of non-overlapping
geographically distinct places, enabling scalable networks for large
environments. Secondly, we present Ensembles of Modular SNNs, where multiple
networks represent the same place, significantly enhancing accuracy compared to
single-network models. Each of our Modular SNN modules is compact, comprising
only 1500 neurons and 474k synapses, making them ideally suited for ensembling
due to their small size. Lastly, we investigate the role of sequence matching
in SNN-based VPR, a technique where consecutive images are used to refine place
recognition. We demonstrate competitive performance of our method on a range of
datasets, including higher responsiveness to ensembling compared to
conventional VPR techniques and higher R@1 improvements with sequence matching
than VPR techniques with comparable baseline performance. Our contributions
highlight the viability of SNNs for VPR, offering scalable and robust
solutions, and paving the way for their application in various energy-sensitive
robotic tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figures, IEEE Transactions on Robotics (TRO)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> AVR: Active <span class="highlight-title">Vision</span>-Driven <span class="highlight-title">Robo</span>tic Precision <span class="highlight-title">Manipulation</span> with Viewpoint
  and Focal Length Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01439v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01439v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushan Liu, Shilong Mu, Xintao Chao, Zizhen Li, <span class="highlight-author">Yao Mu</span>, Tianxing Chen, Shoujie Li, Chuqiao Lyu, Xiao-ping Zhang, Wenbo Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation within dynamic environments presents challenges to
precise control and adaptability. Traditional fixed-view camera systems face
challenges adapting to change viewpoints and scale variations, limiting
perception and manipulation precision. To tackle these issues, we propose the
Active Vision-driven Robotic (AVR) framework, a teleoperation hardware solution
that supports dynamic viewpoint and dynamic focal length adjustments to
continuously center targets and maintain optimal scale, accompanied by a
corresponding algorithm that effectively enhances the success rates of various
operational tasks. Using the RoboTwin platform with a real-time image
processing plugin, AVR framework improves task success rates by 5%-16% on five
manipulation tasks. Physical deployment on a dual-arm system demonstrates in
collaborative tasks and 36% precision in screwdriver insertion, outperforming
baselines by over 25%. Experimental results confirm that AVR framework enhances
environmental perception, manipulation repeatability (40% $\le $1 cm error),
and robustness in complex scenarios, paving the way for future robotic
precision manipulation methods in the pursuit of human-level robot dexterity
and precision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Previously, there were some problems with our experimental data, and
  the conclusions need to be further verified. Now that we have completed a
  full-scale experiment and analysis, and added supporting materials to our
  website, we hope to be able to resubmit it</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D-MVP: 3D Multiview Pretraining for <span class="highlight-title">Robo</span>tic <span class="highlight-title">Manipulation</span> <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyi Qian, Kaichun Mo, Valts Blukis, David F. Fouhey, Dieter Fox, Ankit Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that visual pretraining on egocentric datasets using
masked autoencoders (MAE) can improve generalization for downstream robotics
tasks. However, these approaches pretrain only on 2D images, while many
robotics applications require 3D scene understanding. In this work, we propose
3D-MVP, a novel approach for 3D Multi-View Pretraining using masked
autoencoders. We leverage Robotic View Transformer (RVT), which uses a
multi-view transformer to understand the 3D scene and predict gripper pose
actions. We split RVT's multi-view transformer into visual encoder and action
decoder, and pretrain its visual encoder using masked autoencoding on
large-scale 3D datasets such as Objaverse. We evaluate 3D-MVP on a suite of
virtual robot manipulation tasks and demonstrate improved performance over
baselines. Our results suggest that 3D-aware pretraining is a promising
approach to improve generalization of vision-based robotic manipulation
policies. Project site: https://jasonqsy.github.io/3DMVP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MVCTrack: Boosting 3D Point Cloud Tracking via Multimodal-Guided Virtual
  Cues <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02734v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02734v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaofeng Hu, Sifan Zhou, Shibo Zhao, Zhihang Yuan, Ci-Jyun Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D single object tracking is essential in autonomous driving and robotics.
Existing methods often struggle with sparse and incomplete point cloud
scenarios. To address these limitations, we propose a Multimodal-guided Virtual
Cues Projection (MVCP) scheme that generates virtual cues to enrich sparse
point clouds. Additionally, we introduce an enhanced tracker MVCTrack based on
the generated virtual cues. Specifically, the MVCP scheme seamlessly integrates
RGB sensors into LiDAR-based systems, leveraging a set of 2D detections to
create dense 3D virtual cues that significantly improve the sparsity of point
clouds. These virtual cues can naturally integrate with existing LiDAR-based 3D
trackers, yielding substantial performance gains. Extensive experiments
demonstrate that our method achieves competitive performance on the NuScenes
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NextStop: An Improved Tracker For Panoptic <span class="highlight-title">LIDAR</span> Segmentation Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nirit Alkalay, Roy Orfaig, Ben-Zion Bobrovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  4D panoptic LiDAR segmentation is essential for scene understanding in
autonomous driving and robotics, combining semantic and instance segmentation
with temporal consistency. Current methods, like 4D-PLS and 4D-STOP, use a
tracking-by-detection methodology, employing deep learning networks to perform
semantic and instance segmentation on each frame. To maintain temporal
consistency, large-size instances detected in the current frame are compared
and associated with instances within a temporal window that includes the
current and preceding frames. However, their reliance on short-term instance
detection, lack of motion estimation, and exclusion of small-sized instances
lead to frequent identity switches and reduced tracking performance. We address
these issues with the NextStop1 tracker, which integrates Kalman filter-based
motion estimation, data association, and lifespan management, along with a
tracklet state concept to improve prioritization. Evaluated using the LiDAR
Segmentation and Tracking Quality (LSTQ) metric on the SemanticKITTI validation
set, NextStop demonstrated enhanced tracking performance, particularly for
small-sized objects like people and bicyclists, with fewer ID switches, earlier
tracking initiation, and improved reliability in complex environments. The
source code is available at https://github.com/AIROTAU/NextStop
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MGSO: Monocular Real-time Photometric SLAM with Efficient 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Song Hu, Nicolas Abboud, Muhammad Qasim Ali, Adam Srebrnjak Yang, Imad Elhajj, Daniel Asmar, Yuhao Chen, John S. Zelek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time SLAM with dense 3D mapping is computationally challenging,
especially on resource-limited devices. The recent development of 3D Gaussian
Splatting (3DGS) offers a promising approach for real-time dense 3D
reconstruction. However, existing 3DGS-based SLAM systems struggle to balance
hardware simplicity, speed, and map quality. Most systems excel in one or two
of the aforementioned aspects but rarely achieve all. A key issue is the
difficulty of initializing 3D Gaussians while concurrently conducting SLAM. To
address these challenges, we present Monocular GSO (MGSO), a novel real-time
SLAM system that integrates photometric SLAM with 3DGS. Photometric SLAM
provides dense structured point clouds for 3DGS initialization, accelerating
optimization and producing more efficient maps with fewer Gaussians. As a
result, experiments show that our system generates reconstructions with a
balance of quality, memory efficiency, and speed that outperforms the
state-of-the-art. Furthermore, our system achieves all results using RGB
inputs. We evaluate the Replica, TUM-RGBD, and EuRoC datasets against current
live dense reconstruction systems. Not only do we surpass contemporary systems,
but experiments also show that we maintain our performance on laptop hardware,
making it a practical solution for robotics, A/R, and other real-time
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The final version of this work has been approved by the IEEE for
  publication. This version may no longer be accessible without notice.
  Copyright 2025 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAHSOR: Competence-Aware High-Speed Off-Road Ground Navigation in SE(3) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07065v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07065v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuj Pokhrel, Aniket Datar, Mohammad Nazeri, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the workspace of traditional ground vehicles is usually assumed to be
in a 2D plane, i.e., SE(2), such an assumption may not hold when they drive at
high speeds on unstructured off-road terrain: High-speed sharp turns on
high-friction surfaces may lead to vehicle rollover; Turning aggressively on
loose gravel or grass may violate the non-holonomic constraint and cause
significant lateral sliding; Driving quickly on rugged terrain will produce
extensive vibration along the vertical axis. Therefore, most offroad vehicles
are currently limited to drive only at low speeds to assure vehicle stability
and safety. In this work, we aim at empowering high-speed off-road vehicles
with competence awareness in SE(3) so that they can reason about the
consequences of taking aggressive maneuvers on different terrain with a 6-DoF
forward kinodynamic model. The model is learned from visual and inertial
Terrain Representation for Off-road Navigation (TRON) using multimodal,
self-supervised vehicle-terrain interactions. We demonstrate the efficacy of
our Competence-Aware High-Speed Off-Road (CAHSOR) navigation approach on a
physical ground robot in both an autonomous navigation and a human
shared-control setup and show that CAHSOR can efficiently reduce vehicle
instability by 62% while only compromising 8.6% average speed with the help of
TRON.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Rapid Trajectory Optimization and Control Framework for
  Resource-Constrained Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07413v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07413v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deep Parikh, Thomas L. Ahrens, Manoranjan Majji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a computationally efficient model predictive control
formulation that uses an integral Chebyshev collocation method to enable rapid
operations of autonomous agents. By posing the finite-horizon optimal control
problem and recursive re-evaluation of the optimal trajectories, minimization
of the L2 norms of the state and control errors are transcribed into a
quadratic program. Control and state variable constraints are parameterized
using Chebyshev polynomials and are accommodated in the optimal trajectory
generation programs to incorporate the actuator limits and keep-out
constraints. Differentiable collision detection of polytopes is leveraged for
optimal collision avoidance. Results obtained from the collocation methods are
benchmarked against the existing approaches on an edge computer to outline the
performance improvements. Finally, collaborative control scenarios involving
multi-agent space systems are considered to demonstrate the technical merits of
the proposed work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication at the IEEE ACC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morphological Symmetries in <span class="highlight-title">Robo</span>tics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15552v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15552v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Ordoñez-Apraez, Giulio Turrisi, Vladimir Kostic, Mario Martin, Antonio Agudo, Francesc Moreno-Noguer, Massimiliano Pontil, Claudio Semini, Carlos Mastalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive framework for studying and leveraging
morphological symmetries in robotic systems. These are intrinsic properties of
the robot's morphology, frequently observed in animal biology and robotics,
which stem from the replication of kinematic structures and the symmetrical
distribution of mass. We illustrate how these symmetries extend to the robot's
state space and both proprioceptive and exteroceptive sensor measurements,
resulting in the equivariance of the robot's equations of motion and optimal
control policies. Thus, we recognize morphological symmetries as a relevant and
previously unexplored physics-informed geometric prior, with significant
implications for both data-driven and analytical methods used in modeling,
control, estimation and design in robotics. For data-driven methods, we
demonstrate that morphological symmetries can enhance the sample efficiency and
generalization of machine learning models through data augmentation, or by
applying equivariant/invariant constraints on the model's architecture. In the
context of analytical methods, we employ abstract harmonic analysis to
decompose the robot's dynamics into a superposition of lower-dimensional,
independent dynamics. We substantiate our claims with both synthetic and
real-world experiments conducted on bipedal and quadrupedal robots. Lastly, we
introduce the repository MorphoSymm to facilitate the practical use of the
theory and applications outlined in this work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Target-Aware Video <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeksoo Kim, Hanbyul Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a target-aware video diffusion model that generates videos from an
input image in which an actor interacts with a specified target while
performing a desired action. The target is defined by a segmentation mask and
the desired action is described via a text prompt. Unlike existing controllable
image-to-video diffusion models that often rely on dense structural or motion
cues to guide the actor's movements toward the target, our target-aware model
requires only a simple mask to indicate the target, leveraging the
generalization capabilities of pretrained models to produce plausible actions.
This makes our method particularly effective for human-object interaction (HOI)
scenarios, where providing precise action guidance is challenging, and further
enables the use of video diffusion models for high-level action planning in
applications such as robotics. We build our target-aware model by extending a
baseline model to incorporate the target mask as an additional input. To
enforce target awareness, we introduce a special token that encodes the
target's spatial information within the text prompt. We then fine-tune the
model with our curated dataset using a novel cross-attention loss that aligns
the cross-attention maps associated with this token with the input target mask.
To further improve performance, we selectively apply this loss to the most
semantically relevant transformer blocks and attention regions. Experimental
results show that our target-aware model outperforms existing solutions in
generating videos where actors interact accurately with the specified targets.
We further demonstrate its efficacy in two downstream applications: video
content creation and zero-shot 3D HOI motion synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project page is available at https://taeksuu.github.io/tavid/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equivariant Image Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiao Dong, Mengde Xu, Zigang Geng, Li Li, Han Hu, Shuyang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current generative models, such as autoregressive and diffusion approaches,
decompose high-dimensional data distribution learning into a series of simpler
subtasks. However, inherent conflicts arise during the joint optimization of
these subtasks, and existing solutions fail to resolve such conflicts without
sacrificing efficiency or scalability. We propose a novel equivariant image
modeling framework that inherently aligns optimization targets across subtasks
by leveraging the translation invariance of natural visual signals. Our method
introduces (1) column-wise tokenization which enhances translational symmetry
along the horizontal axis, and (2) windowed causal attention which enforces
consistent contextual relationships across positions. Evaluated on
class-conditioned ImageNet generation at 256x256 resolution, our approach
achieves performance comparable to state-of-the-art AR models while using fewer
computational resources. Systematic analysis demonstrates that enhanced
equivariance reduces inter-task conflicts, significantly improving zero-shot
generalization and enabling ultra-long image synthesis. This work establishes
the first framework for task-aligned decomposition in generative modeling,
offering insights into efficient parameter sharing and conflict-free
optimization. The code and models are publicly available at
https://github.com/drx-code/EquivariantModeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tuning-Free Amodal Segmentation via the Occlusion-Free Bias of
  Inpainting Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jae Joong Lee, Bedrich Benes, Raymond A. Yeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amodal segmentation aims to predict segmentation masks for both the visible
and occluded regions of an object. Most existing works formulate this as a
supervised learning problem, requiring manually annotated amodal masks or
synthetic training data. Consequently, their performance depends on the quality
of the datasets, which often lack diversity and scale. This work introduces a
tuning-free approach that repurposes pretrained diffusion-based inpainting
models for amodal segmentation. Our approach is motivated by the
"occlusion-free bias" of inpainting models, i.e., the inpainted objects tend to
be complete objects without occlusions. Specifically, we reconstruct the
occluded regions of an object via inpainting and then apply segmentation, all
without additional training or fine-tuning. Experiments on five datasets
demonstrate the generalizability and robustness of our approach. On average,
our approach achieves 5.3% more accurate masks over the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aether: Geometric-Aware Unified World Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Tong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of geometric reconstruction and generative modeling remains a
critical challenge in developing AI systems capable of human-like spatial
reasoning. This paper proposes Aether, a unified framework that enables
geometry-aware reasoning in world models by jointly optimizing three core
capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video
prediction, and (3) goal-conditioned visual planning. Through task-interleaved
feature learning, Aether achieves synergistic knowledge sharing across
reconstruction, prediction, and planning objectives. Building upon video
generation models, our framework demonstrates unprecedented synthetic-to-real
generalization despite never observing real-world data during training.
Furthermore, our approach achieves zero-shot generalization in both action
following and reconstruction tasks, thanks to its intrinsic geometric modeling.
Remarkably, even without real-world data, its reconstruction performance far
exceeds that of domain-specific models. Additionally, Aether leverages a
geometry-informed action space to seamlessly translate predictions into
actions, enabling effective autonomous trajectory planning. We hope our work
inspires the community to explore new frontiers in physically-reasonable world
modeling and its applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://aether-world.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Abou Zeid, Kadir Yilmaz, Daan de Geus, Alexander Hermans, David Adrian, Timm Linder, Bastian Leibe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision foundation models (VFMs) trained on large-scale image datasets provide
high-quality features that have significantly advanced 2D visual recognition.
However, their potential in 3D vision remains largely untapped, despite the
common availability of 2D images alongside 3D point cloud datasets. While
significant research has been dedicated to 2D-3D fusion, recent
state-of-the-art 3D methods predominantly focus on 3D data, leaving the
integration of VFMs into 3D models underexplored. In this work, we challenge
this trend by introducing DITR, a simple yet effective approach that extracts
2D foundation model features, projects them to 3D, and finally injects them
into a 3D point cloud segmentation model. DITR achieves state-of-the-art
results on both indoor and outdoor 3D semantic segmentation benchmarks. To
enable the use of VFMs even when images are unavailable during inference, we
further propose to distill 2D foundation models into a 3D backbone as a
pretraining task. By initializing the 3D backbone with knowledge distilled from
2D VFMs, we create a strong basis for downstream 3D segmentation tasks,
ultimately boosting performance across various datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://vision.rwth-aachen.de/DITR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language
  Models for Long-Form Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, Meng Cao, Kai Kang, Yinfei Yang, Afshin Dehghan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of
video large language models (LLMs) offering a token-efficient solution for
long-form video understanding. This model family employs the two-stream
SlowFast mechanism, enabling efficient modeling of long-range temporal context
to meet the demand for lightweight, mobile-friendly Video LLMs. We provide
models ranging from 1B to 7B parameters, optimized through a streamlined
training pipeline and a high-quality data mixture composed of publicly
available datasets. Experimental results demonstrate that SF-LLaVA-1.5 achieves
competitive performance on a wide range of video and image benchmarks, with
robust results across all model sizes. Notably, SF-LLaVA-1.5 achieves
state-of-the-art results in long-form video understanding (e.g., LongVideoBench
and MLVU) and excels at small scales (1B and 3B) across various video
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video-T1: Test-Time Scaling for Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, Yueqi Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the scale capability of increasing training data, model size, and
computational cost, video generation has achieved impressive results in digital
creation, enabling users to express creativity across various domains.
Recently, researchers in Large Language Models (LLMs) have expanded the scaling
to test-time, which can significantly improve LLM performance by using more
inference-time computation. Instead of scaling up video foundation models
through expensive training costs, we explore the power of Test-Time Scaling
(TTS) in video generation, aiming to answer the question: if a video generation
model is allowed to use non-trivial amount of inference-time compute, how much
can it improve generation quality given a challenging text prompt. In this
work, we reinterpret the test-time scaling of video generation as a searching
problem to sample better trajectories from Gaussian noise space to the target
video distribution. Specifically, we build the search space with test-time
verifiers to provide feedback and heuristic algorithms to guide searching
process. Given a text prompt, we first explore an intuitive linear search
strategy by increasing noise candidates at inference time. As full-step
denoising all frames simultaneously requires heavy test-time computation costs,
we further design a more efficient TTS method for video generation called
Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an
autoregressive manner. Extensive experiments on text-conditioned video
generation benchmarks demonstrate that increasing test-time compute
consistently leads to significant improvements in the quality of videos.
Project page: https://liuff19.github.io/Video-T1
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://liuff19.github.io/Video-T1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-free <span class="highlight-title">Diffusion</span> Acceleration with Bottleneck Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Yunhai Tong, Ling Yang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable capabilities in visual content
generation but remain challenging to deploy due to their high computational
cost during inference. This computational burden primarily arises from the
quadratic complexity of self-attention with respect to image or video
resolution. While existing acceleration methods often compromise output quality
or necessitate costly retraining, we observe that most diffusion models are
pre-trained at lower resolutions, presenting an opportunity to exploit these
low-resolution priors for more efficient inference without degrading
performance. In this work, we introduce Bottleneck Sampling, a training-free
framework that leverages low-resolution priors to reduce computational overhead
while preserving output fidelity. Bottleneck Sampling follows a high-low-high
denoising workflow: it performs high-resolution denoising in the initial and
final stages while operating at lower resolutions in intermediate steps. To
mitigate aliasing and blurring artifacts, we further refine the resolution
transition points and adaptively shift the denoising timesteps at each stage.
We evaluate Bottleneck Sampling on both image and video generation tasks, where
extensive experiments demonstrate that it accelerates inference by up to
3$\times$ for image generation and 2.5$\times$ for video generation, all while
maintaining output quality comparable to the standard full-resolution sampling
process across multiple evaluation metrics. Code is available at:
https://github.com/tyfeld/Bottleneck-Sampling
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Repo: https://github.com/tyfeld/Bottleneck-Sampling ,Project
  Page: https://tyfeld.github.io/BottleneckSampling.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaWorld: Learning Adaptable World Models with Latent Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models aim to learn action-controlled prediction models and have proven
essential for the development of intelligent agents. However, most existing
world models rely heavily on substantial action-labeled data and costly
training, making it challenging to adapt to novel environments with
heterogeneous actions through limited interactions. This limitation can hinder
their applicability across broader domains. To overcome this challenge, we
propose AdaWorld, an innovative world model learning approach that enables
efficient adaptation. The key idea is to incorporate action information during
the pretraining of world models. This is achieved by extracting latent actions
from videos in a self-supervised manner, capturing the most critical
transitions between frames. We then develop an autoregressive world model that
conditions on these latent actions. This learning paradigm enables highly
adaptable world models, facilitating efficient transfer and learning of new
actions even with limited interactions and finetuning. Our comprehensive
experiments across multiple environments demonstrate that AdaWorld achieves
superior performance in both simulation quality and visual planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://adaptable-world-model.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SyncVP: Joint <span class="highlight-title">Diffusion</span> for Synchronous Multi-Modal Video Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico Pallotta, Sina Mokhtarzadeh Azar, Shuai Li, Olga Zatsarynna, Juergen Gall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting future video frames is essential for decision-making systems, yet
RGB frames alone often lack the information needed to fully capture the
underlying complexities of the real world. To address this limitation, we
propose a multi-modal framework for Synchronous Video Prediction (SyncVP) that
incorporates complementary data modalities, enhancing the richness and accuracy
of future predictions. SyncVP builds on pre-trained modality-specific diffusion
models and introduces an efficient spatio-temporal cross-attention module to
enable effective information sharing across modalities. We evaluate SyncVP on
standard benchmark datasets, such as Cityscapes and BAIR, using depth as an
additional modality. We furthermore demonstrate its generalization to other
modalities on SYNTHIA with semantic information and ERA5-Land with climate
data. Notably, SyncVP achieves state-of-the-art performance, even in scenarios
where only one modality is present, demonstrating its robustness and potential
for a wide range of applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoMP: Continual Multimodal Pre-training for <span class="highlight-title">Vision</span> Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitong Chen, Lingchen Meng, Wujian Peng, Zuxuan Wu, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Vision Foundation Models (VFMs) provide strong visual
representations for a wide range of applications. In this paper, we continually
pre-train prevailing VFMs in a multimodal manner such that they can
effortlessly process visual inputs of varying sizes and produce visual
representations that are more aligned with language representations, regardless
of their original pre-training process. To this end, we introduce CoMP, a
carefully designed multimodal pre-training pipeline. CoMP uses a Continual
Rotary Position Embedding to support native resolution continual pre-training,
and an Alignment Loss between visual and textual features through language
prototypes to align multimodal representations. By three-stage training, our
VFMs achieve remarkable improvements not only in multimodal understanding but
also in other downstream tasks such as classification and segmentation.
Remarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA
with a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5
mIoU on ADE20K under frozen chunk evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available in https://github.com/SliMM-X/CoMP-MM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video SimpleQA: Towards Factuality Evaluation in Large Video Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Cao, Pengfei Hu, Yingyao Wang, Jihao Gu, Haoran Tang, Haoze Zhao, Jiahua Dong, Wangbo Yu, Ge Zhang, Ian Reid, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Video Language Models (LVLMs) have highlighted
their potential for multi-modal understanding, yet evaluating their factual
grounding in video contexts remains a critical unsolved challenge. To address
this gap, we introduce Video SimpleQA, the first comprehensive benchmark
tailored for factuality evaluation of LVLMs. Our work distinguishes from
existing video benchmarks through the following key features: 1) Knowledge
required: demanding integration of external knowledge beyond the explicit
narrative; 2) Fact-seeking question: targeting objective, undisputed events or
relationships, avoiding subjective interpretation; 3) Definitive & short-form
answer: Answers are crafted as unambiguous and definitively correct in a short
format, enabling automated evaluation through LLM-as-a-judge frameworks with
minimal scoring variance; 4) External-source verified: All annotations undergo
rigorous validation against authoritative external references to ensure the
reliability; 5) Temporal reasoning required: The annotated question types
encompass both static single-frame understanding and dynamic temporal
reasoning, explicitly evaluating LVLMs factuality under the long-context
dependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize
key findings as follows: 1) Current LVLMs exhibit notable deficiencies in
factual adherence, particularly for open-source models. The best-performing
model Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute
paradigms show insignificant performance gains, revealing fundamental
constraints for enhancing factuality through post-hoc computation; 3)
Retrieval-Augmented Generation demonstrates consistent improvements at the cost
of additional inference time overhead, presenting a critical
efficiency-performance trade-off.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Blocks for Robust and Effective Semi-Supervised Real-World
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moussa Kassem Sbeyti, Nadja Klein, Azarm Nowzad, Fikret Sivrikaya, Sahin Albayrak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised object detection (SSOD) based on pseudo-labeling
significantly reduces dependence on large labeled datasets by effectively
leveraging both labeled and unlabeled data. However, real-world applications of
SSOD often face critical challenges, including class imbalance, label noise,
and labeling errors. We present an in-depth analysis of SSOD under real-world
conditions, uncovering causes of suboptimal pseudo-labeling and key trade-offs
between label quality and quantity. Based on our findings, we propose four
building blocks that can be seamlessly integrated into an SSOD framework. Rare
Class Collage (RCC): a data augmentation method that enhances the
representation of rare classes by creating collages of rare objects. Rare Class
Focus (RCF): a stratified batch sampling strategy that ensures a more balanced
representation of all classes during training. Ground Truth Label Correction
(GLC): a label refinement method that identifies and corrects false, missing,
and noisy ground truth labels by leveraging the consistency of teacher model
predictions. Pseudo-Label Selection (PLS): a selection method for removing
low-quality pseudo-labeled images, guided by a novel metric estimating the
missing detection rate while accounting for class rarity. We validate our
methods through comprehensive experiments on autonomous driving datasets,
resulting in up to 6% increase in SSOD performance. Overall, our investigation
and novel, data-centric, and broadly applicable building blocks enable robust
and effective SSOD in complex, real-world scenarios. Code is available at
https://mos-ks.github.io/publications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research (TMLR).
  OpenReview: https://openreview.net/forum?id=vRYt8QLKqK</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online 3D Scene Reconstruction Using Neural Object Priors <span class="chip">3DV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Chabal, Shizhe Chen, Jean Ponce, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of reconstructing a scene online at the
level of objects given an RGB-D video sequence. While current object-aware
neural implicit representations hold promise, they are limited in online
reconstruction efficiency and shape completion. Our main contributions to
alleviate the above limitations are twofold. First, we propose a feature grid
interpolation mechanism to continuously update grid-based object-centric neural
implicit representations as new object parts are revealed. Second, we construct
an object library with previously mapped objects in advance and leverage the
corresponding shape priors to initialize geometric object models in new videos,
subsequently completing them with novel views as well as synthesized past views
to avoid losing original object details. Extensive experiments on synthetic
environments from the Replica dataset, real-world ScanNet sequences and videos
captured in our laboratory demonstrate that our approach outperforms
state-of-the-art neural implicit models for this task in terms of
reconstruction accuracy and completeness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3DV 2025. Project page:
  https://www.di.ens.fr/willow/research/online-scene-reconstruction/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichen Fan, Amber Yijia Zheng, Raymond A. Yeh, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifier-Free Guidance (CFG) is a widely adopted technique in
diffusion/flow models to improve image fidelity and controllability. In this
work, we first analytically study the effect of CFG on flow matching models
trained on Gaussian mixtures where the ground-truth flow can be derived. We
observe that in the early stages of training, when the flow estimation is
inaccurate, CFG directs samples toward incorrect trajectories. Building on this
observation, we propose CFG-Zero*, an improved CFG with two contributions: (a)
optimized scale, where a scalar is optimized to correct for the inaccuracies in
the estimated velocity, hence the * in the name; and (b) zero-init, which
involves zeroing out the first few steps of the ODE solver. Experiments on both
text-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video
(Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG,
highlighting its effectiveness in guiding Flow Matching models. (Code is
available at github.com/WeichenFan/CFG-Zero-star)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Accurate Scene Text Recognition with Cascaded-Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savas Ozkan, Andrea Maracani, Hyowon Kim, Sijun Cho, Eunchung Noh, Jeongwon Min, Jung Min Cho, Mete Ozay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, vision transformers with text decoder have demonstrated
remarkable performance on Scene Text Recognition (STR) due to their ability to
capture long-range dependencies and contextual relationships with high learning
capacity. However, the computational and memory demands of these models are
significant, limiting their deployment in resource-constrained applications. To
address this challenge, we propose an efficient and accurate STR system.
Specifically, we focus on improving the efficiency of encoder models by
introducing a cascaded-transformers structure. This structure progressively
reduces the vision token size during the encoding step, effectively eliminating
redundant tokens and reducing computational cost. Our experimental results
confirm that our STR system achieves comparable performance to state-of-the-art
baselines while substantially decreasing computational requirements. In
particular, for large-models, the accuracy remains same, 92.77 to 92.68, while
computational complexity is almost halved with our structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM-MMSys2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Speech and Sound: Distinguishing and Locating Audios in <span class="highlight-title">Visual</span>
  Scenes <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonggon Ryu, Seongyu Kim, Joon Son Chung, Arda Senocak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a unified model capable of simultaneously grounding both spoken
language and non-speech sounds within a visual scene, addressing key
limitations in current audio-visual grounding models. Existing approaches are
typically limited to handling either speech or non-speech sounds independently,
or at best, together but sequentially without mixing. This limitation prevents
them from capturing the complexity of real-world audio sources that are often
mixed. Our approach introduces a 'mix-and-separate' framework with audio-visual
alignment objectives that jointly learn correspondence and disentanglement
using mixed audio. Through these objectives, our model learns to produce
distinct embeddings for each audio type, enabling effective disentanglement and
grounding across mixed audio sources. Additionally, we created a new dataset to
evaluate simultaneous grounding of mixed audio sources, demonstrating that our
model outperforms prior methods. Our approach also achieves comparable or
better performance in standard segmentation and cross-modal retrieval tasks,
highlighting the benefits of our mix-and-separate approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A semantic communication-based workload-adjustable transceiver for
  wireless AI-generated content (AIGC) delivery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runze Cheng, Yao Sun, Lan Zhang, Lei Feng, Lei Zhang, Muhammad Ali Imran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the significant advances in generative AI (GAI) and the proliferation of
mobile devices, providing high-quality AI-generated content (AIGC) services via
wireless networks is becoming the future direction. However, the primary
challenges of AIGC service delivery in wireless networks lie in unstable
channels, limited bandwidth resources, and unevenly distributed computational
resources. In this paper, we employ semantic communication (SemCom) in
diffusion-based GAI models to propose a Resource-aware wOrkload-adjUstable
TransceivEr (ROUTE) for AIGC delivery in dynamic wireless networks.
Specifically, to relieve the communication resource bottleneck, SemCom is
utilized to prioritize semantic information of the generated content. Then, to
improve computational resource utilization in both edge and local and reduce
AIGC semantic distortion in transmission, modified diffusion-based models are
applied to adjust the computing workload and semantic density in cooperative
content generation. Simulations verify the superiority of our proposed ROUTE in
terms of latency and content quality compared to conventional AIGC approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Self-Supervised Adaptation for Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moein Sorkhei, Emir Konuk, Jingyu Guo, Christos Matsoukas, Kevin Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised adaptation (SSA) improves foundation model transfer to
medical domains but is computationally prohibitive. Although parameter
efficient fine-tuning methods such as LoRA have been explored for supervised
adaptation, their effectiveness for SSA remains unknown. In this work, we
introduce efficient self-supervised adaptation (ESSA), a framework that applies
parameter-efficient fine-tuning techniques to SSA with the aim of reducing
computational cost and improving adaptation performance. Among the methods
tested, Attention Projection Layer Adaptation (APLA) sets a new
state-of-the-art, consistently surpassing full-parameter SSA and supervised
fine-tuning across diverse medical tasks, while reducing GPU memory by up to
40.1% and increasing training throughput by 25.2%, all while maintaining
inference efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curriculum Coarse-to-Fine Selection for High-IPC <span class="highlight-title">Dataset</span> Distillation <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanda Chen, Gongwei Chen, Miao Zhang, Weili Guan, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation (DD) excels in synthesizing a small number of images per
class (IPC) but struggles to maintain its effectiveness in high-IPC settings.
Recent works on dataset distillation demonstrate that combining distilled and
real data can mitigate the effectiveness decay. However, our analysis of the
combination paradigm reveals that the current one-shot and independent
selection mechanism induces an incompatibility issue between distilled and real
images. To address this issue, we introduce a novel curriculum coarse-to-fine
selection (CCFS) method for efficient high-IPC dataset distillation. CCFS
employs a curriculum selection framework for real data selection, where we
leverage a coarse-to-fine strategy to select appropriate real data based on the
current synthetic dataset in each curriculum. Extensive experiments validate
CCFS, surpassing the state-of-the-art by +6.6\% on CIFAR-10, +5.8\% on
CIFAR-100, and +3.4\% on Tiny-ImageNet under high-IPC settings. Notably, CCFS
achieves 60.2\% test accuracy on ResNet-18 with a 20\% compression ratio of
Tiny-ImageNet, closely matching full-dataset training with only 0.3\%
degradation. Code: https://github.com/CYDaaa30/CCFS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Integration of Key-Value Attention Into Pure and Hybrid
  Transformers for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        DeShin Hwa, Tobias Holmes, Klaus Drechsler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While CNNs were long considered state of the art for image processing, the
introduction of Transformer architectures has challenged this position. While
achieving excellent results in image classification and segmentation,
Transformers remain inherently reliant on large training datasets and remain
computationally expensive. A newly introduced Transformer derivative named KV
Transformer shows promising results in synthetic, NLP, and image classification
tasks, while reducing complexity and memory usage. This is especially conducive
to use cases where local inference is required, such as medical screening
applications. We endeavoured to further evaluate the merit of KV Transformers
on semantic segmentation tasks, specifically in the domain of medical imaging.
By directly comparing traditional and KV variants of the same base
architectures, we provide further insight into the practical tradeoffs of
reduced model complexity. We observe a notable reduction in parameter count and
multiply accumulate operations, while achieving similar performance from most
of the KV variant models when directly compared to their QKV implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, Preprint. Final version published in:
  Bildverarbeitung f\"ur die Medizin 2025, Springer. DOI:
  https://doi.org/10.1007/978-3-658-47422-5_71</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HunyuanPortrait: Implicit Condition Control for Enhanced Portrait
  Animation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zunnan Xu, Zhentao Yu, Zixiang Zhou, Jun Zhou, Xiaoyu Jin, Fa-Ting Hong, Xiaozhong Ji, Junwei Zhu, Chengfei Cai, Shiyu Tang, Qin Lin, Xiu Li, Qinglin Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HunyuanPortrait, a diffusion-based condition control method that
employs implicit representations for highly controllable and lifelike portrait
animation. Given a single portrait image as an appearance reference and video
clips as driving templates, HunyuanPortrait can animate the character in the
reference image by the facial expression and head pose of the driving videos.
In our framework, we utilize pre-trained encoders to achieve the decoupling of
portrait motion information and identity in videos. To do so, implicit
representation is adopted to encode motion information and is employed as
control signals in the animation phase. By leveraging the power of stable video
diffusion as the main building block, we carefully design adapter layers to
inject control signals into the denoising unet through attention mechanisms.
These bring spatial richness of details and temporal consistency.
HunyuanPortrait also exhibits strong generalization performance, which can
effectively disentangle appearance and motion under different image styles. Our
framework outperforms existing methods, demonstrating superior temporal
consistency and controllability. Our project is available at
https://kkakkkka.github.io/HunyuanPortrait.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MC-LLaVA: Multi-Concept Personalized <span class="highlight-title">Vision</span>-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, Shanghang Zhang, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current vision-language models (VLMs) show exceptional abilities across
diverse tasks, such as visual question answering. To enhance user experience,
recent studies investigate VLM personalization to understand user-provided
concepts. However, they mainly focus on single-concept personalization,
neglecting the existence and interplay of multiple concepts, which limits
real-world applicability. This paper proposes the first multi-concept
personalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a
multi-concept instruction tuning strategy, effectively integrating multiple
concepts in a single training step. To reduce the costs related to joint
training, we propose a personalized textual prompt that uses visual token
information to initialize concept tokens. Additionally, we introduce a
personalized visual prompt during inference, aggregating location confidence
maps for enhanced recognition and grounding capabilities. To advance
multi-concept personalization research, we further contribute a high-quality
instruction tuning dataset. We carefully collect images with multiple
characters and objects from movies and manually generate question-answer
samples for multi-concept scenarios, featuring superior diversity.
Comprehensive qualitative and quantitative experiments demonstrate that
MC-LLaVA can achieve impressive multi-concept personalized responses, paving
the way for VLMs to become better user-specific assistants. The code and
dataset will be publicly available at
$\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and dataset will be publicly available at
  $\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}$</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DSwapping: Texture Swapping For 3D Object From Single Reference Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Cao, Beibei Lin, Bo Wang, Zhiyong Huang, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D texture swapping allows for the customization of 3D object textures,
enabling efficient and versatile visual transformations in 3D editing. While no
dedicated method exists, adapted 2D editing and text-driven 3D editing
approaches can serve this purpose. However, 2D editing requires frame-by-frame
manipulation, causing inconsistencies across views, while text-driven 3D
editing struggles to preserve texture characteristics from reference images. To
tackle these challenges, we introduce 3DSwapping, a 3D texture swapping method
that integrates: 1) progressive generation, 2) view-consistency gradient
guidance, and 3) prompt-tuned gradient guidance. To ensure view consistency,
our progressive generation process starts by editing a single reference image
and gradually propagates the edits to adjacent views. Our view-consistency
gradient guidance further reinforces consistency by conditioning the generation
model on feature differences between consistent and inconsistent outputs. To
preserve texture characteristics, we introduce prompt-tuning-based gradient
guidance, which learns a token that precisely captures the difference between
the reference image and the 3D object. This token then guides the editing
process, ensuring more consistent texture preservation across views. Overall,
3DSwapping integrates these novel strategies to achieve higher-fidelity texture
transfer while preserving structural coherence across multiple viewpoints.
Extensive qualitative and quantitative evaluations confirm that our three novel
components enable convincing and effective 2D texture swapping for 3D objects.
Code will be available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to segment anatomy and lesions from disparately labeled sources
  in brain MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meva Himmetoglu, Ilja Ciernik, Ender Konukoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting healthy tissue structures alongside lesions in brain Magnetic
Resonance Images (MRI) remains a challenge for today's algorithms due to
lesion-caused disruption of the anatomy and lack of jointly labeled training
datasets, where both healthy tissues and lesions are labeled on the same
images. In this paper, we propose a method that is robust to lesion-caused
disruptions and can be trained from disparately labeled training sets, i.e.,
without requiring jointly labeled samples, to automatically segment both. In
contrast to prior work, we decouple healthy tissue and lesion segmentation in
two paths to leverage multi-sequence acquisitions and merge information with an
attention mechanism. During inference, an image-specific adaptation reduces
adverse influences of lesion regions on healthy tissue predictions. During
training, the adaptation is taken into account through meta-learning and
co-training is used to learn from disparately labeled training images. Our
model shows an improved performance on several anatomical structures and
lesions on a publicly available brain glioblastoma dataset compared to the
state-of-the-art segmentation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-domain Multi-path Self-supervised <span class="highlight-title">Diffusion</span> Model for Accelerated
  MRI Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang, Jinkui Hao, Bo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic resonance imaging (MRI) is a vital diagnostic tool, but its
inherently long acquisition times reduce clinical efficiency and patient
comfort. Recent advancements in deep learning, particularly diffusion models,
have improved accelerated MRI reconstruction. However, existing diffusion
models' training often relies on fully sampled data, models incur high
computational costs, and often lack uncertainty estimation, limiting their
clinical applicability. To overcome these challenges, we propose a novel
framework, called Dual-domain Multi-path Self-supervised Diffusion Model
(DMSM), that integrates a self-supervised dual-domain diffusion model training
scheme, a lightweight hybrid attention network for the reconstruction diffusion
model, and a multi-path inference strategy, to enhance reconstruction accuracy,
efficiency, and explainability. Unlike traditional diffusion-based models, DMSM
eliminates the dependency on training from fully sampled data, making it more
practical for real-world clinical settings. We evaluated DMSM on two human MRI
datasets, demonstrating that it achieves favorable performance over several
supervised and self-supervised baselines, particularly in preserving fine
anatomical structures and suppressing artifacts under high acceleration
factors. Additionally, our model generates uncertainty maps that correlate
reasonably well with reconstruction errors, offering valuable clinically
interpretable guidance and potentially enhancing diagnostic confidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAGait: Generalized Skeleton-Guided Data Alignment for Gait Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxian Wu, Chuanrui Zhang, Hangrui Xu, Peng Jiao, Haoqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gait recognition is emerging as a promising and innovative area within the
field of computer vision, widely applied to remote person identification.
Although existing gait recognition methods have achieved substantial success in
controlled laboratory datasets, their performance often declines significantly
when transitioning to wild datasets.We argue that the performance gap can be
primarily attributed to the spatio-temporal distribution inconsistencies
present in wild datasets, where subjects appear at varying angles, positions,
and distances across the frames. To achieve accurate gait recognition in the
wild, we propose a skeleton-guided silhouette alignment strategy, which uses
prior knowledge of the skeletons to perform affine transformations on the
corresponding silhouettes.To the best of our knowledge, this is the first study
to explore the impact of data alignment on gait recognition. We conducted
extensive experiments across multiple datasets and network architectures, and
the results demonstrate the significant advantages of our proposed alignment
strategy.Specifically, on the challenging Gait3D dataset, our method achieved
an average performance improvement of 7.9% across all evaluated networks.
Furthermore, our method achieves substantial improvements on cross-domain
datasets, with accuracy improvements of up to 24.0%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal
  Representations <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeonghyeon Kim, Sangheum Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research on out-of-distribution detection (OoDD) has primarily focused
on single-modality models. Recently, with the advent of large-scale pretrained
vision-language models such as CLIP, OoDD methods utilizing such multi-modal
representations through zero-shot and prompt learning strategies have emerged.
However, these methods typically involve either freezing the pretrained weights
or only partially tuning them, which can be suboptimal for downstream datasets.
In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve
notable OoDD performance. Despite some recent works demonstrating the impact of
fine-tuning methods for OoDD, there remains significant potential for
performance improvement. We investigate the limitation of na\"ive fine-tuning
methods, examining why they fail to fully leverage the pretrained knowledge.
Our empirical analysis suggests that this issue could stem from the modality
gap within in-distribution (ID) embeddings. To address this, we propose a
training objective that enhances cross-modal alignment by regularizing the
distances between image and text embeddings of ID data. This adjustment helps
in better utilizing pretrained textual information by aligning similar
semantics from different modalities (i.e., text and image) more closely in the
hyperspherical representation space. We theoretically demonstrate that the
proposed regularization corresponds to the maximum likelihood estimation of an
energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark
datasets, we show that our method, combined with post-hoc OoDD approaches
leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms
existing methods, achieving state-of-the-art OoDD performance and leading ID
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SKDU at De-Factify 4.0: <span class="highlight-title">Vision</span> Transformer with Data Augmentation for
  AI-Generated Image Detection <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrikant Malviya, Neelanjan Bhowmik, Stamos Katsigiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this work is to explore the potential of pre-trained
vision-language models, e.g. Vision Transformers (ViT), enhanced with advanced
data augmentation strategies for the detection of AI-generated images. Our
approach leverages a fine-tuned ViT model trained on the Defactify-4.0 dataset,
which includes images generated by state-of-the-art models such as Stable
Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and
MidJourney. We employ perturbation techniques like flipping, rotation, Gaussian
noise injection, and JPEG compression during training to improve model
robustness and generalisation. The experimental results demonstrate that our
ViT-based pipeline achieves state-of-the-art performance, significantly
outperforming competing methods on both validation and test datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>De-Factify 4.0 workshop at the 39th Annual AAAI Conference on
  Artificial Intelligence (AAAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRCL: Causal Representation Consistency Learning for Anomaly Detection
  in Surveillance Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Hongjin Wang, Zepu Wang, Xiaoguang Zhu, Jing Liu, Peng Sun, Rui Tang, Jianwei Du, Victor C. M. Leung, Liang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Anomaly Detection (VAD) remains a fundamental yet formidable task in
the video understanding community, with promising applications in areas such as
information forensics and public safety protection. Due to the rarity and
diversity of anomalies, existing methods only use easily collected regular
events to model the inherent normality of normal spatial-temporal patterns in
an unsupervised manner. Previous studies have shown that existing unsupervised
VAD models are incapable of label-independent data offsets (e.g., scene
changes) in real-world scenarios and may fail to respond to light anomalies due
to the overgeneralization of deep neural networks. Inspired by causality
learning, we argue that there exist causal factors that can adequately
generalize the prototypical patterns of regular events and present significant
deviations when anomalous instances occur. In this regard, we propose Causal
Representation Consistency Learning (CRCL) to implicitly mine potential
scene-robust causal variable in unsupervised video normality learning.
Specifically, building on the structural causal models, we propose
scene-debiasing learning and causality-inspired normality learning to strip
away entangled scene bias in deep representations and learn causal video
normality, respectively. Extensive experiments on benchmarks validate the
superiority of our method over conventional deep representation learning.
Moreover, ablation studies and extension validation show that the CRCL can cope
with label-independent biases in multi-scene settings and maintain stable
performance with only limited training data available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication by IEEE Transactions on Image Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Change3D: Revisiting Change Detection and Captioning from A Video
  Modeling Perspective <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duowang Zhu, Xiaohu Huang, Haiyan Huang, Hao Zhou, Zhenfeng Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present Change3D, a framework that reconceptualizes the
change detection and captioning tasks through video modeling. Recent methods
have achieved remarkable success by regarding each pair of bi-temporal images
as separate frames. They employ a shared-weight image encoder to extract
spatial features and then use a change extractor to capture differences between
the two images. However, image feature encoding, being a task-agnostic process,
cannot attend to changed regions effectively. Furthermore, different change
extractors designed for various change detection and captioning tasks make it
difficult to have a unified framework. To tackle these challenges, Change3D
regards the bi-temporal images as comprising two frames akin to a tiny video.
By integrating learnable perception frames between the bi-temporal images, a
video encoder enables the perception frames to interact with the images
directly and perceive their differences. Therefore, we can get rid of the
intricate change extractors, providing a unified framework for different change
detection and captioning tasks. We verify Change3D on multiple tasks,
encompassing change detection (including binary change detection, semantic
change detection, and building damage assessment) and change captioning, across
eight standard benchmarks. Without bells and whistles, this simple yet
effective framework can achieve superior performance with an ultra-light video
model comprising only ~6%-13% of the parameters and ~8%-34% of the FLOPs
compared to state-of-the-art methods. We hope that Change3D could be an
alternative to 2D-based models and facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>conference paper, accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian
  Splatting <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulong Zheng, Zicheng Jiang, Shengfeng He, Yandu Sun, Junyu Dong, Huaidong Zhang, Yong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have noticeably
advanced photo-realistic novel view synthesis using images from densely spaced
camera viewpoints. However, these methods struggle in few-shot scenarios due to
limited supervision. In this paper, we present NexusGS, a 3DGS-based approach
that enhances novel view synthesis from sparse-view images by directly
embedding depth information into point clouds, without relying on complex
manual regularizations. Exploiting the inherent epipolar geometry of 3DGS, our
method introduces a novel point cloud densification strategy that initializes
3DGS with a dense point cloud, reducing randomness in point placement while
preventing over-smoothing and overfitting. Specifically, NexusGS comprises
three key steps: Epipolar Depth Nexus, Flow-Resilient Depth Blending, and
Flow-Filtered Depth Pruning. These steps leverage optical flow and camera poses
to compute accurate depth maps, while mitigating the inaccuracies often
associated with optical flow. By incorporating epipolar depth priors, NexusGS
ensures reliable dense point cloud coverage and supports stable 3DGS training
under sparse-view conditions. Experiments demonstrate that NexusGS
significantly enhances depth accuracy and rendering quality, surpassing
state-of-the-art methods by a considerable margin. Furthermore, we validate the
superiority of our generated point clouds by substantially boosting the
performance of competing methods. Project page:
https://usmizuki.github.io/NexusGS/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LGI-DETR: Local-Global Interaction for UAV Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifa Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  UAV has been widely used in various fields. However, most of the existing
object detectors used in drones are not end-to-end and require the design of
various complex components and careful fine-tuning. Most of the existing
end-to-end object detectors are designed for natural scenes. It is not ideal to
apply them directly to UAV images. In order to solve the above challenges, we
design an local-global information interaction DETR for UAVs, namely LGI-DETR.
Cross-layer bidirectional low-level and high-level feature information
enhancement, this fusion method is effective especially in the field of small
objection detection. At the initial stage of encoder, we propose a local
spatial enhancement module (LSE), which enhances the low-level rich local
spatial information into the high-level feature, and reduces the loss of local
information in the transmission process of high-level information. At the final
stage of the encoder, we propose a novel global information injection module
(GII) designed to integrate rich high-level global semantic representations
with low-level feature maps. This hierarchical fusion mechanism effectively
addresses the inherent limitations of local receptive fields by propagating
contextual information across the feature hierarchy. Experimental results on
two challenging UAV image object detection benchmarks, VisDrone2019 and UAVDT,
show that our proposed model outperforms the SOTA model. Compared to the
baseline model, AP and AP50 improved by 1.9% and 2.4%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Perturbation Robustness to Enhance Out-of-Distribution
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxi Chen, Raymond A. Yeh, Shaoshuai Mou, Yan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is the task of identifying inputs that
deviate from the training data distribution. This capability is essential for
safely deploying deep computer vision models in open-world environments. In
this work, we propose a post-hoc method, Perturbation-Rectified OOD detection
(PRO), based on the insight that prediction confidence for OOD inputs is more
susceptible to reduction under perturbation than in-distribution (IND) inputs.
Based on the observation, we propose an adversarial score function that
searches for the local minimum scores near the original inputs by applying
gradient descent. This procedure enhances the separability between IND and OOD
samples. Importantly, the approach improves OOD detection performance without
complex modifications to the underlying model architectures. We conduct
extensive experiments using the OpenOOD benchmark~\cite{yang2022openood}. Our
approach further pushes the limit of softmax-based OOD detection and is the
leading post-hoc method for small-scale models. On a CIFAR-10 model with
adversarial training, PRO effectively detects near-OOD inputs, achieving a
reduction of more than 10\% on FPR@95 compared to state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency Dynamic Convolution for Dense Image Prediction <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linwei Chen, Lin Gu, Liang Li, Chenggang Yan, Ying Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Dynamic Convolution (DY-Conv) has shown promising performance by
enabling adaptive weight selection through multiple parallel weights combined
with an attention mechanism, the frequency response of these weights tends to
exhibit high similarity, resulting in high parameter costs but limited
adaptability. In this work, we introduce Frequency Dynamic Convolution
(FDConv), a novel approach that mitigates these limitations by learning a fixed
parameter budget in the Fourier domain. FDConv divides this budget into
frequency-based groups with disjoint Fourier indices, enabling the construction
of frequency-diverse weights without increasing the parameter cost. To further
enhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency
Band Modulation (FBM). KSM dynamically adjusts the frequency response of each
filter at the spatial level, while FBM decomposes weights into distinct
frequency bands in the frequency domain and modulates them dynamically based on
local content. Extensive experiments on object detection, segmentation, and
classification validate the effectiveness of FDConv. We demonstrate that when
applied to ResNet-50, FDConv achieves superior performance with a modest
increase of +3.6M parameters, outperforming previous methods that require
substantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).
Moreover, FDConv seamlessly integrates into a variety of architectures,
including ConvNeXt, Swin-Transformer, offering a flexible and efficient
solution for modern vision tasks. The code is made publicly available at
https://github.com/Linwei-Chen/FDConv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Good Keypoints for the Two-View Geometry Estimation Problem <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Pakulev, Alexander Vakhitov, Gonzalo Ferrer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local features are essential to many modern downstream applications.
Therefore, it is of interest to determine the properties of local features that
contribute to the downstream performance for a better design of feature
detectors and descriptors. In our work, we propose a new theoretical model for
scoring feature points (keypoints) in the context of the two-view geometry
estimation problem. The model determines two properties that a good keypoint
for solving the homography estimation problem should have: be repeatable and
have a small expected measurement error. This result provides key insights into
why maximizing the number of correspondences doesn't always lead to better
homography estimation accuracy. We use the developed model to design a method
that detects keypoints that benefit the homography estimation introducing the
Bounded NeSS-ST (BoNeSS-ST) keypoint detector. The novelty of BoNeSS-ST comes
from strong theoretical foundations, a more accurate keypoint scoring due to
subpixel refinement and a cost designed for superior robustness to low saliency
keypoints. As a result, BoNeSS-ST outperforms prior self-supervised local
feature detectors in both planar homography and epipolar geometry estimation
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version of the CVPR 2025 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoSurgery-HTS: A <span class="highlight-title">Dataset</span> for Egocentric Hand-Tool Segmentation in Open
  Surgery Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Darjana, Ryo Fujii, Hideo Saito, Hiroki Kajita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Egocentric open-surgery videos capture rich, fine-grained details essential
for accurately modeling surgical procedures and human behavior in the operating
room. A detailed, pixel-level understanding of hands and surgical tools is
crucial for interpreting a surgeon's actions and intentions. We introduce
EgoSurgery-HTS, a new dataset with pixel-wise annotations and a benchmark suite
for segmenting surgical tools, hands, and interacting tools in egocentric
open-surgery videos. Specifically, we provide a labeled dataset for (1) tool
instance segmentation of 14 distinct surgical tools, (2) hand instance
segmentation, and (3) hand-tool segmentation to label hands and the tools they
manipulate. Using EgoSurgery-HTS, we conduct extensive evaluations of
state-of-the-art segmentation methods and demonstrate significant improvements
in the accuracy of hand and hand-tool segmentation in egocentric open-surgery
videos compared to existing datasets. The dataset will be released at
https://github.com/Fujiry0/EgoSurgery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Learning based on Transformed Image Reconstruction for
  Equivariance-Coherent Feature Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Wang, Benjamin Bruns, Hanno Scharr, Kai Krajsek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The equivariant behaviour of features is essential in many computer vision
tasks, yet popular self-supervised learning (SSL) methods tend to constrain
equivariance by design. We propose a self-supervised learning approach where
the system learns transformations independently by reconstructing images that
have undergone previously unseen transformations. Specifically, the model is
tasked to reconstruct intermediate transformed images, e.g. translated or
rotated images, without prior knowledge of these transformations. This
auxiliary task encourages the model to develop equivariance-coherent features
without relying on predefined transformation rules. To this end, we apply
transformations to the input image, generating an image pair, and then split
the extracted features into two sets per image. One set is used with a usual
SSL loss encouraging invariance, the other with our loss based on the auxiliary
task to reconstruct the intermediate transformed images. Our loss and the SSL
loss are linearly combined with weighted terms. Evaluating on synthetic tasks
with natural images, our proposed method strongly outperforms all competitors,
regardless of whether they are designed to learn equivariance. Furthermore,
when trained alongside augmentation-based methods as the invariance tasks, such
as iBOT or DINOv2, we successfully learn a balanced combination of invariant
and equivariant features. Our approach performs strong on a rich set of
realistic computer vision downstream tasks, almost always improving over all
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Tube-based Control Strategy for <span class="highlight-title">Vision</span>-guided Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Der-Hau Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A robust control strategy for autonomous vehicles can improve system
stability, enhance riding comfort, and prevent driving accidents. This paper
presents a novel interpolation tube-based constrained iterative linear
quadratic regulator (itube-CILQR) algorithm for autonomous
computer-vision-based vehicle lane-keeping. The goal of the algorithm is to
enhance robustness during high-speed cornering on tight turns. The advantages
of itube-CILQR over the standard tube-approach include reduced system
conservatism and increased computational speed. Numerical and vision-based
experiments were conducted to examine the feasibility of the proposed
algorithm. The proposed itube-CILQR algorithm is better suited to vehicle
lane-keeping than variational CILQR-based methods and model predictive control
(MPC) approaches using a classical interior-point solver. Specifically, in
evaluation experiments, itube-CILQR achieved an average runtime of 3.16 ms to
generate a control signal to guide a self-driving vehicle; itube-MPC typically
required a 4.67-times longer computation time to complete the same task.
Moreover, the influence of conservatism on system behavior was investigated by
exploring the interpolation variable trajectories derived from the proposed
itube-CILQR algorithm during lane-keeping maneuvers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linguistics-aware Masked Image Modeling for Self-supervised Scene Text
  Recognition <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Zhang, Chang Liu, Jin Wei, Xiaomeng Yang, Yu Zhou, Can Ma, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text images are unique in their dual nature, encompassing both visual and
linguistic information. The visual component encompasses structural and
appearance-based features, while the linguistic dimension incorporates
contextual and semantic elements. In scenarios with degraded visual quality,
linguistic patterns serve as crucial supplements for comprehension,
highlighting the necessity of integrating both aspects for robust scene text
recognition (STR). Contemporary STR approaches often use language models or
semantic reasoning modules to capture linguistic features, typically requiring
large-scale annotated datasets. Self-supervised learning, which lacks
annotations, presents challenges in disentangling linguistic features related
to the global context. Typically, sequence contrastive learning emphasizes the
alignment of local features, while masked image modeling (MIM) tends to exploit
local structures to reconstruct visual patterns, resulting in limited
linguistic knowledge. In this paper, we propose a Linguistics-aware Masked
Image Modeling (LMIM) approach, which channels the linguistic information into
the decoding process of MIM through a separate branch. Specifically, we design
a linguistics alignment module to extract vision-independent features as
linguistic guidance using inputs with different visual appearances. As features
extend beyond mere visual structures, LMIM must consider the global context to
achieve reconstruction. Extensive experiments on various benchmarks
quantitatively demonstrate our state-of-the-art performance, and attention
visualizations qualitatively show the simultaneous capture of both visual and
linguistic information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFDLA: Source-Free Document Layout Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Tewes, Yufan Chen, Omar Moured, Jiaming Zhang, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document Layout Analysis (DLA) is a fundamental task in document
understanding. However, existing DLA and adaptation methods often require
access to large-scale source data and target labels. This requirements severely
limiting their real-world applicability, particularly in privacy-sensitive and
resource-constrained domains, such as financial statements, medical records,
and proprietary business documents. According to our observation, directly
transferring source-domain fine-tuned models on target domains often results in
a significant performance drop (Avg. -32.64%). In this work, we introduce
Source-Free Document Layout Analysis (SFDLA), aiming for adapting a pre-trained
source DLA models to an unlabeled target domain, without access to any source
data. To address this challenge, we establish the first SFDLA benchmark,
covering three major DLA datasets for geometric- and content-aware adaptation.
Furthermore, we propose Document Layout Analysis Adapter (DLAdapter), a novel
framework that is designed to improve source-free adaptation across document
domains. Our method achieves a +4.21% improvement over the source-only baseline
and a +2.26% gain over existing source-free methods from PubLayNet to
DocLayNet. We believe this work will inspire the DLA community to further
investigate source-free document understanding. To support future research of
the community, the benchmark, models, and code will be publicly available at
https://github.com/s3setewe/sfdla-DLAdapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The benchmark, models, and code will be publicly available at
  https://github.com/s3setewe/sfdla-DLAdapter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FG$^2$: Fine-Grained Cross-View Localization by Fine-Grained Feature
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zimin Xia, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel fine-grained cross-view localization method that estimates
the 3 Degrees of Freedom pose of a ground-level image in an aerial image of the
surroundings by matching fine-grained features between the two images. The pose
is estimated by aligning a point plane generated from the ground image with a
point plane sampled from the aerial image. To generate the ground points, we
first map ground image features to a 3D point cloud. Our method then learns to
select features along the height dimension to pool the 3D points to a
Bird's-Eye-View (BEV) plane. This selection enables us to trace which feature
in the ground image contributes to the BEV representation. Next, we sample a
set of sparse matches from computed point correspondences between the two point
planes and compute their relative pose using Procrustes alignment. Compared to
the previous state-of-the-art, our method reduces the mean localization error
by 28% on the VIGOR cross-area test set. Qualitative results show that our
method learns semantically consistent matches across ground and aerial views
through weakly supervised learning from the camera pose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Resolution Generalization of <span class="highlight-title">Diffusion</span> Transformers with
  Randomized Positional Encodings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Liu, Liang Hou, Mingwu Zheng, Xin Tao, Pengfei Wan, Di Zhang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resolution generalization in image generation tasks enables the production of
higher-resolution images with lower training resolution overhead. However, a
significant challenge in resolution generalization, particularly in the widely
used Diffusion Transformers, lies in the mismatch between the positional
encodings encountered during testing and those used during training. While
existing methods have employed techniques such as interpolation, extrapolation,
or their combinations, none have fully resolved this issue. In this paper, we
propose a novel two-dimensional randomized positional encodings (RPE-2D)
framework that focuses on learning positional order of image patches instead of
the specific distances between them, enabling seamless high- and low-resolution
image generation without requiring high- and low-resolution image training.
Specifically, RPE-2D independently selects positions over a broader range along
both the horizontal and vertical axes, ensuring that all position encodings are
trained during the inference phase, thus improving resolution generalization.
Additionally, we propose a random data augmentation technique to enhance the
modeling of position order. To address the issue of image cropping caused by
the augmentation, we introduce corresponding micro-conditioning to enable the
model to perceive the specific cropping patterns. On the ImageNet dataset, our
proposed RPE-2D achieves state-of-the-art resolution generalization
performance, outperforming existing competitive methods when trained at a
resolution of $256 \times 256$ and inferred at $384 \times 384$ and $512 \times
512$, as well as when scaling from $512 \times 512$ to $768 \times 768$ and
$1024 \times 1024$. And it also exhibits outstanding capabilities in
low-resolution image generation, multi-stage training acceleration and
multi-resolution inheritance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijiang Li, Jinglu Wang, Xiang Ming, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Generative AI era, safeguarding 3D models has become increasingly
urgent. While invisible watermarking is well-established for 2D images with
encoder-decoder frameworks, generalizable and robust solutions for 3D remain
elusive. The main difficulty arises from the renderer between the 3D encoder
and 2D decoder, which disrupts direct gradient flow and complicates training.
Existing 3D methods typically rely on per-scene iterative optimization,
resulting in time inefficiency and limited generalization. In this work, we
propose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a
well-known yet underexplored representation for watermarking. We identify two
major challenges: (1) ensuring effective training generalized across diverse 3D
models, and (2) reliably extracting watermarks from free-view renderings, even
under distortions. Our framework, named GS-Marker, incorporates a 3D encoder to
embed messages, distortion layers to enhance resilience against various
distortions, and a 2D decoder to extract watermarks from renderings. A key
innovation is the Adaptive Marker Control mechanism that adaptively perturbs
the initially optimized 3DGS, escaping local minima and improving both training
stability and convergence. Extensive experiments show that GS-Marker
outperforms per-scene training approaches in terms of decoding accuracy and
model fidelity, while also significantly reducing computation time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVAction: evaluating and training multi-modal large language models
  for action recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaokai Ye, Haozhe Qi, Alexander Mathis, Mackenzie W. Mathis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding human behavior requires measuring behavioral actions. Due to
its complexity, behavior is best mapped onto a rich, semantic structure such as
language. The recent development of multi-modal large language models (MLLMs)
is a promising candidate for a wide range of action understanding tasks. In
this work, we focus on evaluating and then improving MLLMs to perform action
recognition. We reformulate EPIC-KITCHENS-100, one of the largest and most
challenging egocentric action datasets, to the form of video multiple question
answering (EPIC-KITCHENS-100-MQA). We show that when we sample difficult
incorrect answers as distractors, leading MLLMs struggle to recognize the
correct actions. We propose a series of methods that greatly improve the MLLMs'
ability to perform action recognition, achieving state-of-the-art on both the
EPIC-KITCHENS-100 validation set, as well as outperforming GPT-4o by 21 points
in accuracy on EPIC-KITCHENS-100-MQA. Lastly, we show improvements on other
action-related video benchmarks such as EgoSchema, PerceptionTest,
LongVideoBench, VideoMME and MVBench, suggesting that MLLMs are a promising
path forward for complex action tasks. Code and models are available at:
https://github.com/AdaptiveMotorControlLab/LLaVAction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/AdaptiveMotorControlLab/LLaVAction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accenture-NVS1: A Novel View Synthesis <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Sugg, Kyle O'Brien, Lekh Poudel, Alex Dumouchelle, Michelle Jou, Marc Bosch, Deva Ramanan, Srinivasa Narasimhan, Shubham Tulsiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces ACC-NVS1, a specialized dataset designed for research
on Novel View Synthesis specifically for airborne and ground imagery. Data for
ACC-NVS1 was collected in Austin, TX and Pittsburgh, PA in 2023 and 2024. The
collection encompasses six diverse real-world scenes captured from both
airborne and ground cameras, resulting in a total of 148,000 images. ACC-NVS1
addresses challenges such as varying altitudes and transient objects. This
dataset is intended to supplement existing datasets, providing additional
resources for comprehensive research, rather than serving as a benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Automatic Data Curation for <span class="highlight-title">Vision</span> Foundation Models in
  Digital Pathology <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boqi Chen, Cédric Vincent-Cuaz, Lydia A. Schoenpflug, Manuel Madeira, Lisa Fournier, Vaishnavi Subramanian, Sonali Andani, Samuel Ruiperez-Campillo, Julia E. Vogt, Raphaëlle Luisier, Dorina Thanou, Viktor H. Koelzer, Pascal Frossard, Gabriele Campanella, Gunnar Rätsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision foundation models (FMs) are accelerating the development of digital
pathology algorithms and transforming biomedical research. These models learn,
in a self-supervised manner, to represent histological features in highly
heterogeneous tiles extracted from whole-slide images (WSIs) of real-world
patient samples. The performance of these FMs is significantly influenced by
the size, diversity, and balance of the pre-training data. However, data
selection has been primarily guided by expert knowledge at the WSI level,
focusing on factors such as disease classification and tissue types, while
largely overlooking the granular details available at the tile level. In this
paper, we investigate the potential of unsupervised automatic data curation at
the tile-level, taking into account 350 million tiles. Specifically, we apply
hierarchical clustering trees to pre-extracted tile embeddings, allowing us to
sample balanced datasets uniformly across the embedding space of the pretrained
FM. We further identify these datasets are subject to a trade-off between size
and balance, potentially compromising the quality of representations learned by
FMs, and propose tailored batch sampling strategies to mitigate this effect. We
demonstrate the effectiveness of our method through improved performance on a
diverse range of clinically relevant downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Burst Super-Resolution for Polarization Images: Noise
  <span class="highlight-title">Dataset</span> and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inseung Hwang, Kiseok Choi, Hyunho Ha, Min H. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Snapshot polarization imaging calculates polarization states from linearly
polarized subimages. To achieve this, a polarization camera employs a double
Bayer-patterned sensor to capture both color and polarization. It demonstrates
low light efficiency and low spatial resolution, resulting in increased noise
and compromised polarization measurements. Although burst super-resolution
effectively reduces noise and enhances spatial resolution, applying it to
polarization imaging poses challenges due to the lack of tailored datasets and
reliable ground truth noise statistics. To address these issues, we introduce
PolarNS and PolarBurstSR, two innovative datasets developed specifically for
polarization imaging. PolarNS provides characterization of polarization noise
statistics, facilitating thorough analysis, while PolarBurstSR functions as a
benchmark for burst super-resolution in polarization images. These datasets,
collected under various real-world conditions, enable comprehensive evaluation.
Additionally, we present a model for analyzing polarization noise to quantify
noise propagation, tested on a large dataset captured in a darkroom
environment. As part of our application, we compare the latest burst
super-resolution models, highlighting the advantages of training tailored to
polarization compared to RGB-based methods. This work establishes a benchmark
for polarization burst super-resolution and offers critical insights into noise
propagation, thereby enhancing polarization image reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel Consistency Prior and Self-Reconstruction Strategy Based
  Unsupervised Image Deraining <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanglu Dong, Tianheng Zheng, Yuanzhouhan Cao, Linbo Qing, Chao Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep image deraining models based on paired datasets have made a
series of remarkable progress. However, they cannot be well applied in
real-world applications due to the difficulty of obtaining real paired datasets
and the poor generalization performance. In this paper, we propose a novel
Channel Consistency Prior and Self-Reconstruction Strategy Based Unsupervised
Image Deraining framework, CSUD, to tackle the aforementioned challenges.
During training with unpaired data, CSUD is capable of generating high-quality
pseudo clean and rainy image pairs which are used to enhance the performance of
deraining network. Specifically, to preserve more image background details
while transferring rain streaks from rainy images to the unpaired clean images,
we propose a novel Channel Consistency Loss (CCLoss) by introducing the Channel
Consistency Prior (CCP) of rain streaks into training process, thereby ensuring
that the generated pseudo rainy images closely resemble the real ones.
Furthermore, we propose a novel Self-Reconstruction (SR) strategy to alleviate
the redundant information transfer problem of the generator, further improving
the deraining performance and the generalization capability of our method.
Extensive experiments on multiple synthetic and real-world datasets demonstrate
that the deraining performance of CSUD surpasses other state-of-the-art
unsupervised methods and CSUD exhibits superior generalization capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OCRT: Boosting Foundation Models in the Open World with
  Object-Concept-Relation Triad <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyao Tang, Yuxuan Yuan, Chaoqi Chen, Zeyu Zhang, Yue Huang, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although foundation models (FMs) claim to be powerful, their generalization
ability significantly decreases when faced with distribution shifts, weak
supervision, or malicious attacks in the open world. On the other hand, most
domain generalization or adversarial fine-tuning methods are task-related or
model-specific, ignoring the universality in practical applications and the
transferability between FMs. This paper delves into the problem of generalizing
FMs to the out-of-domain data. We propose a novel framework, the
Object-Concept-Relation Triad (OCRT), that enables FMs to extract sparse,
high-level concepts and intricate relational structures from raw visual inputs.
The key idea is to bind objects in visual scenes and a set of object-centric
representations through unsupervised decoupling and iterative refinement. To be
specific, we project the object-centric representations onto a semantic concept
space that the model can readily interpret and estimate their importance to
filter out irrelevant elements. Then, a concept-based graph, which has a
flexible degree, is constructed to incorporate the set of concepts and their
corresponding importance, enabling the extraction of high-order factors from
informative concepts and facilitating relational reasoning among these
concepts. Extensive experiments demonstrate that OCRT can substantially boost
the generalizability and robustness of SAM and CLIP across multiple downstream
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hardware-Rasterized Ray-Based Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Rota Bulò, Nemanja Bartolovic, Lorenzo Porzi, Peter Kontschieder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel, hardware rasterized rendering approach for ray-based 3D
Gaussian Splatting (RayGS), obtaining both fast and high-quality results for
novel view synthesis. Our work contains a mathematically rigorous and
geometrically intuitive derivation about how to efficiently estimate all
relevant quantities for rendering RayGS models, structured with respect to
standard hardware rasterization shaders. Our solution is the first enabling
rendering RayGS models at sufficiently high frame rates to support
quality-sensitive applications like Virtual and Mixed Reality. Our second
contribution enables alias-free rendering for RayGS, by addressing MIP-related
issues arising when rendering diverging scales during training and testing. We
demonstrate significant performance gains, across different benchmark scenes,
while retaining state-of-the-art appearance quality of RayGS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Wang, Harry Cheng, Xiao Zhang, Yinglong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Suffering from performance bottlenecks in passively detecting high-quality
Deepfake images due to the advancement of generative models, proactive
perturbations offer a promising approach to disabling Deepfake manipulations by
inserting signals into benign images. However, existing proactive perturbation
approaches remain unsatisfactory in several aspects: 1) visual degradation due
to direct element-wise addition; 2) limited effectiveness against face swapping
manipulation; 3) unavoidable reliance on white- and grey-box settings to
involve generative models during training. In this study, we analyze the
essence of Deepfake face swapping and argue the necessity of protecting source
identities rather than target images, and we propose NullSwap, a novel
proactive defense approach that cloaks source image identities and nullifies
face swapping under a pure black-box scenario. We design an Identity Extraction
module to obtain facial identity features from the source image, while a
Perturbation Block is then devised to generate identity-guided perturbations
accordingly. Meanwhile, a Feature Block extracts shallow-level image features,
which are then fused with the perturbation in the Cloaking Block for image
reconstruction. Furthermore, to ensure adaptability across different identity
extractors in face swapping algorithms, we propose Dynamic Loss Weighting to
adaptively balance identity losses. Experiments demonstrate the outstanding
ability of our approach to fool various identity recognition models,
outperforming state-of-the-art proactive perturbations in preventing face
swapping models from generating images with correct source identities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Motion Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo De Matteis, Matteo Migliarini, Alessio Sampieri, Indro Spinelli, Fabio Galasso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the task of human motion unlearning to prevent the synthesis of
toxic animations while preserving the general text-to-motion generative
performance. Unlearning toxic motions is challenging as those can be generated
from explicit text prompts and from implicit toxic combinations of safe motions
(e.g., ``kicking" is ``loading and swinging a leg"). We propose the first
motion unlearning benchmark by filtering toxic motions from the large and
recent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines,
by adapting state-of-the-art image unlearning techniques to process
spatio-temporal signals. Finally, we propose a novel motion unlearning model
based on Latent Code Replacement, which we dub LCR. LCR is training-free and
suitable to the discrete latent spaces of state-of-the-art text-to-motion
diffusion models. LCR is simple and consistently outperforms baselines
qualitatively and quantitatively. Project page:
\href{https://www.pinlab.org/hmu}{https://www.pinlab.org/hmu}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Any6D: Model-free 6D Pose Estimation of Novel Objects <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeyeop Lee, Bowen Wen, Minjun Kang, Gyuree Kang, In So Kweon, Kuk-Jin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Any6D, a model-free framework for 6D object pose estimation that
requires only a single RGB-D anchor image to estimate both the 6D pose and size
of unknown objects in novel scenes. Unlike existing methods that rely on
textured 3D models or multiple viewpoints, Any6D leverages a joint object
alignment process to enhance 2D-3D alignment and metric scale estimation for
improved pose accuracy. Our approach integrates a render-and-compare strategy
to generate and refine pose hypotheses, enabling robust performance in
scenarios with occlusions, non-overlapping views, diverse lighting conditions,
and large cross-environment variations. We evaluate our method on five
challenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,
demonstrating its effectiveness in significantly outperforming state-of-the-art
methods for novel object pose estimation. Project page:
https://taeyeop.com/any6d
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025, Project Page: https://taeyeop.com/any6d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Calibration enhanced Parameter Synthesis for CLIP-based
  Class-incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncen Guo, Xiaoguang Zhu, Lianlong Sun, Liangyu Teng, Di Li, Yang Liu, Liang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental Learning (CIL) enables models to continuously learn new
class knowledge while memorizing previous classes, facilitating their
adaptation and evolution in dynamic environments. Traditional CIL methods are
mainly based on visual features, which limits their ability to handle complex
scenarios. In contrast, Vision-Language Models (VLMs) show promising potential
to promote CIL by integrating pretrained knowledge with textual features.
However, previous methods make it difficult to overcome catastrophic forgetting
while preserving the generalization capabilities of VLMs. To tackle these
challenges, we propose Feature Calibration enhanced Parameter Synthesis (FCPS)
in this paper. Specifically, our FCPS employs a specific parameter adjustment
mechanism to iteratively refine the proportion of original visual features
participating in the final class determination, ensuring the model's
foundational generalization capabilities. Meanwhile, parameter integration
across different tasks achieves a balance between learning new class knowledge
and retaining old knowledge. Experimental results on popular benchmarks (e.g.,
CIFAR100 and ImageNet100) validate the superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure-Aware Correspondence Learning for Relative Pose Estimation <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Chen, Wenfei Yang, Huan Ren, Shifeng Zhang, Tianzhu Zhang, Feng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relative pose estimation provides a promising way for achieving
object-agnostic pose estimation. Despite the success of existing 3D
correspondence-based methods, the reliance on explicit feature matching suffers
from small overlaps in visible regions and unreliable feature estimation for
invisible regions. Inspired by humans' ability to assemble two object parts
that have small or no overlapping regions by considering object structure, we
propose a novel Structure-Aware Correspondence Learning method for Relative
Pose Estimation, which consists of two key modules. First, a structure-aware
keypoint extraction module is designed to locate a set of kepoints that can
represent the structure of objects with different shapes and appearance, under
the guidance of a keypoint based image reconstruction loss. Second, a
structure-aware correspondence estimation module is designed to model the
intra-image and inter-image relationships between keypoints to extract
structure-aware features for correspondence estimation. By jointly leveraging
these two modules, the proposed method can naturally estimate 3D-3D
correspondences for unseen objects without explicit feature matching for
precise relative pose estimation. Experimental results on the CO3D, Objaverse
and LineMOD datasets demonstrate that the proposed method significantly
outperforms prior methods, i.e., with 5.7{\deg}reduction in mean angular error
on the CO3D dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Virtual Agent Learning and Reasoning: A Step-wise,
  Multi-dimensional, and Generalist Reward Model with Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Miao, Yang Wu, Minghe Gao, Qifan Yu, Wendong Bu, Wenqiao Zhang, Yunfei Li, Siliang Tang, Tat-Seng Chua, Juncheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Generalist Virtual Agents (GVAs) powered by Multimodal
Large Language Models (MLLMs) has shown significant promise in autonomous task
execution. However, current training paradigms face critical limitations,
including reliance on outcome supervision and labor-intensive human
annotations. To address these challenges, we propose Similar, a Step-wise
Multi-dimensional Generalist Reward Model, which offers fine-grained signals
for agent training and can choose better action for inference-time scaling.
Specifically, we begin by systematically defining five dimensions for
evaluating agent actions. Building on this framework, we design an MCTS-P
algorithm to automatically collect and annotate step-wise, five-dimensional
agent execution data. Using this data, we train Similar with the Triple-M
strategy. Furthermore, we introduce the first benchmark in the virtual agent
domain for step-wise, multi-dimensional reward model training and evaluation,
named SRM. This benchmark consists of two components: SRMTrain, which serves as
the training set for Similar, and SRMEval, a manually selected test set for
evaluating the reward model. Experimental results demonstrate that Similar,
through its step-wise, multi-dimensional assessment and synergistic gain,
provides GVAs with effective intermediate signals during both training and
inference-time scaling. The code is available at
https://github.com/Galery23/Similar-v1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Land Cover Priors for Isoprene Emission Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Ummerle, Antonio Giganti, Sara Mandelli, Paolo Bestagini, Stefano Tubaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing plays a crucial role in monitoring Earth's ecosystems, yet
satellite-derived data often suffer from limited spatial resolution,
restricting their applicability in atmospheric modeling and climate research.
In this work, we propose a deep learning-based Super-Resolution (SR) framework
that leverages land cover information to enhance the spatial accuracy of
Biogenic Volatile Organic Compounds (BVOCs) emissions, with a particular focus
on isoprene. Our approach integrates land cover priors as emission drivers,
capturing spatial patterns more effectively than traditional methods. We
evaluate the model's performance across various climate conditions and analyze
statistical correlations between isoprene emissions and key environmental
information such as cropland and tree cover data. Additionally, we assess the
generalization capabilities of our SR model by applying it to unseen climate
zones and geographical regions. Experimental results demonstrate that
incorporating land cover data significantly improves emission SR accuracy,
particularly in heterogeneous landscapes. This study contributes to atmospheric
chemistry and climate modeling by providing a cost-effective, data-driven
approach to refining BVOC emission maps. The proposed method enhances the
usability of satellite-based emissions data, supporting applications in air
quality forecasting, climate impact assessments, and environmental studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 16 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust face recognition based on the wing loss and the $\ell_1$
  regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoyao Yun, Jianwen Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, sparse sampling techniques based on regression analysis have
witnessed extensive applications in face recognition research. Presently,
numerous sparse sampling models based on regression analysis have been explored
by various researchers. Nevertheless, the recognition rates of the majority of
these models would be significantly decreased when confronted with highly
occluded and highly damaged face images. In this paper, a new wing-constrained
sparse coding model(WCSC) and its weighted version(WWCSC) are introduced, so as
to deal with the face recognition problem in complex circumstances, where the
alternating direction method of multipliers (ADMM) algorithm is employed to
solve the corresponding minimization problems. In addition, performances of the
proposed method are examined based on the four well-known facial databases,
namely the ORL facial database, the Yale facial database, the AR facial
database and the FERET facial database. Also, compared to the other methods in
the literatures, the WWCSC has a very high recognition rate even in complex
situations where face images have high occlusion or high damage, which
illustrates the robustness of the WWCSC method in facial recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Glaucoma Calibration: Voting-Based Binocular and Metadata
  Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taejin Jeong, Joohyeok Kim, Jaehoon Joo, Yeonwoo Jung, Hyeonmin Kim, Seong Jae Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Glaucoma is an incurable ophthalmic disease that damages the optic nerve,
leads to vision loss, and ranks among the leading causes of blindness
worldwide. Diagnosing glaucoma typically involves fundus photography, optical
coherence tomography (OCT), and visual field testing. However, the high cost of
OCT often leads to reliance on fundus photography and visual field testing,
both of which exhibit inherent inter-observer variability. This stems from
glaucoma being a multifaceted disease that influenced by various factors. As a
result, glaucoma diagnosis is highly subjective, emphasizing the necessity of
calibration, which aligns predicted probabilities with actual disease
likelihood. Proper calibration is essential to prevent overdiagnosis or
misdiagnosis, which are critical concerns for high-risk diseases. Although AI
has significantly improved diagnostic accuracy, overconfidence in models have
worsen calibration performance. Recent study has begun focusing on calibration
for glaucoma. Nevertheless, previous study has not fully considered glaucoma's
systemic nature and the high subjectivity in its diagnostic process. To
overcome these limitations, we propose V-ViT (Voting-based ViT), a novel
framework that enhances calibration by incorporating disease-specific
characteristics. V-ViT integrates binocular data and metadata, reflecting the
multi-faceted nature of glaucoma diagnosis. Additionally, we introduce a MC
dropout-based Voting System to address high subjectivity. Our approach achieves
state-of-the-art performance across all metrics, including accuracy,
demonstrating that our proposed methods are effective in addressing calibration
issues. We validate our method using a custom dataset including binocular data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLGS: Unsupervised Gaussian Splatting for Image Enhancement and
  Reconstruction in Pure Dark Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Wang, Jingwei Huang, Lu Yang, Tianchen Deng, Gaojing Zhang, Mingrui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting has shown remarkable capabilities in novel view
rendering tasks and exhibits significant potential for multi-view
optimization.However, the original 3D Gaussian Splatting lacks color
representation for inputs in low-light environments. Simply using enhanced
images as inputs would lead to issues with multi-view consistency, and current
single-view enhancement systems rely on pre-trained data, lacking scene
generalization. These problems limit the application of 3D Gaussian Splatting
in low-light conditions in the field of robotics, including high-fidelity
modeling and feature matching. To address these challenges, we propose an
unsupervised multi-view stereoscopic system based on Gaussian Splatting, called
Low-Light Gaussian Splatting (LLGS). This system aims to enhance images in
low-light environments while reconstructing the scene. Our method introduces a
decomposable Gaussian representation called M-Color, which separately
characterizes color information for targeted enhancement. Furthermore, we
propose an unsupervised optimization method with zero-knowledge priors, using
direction-based enhancement to ensure multi-view consistency. Experiments
conducted on real-world datasets demonstrate that our system outperforms
state-of-the-art methods in both low-light enhancement and 3D Gaussian
Splatting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unbiasing through Textual Descriptions: Mitigating Representation Bias
  in Video Benchmarks <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nina Shvetsova, Arsha Nagrani, Bernt Schiele, Hilde Kuehne, Christian Rupprecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new "Unbiased through Textual Description (UTD)" video benchmark
based on unbiased subsets of existing video classification and retrieval
datasets to enable a more robust assessment of video understanding
capabilities. Namely, we tackle the problem that current video benchmarks may
suffer from different representation biases, e.g., object bias or single-frame
bias, where mere recognition of objects or utilization of only a single frame
is sufficient for correct prediction. We leverage VLMs and LLMs to analyze and
debias benchmarks from such representation biases. Specifically, we generate
frame-wise textual descriptions of videos, filter them for specific information
(e.g. only objects) and leverage them to examine representation biases across
three dimensions: 1) concept bias - determining if a specific concept (e.g.,
objects) alone suffice for prediction; 2) temporal bias - assessing if temporal
information contributes to prediction; and 3) common sense vs. dataset bias -
evaluating whether zero-shot reasoning or dataset correlations contribute to
prediction. We conduct a systematic analysis of 12 popular video classification
and retrieval datasets and create new object-debiased test splits for these
datasets. Moreover, we benchmark 30 state-of-the-art video models on original
and debiased splits and analyze biases in the models. To facilitate the future
development of more robust video understanding benchmarks and models, we
release: "UTD-descriptions", a dataset with our rich structured descriptions
for each dataset, and "UTD-splits", a dataset of object-debiased test splits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published at CVPR 2025, project webpage
  https://utd-project.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OCCO: LVM-guided Infrared and Visible Image Fusion Framework based on
  Object-aware and Contextual COntrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Li, Congcong Bian, Zeyang Zhang, Xiaoning Song, Xi Li, Xiao-Jun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image fusion is a crucial technique in the field of computer vision, and its
goal is to generate high-quality fused images and improve the performance of
downstream tasks. However, existing fusion methods struggle to balance these
two factors. Achieving high quality in fused images may result in lower
performance in downstream visual tasks, and vice versa. To address this
drawback, a novel LVM (large vision model)-guided fusion framework with
Object-aware and Contextual COntrastive learning is proposed, termed as OCCO.
The pre-trained LVM is utilized to provide semantic guidance, allowing the
network to focus solely on fusion tasks while emphasizing learning salient
semantic features in form of contrastive learning. Additionally, a novel
feature interaction fusion network is also designed to resolve information
conflicts in fusion images caused by modality differences. By learning the
distinction between positive samples and negative samples in the latent feature
space (contextual space), the integrity of target information in fused image is
improved, thereby benefiting downstream performance. Finally, compared with
eight state-of-the-art methods on four datasets, the effectiveness of the
proposed method is validated, and exceptional performance is also demonstrated
on downstream visual task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Lane Detection with Wavelet-Enhanced Context Modeling and
  Adaptive Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunyang Li, Ming Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane detection is critical for autonomous driving and ad-vanced driver
assistance systems (ADAS). While recent methods like CLRNet achieve strong
performance, they struggle under adverse con-ditions such as extreme weather,
illumination changes, occlusions, and complex curves. We propose a
Wavelet-Enhanced Feature Pyramid Net-work (WE-FPN) to address these challenges.
A wavelet-based non-local block is integrated before the feature pyramid to
improve global context modeling, especially for occluded and curved lanes.
Additionally, we de-sign an adaptive preprocessing module to enhance lane
visibility under poor lighting. An attention-guided sampling strategy further
reffnes spa-tial features, boosting accuracy on distant and curved lanes.
Experiments on CULane and TuSimple demonstrate that our approach signiffcantly
outperforms baselines in challenging scenarios, achieving better robust-ness
and accuracy in real-world driving conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Human-Understandable Multi-Dimensional Concept Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arne Grobrügge, Niklas Kühl, Gerhard Satzger, Philipp Spitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept-based eXplainable AI (C-XAI) aims to overcome the limitations of
traditional saliency maps by converting pixels into human-understandable
concepts that are consistent across an entire dataset. A crucial aspect of
C-XAI is completeness, which measures how well a set of concepts explains a
model's decisions. Among C-XAI methods, Multi-Dimensional Concept Discovery
(MCD) effectively improves completeness by breaking down the CNN latent space
into distinct and interpretable concept subspaces. However, MCD's explanations
can be difficult for humans to understand, raising concerns about their
practical utility. To address this, we propose Human-Understandable
Multi-dimensional Concept Discovery (HU-MCD). HU-MCD uses the Segment Anything
Model for concept identification and implements a CNN-specific input masking
technique to reduce noise introduced by traditional masking methods. These
changes to MCD, paired with the completeness relation, enable HU-MCD to enhance
concept understandability while maintaining explanation faithfulness. Our
experiments, including human subject studies, show that HU-MCD provides more
precise and reliable explanations than existing C-XAI methods. The code is
available at https://github.com/grobruegge/hu-mcd.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dig2DIG: Dig into <span class="highlight-title">Diffusion</span> Information Gains for Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bing Cao, Baoshuo Cai, Changqing Zhang, Qinghua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image fusion integrates complementary information from multi-source images to
generate more informative results. Recently, the diffusion model, which
demonstrates unprecedented generative potential, has been explored in image
fusion. However, these approaches typically incorporate predefined multimodal
guidance into diffusion, failing to capture the dynamically changing
significance of each modality, while lacking theoretical guarantees. To address
this issue, we reveal a significant spatio-temporal imbalance in image
denoising; specifically, the diffusion model produces dynamic information gains
in different image regions with denoising steps. Based on this observation, we
Dig into the Diffusion Information Gains (Dig2DIG) and theoretically derive a
diffusion-based dynamic image fusion framework that provably reduces the upper
bound of the generalization error. Accordingly, we introduce diffusion
information gains (DIG) to quantify the information contribution of each
modality at different denoising steps, thereby providing dynamic guidance
during the fusion process. Extensive experiments on multiple fusion scenarios
confirm that our method outperforms existing diffusion-based approaches in
terms of both fusion quality and inference efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative <span class="highlight-title">Dataset</span> Distillation using Min-Max <span class="highlight-title">Diffusion</span> Model <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junqiao Fan, Yunjiao Zhou, Min Chang Jordan Ren, Jianfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the problem of generative dataset distillation that
utilizes generative models to synthesize images. The generator may produce any
number of images under a preserved evaluation time. In this work, we leverage
the popular diffusion model as the generator to compute a surrogate dataset,
boosted by a min-max loss to control the dataset's diversity and
representativeness during training. However, the diffusion model is
time-consuming when generating images, as it requires an iterative generation
process. We observe a critical trade-off between the number of image samples
and the image quality controlled by the diffusion steps and propose Diffusion
Step Reduction to achieve optimal performance. This paper details our
comprehensive method and its performance. Our model achieved $2^{nd}$ place in
the generative track of \href{https://www.dd-challenge.com/#/}{The First
Dataset Distillation Challenge of ECCV2024}, demonstrating its superior
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is accepted as the ECCV2024 workshop paper and achieved
  second place in the generative track of The First Dataset Distillation
  Challenge of ECCV2024, https://www.dd-challenge.com/#/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-Free Personalization via Retrieval and Reasoning on
  Fingerprints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepayan Das, Davide Talon, Yiming Wang, Massimiliano Mancini, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have lead to major improvements in multimodal
reasoning, yet they still struggle to understand user-specific concepts.
Existing personalization methods address this limitation but heavily rely on
training procedures, that can be either costly or unpleasant to individual
users. We depart from existing work, and for the first time explore the
training-free setting in the context of personalization. We propose a novel
method, Retrieval and Reasoning for Personalization (R2P), leveraging internal
knowledge of VLMs. First, we leverage VLMs to extract the concept fingerprint,
i.e., key attributes uniquely defining the concept within its semantic class.
When a query arrives, the most similar fingerprints are retrieved and scored
via chain-of-thought-reasoning. To reduce the risk of hallucinations, the
scores are validated through cross-modal verification at the attribute level:
in case of a discrepancy between the scores, R2P refines the concept
association via pairwise multimodal matching, where the retrieved fingerprints
and their images are directly compared with the query. We validate R2P on two
publicly available benchmarks and a newly introduced dataset, Personal Concepts
with Visual Ambiguity (PerVA), for concept identification highlighting
challenges in visual ambiguity. R2P consistently outperforms state-of-the-art
approaches on various downstream tasks across all benchmarks. Code will be
available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Uncertainty-Aware <span class="highlight-title">Diffusion</span> for Multi-Agent Trajectory Modeling <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillem Capellera, Antonio Rubio, Luis Ferraz, Antonio Agudo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent trajectory modeling has primarily focused on forecasting future
states, often overlooking broader tasks like trajectory completion, which are
crucial for real-world applications such as correcting tracking data. Existing
methods also generally predict agents' states without offering any state-wise
measure of uncertainty. Moreover, popular multi-modal sampling methods lack any
error probability estimates for each generated scene under the same prior
observations, making it difficult to rank the predictions during inference
time. We introduce U2Diff, a \textbf{unified} diffusion model designed to
handle trajectory completion while providing state-wise \textbf{uncertainty}
estimates jointly. This uncertainty estimation is achieved by augmenting the
simple denoising loss with the negative log-likelihood of the predicted noise
and propagating latent space uncertainty to the real state space. Additionally,
we incorporate a Rank Neural Network in post-processing to enable \textbf{error
probability} estimation for each generated mode, demonstrating a strong
correlation with the error relative to ground truth. Our method outperforms the
state-of-the-art solutions in trajectory completion and forecasting across four
challenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),
highlighting the effectiveness of uncertainty and error probability estimation.
Video at https://youtu.be/ngw4D4eJToE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting Video <span class="highlight-title">Diffusion</span> Models for Time-Lapse Microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Holmberg, Nils Mechtel, Wei Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a domain adaptation of video diffusion models to generate highly
realistic time-lapse microscopy videos of cell division in HeLa cells. Although
state-of-the-art generative video models have advanced significantly for
natural videos, they remain underexplored in microscopy domains. To address
this gap, we fine-tune a pretrained video diffusion model on
microscopy-specific sequences, exploring three conditioning strategies: (1)
text prompts derived from numeric phenotypic measurements (e.g., proliferation
rates, migration speeds, cell-death frequencies), (2) direct numeric embeddings
of phenotype scores, and (3) image-conditioned generation, where an initial
microscopy frame is extended into a complete video sequence. Evaluation using
biologically meaningful morphological, proliferation, and migration metrics
demonstrates that fine-tuning substantially improves realism and accurately
captures critical cellular behaviors such as mitosis and migration. Notably,
the fine-tuned model also generalizes beyond the training horizon, generating
coherent cell dynamics even in extended sequences. However, precisely
controlling specific phenotypic characteristics remains challenging,
highlighting opportunities for future work to enhance conditioning methods. Our
results demonstrate the potential for domain-specific fine-tuning of generative
video models to produce biologically plausible synthetic microscopy data,
supporting applications such as in-silico hypothesis testing and data
augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Chen, Xingcheng Fu, Yisen Gao, Haodong Qian, Yuecen Wei, Kun Yan, Haoyi Zhou, Jianxin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern vision-language models (VLMs) develop patch embedding and convolution
backbone within vector space, especially Euclidean ones, at the very founding.
When expanding VLMs to a galaxy scale for understanding astronomical phenomena,
the integration of spherical space for planetary orbits and hyperbolic spaces
for black holes raises two formidable challenges. a) The current pre-training
model is confined to Euclidean space rather than a comprehensive geometric
embedding. b) The predominant architecture lacks suitable backbones for
anisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a
geometry-aware VLM, for the universe-level vision understanding tasks. We
proposed the geometry prompt that generates geometry tokens by random walks
across diverse spaces on a multi-scale physical graph, along with a geometry
adapter that compresses and reshapes the space anisotropy in a
mixture-of-experts manner. Extensive experiments demonstrate the effectiveness
of our approach, with Galaxy-Walker achieving state-of-the-art performance in
both galaxy property estimation ($R^2$ scores up to $0.91$) and morphology
classification tasks (up to $+0.17$ F1 improvement in challenging features),
significantly outperforming both domain-specific models and general-purpose
VLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Cross-Organ Domain Generalization with Test-Time Style
  Transfer and Diversity Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biwen Meng, Xi Long, Wanrong Yang, Ruochen Liu, Yi Tian, Yalin Zheng, Jingxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has made significant progress in addressing challenges in
various fields including computational pathology (CPath). However, due to the
complexity of the domain shift problem, the performance of existing models will
degrade, especially when it comes to multi-domain or cross-domain tasks. In
this paper, we propose a Test-time style transfer (T3s) that uses a
bidirectional mapping mechanism to project the features of the source and
target domains into a unified feature space, enhancing the generalization
ability of the model. To further increase the style expression space, we
introduce a Cross-domain style diversification module (CSDM) to ensure the
orthogonality between style bases. In addition, data augmentation and low-rank
adaptation techniques are used to improve feature alignment and sensitivity,
enabling the model to adapt to multi-domain inputs effectively. Our method has
demonstrated effectiveness on three unseen datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE International Symposium on Biomedical Imaging (ISBI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMD-Hummingbird: Towards an Efficient Text-to-Video Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Isobe, He Cui, Dong Zhou, Mengmeng Ge, Dong Li, Emad Barsoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Video (T2V) generation has attracted significant attention for its
ability to synthesize realistic videos from textual descriptions. However,
existing models struggle to balance computational efficiency and high visual
quality, particularly on resource-limited devices, e.g.,iGPUs and mobile
phones. Most prior work prioritizes visual fidelity while overlooking the need
for smaller, more efficient models suitable for real-world deployment. To
address this challenge, we propose a lightweight T2V framework, termed
Hummingbird, which prunes existing models and enhances visual quality through
visual feedback learning. Our approach reduces the size of the U-Net from 1.4
billion to 0.7 billion parameters, significantly improving efficiency while
preserving high-quality video generation. Additionally, we introduce a novel
data processing pipeline that leverages Large Language Models (LLMs) and Video
Quality Assessment (VQA) models to enhance the quality of both text prompts and
video data. To support user-driven training and style customization, we
publicly release the full training code, including data processing and model
training. Extensive experiments show that our method achieves a 31X speedup
compared to state-of-the-art models such as VideoCrafter2, while also attaining
the highest overall score on VBench. Moreover, our method supports the
generation of videos with up to 26 frames, addressing the limitations of
existing U-Net-based methods in long video generation. Notably, the entire
training process requires only four GPUs, yet delivers performance competitive
with existing leading methods. Hummingbird presents a practical and efficient
solution for T2V generation, combining high performance, scalability, and
flexibility for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage:
  https://www.amd.com/en/developer/resources/technical-articles/amd-hummingbird-0-9b-text-to-video-diffusion-model-with-4-step-inferencing.html|
  GitHub: https://github.com/AMD-AIG-AIMA/AMD-Hummingbird-T2V</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LeanStereo: A Leaner Backbone based Stereo Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafia Rahim, Samuel Woerz, Andreas Zell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, end-to-end deep networks based stereo matching methods, mainly
because of their performance, have gained popularity. However, this improvement
in performance comes at the cost of increased computational and memory
bandwidth requirements, thus necessitating specialized hardware (GPUs); even
then, these methods have large inference times compared to classical methods.
This limits their applicability in real-world applications. Although we desire
high accuracy stereo methods albeit with reasonable inference time. To this
end, we propose a fast end-to-end stereo matching method. Majority of this
speedup comes from integrating a leaner backbone. To recover the performance
lost because of a leaner backbone, we propose to use learned attention weights
based cost volume combined with LogL1 loss for stereo matching. Using LogL1
loss not only improves the overall performance of the proposed network but also
leads to faster convergence. We do a detailed empirical evaluation of different
design choices and show that our method requires 4x less operations and is also
about 9 to 14x faster compared to the state of the art methods like ACVNet [1],
LEAStereo [2] and CFNet [3] while giving comparable performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruction-Aligned <span class="highlight-title">Visual</span> Attention for Mitigating Hallucinations in
  Large <span class="highlight-title">Vision</span>-Language Models <span class="chip">ICME2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Li, Dehong Gao, Yeyuan Wang, Linbo Jin, Shanqing Yu, Xiaoyan Cai, Libin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the significant success of Large Vision-Language models(LVLMs), these
models still suffer hallucinations when describing images, generating answers
that include non-existent objects. It is reported that these models tend to
over-focus on certain irrelevant image tokens that do not contain critical
information for answering the question and distort the output. To address this,
we propose an Instruction-Aligned Visual Attention(IAVA) approach, which
identifies irrelevant tokens by comparing changes in attention weights under
two different instructions. By applying contrastive decoding, we dynamically
adjust the logits generated from original image tokens and irrelevant image
tokens, reducing the model's over-attention to irrelevant information. The
experimental results demonstrate that IAVA consistently outperforms existing
decoding techniques on benchmarks such as MME, POPE, and TextVQA in mitigating
object hallucinations. Our IAVA approach is available online at
https://github.com/Lee-lab558/IAVA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ATARS: An Aerial Traffic Atomic Activity Recognition and Temporal
  Segmentation <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Chen, Hsuanyu Wu, Chi-Hsi Kung, Yi-Ting Chen, Yan-Tsung Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic Atomic Activity which describes traffic patterns for topological
intersection dynamics is a crucial topic for the advancement of intelligent
driving systems. However, existing atomic activity datasets are collected from
an egocentric view, which cannot support the scenarios where traffic activities
in an entire intersection are required. Moreover, existing datasets only
provide video-level atomic activity annotations, which require exhausting
efforts to manually trim the videos for recognition and limit their
applications to untrimmed videos. To bridge this gap, we introduce the Aerial
Traffic Atomic Activity Recognition and Segmentation (ATARS) dataset, the first
aerial dataset designed for multi-label atomic activity analysis. We offer
atomic activity labels for each frame, which accurately record the intervals
for traffic activities. Moreover, we propose a novel task, Multi-label Temporal
Atomic Activity Recognition, enabling the study of accurate temporal
localization for atomic activity and easing the burden of manual video trimming
for recognition. We conduct extensive experiments to evaluate existing
state-of-the-art models on both atomic activity recognition and temporal atomic
activity segmentation. The results highlight the unique challenges of our ATARS
dataset, such as recognizing extremely small objects' activities. We further
provide comprehensive discussion analyzing these challenges and offer valuable
insights for future direction to improve recognizing atomic activity in aerial
view. Our source code and dataset are available at
https://github.com/magecliff96/ATARS/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EvAnimate: Event-conditioned Image-to-Video Generation for Human
  Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Qu, Ming Li, Xiaoming Chen, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional human animation transforms a static reference image into a
dynamic sequence by applying motion cues such as poses. These motion cues are
typically derived from video data but are susceptible to limitations including
low temporal resolution, motion blur, overexposure, and inaccuracies under
low-light conditions. In contrast, event cameras provide data streams with
exceptionally high temporal resolution, a wide dynamic range, and inherent
resistance to motion blur and exposure issues. In this work, we propose
EvAnimate, a framework that leverages event streams as motion cues to animate
static human images. Our approach employs a specialized event representation
that transforms asynchronous event streams into 3-channel slices with
controllable slicing rates and appropriate slice density, ensuring
compatibility with diffusion models. Subsequently, a dual-branch architecture
generates high-quality videos by harnessing the inherent motion dynamics of the
event streams, thereby enhancing both video quality and temporal consistency.
Specialized data augmentation strategies further enhance cross-person
generalization. Finally, we establish a new benchmarking, including simulated
event data for training and validation, and a real-world event dataset
capturing human actions under normal and extreme scenarios. The experiment
results demonstrate that EvAnimate achieves high temporal fidelity and robust
performance in scenarios where traditional video-derived cues fall short.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Post-Hoc Unknown-Category Detection in Food Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lubnaa Abdur Rahman, Ioannis Papathanail, Lorenzo Brigato, Stavroula Mougiakakou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Food recognition models often struggle to distinguish between seen and unseen
samples, frequently misclassifying samples from unseen categories by assigning
them an in-distribution (ID) label. This misclassification presents significant
challenges when deploying these models in real-world applications, particularly
within automatic dietary assessment systems, where incorrect labels can lead to
cascading errors throughout the system. Ideally, such models should prompt the
user when an unknown sample is encountered, allowing for corrective action.
Given no prior research exploring food recognition in real-world settings, in
this work we conduct an empirical analysis of various post-hoc
out-of-distribution (OOD) detection methods for fine-grained food recognition.
Our findings indicate that virtual logit matching (ViM) performed the best
overall, likely due to its combination of logits and feature-space
representations. Additionally, our work reinforces prior notions in the OOD
domain, noting that models with higher ID accuracy performed better across the
evaluated OOD detection methods. Furthermore, transformer-based architectures
consistently outperformed convolution-based models in detecting OOD samples
across various methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Stereo Networks for Performant and Efficient Leaner Networks <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafia Rahim, Samuel Woerz, Andreas Zell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation has been quite popular in vision for tasks like
classification and segmentation however not much work has been done for
distilling state-of-the-art stereo matching methods despite their range of
applications. One of the reasons for its lack of use in stereo matching
networks is due to the inherent complexity of these networks, where a typical
network is composed of multiple two- and three-dimensional modules. In this
work, we systematically combine the insights from state-of-the-art stereo
methods with general knowledge-distillation techniques to develop a joint
framework for stereo networks distillation with competitive results and faster
inference. Moreover, we show, via a detailed empirical analysis, that
distilling knowledge from the stereo network requires careful design of the
complete distillation pipeline starting from backbone to the right selection of
distillation points and corresponding loss functions. This results in the
student networks that are not only leaner and faster but give excellent
performance . For instance, our student network while performing better than
the performance oriented methods like PSMNet [1], CFNet [2], and LEAStereo [3])
on benchmark SceneFlow dataset, is 8x, 5x, and 8x faster respectively.
Furthermore, compared to speed oriented methods having inference time less than
100ms, our student networks perform better than all the tested methods. In
addition, our student network also shows better generalization capabilities
when tested on unseen datasets like ETH3D and Middlebury.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures. Published in: 2023 International Joint Conference
  on Neural Networks (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniPCGC: Towards Practical Point Cloud Geometry Compression via an
  Efficient Unified Approach <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangli Wang, Wei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based point cloud compression methods have made significant progress
in terms of performance. However, these methods still encounter challenges
including high complexity, limited compression modes, and a lack of support for
variable rate, which restrict the practical application of these methods. In
order to promote the development of practical point cloud compression, we
propose an efficient unified point cloud geometry compression framework, dubbed
as UniPCGC. It is a lightweight framework that supports lossy compression,
lossless compression, variable rate and variable complexity. First, we
introduce the Uneven 8-Stage Lossless Coder (UELC) in the lossless mode, which
allocates more computational complexity to groups with higher coding
difficulty, and merges groups with lower coding difficulty. Second, Variable
Rate and Complexity Module (VRCM) is achieved in the lossy mode through joint
adoption of a rate modulation module and dynamic sparse convolution. Finally,
through the dynamic combination of UELC and VRCM, we achieve lossy compression,
lossless compression, variable rate and complexity within a unified framework.
Compared to the previous state-of-the-art method, our method achieves a
compression ratio (CR) gain of 8.1\% on lossless compression, and a Bjontegaard
Delta Rate (BD-Rate) gain of 14.02\% on lossy compression, while also
supporting variable rate and variable complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for
  Building-Level Remote Sensing Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guneet Mutreja, Philipp Schuegraf, Ksenia Bittner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in self-supervised learning have led to the development of
foundation models that have significantly advanced performance in various
computer vision tasks. However, despite their potential, these models often
overlook the crucial role of high-resolution digital surface models (DSMs) in
understanding urban environments, particularly for building-level analysis,
which is essential for applications like digital twins. To address this gap, we
introduce HiRes-FusedMIM, a novel pre-trained model specifically designed to
leverage the rich information contained within high-resolution RGB and DSM
data. HiRes-FusedMIM utilizes a dual-encoder simple masked image modeling
(SimMIM) architecture with a multi-objective loss function that combines
reconstruction and contrastive objectives, enabling it to learn powerful, joint
representations from both modalities. We conducted a comprehensive evaluation
of HiRes-FusedMIM on a diverse set of downstream tasks, including
classification, semantic segmentation, and instance segmentation. Our results
demonstrate that: 1) HiRes-FusedMIM outperforms previous state-of-the-art
geospatial methods on several building-related datasets, including WHU Aerial
and LoveDA, demonstrating its effectiveness in capturing and leveraging
fine-grained building information; 2) Incorporating DSMs during pre-training
consistently improves performance compared to using RGB data alone,
highlighting the value of elevation information for building-level analysis; 3)
The dual-encoder architecture of HiRes-FusedMIM, with separate encoders for RGB
and DSM data, significantly outperforms a single-encoder model on the Vaihingen
segmentation task, indicating the benefits of learning specialized
representations for each modality. To facilitate further research and
applications in this direction, we will publicly release the trained model
weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiN: <span class="highlight-title">Diffusion</span> Model for Robust Medical VQA with Semantic Noisy Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erjian Guo, Zhen Zhao, Zicheng Wang, Tong Chen, Yunyi Liu, Luping Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical Visual Question Answering (Med-VQA) systems benefit the
interpretation of medical images containing critical clinical information.
However, the challenge of noisy labels and limited high-quality datasets
remains underexplored. To address this, we establish the first benchmark for
noisy labels in Med-VQA by simulating human mislabeling with semantically
designed noise types. More importantly, we introduce the DiN framework, which
leverages a diffusion model to handle noisy labels in Med-VQA. Unlike the
dominant classification-based VQA approaches that directly predict answers, our
Answer Diffuser (AD) module employs a coarse-to-fine process, refining answer
candidates with a diffusion model for improved accuracy. The Answer Condition
Generator (ACG) further enhances this process by generating task-specific
conditional information via integrating answer embeddings with fused
image-question features. To address label noise, our Noisy Label
Refinement(NLR) module introduces a robust loss function and dynamic answer
adjustment to further boost the performance of the AD module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ k-NN as a Simple and Effective Estimator of Transferability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moein Sorkhei, Christos Matsoukas, Johan Fredin Haslum, Kevin Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How well can one expect transfer learning to work in a new setting where the
domain is shifted, the task is different, and the architecture changes? Many
transfer learning metrics have been proposed to answer this question. But how
accurate are their predictions in a realistic new setting? We conducted an
extensive evaluation involving over 42,000 experiments comparing 23
transferability metrics across 16 different datasets to assess their ability to
predict transfer performance. Our findings reveal that none of the existing
metrics perform well across the board. However, we find that a simple k-nearest
neighbor evaluation -- as is commonly used to evaluate feature quality for
self-supervision -- not only surpasses existing metrics, but also offers better
computational efficiency and ease of implementation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIM2PC: Aerial Image to 3D Building Point Cloud Reconstruction <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soulaimene Turki, Daniel Panangian, Houda Chaabouni-Chouayakh, Ksenia Bittner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional urban reconstruction of buildings from single-view images
has attracted significant attention over the past two decades. However, recent
methods primarily focus on rooftops from aerial images, often overlooking
essential geometrical details. Additionally, there is a notable lack of
datasets containing complete 3D point clouds for entire buildings, along with
challenges in obtaining reliable camera pose information for aerial images.
This paper addresses these challenges by presenting a novel methodology, AIM2PC
, which utilizes our generated dataset that includes complete 3D point clouds
and determined camera poses. Our approach takes features from a single aerial
image as input and concatenates them with essential additional conditions, such
as binary masks and Sobel edge maps, to enable more edge-aware reconstruction.
By incorporating a point cloud diffusion model based on Centered denoising
Diffusion Probabilistic Models (CDPM), we project these concatenated features
onto the partially denoised point cloud using our camera poses at each
diffusion step. The proposed method is able to reconstruct the complete 3D
building point cloud, including wall information and demonstrates superior
performance compared to existing baseline techniques. To allow further
comparisons with our methodology the dataset has been made available at
https://github.com/Soulaimene/AIM2PCDataset
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISPRS Geospatial Week 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhang, Weihong Pan, Chong Bao, Xiyu Zhang, Xiaojun Xiang, Hanqing Jiang, Hujun Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans perceive and comprehend their surroundings through information
spanning multiple frequencies. In immersive scenes, people naturally scan their
environment to grasp its overall structure while examining fine details of
objects that capture their attention. However, current NeRF frameworks
primarily focus on modeling either high-frequency local views or the broad
structure of scenes with low-frequency information, which is limited to
balancing both. We introduce FA-NeRF, a novel frequency-aware framework for
view synthesis that simultaneously captures the overall scene structure and
high-definition details within a single NeRF model. To achieve this, we propose
a 3D frequency quantification method that analyzes the scene's frequency
distribution, enabling frequency-aware rendering. Our framework incorporates a
frequency grid for fast convergence and querying, a frequency-aware feature
re-weighting strategy to balance features across different frequency contents.
Extensive experiments show that our method significantly outperforms existing
approaches in modeling entire scenes while preserving fine details.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures. Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-guided Perturbation for Image Super-Resolution <span class="highlight-title">Diffusion</span>
  Model <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leheng Zhang, Weiyi You, Kexuan Shi, Shuhang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based image super-resolution methods have demonstrated significant
advantages over GAN-based approaches, particularly in terms of perceptual
quality. Building upon a lengthy Markov chain, diffusion-based methods possess
remarkable modeling capacity, enabling them to achieve outstanding performance
in real-world scenarios. Unlike previous methods that focus on modifying the
noise schedule or sampling process to enhance performance, our approach
emphasizes the improved utilization of LR information. We find that different
regions of the LR image can be viewed as corresponding to different timesteps
in a diffusion process, where flat areas are closer to the target HR
distribution but edge and texture regions are farther away. In these flat
areas, applying a slight noise is more advantageous for the reconstruction. We
associate this characteristic with uncertainty and propose to apply uncertainty
estimate to guide region-specific noise level control, a technique we refer to
as Uncertainty-guided Noise Weighting. Pixels with lower uncertainty (i.e.,
flat regions) receive reduced noise to preserve more LR information, therefore
improving performance. Furthermore, we modify the network architecture of
previous methods to develop our Uncertainty-guided Perturbation
Super-Resolution (UPSR) model. Extensive experimental results demonstrate that,
despite reduced model size and training overhead, the proposed UWSR method
outperforms current state-of-the-art methods across various datasets, both
quantitatively and qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Text-to-Video Generation help Video-Language Alignment? <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Zanella, Massimiliano Mancini, Willi Menapace, Sergey Tulyakov, Yiming Wang, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent video-language alignment models are trained on sets of videos, each
with an associated positive caption and a negative caption generated by large
language models. A problem with this procedure is that negative captions may
introduce linguistic biases, i.e., concepts are seen only as negatives and
never associated with a video. While a solution would be to collect videos for
the negative captions, existing databases lack the fine-grained variations
needed to cover all possible negatives. In this work, we study whether
synthetic videos can help to overcome this issue. Our preliminary analysis with
multiple generators shows that, while promising on some tasks, synthetic videos
harm the performance of the model on others. We hypothesize this issue is
linked to noise (semantic and visual) in the generated videos and develop a
method, SynViTA, that accounts for those. SynViTA dynamically weights the
contribution of each synthetic video based on how similar its target caption is
w.r.t. the real counterpart. Moreover, a semantic consistency loss makes the
model focus on fine-grained differences across captions, rather than
differences in video appearance. Experiments show that, on average, SynViTA
improves over existing methods on VideoCon test sets and SSv2-Temporal,
SSv2-Events, and ATP-Hard benchmarks, being a first promising step for using
synthetic videos when learning video-language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Project website at https://lucazanella.github.io/synvita/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GroundCap: A <span class="highlight-title">Visual</span>ly Grounded Image Captioning <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel A. P. Oliveira, Lourenço Teodoro, David Martins de Matos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current image captioning systems lack the ability to link descriptive text to
specific visual elements, making their outputs difficult to verify. While
recent approaches offer some grounding capabilities, they cannot track object
identities across multiple references or ground both actions and objects
simultaneously. We propose a novel ID-based grounding system that enables
consistent object reference tracking and action-object linking, and present
GroundCap, a dataset containing 52,016 images from 77 movies, with 344
human-annotated and 52,016 automatically generated captions. Each caption is
grounded on detected objects (132 classes) and actions (51 classes) using a tag
system that maintains object identity while linking actions to the
corresponding objects. Our approach features persistent object IDs for
reference tracking, explicit action-object linking, and segmentation of
background elements through K-means clustering. We propose gMETEOR, a metric
combining caption quality with grounding accuracy, and establish baseline
performance by fine-tuning Pixtral-12B. Human evaluation demonstrates our
approach's effectiveness in producing verifiable descriptions with coherent
object references.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Styled Text Image Generation, but Make It Autoregressive <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vittorio Pippi, Fabio Quattrini, Silvia Cascianelli, Alessio Tonioni, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Styled Handwritten Text Generation (HTG) has recently received attention from
the computer vision and document analysis communities, which have developed
several solutions, either GAN- or diffusion-based, that achieved promising
results. Nonetheless, these strategies fail to generalize to novel styles and
have technical constraints, particularly in terms of maximum output length and
training efficiency. To overcome these limitations, in this work, we propose a
novel framework for text image generation, dubbed Emuru. Our approach leverages
a powerful text image representation model (a variational autoencoder) combined
with an autoregressive Transformer. Our approach enables the generation of
styled text images conditioned on textual content and style examples, such as
specific fonts or handwriting styles. We train our model solely on a diverse,
synthetic dataset of English text rendered in over 100,000 typewritten and
calligraphy fonts, which gives it the capability to reproduce unseen styles
(both fonts and users' handwriting) in zero-shot. To the best of our knowledge,
Emuru is the first autoregressive model for HTG, and the first designed
specifically for generalization to novel styles. Moreover, our model generates
images without background artifacts, which are easier to use for downstream
applications. Extensive evaluation on both typewritten and handwritten,
any-length text image generation scenarios demonstrates the effectiveness of
our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relative Pose Estimation through Affine Corrections of Monocular Depth
  Priors <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yu, Shaohui Liu, Rémi Pautrat, Marc Pollefeys, Viktor Larsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation (MDE) models have undergone significant
advancements over recent years. Many MDE models aim to predict affine-invariant
relative depth from monocular images, while recent developments in large-scale
training and vision foundation models enable reasonable estimation of metric
(absolute) depth. However, effectively leveraging these predictions for
geometric vision tasks, in particular relative pose estimation, remains
relatively under explored. While depths provide rich constraints for cross-view
image alignment, the intrinsic noise and ambiguity from the monocular depth
priors present practical challenges to improving upon classic keypoint-based
solutions. In this paper, we develop three solvers for relative pose estimation
that explicitly account for independent affine (scale and shift) ambiguities,
covering both calibrated and uncalibrated conditions. We further propose a
hybrid estimation pipeline that combines our proposed solvers with classic
point-based solvers and epipolar constraints. We find that the affine
correction modeling is beneficial to not only the relative depth priors but
also, surprisingly, the "metric" ones. Results across multiple datasets
demonstrate large improvements of our approach over classic keypoint-based
baselines and PnP-based solutions, under both calibrated and uncalibrated
setups. We also show that our method improves consistently with different
feature matchers and MDE models, and can further benefit from very recent
advances on both modules. Code is available at
https://github.com/MarkYu98/madpose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complementary Advantages: Exploiting Cross-Field Frequency Correlation
  for NIR-Assisted Image Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Wang, Hongyuan Wang, Lizhi Wang, Xin Wang, Lin Zhu, Wanxuan Lu, Hua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing single-image denoising algorithms often struggle to restore details
when dealing with complex noisy images. The introduction of near-infrared (NIR)
images offers new possibilities for RGB image denoising. However, due to the
inconsistency between NIR and RGB images, the existing works still struggle to
balance the contributions of two fields in the process of image fusion. In
response to this, in this paper, we develop a cross-field Frequency Correlation
Exploiting Network (FCENet) for NIR-assisted image denoising. We first propose
the frequency correlation prior based on an in-depth statistical frequency
analysis of NIR-RGB image pairs. The prior reveals the complementary
correlation of NIR and RGB images in the frequency domain. Leveraging frequency
correlation prior, we then establish a frequency learning framework composed of
Frequency Dynamic Selection Mechanism (FDSM) and Frequency Exhaustive Fusion
Mechanism (FEFM). FDSM dynamically selects complementary information from NIR
and RGB images in the frequency domain, and FEFM strengthens the control of
common and differential features during the fusion process of NIR and RGB
features. Extensive experiments on simulated and real data validate that the
proposed method outperforms other state-of-the-art methods. The code will be
released at https://github.com/yuchenwang815/FCENet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Merging synthetic and real embryo data for advanced AI predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriana Presacan, Alexandru Dorobantiu, Vajira Thambawita, Michael A. Riegler, Mette H. Stensen, Mario Iliceto, Alexandru C. Aldea, Akriti Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate embryo morphology assessment is essential in assisted reproductive
technology for selecting the most viable embryo. Artificial intelligence has
the potential to enhance this process. However, the limited availability of
embryo data presents challenges for training deep learning models. To address
this, we trained two generative models using two datasets-one we created and
made publicly available, and one existing public dataset-to generate synthetic
embryo images at various cell stages, including 2-cell, 4-cell, 8-cell, morula,
and blastocyst. These were combined with real images to train classification
models for embryo cell stage prediction. Our results demonstrate that
incorporating synthetic images alongside real data improved classification
performance, with the model achieving 97% accuracy compared to 94.5% when
trained solely on real data. This trend remained consistent when tested on an
external Blastocyst dataset from a different clinic. Notably, even when trained
exclusively on synthetic data and tested on real data, the model achieved a
high accuracy of 92%. Furthermore, combining synthetic data from both
generative models yielded better classification results than using data from a
single generative model. Four embryologists evaluated the fidelity of the
synthetic images through a Turing test, during which they annotated
inaccuracies and offered feedback. The analysis showed the diffusion model
outperformed the generative adversarial network, deceiving embryologists 66.6%
versus 25.3% and achieving lower Frechet inception distance scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RaCFormer: Towards High-Quality 3D Object Detection via Query-based
  Radar-<span class="highlight-title">Camera</span> Fusion <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomeng Chu, Jiajun Deng, Guoliang You, Yifan Duan, Houqiang Li, Yanyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Radar-Camera fusion transformer (RaCFormer) to boost the accuracy
of 3D object detection by the following insight. The Radar-Camera fusion in
outdoor 3D scene perception is capped by the image-to-BEV transformation--if
the depth of pixels is not accurately estimated, the naive combination of BEV
features actually integrates unaligned visual content. To avoid this problem,
we propose a query-based framework that enables adaptive sampling of
instance-relevant features from both the bird's-eye view (BEV) and the original
image view. Furthermore, we enhance system performance by two key designs:
optimizing query initialization and strengthening the representational capacity
of BEV. For the former, we introduce an adaptive circular distribution in polar
coordinates to refine the initialization of object queries, allowing for a
distance-based adjustment of query density. For the latter, we initially
incorporate a radar-guided depth head to refine the transformation from image
view to BEV. Subsequently, we focus on leveraging the Doppler effect of radar
and introduce an implicit dynamic catcher to capture the temporal elements
within the BEV. Extensive experiments on nuScenes and View-of-Delft (VoD)
datasets validate the merits of our design. Remarkably, our method achieves
superior results of 64.9% mAP and 70.2% NDS on nuScenes. RaCFormer also secures
the state-of-the-art performance on the VoD dataset. Code is available at
https://github.com/cxmomo/RaCFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MotionMap: Representing Multimodality in Human Pose Forecasting <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reyhaneh Hosseininejad, Megh Shukla, Saeed Saadatnejad, Mathieu Salzmann, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human pose forecasting is inherently multimodal since multiple futures exist
for an observed pose sequence. However, evaluating multimodality is challenging
since the task is ill-posed. Therefore, we first propose an alternative
paradigm to make the task well-posed. Next, while state-of-the-art methods
predict multimodality, this requires oversampling a large volume of
predictions. This raises key questions: (1) Can we capture multimodality by
efficiently sampling a smaller number of predictions? (2) Subsequently, which
of the predicted futures is more likely for an observed pose sequence? We
address these questions with MotionMap, a simple yet effective heatmap based
representation for multimodality. We extend heatmaps to represent a spatial
distribution over the space of all possible motions, where different local
maxima correspond to different forecasts for a given observation. MotionMap can
capture a variable number of modes per observation and provide confidence
measures for different modes. Further, MotionMap allows us to introduce the
notion of uncertainty and controllability over the forecasted pose sequence.
Finally, MotionMap captures rare modes that are non-trivial to evaluate yet
critical for safety. We support our claims through multiple qualitative and
quantitative experiments using popular 3D human pose datasets: Human3.6M and
AMASS, highlighting the strengths and limitations of our proposed method.
Project Page: https://vita-epfl.github.io/MotionMap
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. We propose a new representation for learning multimodality
  in human pose forecasting which does not depend on generative models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17808v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17808v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, Ping Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent 3D content generation pipelines commonly employ Variational
Autoencoders (VAEs) to encode shapes into compact latent representations for
diffusion-based generation. However, the widely adopted uniform point sampling
strategy in Shape VAE training often leads to a significant loss of geometric
details, limiting the quality of shape reconstruction and downstream generation
tasks. We present Dora-VAE, a novel approach that enhances VAE reconstruction
through our proposed sharp edge sampling strategy and a dual cross-attention
mechanism. By identifying and prioritizing regions with high geometric
complexity during training, our method significantly improves the preservation
of fine-grained shape features. Such sampling strategy and the dual attention
mechanism enable the VAE to focus on crucial geometric details that are
typically missed by uniform sampling approaches. To systematically evaluate VAE
reconstruction quality, we additionally propose Dora-bench, a benchmark that
quantifies shape complexity through the density of sharp edges, introducing a
new metric focused on reconstruction accuracy at these salient geometric
features. Extensive experiments on the Dora-bench demonstrate that Dora-VAE
achieves comparable reconstruction quality to the state-of-the-art dense
XCube-VAE while requiring a latent space at least 8$\times$ smaller (1,280 vs.
> 10,000 codes).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025. Project page: https://aruichen.github.io/Dora/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Visual</span> Position Prompt for MLLM based <span class="highlight-title">Visual</span> Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Tang, Yanpeng Sun, Qinying Gu, Zechao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Multimodal Large Language Models (MLLMs) excel at various
image-related tasks, they encounter challenges in precisely aligning
coordinates with spatial information within images, particularly in
position-aware tasks such as visual grounding. This limitation arises from two
key factors. First, MLLMs lack explicit spatial references, making it difficult
to associate textual descriptions with precise image locations. Second, their
feature extraction processes prioritize global context over fine-grained
spatial details, leading to weak localization capability. To address this
issue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt
(VPP) to improve its grounding capability. VPP-LLaVA integrates two
complementary mechanisms. The global VPP overlays learnable, axis-like
embeddings onto the input image to provide structured spatial cues. The local
VPP focuses on fine-grained localization by incorporating position-aware
queries, which suggests probable object locations. We also introduce a VPP-SFT
dataset with 0.6M samples, consolidating high-quality visual grounding data
into a compact format for efficient model training. Training on this dataset
with VPP enhances the model's performance, achieving state-of-the-art results
on standard grounding benchmarks despite using fewer training samples compared
to other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\sim$21M
samples). The code and VPP-SFT dataset will be available at
https://github.com/WayneTomas/VPP-LLaVA upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRISM: Privacy-Preserving Improved Stochastic Masking for Federated
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyeongkook Seo, Dong-Jun Han, Jaejun Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements in federated learning (FL), the integration of
generative models into FL has been limited due to challenges such as high
communication costs and unstable training in heterogeneous data environments.
To address these issues, we propose PRISM, a FL framework tailored for
generative models that ensures (i) stable performance in heterogeneous data
distributions and (ii) resource efficiency in terms of communication cost and
final model size. The key of our method is to search for an optimal stochastic
binary mask for a random network rather than updating the model weights,
identifying a sparse subnetwork with high generative performance; i.e., a
``strong lottery ticket''. By communicating binary masks in a stochastic
manner, PRISM minimizes communication overhead. This approach, combined with
the utilization of maximum mean discrepancy (MMD) loss and a mask-aware dynamic
moving average aggregation method (MADA) on the server side, facilitates stable
and strong generative capabilities by mitigating local divergence in FL
scenarios. Moreover, thanks to its sparsifying characteristic, PRISM yields a
lightweight model without extra pruning or quantization, making it ideal for
environments such as edge devices. Experiments on MNIST, FMNIST, CelebA, and
CIFAR10 demonstrate that PRISM outperforms existing methods, while maintaining
privacy with minimal communication costs. PRISM is the first to successfully
generate images under challenging non-IID and privacy-preserving FL
environments on complex datasets, where previous methods have struggled.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STEVE: A Step Verification Pipeline for Computer-use Agent Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanbin Lu, Zhisheng Zhong, Ziqin Wei, Shu Liu, Chi-Wing Fu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing AI agents to autonomously manipulate graphical user interfaces is
a long challenging task. Recent advances in data scaling law inspire us to
train computer-use agents with a scaled instruction set, yet using behavior
cloning to train agents still requires immense high-quality trajectories. To
meet the scalability need, we designed STEVE, a step verification pipeline for
computer-use agent training. First, we establish a large instruction set for
computer-use agents and collect trajectory data with some suboptimal agents.
GPT-4o is used to verify the correctness of each step in the trajectories based
on the screens before and after the action execution, assigning each step with
a binary label. Last, we adopt the Kahneman and Tversky Optimization to
optimize the agent from the binary stepwise labels. Extensive experiments
manifest that our agent outperforms supervised finetuning by leveraging both
positive and negative actions within a trajectory. Also, STEVE enables us to
train a 7B vision-language model as a computer-use agent, achieving leading
performance in the challenging live desktop environment WinAgentArena with
great efficiency at a reduced cost. Code and data:
https://github.com/FanbinLu/STEVE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Positive2Negative: Breaking the Information-Lossy Barrier in
  Self-Supervised Single Image Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16460v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16460v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Li, Lizhi Wang, Zhiyuan Xu, Lin Zhu, Wanxuan Lu, Hua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image denoising enhances image quality, serving as a foundational technique
across various computational photography applications. The obstacle to clean
image acquisition in real scenarios necessitates the development of
self-supervised image denoising methods only depending on noisy images,
especially a single noisy image. Existing self-supervised image denoising
paradigms (Noise2Noise and Noise2Void) rely heavily on information-lossy
operations, such as downsampling and masking, culminating in low quality
denoising performance. In this paper, we propose a novel self-supervised single
image denoising paradigm, Positive2Negative, to break the information-lossy
barrier. Our paradigm involves two key steps: Renoised Data Construction (RDC)
and Denoised Consistency Supervision (DCS). RDC renoises the predicted denoised
image by the predicted noise to construct multiple noisy images, preserving all
the information of the original image. DCS ensures consistency across the
multiple denoised images, supervising the network to learn robust denoising.
Our Positive2Negative paradigm achieves state-of-the-art performance in
self-supervised single image denoising with significant speed improvements. The
code is released to the public at https://github.com/Li-Tong-621/P2N.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 figures, 5 tables, 11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interleaved Scene Graphs for Interleaved Text-and-Image Generation
  Assessment <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongping Chen, Ruoxi Chen, Shu Pu, Zhaoyi Liu, Yanru Wu, Caixi Chen, Benlin Liu, Yue Huang, Yao Wan, Pan Zhou, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world user queries (e.g. "How do to make egg fried rice?") could
benefit from systems capable of generating responses with both textual steps
with accompanying images, similar to a cookbook. Models designed to generate
interleaved text and images face challenges in ensuring consistency within and
across these modalities. To address these challenges, we present ISG, a
comprehensive evaluation framework for interleaved text-and-image generation.
ISG leverages a scene graph structure to capture relationships between text and
image blocks, evaluating responses on four levels of granularity: holistic,
structural, block-level, and image-specific. This multi-tiered evaluation
allows for a nuanced assessment of consistency, coherence, and accuracy, and
provides interpretable question-answer feedback. In conjunction with ISG, we
introduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8
categories and 21 subcategories. This benchmark dataset includes complex
language-vision dependencies and golden answers to evaluate models effectively
on vision-centric tasks such as style transfer, a challenging area for current
models. Using ISG-Bench, we demonstrate that recent unified vision-language
models perform poorly on generating interleaved content. While compositional
approaches that combine separate language and image models show a 111%
improvement over unified models at the holistic level, their performance
remains suboptimal at both block and image levels. To facilitate future work,
we develop ISG-Agent, a baseline agent employing a "plan-execute-refine"
pipeline to invoke tools, achieving a 122% performance improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025 as Spotlight. Project homepage:
  https://interleave-eval.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Omnimatte: Learning to Decompose Video into Layers <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao-Chih Lee, Erika Lu, Sarah Rumbley, Michal Geyer, Jia-Bin Huang, Tali Dekel, Forrester Cole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a video and a set of input object masks, an omnimatte method aims to
decompose the video into semantically meaningful layers containing individual
objects along with their associated effects, such as shadows and reflections.
Existing omnimatte methods assume a static background or accurate pose and
depth estimation and produce poor decompositions when these assumptions are
violated. Furthermore, due to the lack of generative prior on natural videos,
existing methods cannot complete dynamic occluded regions. We present a novel
generative layered video decomposition framework to address the omnimatte
problem. Our method does not assume a stationary scene or require camera pose
or depth information and produces clean, complete layers, including convincing
completions of occluded dynamic regions. Our core idea is to train a video
diffusion model to identify and remove scene effects caused by a specific
object. We show that this model can be finetuned from an existing video
inpainting model with a small, carefully curated dataset, and demonstrate
high-quality decompositions and editing results for a wide range of casually
captured videos containing soft shadows, glossy reflections, splashing water,
and more.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Project page: https://gen-omnimatte.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learned, uncertainty-driven adaptive acquisition for photon-efficient
  scanning microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cassandra Tong Ye, Jiashu Han, Kunzan Liu, Anastasios Angelopoulos, Linda Griffith, Kristina Monakhova, Sixian You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scanning microscopy systems, such as confocal and multiphoton microscopy, are
powerful imaging tools for probing deep into biological tissue. However,
scanning systems have an inherent trade-off between acquisition time, field of
view, phototoxicity, and image quality, often resulting in noisy measurements
when fast, large field of view, and/or gentle imaging is needed. Deep learning
could be used to denoise noisy microscopy measurements, but these algorithms
can be prone to hallucination, which can be disastrous for medical and
scientific applications. We propose a method to simultaneously denoise and
predict pixel-wise uncertainty for scanning microscopy systems, improving
algorithm trustworthiness and providing statistical guarantees for deep
learning predictions. Furthermore, we propose to leverage this learned,
pixel-wise uncertainty to drive an adaptive acquisition technique that rescans
only the most uncertain regions of a sample, saving time and reducing the total
light dose to the sample. We demonstrate our method on experimental confocal
and multiphoton microscopy systems, showing that our uncertainty maps can
pinpoint hallucinations in the deep learned predictions. Finally, with our
adaptive acquisition technique, we demonstrate up to 16X reduction in
acquisition time and total light dose while successfully recovering fine
features in the sample and reducing hallucinations. We are the first to
demonstrate distribution-free uncertainty quantification for a denoising task
with real experimental data and the first to propose adaptive acquisition based
on reconstruction uncertainty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lightweight Embedded FPGA Deployment of Learned Image Compression with
  Knowledge Distillation and Hybrid Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04832v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04832v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alaa Mazouz, Sumanta Chaudhuri, Marco Cagnanzzo, Mihai Mitrea, Enzo Tartaglione, Attilio Fiandrotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learnable Image Compression (LIC) has shown the potential to outperform
standardized video codecs in RD efficiency, prompting the research for
hardware-friendly implementations. Most existing LIC hardware implementations
prioritize latency to RD-efficiency and through an extensive exploration of the
hardware design space. We present a novel design paradigm where the burden of
tuning the design for a specific hardware platform is shifted towards model
dimensioning and without compromising on RD-efficiency. First, we design a
framework for distilling a leaner student LIC model from a reference teacher:
by tuning a single model hyperparameters, we can meet the constraints of
different hardware platforms without a complex hardware design exploration.
Second, we propose a hardware-friendly implementation of the Generalized
Divisive Normalization - GDN activation that preserves RD efficiency even post
parameter quantization. Third, we design a pipelined FPGA configuration which
takes full advantage of available FPGA resources by leveraging parallel
processing and optimizing resource allocation. Our experiments with a state of
the art LIC model show that we outperform all existing FPGA implementations
while performing very close to the original model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Circuits and Systems for Video
  Technology in March 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mani-GS: Gaussian Splatting <span class="highlight-title">Manipulation</span> with Triangular Mesh <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangjun Gao, Xiaoyu Li, Yiyu Zhuang, Qi Zhang, Wenbo Hu, Chaopeng Zhang, Yao Yao, Ying Shan, Long Quan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural 3D representations such as Neural Radiance Fields (NeRF), excel at
producing photo-realistic rendering results but lack the flexibility for
manipulation and editing which is crucial for content creation. Previous works
have attempted to address this issue by deforming a NeRF in canonical space or
manipulating the radiance field based on an explicit mesh. However,
manipulating NeRF is not highly controllable and requires a long training and
inference time. With the emergence of 3D Gaussian Splatting (3DGS), extremely
high-fidelity novel view synthesis can be achieved using an explicit
point-based 3D representation with much faster training and rendering speed.
However, there is still a lack of effective means to manipulate 3DGS freely
while maintaining rendering quality. In this work, we aim to tackle the
challenge of achieving manipulable photo-realistic rendering. We propose to
utilize a triangular mesh to manipulate 3DGS directly with self-adaptation.
This approach reduces the need to design various algorithms for different types
of Gaussian manipulation. By utilizing a triangle shape-aware Gaussian binding
and adapting method, we can achieve 3DGS manipulation and preserve
high-fidelity rendering after manipulation. Our approach is capable of handling
large deformations, local manipulations, and soft body simulations while
keeping high-quality rendering. Furthermore, we demonstrate that our method is
also effective with inaccurate meshes extracted from 3DGS. Experiments
conducted demonstrate the effectiveness of our method and its superiority over
baseline approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Project page here: https://gaoxiangjun.github.io/mani_gs/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaRS: A Fast Sampler for Mean Reverting <span class="highlight-title">Diffusion</span> based on ODE and SDE
  Solvers <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07856v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07856v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Li, Wei Fang, Hongbo Zhao, Le Lu, Ge Yang, Minfeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In applications of diffusion models, controllable generation is of practical
significance, but is also challenging. Current methods for controllable
generation primarily focus on modifying the score function of diffusion models,
while Mean Reverting (MR) Diffusion directly modifies the structure of the
stochastic differential equation (SDE), making the incorporation of image
conditions simpler and more natural. However, current training-free fast
samplers are not directly applicable to MR Diffusion. And thus MR Diffusion
requires hundreds of NFEs (number of function evaluations) to obtain
high-quality samples. In this paper, we propose a new algorithm named MaRS (MR
Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time
SDE and the probability flow ordinary differential equation (PF-ODE) associated
with MR Diffusion, and derive semi-analytical solutions. The solutions consist
of an analytical function and an integral parameterized by a neural network.
Based on this solution, we can generate high-quality samples in fewer steps.
Our approach does not require training and supports all mainstream
parameterizations, including noise prediction, data prediction and velocity
prediction. Extensive experiments demonstrate that MR Sampler maintains high
sampling quality with a speedup of 10 to 20 times across ten different image
restoration tasks. Our algorithm accelerates the sampling procedure of MR
Diffusion, making it more practical in controllable generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhang, Zhuokai Zhao, Zhaorun Chen, Zenghui Ding, Xianjun Yang, Yining Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal large language models (MLLMs) have opened
new avenues for video understanding. However, achieving high fidelity in
zero-shot video tasks remains challenging. Traditional video processing methods
rely heavily on fine-tuning to capture nuanced spatial-temporal details, which
incurs significant data and computation costs. In contrast, training-free
approaches, though efficient, often lack robustness in preserving context-rich
features across complex video content. To this end, we propose DYTO, a novel
dynamic token merging framework for zero-shot video understanding that
adaptively optimizes token efficiency while preserving crucial scene details.
DYTO integrates a hierarchical frame selection and a bipartite token merging
strategy to dynamically cluster key frames and selectively compress token
sequences, striking a balance between computational efficiency with semantic
richness. Extensive experiments across multiple benchmarks demonstrate the
effectiveness of DYTO, achieving superior performance compared to both
fine-tuned and training-free methods and setting a new state-of-the-art for
zero-shot video understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/Jam1ezhang/DYTO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive
  Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19824v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19824v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Su, Xiaoxuan Ma, Jiajun Su, Yizhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a one-stage framework for real-time multi-person 3D human mesh
estimation from a single RGB image. While current one-stage methods, which
follow a DETR-style pipeline, achieve state-of-the-art (SOTA) performance with
high-resolution inputs, we observe that this particularly benefits the
estimation of individuals in smaller scales of the image (e.g., those far from
the camera), but at the cost of significantly increased computation overhead.
To address this, we introduce scale-adaptive tokens that are dynamically
adjusted based on the relative scale of each individual in the image within the
DETR framework. Specifically, individuals in smaller scales are processed at
higher resolutions, larger ones at lower resolutions, and background regions
are further distilled. These scale-adaptive tokens more efficiently encode the
image features, facilitating subsequent decoding to regress the human mesh,
while allowing the model to allocate computational resources more effectively
and focus on more challenging cases. Experiments show that our method preserves
the accuracy benefits of high-resolution processing while substantially
reducing computational cost, achieving real-time inference with performance
comparable to SOTA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RankCLIP: Ranking-Consistent Language-Image Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhang, Zhuokai Zhao, Zhaorun Chen, Zhili Feng, Zenghui Ding, Yining Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised contrastive learning models, such as CLIP, have set new
benchmarks for vision-language models in many downstream tasks. However, their
dependency on rigid one-to-one mappings overlooks the complex and often
multifaceted relationships between and within texts and images. To this end, we
introduce RankCLIP, a novel pre-training method that extends beyond the rigid
one-to-one matching framework of CLIP and its variants. By extending the
traditional pair-wise loss to list-wise, and leveraging both in-modal and
cross-modal ranking consistency, RankCLIP improves the alignment process,
enabling it to capture the nuanced many-to-many relationships between and
within each modality. Through comprehensive experiments, we demonstrate the
effectiveness of RankCLIP in various downstream tasks, notably achieving
significant gains in zero-shot classifications over state-of-the-art methods,
underscoring the importance of this enhanced learning process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and model checkpoints are available at
  https://github.com/Jam1ezhang/RankCLIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DUNE: Distilling a Universal Encoder from Heterogeneous 2D and 3D
  Teachers <span class="chip">CVPR-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Bulent Sariyildiz, Philippe Weinzaepfel, Thomas Lucas, Pau de Jorge, Diane Larlus, Yannis Kalantidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multi-teacher distillation methods have unified the encoders of
multiple foundation models into a single encoder, achieving competitive
performance on core vision tasks like classification, segmentation, and depth
estimation. This led us to ask: Could similar success be achieved when the pool
of teachers also includes vision models specialized in diverse tasks across
both 2D and 3D perception? In this paper, we define and investigate the problem
of heterogeneous teacher distillation, or co-distillation, a challenging
multi-teacher distillation scenario where teacher models vary significantly in
both (a) their design objectives and (b) the data they were trained on. We
explore data-sharing strategies and teacher-specific encoding, and introduce
DUNE, a single encoder excelling in 2D vision, 3D understanding, and 3D human
perception. Our model achieves performance comparable to that of its larger
teachers, sometimes even outperforming them, on their respective tasks.
Notably, DUNE surpasses MASt3R in Map-free Visual Relocalization with a much
smaller encoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR-2025. Project page:
  https://europe.naverlabs.com/dune</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Model Calibration -- A gentle introduction and <span class="highlight-title">visual</span>
  exploration of calibration and the expected calibration error (ECE) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19047v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19047v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maja Pavlovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To be considered reliable, a model must be calibrated so that its confidence
in each decision closely reflects its true outcome. In this blogpost we'll take
a look at the most commonly used definition for calibration and then dive into
a frequently used evaluation measure for model calibration. We'll then cover
some of the drawbacks of this measure and how these surfaced the need for
additional notions of calibration, which require their own new evaluation
measures. This post is not intended to be an in-depth dissection of all works
on calibration, nor does it focus on how to calibrate models. Instead, it is
meant to provide a gentle introduction to the different notions and their
evaluation measures as well as to re-highlight some issues with a measure that
is still widely used to evaluate calibration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAEA: A Geolocation Aware Conversational Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16423v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16423v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ron Campos, Ashmal Vayani, Parth Parag Kulkarni, Rohit Gupta, Aritra Dutta, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image geolocalization, in which, traditionally, an AI model predicts the
precise GPS coordinates of an image is a challenging task with many downstream
applications. However, the user cannot utilize the model to further their
knowledge other than the GPS coordinate; the model lacks an understanding of
the location and the conversational ability to communicate with the user. In
recent days, with tremendous progress of large multimodal models (LMMs) --
proprietary and open-source -- researchers have attempted to geolocalize images
via LMMs. However, the issues remain unaddressed; beyond general tasks, for
more specialized downstream tasks, one of which is geolocalization, LMMs
struggle. In this work, we propose to solve this problem by introducing a
conversational model GAEA that can provide information regarding the location
of an image, as required by a user. No large-scale dataset enabling the
training of such a model exists. Thus we propose GAEA-1.6M, a comprehensive
dataset with 800K images and around 1.6M question-answer pairs constructed by
leveraging OpenStreetMap (OSM) attributes and geographical context clues. For
quantitative evaluation, we propose a diverse benchmark, GAEA-Bench, comprising
4K image-text pairs to evaluate conversational capabilities equipped with
diverse question types. We consider 11 state-of-the-art open-source and
proprietary LMMs and demonstrate that GAEA significantly outperforms the best
open-source model, LLaVA-OneVision by 25.69% and the best proprietary model,
GPT-4o by 8.28%. Our dataset, model and codes are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The dataset and code used in this submission is available at:
  https://ucf-crcv.github.io/GAEA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiple Object Tracking as ID Prediction <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruopeng Gao, Ji Qi, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Object Tracking (MOT) has been a long-standing challenge in video
understanding. A natural and intuitive approach is to split this task into two
parts: object detection and association. Most mainstream methods employ
meticulously crafted heuristic techniques to maintain trajectory information
and compute cost matrices for object matching. Although these methods can
achieve notable tracking performance, they often require a series of elaborate
handcrafted modifications while facing complicated scenarios. We believe that
manually assumed priors limit the method's adaptability and flexibility in
learning optimal tracking capabilities from domain-specific data. Therefore, we
introduce a new perspective that treats Multiple Object Tracking as an
in-context ID Prediction task, transforming the aforementioned object
association into an end-to-end trainable task. Based on this, we propose a
simple yet effective method termed MOTIP. Given a set of trajectories carried
with ID information, MOTIP directly decodes the ID labels for current
detections to accomplish the association process. Without using tailored or
sophisticated architectures, our method achieves state-of-the-art results
across multiple benchmarks by solely leveraging object-level features as
tracking cues. The simplicity and impressive results of MOTIP leave substantial
room for future advancements, thereby making it a promising baseline for
subsequent research. Our code and checkpoints are released at
https://github.com/MCG-NJU/MOTIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Kalib: Easy Hand-Eye Calibration with Reference Point Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tutian Tang, Minghao Liu, Wenqiang Xu, <span class="highlight-author">Cewu Lu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand-eye calibration aims to estimate the transformation between a camera and
a robot. Traditional methods rely on fiducial markers, which require
considerable manual effort and precise setup. Recent advances in deep learning
have introduced markerless techniques but come with more prerequisites, such as
retraining networks for each robot, and accessing accurate mesh models for data
generation. In this paper, we propose Kalib, an automatic and easy-to-setup
hand-eye calibration method that leverages the generalizability of visual
foundation models to overcome these challenges. It features only two basic
prerequisites, the robot's kinematic chain and a predefined reference point on
the robot. During calibration, the reference point is tracked in the camera
space. Its corresponding 3D coordinates in the robot coordinate can be inferred
by forward kinematics. Then, a PnP solver directly estimates the transformation
between the camera and the robot without training new networks or accessing
mesh models. Evaluations in simulated and real-world benchmarks show that Kalib
achieves good accuracy with a lower manual workload compared with recent
baseline methods. We also demonstrate its application in multiple real-world
settings with various robot arms and grippers. Kalib's user-friendly design and
minimal setup requirements make it a possible solution for continuous operation
in unstructured environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code, data, and supplementary materials are available at
  https://sites.google.com/view/hand-eye-kalib</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Universal Soccer Video Understanding <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01820v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01820v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayuan Rao, Haoning Wu, Hao Jiang, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a globally celebrated sport, soccer has attracted widespread interest from
fans all over the world. This paper aims to develop a comprehensive multi-modal
framework for soccer video understanding. Specifically, we make the following
contributions in this paper: (i) we introduce SoccerReplay-1988, the largest
multi-modal soccer dataset to date, featuring videos and detailed annotations
from 1,988 complete matches, with an automated annotation pipeline; (ii) we
present an advanced soccer-specific visual encoder, MatchVision, which
leverages spatiotemporal information across soccer videos and excels in various
downstream tasks; (iii) we conduct extensive experiments and ablation studies
on event classification, commentary generation, and multi-view foul
recognition. MatchVision demonstrates state-of-the-art performance on all of
them, substantially outperforming existing models, which highlights the
superiority of our proposed data and model. We believe that this work will
offer a standard paradigm for sports understanding research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025; Project Page: https://jyrao.github.io/UniSoccer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAGE: Unsupervised <span class="highlight-title">Visual</span> Composition and Animation for Controllable
  Video Generation <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aram Davtyan, Sepehr Sameni, Björn Ommer, Paolo Favaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of video generation has expanded significantly in recent years,
with controllable and compositional video generation garnering considerable
interest. Most methods rely on leveraging annotations such as text, objects'
bounding boxes, and motion cues, which require substantial human effort and
thus limit their scalability. In contrast, we address the challenge of
controllable and compositional video generation without any annotations by
introducing a novel unsupervised approach. Our model is trained from scratch on
a dataset of unannotated videos. At inference time, it can compose plausible
novel scenes and animate objects by placing object parts at the desired
locations in space and time. The core innovation of our method lies in the
unified control format and the training process, where video generation is
conditioned on a randomly selected subset of pre-trained self-supervised local
features. This conditioning compels the model to learn how to inpaint the
missing information in the video both spatially and temporally, thereby
learning the inherent compositionality of a scene and the dynamics of moving
objects. The abstraction level and the imposed invariance of the conditioning
input to minor visual perturbations enable control over object motion by simply
using the same features at all the desired future locations. We call our model
CAGE, which stands for visual Composition and Animation for video GEneration.
We conduct extensive experiments to validate the effectiveness of CAGE across
various scenarios, demonstrating its capability to accurately follow the
control and to generate high-quality videos that exhibit coherent scene
composition and realistic animation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at AAAI2025; Project website:
  https://araachie.github.io/cage</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re-HOLD: Video Hand Object Interaction Reenactment via adaptive
  Layout-instructed <span class="highlight-title">Diffusion</span> Model <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingying Fan, Quanwei Yang, Kaisiyuan Wang, Hang Zhou, Yingying Li, Haocheng Feng, Errui Ding, Yu Wu, Jingdong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current digital human studies focusing on lip-syncing and body movement are
no longer sufficient to meet the growing industrial demand, while human video
generation techniques that support interacting with real-world environments
(e.g., objects) have not been well investigated. Despite human hand synthesis
already being an intricate problem, generating objects in contact with hands
and their interactions presents an even more challenging task, especially when
the objects exhibit obvious variations in size and shape. To cope with these
issues, we present a novel video Reenactment framework focusing on Human-Object
Interaction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD).
Our key insight is to employ specialized layout representation for hands and
objects, respectively. Such representations enable effective disentanglement of
hand modeling and object adaptation to diverse motion sequences. To further
improve the generation quality of HOI, we have designed an interactive textural
enhancement module for both hands and objects by introducing two independent
memory banks. We also propose a layout-adjusting strategy for the cross-object
reenactment scenario to adaptively adjust unreasonable layouts caused by
diverse object sizes during inference. Comprehensive qualitative and
quantitative evaluations demonstrate that our proposed framework significantly
outperforms existing methods. Project page: https://fyycs.github.io/Re-HOLD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IncEventGS: Pose-Free Gaussian Splatting from a Single Event <span class="highlight-title">Camera</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08107v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08107v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Huang, Chengrui Dong, Peidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for
novel view synthesis have achieved remarkable progress with frame-based camera
(e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel
type of bio-inspired visual sensor, i.e. event camera, has demonstrated
advantages in high temporal resolution, high dynamic range, low power
consumption and low latency. Due to its unique asynchronous and irregular data
capturing process, limited work has been proposed to apply neural
representation or 3D Gaussian splatting for an event camera. In this work, we
present IncEventGS, an incremental 3D Gaussian Splatting reconstruction
algorithm with a single event camera. To recover the 3D scene representation
incrementally, we exploit the tracking and mapping paradigm of conventional
SLAM pipelines for IncEventGS. Given the incoming event stream, the tracker
firstly estimates an initial camera motion based on prior reconstructed 3D-GS
scene representation. The mapper then jointly refines both the 3D scene
representation and camera motion based on the previously estimated motion
trajectory from the tracker. The experimental results demonstrate that
IncEventGS delivers superior performance compared to prior NeRF-based methods
and other related baselines, even we do not have the ground-truth camera poses.
Furthermore, our method can also deliver better performance compared to
state-of-the-art event visual odometry methods in terms of camera motion
estimation. Code is publicly available at:
https://github.com/wu-cvgl/IncEventGS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Page: https://github.com/wu-cvgl/IncEventGS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CholecTrack20: A Multi-Perspective Tracking <span class="highlight-title">Dataset</span> for Surgical Tools <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinedu Innocent Nwoye, Kareem Elgohary, Anvita Srinivas, Fauzan Zaid, Joël L. Lavanchy, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool tracking in surgical videos is essential for advancing computer-assisted
interventions, such as skill assessment, safety zone estimation, and
human-machine collaboration. However, the lack of context-rich datasets limits
AI applications in this field. Existing datasets rely on overly generic
tracking formalizations that fail to capture surgical-specific dynamics, such
as tools moving out of the camera's view or exiting the body. This results in
less clinically relevant trajectories and a lack of flexibility for real-world
surgical applications. Methods trained on these datasets often struggle with
visual challenges such as smoke, reflection, and bleeding, further exposing the
limitations of current approaches. We introduce CholecTrack20, a specialized
dataset for multi-class, multi-tool tracking in surgical procedures. It
redefines tracking formalization with three perspectives: (i) intraoperative,
(ii) intracorporeal, and (iii) visibility, enabling adaptable and clinically
meaningful tool trajectories. The dataset comprises 20 full-length surgical
videos, annotated at 1 fps, yielding over 35K frames and 65K labeled tool
instances. Annotations include spatial location, category, identity, operator,
phase, and scene visual challenge. Benchmarking state-of-the-art methods on
CholecTrack20 reveals significant performance gaps, with current approaches (<
45\% HOTA) failing to meet the accuracy required for clinical translation.
These findings motivate the need for advanced and intuitive tracking algorithms
and establish CholecTrack20 as a foundation for developing robust AI-driven
surgical assistance systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Surgical tool tracking dataset paper, 11 pages, 10 figures, 3 tables,
  CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ETAP: Event-based Tracking of Any Point 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Friedhelm Hamann, Daniel Gehrig, Filbert Febryanto, Kostas Daniilidis, Guillermo Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking any point (TAP) recently shifted the motion estimation paradigm from
focusing on individual salient points with local templates to tracking
arbitrary points with global image contexts. However, while research has mostly
focused on driving the accuracy of models in nominal settings, addressing
scenarios with difficult lighting conditions and high-speed motions remains out
of reach due to the limitations of the sensor. This work addresses this
challenge with the first event camera-based TAP method. It leverages the high
temporal resolution and high dynamic range of event cameras for robust
high-speed tracking, and the global contexts in TAP methods to handle
asynchronous and sparse event measurements. We further extend the TAP framework
to handle event feature variations induced by motion -- thereby addressing an
open challenge in purely event-based tracking -- with a novel feature-alignment
loss which ensures the learning of motion-robust features. Our method is
trained with data from a new data generation pipeline and systematically
ablated across all design decisions. Our method shows strong cross-dataset
generalization and performs 136% better on the average Jaccard metric than the
baselines. Moreover, on an established feature tracking benchmark, it achieves
a 20% improvement over the previous best event-only method and even surpasses
the previous best events-and-frames method by 4.1%. Our code is available at
https://github.com/tub-rip/ETAP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 15 figures, 8 tables. Project page:
  https://github.com/tub-rip/ETAP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeSS-ST: Detecting Good and Stable Keypoints with a Neural Stability
  Score and the Shi-Tomasi Detector <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Pakulev, Alexander Vakhitov, Gonzalo Ferrer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a feature point detector presents a challenge both due to the
ambiguity of the definition of a keypoint and, correspondingly, the need for
specially prepared ground truth labels for such points. In our work, we address
both of these issues by utilizing a combination of a hand-crafted Shi-Tomasi
detector, a specially designed metric that assesses the quality of keypoints,
the stability score (SS), and a neural network. We build on the principled and
localized keypoints provided by the Shi-Tomasi detector and learn the neural
network to select good feature points via the stability score. The neural
network incorporates the knowledge from the training targets in the form of the
neural stability score (NeSS). Therefore, our method is named NeSS-ST since it
combines the Shi-Tomasi detector and the properties of the neural stability
score. It only requires sets of images for training without dataset
pre-labeling or the need for reconstructed correspondence labels. We evaluate
NeSS-ST on HPatches, ScanNet, MegaDepth and IMC-PT demonstrating
state-of-the-art performance and good generalization on downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version of ICCV 2023 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CarPlanner: Consistent Auto-regressive Trajectory Planning for
  Large-scale <span class="highlight-title">Reinforcement Learning</span> in Autonomous Driving <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19908v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19908v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongkun Zhang, Jiaming Liang, Ke Guo, Sha Lu, Qi Wang, Rong Xiong, Zhenwei Miao, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory planning is vital for autonomous driving, ensuring safe and
efficient navigation in complex environments. While recent learning-based
methods, particularly reinforcement learning (RL), have shown promise in
specific scenarios, RL planners struggle with training inefficiencies and
managing large-scale, real-world driving scenarios. In this paper, we introduce
\textbf{CarPlanner}, a \textbf{C}onsistent \textbf{a}uto-\textbf{r}egressive
\textbf{Planner} that uses RL to generate multi-modal trajectories. The
auto-regressive structure enables efficient large-scale RL training, while the
incorporation of consistency ensures stable policy learning by maintaining
coherent temporal consistency across time steps. Moreover, CarPlanner employs a
generation-selection framework with an expert-guided reward function and an
invariant-view module, simplifying RL training and enhancing policy
performance. Extensive analysis demonstrates that our proposed RL framework
effectively addresses the challenges of training efficiency and performance
enhancement, positioning CarPlanner as a promising solution for trajectory
planning in autonomous driving. To the best of our knowledge, we are the first
to demonstrate that the RL-based planner can surpass both IL- and rule-based
state-of-the-arts (SOTAs) on the challenging large-scale real-world dataset
nuPlan. Our proposed CarPlanner surpasses RL-, IL-, and rule-based SOTA
approaches within this demanding dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Monocular 3D Object Detection with Depth Thickness Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19165v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19165v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiude Zhang, Chunyu Lin, Zhijie Shen, Nie Lang, Yao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D object detection is challenging due to the lack of accurate
depth. However, existing depth-assisted solutions still exhibit inferior
performance, whose reason is universally acknowledged as the unsatisfactory
accuracy of monocular depth estimation models. In this paper, we revisit
monocular 3D object detection from the depth perspective and formulate an
additional issue as the limited 3D structure-aware capability of existing depth
representations (e.g., depth one-hot encoding or depth distribution). To
address this issue, we introduce a novel Depth Thickness Field approach to
embed clear 3D structures of the scenes. Specifically, we present MonoDTF, a
scene-to-instance depth-adapted network for monocular 3D object detection. The
framework mainly comprises a Scene-Level Depth Retargeting (SDR) module and an
Instance-Level Spatial Refinement (ISR) module. The former retargets
traditional depth representations to the proposed depth thickness field,
incorporating the scene-level perception of 3D structures. The latter refines
the voxel space with the guidance of instances, enhancing the 3D instance-aware
capability of the depth thickness field and thus improving detection accuracy.
Extensive experiments on the KITTI and Waymo datasets demonstrate our
superiority to existing state-of-the-art (SoTA) methods and the universality
when equipped with different depth estimation models. The code will be
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OnlineAnySeg: Online Zero-Shot 3D Segmentation by <span class="highlight-title">Visual</span> Foundation
  Model Guided 2D Mask Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijie Tang, Jiazhao Zhang, Yuqing Lan, Yulan Guo, Dezun Dong, Chenyang Zhu, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online 3D open-vocabulary segmentation of a progressively reconstructed scene
is both a critical and challenging task for embodied applications. With the
success of visual foundation models (VFMs) in the image domain, leveraging 2D
priors to address 3D online segmentation has become a prominent research focus.
Since segmentation results provided by 2D priors often require spatial
consistency to be lifted into final 3D segmentation, an efficient method for
identifying spatial overlap among 2D masks is essential - yet existing methods
rarely achieve this in real time, mainly limiting its use to offline
approaches. To address this, we propose an efficient method that lifts 2D masks
generated by VFMs into a unified 3D instance using a hashing technique. By
employing voxel hashing for efficient 3D scene querying, our approach reduces
the time complexity of costly spatial overlap queries from $O(n^2)$ to $O(n)$.
Accurate spatial associations further enable 3D merging of 2D masks through
simple similarity-based filtering in a zero-shot manner, making our approach
more robust to incomplete and noisy data. Evaluated on the ScanNet and SceneNN
benchmarks, our approach achieves state-of-the-art performance in online,
open-vocabulary 3D instance segmentation with leading efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoCha-Stereo: Motif Channel Attention Network for Stereo Matching <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06842v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06842v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Chen, Wei Long, He Yao, Yongjun Zhang, Bingshu Wang, Yongbin Qin, Jia Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based stereo matching techniques have made significant progress.
However, existing methods inevitably lose geometrical structure information
during the feature channel generation process, resulting in edge detail
mismatches. In this paper, the Motif Cha}nnel Attention Stereo Matching Network
(MoCha-Stereo) is designed to address this problem. We provide the Motif
Channel Correlation Volume (MCCV) to determine more accurate edge matching
costs. MCCV is achieved by projecting motif channels, which capture common
geometric structures in feature channels, onto feature maps and cost volumes.
In addition, edge variations in %potential feature channels of the
reconstruction error map also affect details matching, we propose the
Reconstruction Error Motif Penalty (REMP) module to further refine the
full-resolution disparity estimation. REMP integrates the frequency information
of typical channel features from the reconstruction error. MoCha-Stereo ranks
1st on the KITTI-2015 and KITTI-2012 Reflective leaderboards. Our structure
also shows excellent performance in Multi-View Stereo. Code is avaliable at
https://github.com/ZYangChen/MoCha-Stereo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Believing is Seeing: Unobserved Object Detection using Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05869v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05869v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhransu S. Bhattacharjee, Dylan Campbell, Rahul Shome
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can objects that are not visible in an image -- but are in the vicinity of
the camera -- be detected? This study introduces the novel tasks of 2D, 2.5D
and 3D unobserved object detection for predicting the location of nearby
objects that are occluded or lie outside the image frame. We adapt several
state-of-the-art pre-trained generative models to address this task, including
2D and 3D diffusion models and vision-language models, and show that they can
be used to infer the presence of objects that are not directly observed. To
benchmark this task, we propose a suite of metrics that capture different
aspects of performance. Our empirical evaluation on indoor scenes from the
RealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the
use of generative models for the unobserved object detection task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/CVF Computer Vision and Pattern Recognition 2025; 22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Over Text: Transforming Formula Recognition Evaluation with
  Character Detection Matching <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wang, Fan Wu, Linke Ouyang, Zhuangcheng Gu, Rui Zhang, Renqiu Xia, Bo Zhang, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formula recognition presents significant challenges due to the complicated
structure and varied notation of mathematical expressions. Despite continuous
advancements in formula recognition models, the evaluation metrics employed by
these models, such as BLEU and Edit Distance, still exhibit notable
limitations. They overlook the fact that the same formula has diverse
representations and is highly sensitive to the distribution of training data,
thereby causing unfairness in formula recognition evaluation. To this end, we
propose a Character Detection Matching (CDM) metric, ensuring the evaluation
objectivity by designing an image-level rather than a LaTeX-level metric score.
Specifically, CDM renders both the model-predicted LaTeX and the ground-truth
LaTeX formulas into image-formatted formulas, then employs visual feature
extraction and localization techniques for precise character-level matching,
incorporating spatial position information. Such a spatially-aware and
character-matching method offers a more accurate and equitable evaluation
compared with previous BLEU and Edit Distance metrics that rely solely on
text-based character matching. Experimentally, we evaluated various formula
recognition models using CDM, BLEU, and ExpRate metrics. Their results
demonstrate that the CDM aligns more closely with human evaluation standards
and provides a fairer comparison across different models by eliminating
discrepancies caused by diverse formula representations. Code is available at
https://github.com/opendatalab/UniMERNet/tree/main/cdm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Polycuboid Fitting for Compact 3D Representation of Indoor Scenes <span class="chip">3DV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gahye Lee, Hyejeong Yoon, Jungeon Kim, Seungyong Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel framework for compactly representing a 3D indoor
scene using a set of polycuboids through a deep learning-based fitting method.
Indoor scenes mainly consist of man-made objects, such as furniture, which
often exhibit rectilinear geometry. This property allows indoor scenes to be
represented using combinations of polycuboids, providing a compact
representation that benefits downstream applications like furniture
rearrangement. Our framework takes a noisy point cloud as input and first
detects six types of cuboid faces using a transformer network. Then, a graph
neural network is used to validate the spatial relationships of the detected
faces to form potential polycuboids. Finally, each polycuboid instance is
reconstructed by forming a set of boxes based on the aggregated face labels. To
train our networks, we introduce a synthetic dataset encompassing a diverse
range of cuboid and polycuboid shapes that reflect the characteristics of
indoor scenes. Our framework generalizes well to real-world indoor scene
datasets, including Replica, ScanNet, and scenes captured with an iPhone. The
versatility of our method is demonstrated through practical applications, such
as virtual room tours and scene editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 3DV 2025. For project page, see this
  https://waldstein94.github.io/deep-polycuboid-fitting/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Physics From Video: Unsupervised Physical Parameter Estimation
  for Continuous Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Castañeda Garcia, Jan van Gemert, Daan Brinks, Nergis Tömen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting physical dynamical system parameters from recorded observations is
key in natural science. Current methods for automatic parameter estimation from
video train supervised deep networks on large datasets. Such datasets require
labels, which are difficult to acquire. While some unsupervised
techniques--which depend on frame prediction--exist, they suffer from long
training times, initialization instabilities, only consider motion-based
dynamical systems, and are evaluated mainly on synthetic data. In this work, we
propose an unsupervised method to estimate the physical parameters of known,
continuous governing equations from single videos suitable for different
dynamical systems beyond motion and robust to initialization. Moreover, we
remove the need for frame prediction by implementing a KL-divergence-based loss
function in the latent space, which avoids convergence to trivial solutions and
reduces model size and compute. We first evaluate our model on synthetic data,
as commonly done. After which, we take the field closer to reality by recording
Delfys75: our own real-world dataset of 75 videos for five different types of
dynamical systems to evaluate our method and others. Our method compares
favorably to others. %, yet, and real-world video datasets and demonstrate
improved parameter estimation accuracy compared to existing methods. Code and
data are available
online:https://github.com/Alejandro-neuro/Learning_physics_from_video.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learnable Infinite Taylor Gaussian for Dynamic View Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingbing Hu, Yanyan Li, Rui Xie, Bo Xu, Haoye Dong, Junfeng Yao, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing the temporal evolution of Gaussian properties such as position,
rotation, and scale is a challenging task due to the vast number of
time-varying parameters and the limited photometric data available, which
generally results in convergence issues, making it difficult to find an optimal
solution. While feeding all inputs into an end-to-end neural network can
effectively model complex temporal dynamics, this approach lacks explicit
supervision and struggles to generate high-quality transformation fields. On
the other hand, using time-conditioned polynomial functions to model Gaussian
trajectories and orientations provides a more explicit and interpretable
solution, but requires significant handcrafted effort and lacks
generalizability across diverse scenes. To overcome these limitations, this
paper introduces a novel approach based on a learnable infinite Taylor Formula
to model the temporal evolution of Gaussians. This method offers both the
flexibility of an implicit network-based approach and the interpretability of
explicit polynomial functions, allowing for more robust and generalizable
modeling of Gaussian dynamics across various dynamic scenes. Extensive
experiments on dynamic novel view rendering tasks are conducted on public
datasets, demonstrating that the proposed method achieves state-of-the-art
performance in this domain. More information is available on our project
page(https://ellisonking.github.io/TaylorGaussian).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAMKD: Spatial-aware Adaptive Masking Knowledge Distillation for Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhourui Zhang, Jun Li, Jiayan Li, Jianhua Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of recent attention-guided feature masking distillation methods perform
knowledge transfer via global teacher attention maps without delving into
fine-grained clues. Instead, performing distillation at finer granularity is
conducive to uncovering local details supplementary to global knowledge
transfer and reconstructing comprehensive student features. In this study, we
propose a Spatial-aware Adaptive Masking Knowledge Distillation (SAMKD)
framework for accurate object detection. Different from previous feature
distillation methods which mainly perform single-scale feature masking, we
develop spatially hierarchical feature masking distillation scheme, such that
the object-aware locality is encoded during coarse-to-fine distillation process
for improved feature reconstruction. In addition, our spatial-aware feature
distillation strategy is combined with a masking logit distillation scheme in
which region-specific feature difference between teacher and student networks
is utilized to adaptively guide the distillation process. Thus, it can help the
student model to better learn from the teacher counterpart with improved
knowledge transfer and reduced gap. Extensive experiments for detection task
demonstrate the superiority of our method. For example, when FCOS is used as
teacher detector with ResNet101 backbone, our method improves the student
network from 35.3\% to 38.8\% mAP, outperforming state-of-the-art distillation
methods including MGD, FreeKD and DMKD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GaussTR: Foundation Model-Aligned Gaussian Transformer for
  Self-Supervised 3D Spatial Understanding <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyi Jiang, Liu Liu, Tianheng Cheng, Xinjie Wang, Tianwei Lin, Zhizhong Su, Wenyu Liu, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Semantic Occupancy Prediction is fundamental for spatial understanding,
yet existing approaches face challenges in scalability and generalization due
to their reliance on extensive labeled data and computationally intensive
voxel-wise representations. In this paper, we introduce GaussTR, a novel
Gaussian-based Transformer framework that unifies sparse 3D modeling with
foundation model alignment through Gaussian representations to advance 3D
spatial understanding. GaussTR predicts sparse sets of Gaussians in a
feed-forward manner to represent 3D scenes. By splatting the Gaussians into 2D
views and aligning the rendered features with foundation models, GaussTR
facilitates self-supervised 3D representation learning and enables
open-vocabulary semantic occupancy prediction without requiring explicit
annotations. Empirical experiments on the Occ3D-nuScenes dataset demonstrate
GaussTR's state-of-the-art zero-shot performance of 12.27 mIoU, along with a
40% reduction in training time. These results highlight the efficacy of GaussTR
for scalable and holistic 3D spatial understanding, with promising implications
in autonomous driving and embodied agents. The code is available at
https://github.com/hustvl/GaussTR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalization Toolkit: Training Free Personalization of Large <span class="highlight-title">Vision</span>
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Seifi, Vaggelis Dorovatas, Daniel Olmeda Reino, Rahaf Aljundi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision Language Models (LVLMs) have significant potential to provide
personalized assistance by adapting to the unique needs and preferences of
individual users. The personalization of LVLMs has emerged as a field that
focuses on customizing models to recognize specific object instances and
provide tailored responses. However, current methodologies depend on
time-consuming test-time training for each user and object, which proves to be
impractical. This paper introduces a novel, training-free approach to LVLM
personalization by leveraging pre-trained vision foundation models to extract
distinct features, retrieval-augmented generation (RAG) techniques to recognize
instances in the visual input, and visual prompting methods. Our model-agnostic
vision toolkit enables flexible and efficient personalization without the need
for extensive retraining. We demonstrate state-of-the-art results, surpassing
conventional training-based approaches, and set a new benchmark for LVLM
personalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BI-RADS prediction of mammographic masses using uncertainty information
  extracted from a Bayesian Deep Learning model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohaddeseh Chegini, Ali Mahloojifar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The BI_RADS score is a probabilistic reporting tool used by radiologists to
express the level of uncertainty in predicting breast cancer based on some
morphological features in mammography images. There is a significant
variability in describing masses which sometimes leads to BI_RADS
misclassification. Using a BI_RADS prediction system is required to support the
final radiologist decisions. In this study, the uncertainty information
extracted by a Bayesian deep learning model is utilized to predict the BI_RADS
score. The investigation results based on the pathology information demonstrate
that the f1-scores of the predictions of the radiologist are 42.86%, 48.33% and
48.28%, meanwhile, the f1-scores of the model performance are 73.33%, 59.60%
and 59.26% in the BI_RADS 2, 3 and 5 dataset samples, respectively. Also, the
model can distinguish malignant from benign samples in the BI_RADS 0 category
of the used dataset with an accuracy of 75.86% and correctly identify all
malignant samples as BI_RADS 5. The Grad-CAM visualization shows the model pays
attention to the morphological features of the lesions. Therefore, this study
shows the uncertainty-aware Bayesian Deep Learning model can report his
uncertainty about the malignancy of a lesion based on morphological features,
like a radiologist.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Objects to Events: Unlocking Complex <span class="highlight-title">Visual</span> Understanding in Object
  Detectors via LLM-guided Symbolic Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05843v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05843v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Zeng, Haoxiang Wu, Wenjie Nie, Xiawu Zheng, Guangyao Chen, Yunhang Shen, Jun Peng, Yonghong Tian, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our key innovation lies in bridging the semantic gap between object detection
and event understanding without requiring expensive task-specific training. The
proposed plug-and-play framework interfaces with any open-vocabulary detector
while extending their inherent capabilities across architectures. At its core,
our approach combines (i) a symbolic regression mechanism exploring
relationship patterns among detected entities and (ii) a LLM-guided
strategically guiding the search toward meaningful expressions. These
discovered symbolic rules transform low-level visual perception into
interpretable event understanding, providing a transparent reasoning path from
objects to events with strong transferability across domains.We compared our
training-free framework against specialized event recognition systems across
diverse application domains. Experiments demonstrate that our framework
enhances multiple object detector architectures to recognize complex events
such as illegal fishing activities (75% AUROC, +8.36% improvement),
construction safety violations (+15.77%), and abnormal crowd behaviors
(+23.16%). The code will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Vision</span> Centric Remote Sensing Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abduljaleel Adejumo, Faegheh Yeganli, Clifford Broni-bediako, Aoran Xiao, Naoto Yokoya, Mennatullah Siam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have achieved remarkable success in
vision-language tasks but their remote sensing (RS) counterpart are relatively
under explored. Unlike natural images, RS imagery presents unique challenges
that current MLLMs struggle to handle, particularly in visual grounding and
spatial reasoning. This study investigates the limitations of CLIP-based MLLMs
in RS, highlighting their failure to differentiate visually distinct yet
semantically similar RS images. To address this, we introduce a remote sensing
multimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs
in RS tasks by identifying the CLIP-blind pairs, where CLIP-based models
incorrectly assign high similarity scores to visually distinct RS images.
Through a visual question answering (VQA) evaluation, we analyze the
performance of state-of-the-art MLLMs, revealing significant limitations in RS
specific representation learning. The results provide valuable insights into
the weaknesses of CLIP-based visual encoding and offer a foundation for future
research to develop more effective MLLMs tailored for remote sensing
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Thing, Nothing: Highlighting Safety-Critical Classes for Robust <span class="highlight-title">LiDAR</span>
  Semantic Segmentation in Adverse Weather <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsung Park, Hwijeong Lee, Inha Kang, Hyunjung Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing domain generalization methods for LiDAR semantic segmentation under
adverse weather struggle to accurately predict "things" categories compared to
"stuff" categories. In typical driving scenes, "things" categories can be
dynamic and associated with higher collision risks, making them crucial for
safe navigation and planning. Recognizing the importance of "things"
categories, we identify their performance drop as a serious bottleneck in
existing approaches. We observed that adverse weather induces degradation of
semantic-level features and both corruption of local features, leading to a
misprediction of "things" as "stuff". To mitigate these corruptions, we suggest
our method, NTN - segmeNt Things for No-accident. To address semantic-level
feature corruption, we bind each point feature to its superclass, preventing
the misprediction of things classes into visually dissimilar categories.
Additionally, to enhance robustness against local corruption caused by adverse
weather, we define each LiDAR beam as a local region and propose a
regularization term that aligns the clean data with its corrupted counterpart
in feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU
gain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the
SemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9
mIoU improvement on "things" classes, respectively, highlighting its
effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, accepted in CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention Overlap Is Responsible for The Entity Missing Problem in
  Text-to-image <span class="highlight-title">Diffusion</span> Models! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arash Marioriyad, Mohammadali Banayeeanzade, Reza Abbasi, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models, such as Stable Diffusion and DALL-E, are
capable of generating high-quality, diverse, and realistic images from textual
prompts. However, they sometimes struggle to accurately depict specific
entities described in prompts, a limitation known as the entity missing problem
in compositional generation. While prior studies suggested that adjusting
cross-attention maps during the denoising process could alleviate this problem,
they did not systematically investigate which objective functions could best
address it. This study examines three potential causes of the entity-missing
problem, focusing on cross-attention dynamics: (1) insufficient attention
intensity for certain entities, (2) overly broad attention spread, and (3)
excessive overlap between attention maps of different entities. We found that
reducing overlap in attention maps between entities can effectively minimize
the rate of entity missing. Specifically, we hypothesize that tokens related to
specific entities compete for attention on certain image regions during the
denoising process, which can lead to divided attention across tokens and
prevent accurate representation of each entity. To address this issue, we
introduced four loss functions, Intersection over Union (IoU), center-of-mass
(CoM) distance, Kullback-Leibler (KL) divergence, and clustering compactness
(CC) to regulate attention overlap during denoising steps without the need for
retraining. Experimental results across a wide variety of benchmarks reveal
that these proposed training-free methods significantly improve compositional
accuracy, outperforming previous approaches in visual question answering (VQA),
captioning scores, CLIP similarity, and human evaluations. Notably, these
methods improved human evaluation scores by 9% over the best baseline,
demonstrating substantial improvements in compositional alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR - 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superpixel Tokenization for <span class="highlight-title">Vision</span> Transformers: Preserving Semantic
  Integrity in <span class="highlight-title">Visual</span> Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04680v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04680v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaihyun Lew, Soohyuk Jang, Jaehoon Lee, Seungryong Yoo, Eunji Kim, Saehyung Lee, Jisoo Mok, Siwon Kim, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers, a groundbreaking architecture proposed for Natural Language
Processing (NLP), have also achieved remarkable success in Computer Vision. A
cornerstone of their success lies in the attention mechanism, which models
relationships among tokens. While the tokenization process in NLP inherently
ensures that a single token does not contain multiple semantics, the
tokenization of Vision Transformer (ViT) utilizes tokens from uniformly
partitioned square image patches, which may result in an arbitrary mixing of
visual concepts in a token. In this work, we propose to substitute the
grid-based tokenization in ViT with superpixel tokenization, which employs
superpixels to generate a token that encapsulates a sole visual concept.
Unfortunately, the diverse shapes, sizes, and locations of superpixels make
integrating superpixels into ViT tokenization rather challenging. Our
tokenization pipeline, comprised of pre-aggregate extraction and
superpixel-aware aggregation, overcomes the challenges that arise in superpixel
tokenization. Extensive experiments demonstrate that our approach, which
exhibits strong compatibility with existing frameworks, enhances the accuracy
and robustness of ViT on various downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> GUI-World: A Video Benchmark and <span class="highlight-title">Dataset</span> for Multimodal GUI-oriented
  Understanding <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, Tianshuo Zhou, Yue Yu, Chujie Gao, Qihui Zhang, Yi Gui, Zhen Li, Yao Wan, Pan Zhou, Jianfeng Gao, Lic<span class="highlight-author">hao Su</span>n
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Multimodal Large Language Models (MLLMs) have been used as agents
to control keyboard and mouse inputs by directly perceiving the Graphical User
Interface (GUI) and generating corresponding commands. However, current agents
primarily demonstrate strong understanding capabilities in static environments
and are mainly applied to relatively simple domains, such as Web or mobile
interfaces. We argue that a robust GUI agent should be capable of perceiving
temporal information on the GUI, including dynamic Web content and multi-step
tasks. Additionally, it should possess a comprehensive understanding of various
GUI scenarios, including desktop software and multi-window interactions. To
this end, this paper introduces a new dataset, termed GUI-World, which features
meticulously crafted Human-MLLM annotations, extensively covering six GUI
scenarios and eight types of GUI-oriented questions in three formats. We
evaluate the capabilities of current state-of-the-art MLLMs, including Image
LLMs and Video LLMs, in understanding various types of GUI content, especially
dynamic and sequential content. Our findings reveal that current models
struggle with dynamic GUI content without manually annotated keyframes or
operation history. On the other hand, Video LLMs fall short in all GUI-oriented
tasks given the sparse GUI video dataset. Therefore, we take the initial step
of leveraging a fine-tuned Video LLM, GUI-Vid, as a GUI-oriented assistant,
demonstrating an improved understanding of various GUI tasks. However, due to
the limitations in the performance of base LLMs, we conclude that using video
LLMs as GUI agents remains a significant challenge. We believe our work
provides valuable insights for future research in dynamic GUI content
understanding. All the dataset and code are publicly available at:
https://gui-world.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park, Eunbyung Park, Sangpil Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D
reconstruction and novel view synthesis, leading to its widespread commercial
use. Consequently, copyright protection via watermarking has become critical.
However, because 3DGS relies on millions of Gaussians, which require gigabytes
of storage, efficient transfer and storage require compression. Existing 3DGS
watermarking methods are vulnerable to quantization-based compression, often
resulting in the loss of the embedded watermark. To address this challenge, we
propose a novel watermarking method that ensures watermark robustness after
model compression while maintaining high rendering quality. In detail, we
incorporate a quantization distortion layer that simulates compression during
training, preserving the watermark under quantization-based compression. Also,
we propose a learnable watermark embedding feature that embeds the watermark
into the anchor feature, ensuring structural consistency and seamless
integration into the 3D scene. Furthermore, we present a frequency-aware anchor
growing mechanism to enhance image quality in high-frequency regions by
effectively identifying Guassians within these regions. Experimental results
confirm that our method preserves the watermark and maintains superior image
quality under high compression, validating it as a promising approach for a
secure 3DGS model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grounded Chain-of-Thought for Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Wu, Xiangcong Yang, Yiyi Zhou, Chenxin Fang, Baiyang Song, Xiaoshuai Sun, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite great progress, existing multimodal large language models (MLLMs) are
prone to visual hallucination, greatly impeding their trustworthy applications.
In this paper, we study this problem from the perspective of visual-spatial
reasoning, and propose a new learning task for MLLMs, termed Grounded
Chain-of-Thought (GCoT). Different from recent visual CoT studies, which focus
more on visual knowledge reasoning, GCoT is keen to helping MLLMs to recognize
and ground the relevant visual cues step by step, thereby predicting the
correct answer with grounding coordinates as the intuitive basis. To facilitate
this task, we also carefully design and construct a dataset called multimodal
grounded chain-of-thought (MM-GCoT) consisting of 24,022 GCoT examples for
5,033 images. Besides, a comprehensive consistency evaluation system is also
introduced, including the metrics of answer accuracy, grounding accuracy and
answer-grounding consistency. We further design and conduct a bunch of
experiments on 12 advanced MLLMs, and reveal some notable findings: i. most
MLLMs performs poorly on the consistency evaluation, indicating obvious visual
hallucination; ii. visual hallucination is not directly related to the
parameter size and general multimodal performance, i.e., a larger and stronger
MLLM is not less affected by this issue. Lastly, we also demonstrate that the
proposed dataset can help existing MLLMs to well cultivate their GCoT
capability and reduce the inconsistent answering significantly. Moreover, their
GCoT can be also generalized to exiting multimodal tasks, such as open-world QA
and REC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLARE: Feed-forward Geometry, Appearance and <span class="highlight-title">Camera</span> Estimation from
  Uncalibrated Sparse Views <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12138v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12138v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, Gordon Wetzstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FLARE, a feed-forward model designed to infer high-quality camera
poses and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8
inputs), which is a challenging yet practical setting in real-world
applications. Our solution features a cascaded learning paradigm with camera
pose serving as the critical bridge, recognizing its essential role in mapping
3D structures onto 2D image planes. Concretely, FLARE starts with camera pose
estimation, whose results condition the subsequent learning of geometric
structure and appearance, optimized through the objectives of geometry
reconstruction and novel-view synthesis. Utilizing large-scale public datasets
for training, our method delivers state-of-the-art performance in the tasks of
pose estimation, geometry reconstruction, and novel view synthesis, while
maintaining the inference efficiency (i.e., less than 0.5 seconds). The project
page and code can be found at: https://zhanghe3z.github.io/FLARE/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Website: https://zhanghe3z.github.io/FLARE/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-driven Image Fusion with Learnable Fusion Loss <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowen Bai, Jiangshe Zhang, Zixiang Zhao, Yichen Wu, Lilun Deng, Yukun Cui, Tao Feng, Shuang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal image fusion aggregates information from multiple sensor sources,
achieving superior visual quality and perceptual features compared to
single-source images, often improving downstream tasks. However, current fusion
methods for downstream tasks still use predefined fusion objectives that
potentially mismatch the downstream tasks, limiting adaptive guidance and
reducing model flexibility. To address this, we propose Task-driven Image
Fusion (TDFusion), a fusion framework incorporating a learnable fusion loss
guided by task loss. Specifically, our fusion loss includes learnable
parameters modeled by a neural network called the loss generation module. This
module is supervised by the downstream task loss in a meta-learning manner. The
learning objective is to minimize the task loss of fused images after
optimizing the fusion module with the fusion loss. Iterative updates between
the fusion module and the loss module ensure that the fusion network evolves
toward minimizing task loss, guiding the fusion process toward the task
objectives. TDFusion's training relies entirely on the downstream task loss,
making it adaptable to any specific task. It can be applied to any architecture
of fusion and task networks. Experiments demonstrate TDFusion's performance
through fusion experiments conducted on four different datasets, in addition to
evaluations on semantic segmentation and object detection tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ComicsPAP: understanding comic strips by picking the correct panel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Vivoli, Artemis Llabrés, Mohamed Ali Souibgui, Marco Bertini, Ernest Valveny Llobet, Dimosthenis Karatzas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) have made impressive strides in image
captioning, VQA, and video comprehension, yet they still struggle with the
intricate temporal and spatial cues found in comics. To address this gap, we
introduce ComicsPAP, a large-scale benchmark designed for comic strip
understanding. Comprising over 100k samples and organized into 5 subtasks under
a Pick-a-Panel framework, ComicsPAP demands models to identify the missing
panel in a sequence. Our evaluations, conducted under both multi-image and
single-image protocols, reveal that current state-of-the-art LMMs perform near
chance on these tasks, underscoring significant limitations in capturing
sequential and contextual dependencies. To close the gap, we adapted LMMs for
comic strip understanding, obtaining better results on ComicsPAP than 10x
bigger models, demonstrating that ComicsPAP offers a robust resource to drive
future research in multimodal comic comprehension.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Low-Rank Scaled Dot-product Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03214v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03214v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ginés Carreto Picón, Illia Oleksiienko, Lukas Hedegaard, Arian Bakhtiarnia, Alexandros Iosifidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers are widely used for their ability to capture data relations in
sequence processing, with great success for a wide range of static tasks.
However, the computational and memory footprint of their main component, i.e.,
the Scaled Dot-product Attention, is commonly overlooked. This makes their
adoption in applications involving stream data processing with constraints in
response latency, computational and memory resources infeasible. Some works
have proposed methods to lower the computational cost of Transformers, i.e.
low-rank approximations, sparsity in attention, and efficient formulations for
Continual Inference. In this paper, we introduce a new formulation of the
Scaled Dot-product Attention based on the Nystr\"om approximation that is
suitable for Continual Inference. In experiments on Online Audio Classification
and Online Action Detection tasks, the proposed Continual Scaled Dot-product
Attention can lower the number of operations by up to three orders of magnitude
compared to the original Transformers while retaining the predictive
performance of competing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fractal Calibration for long-tailed object detection <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Panagiotis Alexandridis, Ismail Elezi, Jiankang Deng, Anh Nguyen, Shan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world datasets follow an imbalanced distribution, which poses
significant challenges in rare-category object detection. Recent studies tackle
this problem by developing re-weighting and re-sampling methods, that utilise
the class frequencies of the dataset. However, these techniques focus solely on
the frequency statistics and ignore the distribution of the classes in image
space, missing important information. In contrast to them, we propose FRActal
CALibration (FRACAL): a novel post-calibration method for long-tailed object
detection. FRACAL devises a logit adjustment method that utilises the fractal
dimension to estimate how uniformly classes are distributed in image space.
During inference, it uses the fractal dimension to inversely downweight the
probabilities of uniformly spaced class predictions achieving balance in two
axes: between frequent and rare categories, and between uniformly spaced and
sparsely spaced classes. FRACAL is a post-processing method and it does not
require any training, also it can be combined with many off-the-shelf models
such as one-stage sigmoid detectors and two-stage instance segmentation models.
FRACAL boosts the rare class performance by up to 8.6% and surpasses all
previous methods on LVIS dataset, while showing good generalisation to other
datasets such as COCO, V3Det and OpenImages. We provide the code at
https://github.com/kostas1515/FRACAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2025 (camera-ready)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constraint-Aware Feature Learning for Parametric Point Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07747v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07747v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Cheng, Ruiqi Lei, Di Huang, Zhichao Liao, Fengyuan Piao, Yan Chen, Pingfa Feng, Long Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parametric point clouds are sampled from CAD shapes and are becoming
increasingly common in industrial manufacturing. Most existing CAD-specific
deep learning methods only focus on geometric features, while overlooking
constraints which are inherent and important in CAD shapes. This limits their
ability to discern CAD shapes with similar appearance but different
constraints. To tackle this challenge, we first analyze the constraint
importance via a simple validation experiment. Then, we introduce a deep
learning-friendly constraints representation with three vectorized components,
and design a constraint-aware feature learning network (CstNet), which includes
two stages. Stage 1 extracts constraint feature from B-Rep data or point cloud
based on shape local information. It enables better generalization ability to
unseen dataset after model pre-training. Stage 2 employs attention layers to
adaptively adjust the weights of three constraints' components. It facilitates
the effective utilization of constraints. In addition, we built the first
multi-modal parametric-purpose dataset, i.e. Param20K, comprising about 20K
shape instances of 75 classes. On this dataset, we performed the classification
and rotation robustness experiments, and CstNet achieved 3.52\% and 26.17\%
absolute improvements in instance accuracy over the state-of-the-art methods,
respectively. To the best of our knowledge, CstNet is the first
constraint-aware deep learning method tailored for parametric point cloud
analysis in CAD domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ APLA: A Simple Adaptation Method for <span class="highlight-title">Vision</span> Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moein Sorkhei, Emir Konuk, Kevin Smith, Christos Matsoukas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing adaptation techniques typically require architectural modifications
or added parameters, leading to high computational costs and complexity. We
introduce Attention Projection Layer Adaptation (APLA), a simple approach to
adapt vision transformers (ViTs) without altering the architecture or adding
parameters. Through a systematic analysis, we find that the layer immediately
after the attention mechanism is crucial for adaptation. By updating only this
projection layer, or even just a random subset of this layer's weights, APLA
achieves state-of-the-art performance while reducing GPU memory usage by up to
52.63% and training time by up to 43.0%, with no extra cost at inference.
Across 46 datasets covering a variety of tasks including scene classification,
medical imaging, satellite imaging, and fine-grained classification, APLA
consistently outperforms 17 other leading adaptation methods, including full
fine-tuning, on classification, segmentation, and detection tasks. The code is
available at https://github.com/MoeinSorkhei/APLA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auto Cherry-Picker: Learning from High-quality Generative Data Driven by
  Language <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Chen, Xiangtai Li, Yining Li, Yanhong Zeng, Jianzong Wu, Xiangyu Zhao, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models can generate realistic and diverse images, potentially
facilitating data availability for data-intensive perception tasks. However,
leveraging these models to boost performance on downstream tasks with synthetic
data poses several challenges, including aligning with real data distribution,
scaling synthetic sample volumes, and ensuring their quality. To bridge these
gaps, we present \textbf{A}uto \textbf{C}herry-\textbf{P}icker (ACP), a novel
framework that generates high-quality cross-modality training samples at scale
to augment perception and multi-modal training. ACP first uses LLMs to sample
descriptions and layouts based on object combinations from real data priors,
eliminating the need for ground truth image captions or annotations. Next, we
use an off-the-shelf controllable diffusion model to generate multiple images.
Then, the generated data are refined using a comprehensively designed metric,
Composite Layout and Image Score (CLIS), to ensure quality. Our customized
synthetic high-quality samples boost performance in various scenarios,
especially in addressing challenges associated with long-tailed distribution
and imbalanced datasets. Experiment results on downstream tasks demonstrate
that ACP can significantly improve the performance of existing models. In
addition, we find a positive correlation between CLIS and performance gains in
downstream tasks. This finding shows the potential for evaluation metrics as
the role for various visual perception and MLLM tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forensics Adapter: Adapting CLIP for Generalizable Face Forgery
  Detection <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Cui, Yuezun Li, Ao Luo, Jiaran Zhou, Junyu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe the Forensics Adapter, an adapter network designed to transform
CLIP into an effective and generalizable face forgery detector. Although CLIP
is highly versatile, adapting it for face forgery detection is non-trivial as
forgery-related knowledge is entangled with a wide range of unrelated
knowledge. Existing methods treat CLIP merely as a feature extractor, lacking
task-specific adaptation, which limits their effectiveness. To address this, we
introduce an adapter to learn face forgery traces -- the blending boundaries
unique to forged faces, guided by task-specific objectives. Then we enhance the
CLIP visual tokens with a dedicated interaction strategy that communicates
knowledge across CLIP and the adapter. Since the adapter is alongside CLIP, its
versatility is highly retained, naturally ensuring strong generalizability in
face forgery detection. With only 5.7M trainable parameters, our method
achieves a significant performance boost, improving by approximately 7% on
average across five standard datasets. We believe the proposed method can serve
as a baseline for future CLIP-based face forgery detection methods. The code is
available at https://github.com/OUC-VAS/ForensicsAdapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-23T00:00:00Z">2025-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">24</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Robo</span>t-Led Intervention for Emotion Regulation: From Expression to
  Reappraisal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Laban, Julie Wang, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion regulation is a crucial skill for managing emotions in everyday life,
yet finding a constructive and accessible method to support these processes
remains challenging due to their cognitive demands. In this study, we explore
how regular interactions with a social robot, conducted in a structured yet
familiar environment within university halls and departments, can provide
effective support for emotion regulation through cognitive reappraisal.
Twenty-one students participated in a five-session study at a university hall
or department, where the robot facilitated structured conversations,
encouraging the students to reinterpret emotionally charged situations that
they shared with the robot. Quantitative and qualitative results indicate
significant improvements in emotion self-regulation, with participants
reporting better understanding and control of their emotions. The intervention
led to significant changes in constructive emotion regulation tendencies and
positive effects on mood and sentiment after each session. The findings also
demonstrate that repeated interactions with the robot encouraged greater
emotional expressiveness, including longer speech disclosures, increased use of
affective language, and heightened facial arousal. Notably, expressiveness
followed structured patterns aligned with the reappraisal process, with
expression peaking during key reappraisal moments, particularly when
participants were prompted to reinterpret negative experiences. The qualitative
feedback further highlighted how the robot fostered introspection and provided
a supportive space for discussing emotions, enabling participants to confront
long-avoided emotional challenges. These findings demonstrate the potential of
robots to effectively assist in emotion regulation in familiar environments,
offering both emotional support and cognitive guidance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Navigation of a Cable-Towed Load using Quadrupedal <span class="highlight-title">Robo</span>t
  Team via MARL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Tse Chen, Minh Nguyen, Zhongyu Li, Guo Ning Sue, Koushil Sreenath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the challenge of enabling a team of quadrupedal robots to
collaboratively tow a cable-connected load through cluttered and unstructured
environments while avoiding obstacles. Leveraging cables allows the multi-robot
system to navigate narrow spaces by maintaining slack when necessary. However,
this introduces hybrid physical interactions due to alternating taut and slack
states, with computational complexity that scales exponentially as the number
of agents increases. To tackle these challenges, we developed a scalable and
decentralized system capable of dynamically coordinating a variable number of
quadrupedal robots while managing the hybrid physical interactions inherent in
the load-towing task. At the core of this system is a novel multi-agent
reinforcement learning (MARL)-based planner, designed for decentralized
coordination. The MARL-based planner is trained using a centralized training
with decentralized execution (CTDE) framework, enabling each robot to make
decisions autonomously using only local (ego) observations. To accelerate
learning and ensure effective collaboration across varying team sizes, we
introduce a tailored training curriculum for MARL. Experimental results
highlight the flexibility and scalability of the framework, demonstrating
successful deployment with one to four robots in real-world scenarios and up to
twelve robots in simulation. The decentralized planner maintains consistent
inference times, regardless of the team size. Additionally, the proposed system
demonstrates robustness to environment perturbations and adaptability to
varying load weights. This work represents a step forward in achieving flexible
and efficient multi-legged robotic collaboration in complex and real-world
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extended Visibility of Autonomous Vehicles via Optimized Cooperative
  Perception under Imperfect Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Sarlak, Rahul Amin, Abolfazl Razi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Vehicles (AVs) rely on individual perception systems to navigate
safely. However, these systems face significant challenges in adverse weather
conditions, complex road geometries, and dense traffic scenarios. Cooperative
Perception (CP) has emerged as a promising approach to extending the perception
quality of AVs by jointly processing shared camera feeds and sensor readings
across multiple vehicles. This work presents a novel CP framework designed to
optimize vehicle selection and networking resource utilization under imperfect
communications. Our optimized CP formation considers critical factors such as
the helper vehicles' spatial position, visual range, motion blur, and available
communication budgets. Furthermore, our resource optimization module allocates
communication channels while adjusting power levels to maximize data flow
efficiency between the ego and helper vehicles, considering realistic models of
modern vehicular communication systems, such as LTE and 5G NR-V2X. We validate
our approach through extensive experiments on pedestrian detection in
challenging scenarios, using synthetic data generated by the CARLA simulator.
The results demonstrate that our method significantly improves upon the
perception quality of individual AVs with about 10% gain in detection accuracy.
This substantial gain uncovers the unleashed potential of CP to enhance AV
safety and performance in complex situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 13 figures, 3 tables, Elsevier Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint State-Parameter Observer-Based Robust Control of a UAV for Heavy
  Load Transportation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brenner S. Rego, Daniel N. Cardoso, Marco. H. Terra, Guilherme V. Raffo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a joint state-parameter observer-based controller for
trajectory tracking of an octocopter unmanned aerial vehicle (OUAV), for
transportation of a heavy load with unknown mass and size. The multi-body
dynamic model of the OUAV with a rigidly attached load is obtained, effectively
considering the effects of the load parameters into the dynamics of the system.
A robust nonlinear W-infinity control strategy is designed for optimal
trajectory tracking of the OUAV, with information of the states and load
parameters provided by a joint estimation unscented Kalman filter. The
effectiveness of the proposed strategy is corroborated by numerical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures. This is a preprint of a paper presented at the
  2023 Conference on Climbing and Walking Robots (CLAWAR) and published later
  by Springer Nature Switzerland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unraveling the Effects of Synthetic Data on End-to-End Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Ge, Zuhong Liu, Longteng Fan, Yifan Jiang, Jiaqi Su, Yiming Li, Zhejun Zhang, Siheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end (E2E) autonomous driving (AD) models require diverse, high-quality
data to perform well across various driving scenarios. However, collecting
large-scale real-world data is expensive and time-consuming, making
high-fidelity synthetic data essential for enhancing data diversity and model
robustness. Existing driving simulators for synthetic data generation have
significant limitations: game-engine-based simulators struggle to produce
realistic sensor data, while NeRF-based and diffusion-based methods face
efficiency challenges. Additionally, recent simulators designed for closed-loop
evaluation provide limited interaction with other vehicles, failing to simulate
complex real-world traffic dynamics. To address these issues, we introduce
SceneCrafter, a realistic, interactive, and efficient AD simulator based on 3D
Gaussian Splatting (3DGS). SceneCrafter not only efficiently generates
realistic driving logs across diverse traffic scenarios but also enables robust
closed-loop evaluation of end-to-end models. Experimental results demonstrate
that SceneCrafter serves as both a reliable evaluation platform and a efficient
data generator that significantly improves end-to-end model generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PanopticSplatting: End-to-End Panoptic Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Xie, Xuan Yu, Changjian Jiang, Sitong Mao, Shunbo Zhou, Rui Fan, Rong Xiong, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary panoptic reconstruction is a challenging task for
simultaneous scene reconstruction and understanding. Recently, methods have
been proposed for 3D scene understanding based on Gaussian splatting. However,
these methods are multi-staged, suffering from the accumulated errors and the
dependence of hand-designed components. To streamline the pipeline and achieve
global optimization, we propose PanopticSplatting, an end-to-end system for
open-vocabulary panoptic reconstruction. Our method introduces query-guided
Gaussian segmentation with local cross attention, lifting 2D instance masks
without cross-frame association in an end-to-end way. The local cross attention
within view frustum effectively reduces the training memory, making our model
more accessible to large scenes with more Gaussians and objects. In addition,
to address the challenge of noisy labels in 2D pseudo masks, we propose label
blending to promote consistent 3D segmentation with less noisy floaters, as
well as label warping on 2D predictions which enhances multi-view coherence and
segmentation accuracy. Our method demonstrates strong performances in 3D scene
panoptic reconstruction on the ScanNet-V2 and ScanNet++ datasets, compared with
both NeRF-based and Gaussian-based panoptic reconstruction methods. Moreover,
PanopticSplatting can be easily generalized to numerous variants of Gaussian
splatting, and we demonstrate its robustness on different Gaussian base models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unseen from Seen: Rewriting Observation-Instruction Using Foundation
  Models for Augmenting <span class="highlight-title">Vision</span>-Language Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Wei, Bingqian Lin, Yunshuang Nie, Jiaqi Chen, Shikui Ma, Hang Xu, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity is a long-standing challenge in the Vision-Language Navigation
(VLN) field, which extremely hinders the generalization of agents to unseen
environments. Previous works primarily rely on additional simulator data or
web-collected images/videos to improve the generalization. However, the
simulator environments still face limited diversity, and the web-collected data
often requires extensive labor to remove the noise. In this paper, we propose a
Rewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates
the unseen observation-instruction pairs via rewriting human-annotated training
data. Benefiting from our rewriting mechanism, new observation-instruction can
be obtained in both simulator-free and labor-saving manners to promote
generalization. Specifically, we first introduce Object-Enriched Observation
Rewriting, where we combine Vision-Language Models (VLMs) and Large Language
Models (LLMs) to derive rewritten object-enriched scene descriptions, enabling
observation synthesis with diverse objects and spatial layouts via
Text-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast
Instruction Rewriting, which generates observation-aligned rewritten
instructions by requiring LLMs to reason the difference between original and
new observations. We further develop a mixing-then-focusing training strategy
with a random observation cropping scheme, effectively enhancing data
distribution diversity while suppressing augmentation data noise during
training. Experiments on both the discrete environments (R2R, REVERIE, and R4R
datasets) and continuous environments (R2R-CE dataset) show the superior
performance and impressive generalization ability of our method. Code is
available at https://github.com/SaDil13/VLN-RAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assist-as-needed Hip Exoskeleton Control for Gait Asymmetry Correction
  via Human-in-the-loop Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuepeng Qian, Jingfeng Xiong, Haoyong Yu, Chenglong Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gait asymmetry is a significant clinical characteristic of hemiplegic gait
that most stroke survivors suffer, leading to limited mobility and long-term
negative impacts on their quality of life. Although a variety of exoskeleton
controls have been developed for robot-assisted gait rehabilitation, little
attention has been paid to correcting the gait asymmetry of stroke patients
following the assist-as-need (AAN) principle, and it is still challenging to
properly share control between the exoskeleton and stroke patients with partial
motor control. In view of this, this article proposes an AAN hip exoskeleton
control with human-in-the-loop optimization to correct gait asymmetry in stroke
patients. To realize the AAN concept, an objective function was designed for
real-time evaluation of the subject's gait performance and active
participation, which considers the variability of natural human movement and
guides the online tuning of control parameters on a subject-specific basis. In
this way, patients were stimulated to contribute as much as possible to
movement, thus maximizing the efficiency and outcomes of post-stroke gait
rehabilitation. Finally, an experimental study was conducted to verify the
feasibility and effectiveness of the proposed AAN control on healthy subjects
with artificial gait impairment. For the first time, the common hypothesis that
AAN controls can improve human active participation was validated from the
biomechanics viewpoint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Navigation And Chemical Application in Precision Agriculture
  With Deep <span class="highlight-title">Reinforcement Learning</span> And Conditional Action Tree 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahsa Khosravi, Zhanhong Jiang, Joshua R Waite, Sarah Jonesc, Hernan Torres, Arti Singh, Baskar Ganapathysubramanian, Asheesh Kumar Singh, Soumik Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel reinforcement learning (RL)-based planning scheme
for optimized robotic management of biotic stresses in precision agriculture.
The framework employs a hierarchical decision-making structure with conditional
action masking, where high-level actions direct the robot's exploration, while
low-level actions optimize its navigation and efficient chemical spraying in
affected areas. The key objectives of optimization include improving the
coverage of infected areas with limited battery power and reducing chemical
usage, thus preventing unnecessary spraying of healthy areas of the field. Our
numerical experimental results demonstrate that the proposed method,
Hierarchical Action Masking Proximal Policy Optimization (HAM-PPO),
significantly outperforms baseline practices, such as LawnMower navigation +
indiscriminate spraying (Carpet Spray), in terms of yield recovery and resource
efficiency. HAM-PPO consistently achieves higher yield recovery percentages and
lower chemical costs across a range of infection scenarios. The framework also
exhibits robustness to observation noise and generalizability under diverse
environmental conditions, adapting to varying infection ranges and spatial
distribution patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable
  Objects from Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxiao Jiang, Hao-Yu Hsu, Kaifeng Zhang, Hsin-Ni Yu, Shenlong Wang, Yunzhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating a physical digital twin of a real-world object has immense potential
in robotics, content creation, and XR. In this paper, we present PhysTwin, a
novel framework that uses sparse videos of dynamic objects under interaction to
produce a photo- and physically realistic, real-time interactive virtual
replica. Our approach centers on two key components: (1) a physics-informed
representation that combines spring-mass models for realistic physical
simulation, generative shape models for geometry, and Gaussian splats for
rendering; and (2) a novel multi-stage, optimization-based inverse modeling
framework that reconstructs complete geometry, infers dense physical
properties, and replicates realistic appearance from videos. Our method
integrates an inverse physics framework with visual perception cues, enabling
high-fidelity reconstruction even from partial, occluded, and limited
viewpoints. PhysTwin supports modeling various deformable objects, including
ropes, stuffed animals, cloth, and delivery packages. Experiments show that
PhysTwin outperforms competing methods in reconstruction, rendering, future
prediction, and simulation under novel interactions. We further demonstrate its
applications in interactive real-time simulation and model-based robotic motion
planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://jianghanxiao.github.io/phystwin-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Koopman Model Predictive Control of Simple Serial <span class="highlight-title">Robo</span>ts <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adriano del Río, Christoph Stoeffler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximating nonlinear systems as linear ones is a common workaround to
apply control tools tailored for linear systems. This motivates our present
work where we developed a data-driven model predictive controller (MPC) based
on the Koopman operator framework, allowing the embedding of nonlinear dynamics
in a higher dimensional, but linear function space. The controller, termed
adaptive Koopman model predictive control (KMPC), uses online closed-loop
feedback to learn and incrementally update a linear representation of nonlinear
system dynamics, without the prior knowledge of a model. Adaptive KMPC differs
from most other Koopman-based control frameworks that aim to identify
high-validity-range models in advance and then enter closed-loop control
without further model adaptations. To validate the controller, trajectory
tracking experiments are conducted with 1R and 2R robots under force
disturbances and changing model parameters. We compare the controller to
classical linearization MPC and Koopman-based MPC without model updates,
denoted static KMPC. The results show that adaptive KMPC can, opposed to static
KMPC, generalize over unforeseen force disturbances and can, opposed to
linearization MPC, handle varying dynamic parameters, while using a small set
of basis functions to approximate the Koopman operator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted to IROS 2025; See supplementary material at
  https://github.com/adrianodelr/adaptive-koopman-mpc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SG-Tailor: Inter-Object Commonsense Relationship Reasoning for Scene
  Graph <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoliang Shang, Hanyu Wu, Guangyao Zhai, Boyang Sun, Fangjinhua Wang, Federico Tombari, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene graphs capture complex relationships among objects, serving as strong
priors for content generation and manipulation. Yet, reasonably manipulating
scene graphs -- whether by adding nodes or modifying edges -- remains a
challenging and untouched task. Tasks such as adding a node to the graph or
reasoning about a node's relationships with all others are computationally
intractable, as even a single edge modification can trigger conflicts due to
the intricate interdependencies within the graph. To address these challenges,
we introduce SG-Tailor, an autoregressive model that predicts the conflict-free
relationship between any two nodes. SG-Tailor not only infers inter-object
relationships, including generating commonsense edges for newly added nodes but
also resolves conflicts arising from edge modifications to produce coherent,
manipulated graphs for downstream tasks. For node addition, the model queries
the target node and other nodes from the graph to predict the appropriate
relationships. For edge modification, SG-Tailor employs a Cut-And-Stitch
strategy to solve the conflicts and globally adjust the graph. Extensive
experiments demonstrate that SG-Tailor outperforms competing methods by a large
margin and can be seamlessly integrated as a plug-in module for scene
generation and robotic manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code will be available at https://github.com/josef5838/SG-Tailor</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Bayesian Estimation in Sensor Networks: Consensus on
  Marginal Densities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01227v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01227v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parth Paritosh, Nikolay Atanasov, Sonia Martinez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we aim to design and analyze distributed Bayesian estimation
algorithms for sensor networks. The challenges we address are to (i) derive a
distributed provably-correct algorithm in the functional space of probability
distributions over continuous variables, and (ii) leverage these results to
obtain new distributed estimators restricted to subsets of variables observed
by individual agents. This relates to applications such as cooperative
localization and federated learning, where the data collected at any agent
depends on a subset of all variables of interest. We present Bayesian density
estimation algorithms using data from non-linear likelihoods at agents in
centralized, distributed, and marginal distributed settings. After setting up a
distributed estimation objective, we prove almost-sure convergence to the
optimal set of pdfs at each agent. Then, we prove the same for a storage-aware
algorithm estimating densities only over relevant variables at each agent.
Finally, we present a Gaussian version of these algorithms and implement it in
a mapping problem using variational inference to handle non-linear likelihood
models associated with LiDAR sensing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Socially-Aware <span class="highlight-title">Robo</span>t Navigation Enhanced by Bidirectional Natural
  Language Conversations Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04965v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04965v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Congcong Wen, Yifan Liu, Geeta Chandra Raju Bethala, Shuaihang Yuan, Hao Huang, Yu Hao, Mengyu Wang, Yu-Shen Liu, Anthony Tzes, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot navigation is crucial across various domains, yet traditional methods
focus on efficiency and obstacle avoidance, often overlooking human behavior in
shared spaces. With the rise of service robots, socially aware navigation has
gained prominence. However, existing approaches primarily predict pedestrian
movements or issue alerts, lacking true human-robot interaction. We introduce
Hybrid Soft Actor-Critic with Large Language Model (HSAC-LLM), a novel
framework for socially aware navigation. By integrating deep reinforcement
learning with large language models, HSAC-LLM enables bidirectional natural
language interactions, predicting both continuous and discrete navigation
actions. When potential collisions arise, the robot proactively communicates
with pedestrians to determine avoidance strategies. Experiments in 2D
simulation, Gazebo, and real-world environments demonstrate that HSAC-LLM
outperforms state-of-the-art DRL methods in interaction, navigation, and
obstacle avoidance. This paradigm advances effective human-robot interactions
in dynamic settings. Videos are available at https://hsacllm.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Safe Mid-Air Drone Interception: Strategies for Tracking &
  Capture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Pliska, Matouš Vrba, Tomáš Báča, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A unique approach for the mid-air autonomous aerial interception of
non-cooperating UAV by a flying robot equipped with a net is presented in this
paper. A novel interception guidance method dubbed EPN is proposed, designed to
catch agile maneuvering targets while relying on onboard state estimation and
tracking. The proposed method is compared with state-of-the-art approaches in
simulations using 100 different trajectories of the target with varying
complexity comprising almost 14 hours of flight data, and EPN demonstrates the
shortest response time and the highest number of interceptions, which are key
parameters of agile interception. To enable robust transfer from theory and
simulation to a real-world implementation, we aim to avoid overfitting to
specific assumptions about the target, and to tackle interception of a target
following an unknown general trajectory. Furthermore, we identify several often
overlooked problems related to tracking and estimation of the target's state
that can have a significant influence on the overall performance of the system.
We propose the use of a novel state estimation filter based on the IMM filter
and a new measurement model. Simulated experiments show that the proposed
solution provides significant improvements in estimation accuracy over the
commonly employed KF approaches when considering general trajectories. Based on
these results, we employ the proposed filtering and guidance methods to
implement a complete autonomous interception system, which is thoroughly
evaluated in realistic simulations and tested in real-world experiments with a
maneuvering target going far beyond the performance of any state-of-the-art
solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode
  Modeling <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Zhou, Hengjian Zhou, Haibo Hu, Zihao Wen, Jianping Wang, Yung-Hui Li, Yu-Kai Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anticipating the multimodality of future events lays the foundation for safe
autonomous driving. However, multimodal motion prediction for traffic agents
has been clouded by the lack of multimodal ground truth. Existing works
predominantly adopt the winner-take-all training strategy to tackle this
challenge, yet still suffer from limited trajectory diversity and uncalibrated
mode confidence. While some approaches address these limitations by generating
excessive trajectory candidates, they necessitate a post-processing stage to
identify the most representative modes, a process lacking universal principles
and compromising trajectory accuracy. We are thus motivated to introduce
ModeSeq, a new multimodal prediction paradigm that models modes as sequences.
Unlike the common practice of decoding multiple plausible trajectories in one
shot, ModeSeq requires motion decoders to infer the next mode step by step,
thereby more explicitly capturing the correlation between modes and
significantly enhancing the ability to reason about multimodality. Leveraging
the inductive bias of sequential mode prediction, we also propose the
Early-Match-Take-All (EMTA) training strategy to diversify the trajectories
further. Without relying on dense mode prediction or heuristic post-processing,
ModeSeq considerably improves the diversity of multimodal output while
attaining satisfactory trajectory accuracy, resulting in balanced performance
on motion prediction benchmarks. Moreover, ModeSeq naturally emerges with the
capability of mode extrapolation, which supports forecasting more behavior
modes when the future is highly uncertain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Omnidirectional Multi-Object Tracking <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04565v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04565v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Luo, Hao Shi, Sheng Wu, Fei Teng, Mengfei Duan, Chang Huang, Yuhang Wang, Kaiwei Wang, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoramic imagery, with its 360{\deg} field of view, offers comprehensive
information to support Multi-Object Tracking (MOT) in capturing spatial and
temporal relationships of surrounding objects. However, most MOT algorithms are
tailored for pinhole images with limited views, impairing their effectiveness
in panoramic settings. Additionally, panoramic image distortions, such as
resolution loss, geometric deformation, and uneven lighting, hinder direct
adaptation of existing MOT methods, leading to significant performance
degradation. To address these challenges, we propose OmniTrack, an
omnidirectional MOT framework that incorporates Tracklet Management to
introduce temporal cues, FlexiTrack Instances for object localization and
association, and the CircularStatE Module to alleviate image and geometric
distortions. This integration enables tracking in panoramic field-of-view
scenarios, even under rapid sensor motion. To mitigate the lack of panoramic
MOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic
dataset collected by a quadruped robot, featuring diverse challenges such as
panoramic fields of view, intense motion, and complex environments. Extensive
experiments on the public JRDB dataset and the newly introduced QuadTrack
benchmark demonstrate the state-of-the-art performance of the proposed
framework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an
improvement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the
baseline by 6.81%. The established dataset and source code are available at
https://github.com/xifen523/OmniTrack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025. The established dataset and source code are
  available at https://github.com/xifen523/OmniTrack</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Data-Centric Revisit of Pre-Trained <span class="highlight-title">Vision</span> Models for <span class="highlight-title">Robo</span>t Learning <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wen, Bingchen Zhao, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained vision models (PVMs) are fundamental to modern robotics, yet
their optimal configuration remains unclear. Through systematic evaluation, we
find that while DINO and iBOT outperform MAE across visuomotor control and
perception tasks, they struggle when trained on non-(single-)object-centric
(NOC) data--a limitation strongly correlated with their diminished ability to
learn object-centric representations. This investigation indicates that the
ability to form object-centric representations from the non-object-centric
robotics dataset is the key to success for PVMs. Motivated by this discovery,
we designed SlotMIM, a method that induces object-centric representations by
introducing a semantic bottleneck to reduce the number of prototypes to
encourage the emergence of objectness as well as cross-view consistency
regularization for encouraging multiview invariance. Our experiments encompass
pre-training on object-centric, scene-centric, web-crawled, and ego-centric
data. Across all settings, our approach learns transferrable representations
and achieves significant improvements over prior work in image recognition,
scene understanding, and robot learning evaluations. When scaled up with
million-scale datasets, our method also demonstrates superior data efficiency
and scalability. Our code and models are publicly available at
https://github.com/CVMI-Lab/SlotMIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot <span class="highlight-title">Manipulation</span> Strategy Learning by Making Contact Analogies <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09627v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09627v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyao Liu, Jiayuan Mao, Joshua Tenenbaum, Tomás Lozano-Pérez, Leslie Pack Kaelbling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach, MAGIC (manipulation analogies for generalizable
intelligent contacts), for one-shot learning of manipulation strategies with
fast and extensive generalization to novel objects. By leveraging a reference
action trajectory, MAGIC effectively identifies similar contact points and
sequences of actions on novel objects to replicate a demonstrated strategy,
such as using different hooks to retrieve distant objects of different shapes
and sizes. Our method is based on a two-stage contact-point matching process
that combines global shape matching using pretrained neural features with local
curvature analysis to ensure precise and physically plausible contact points.
We experiment with three tasks including scooping, hanging, and hooking
objects. MAGIC demonstrates superior performance over existing methods,
achieving significant improvements in runtime speed and generalization to
different object categories. Website: https://magic-2024.github.io/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025; CoRL LEAP Workshop, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligent <span class="highlight-title">LiDAR</span> Navigation: Leveraging External Information and
  Semantic Maps with LLM as Copilot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fujing Xie, Jiajie Zhang, Sören Schwertfeger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional robot navigation systems primarily utilize occupancy grid maps
and laser-based sensing technologies, as demonstrated by the popular move_base
package in ROS. Unlike robots, humans navigate not only through spatial
awareness and physical distances but also by integrating external information,
such as elevator maintenance updates from public notification boards and
experiential knowledge, like the need for special access through certain doors.
With the development of Large Language Models (LLMs), which possesses text
understanding and intelligence close to human performance, there is now an
opportunity to infuse robot navigation systems with a level of understanding
akin to human cognition. In this study, we propose using osmAG (Area Graph in
OpensStreetMap textual format), an innovative semantic topometric hierarchical
map representation, to bridge the gap between the capabilities of ROS move_base
and the contextual understanding offered by LLMs. Our methodology employs LLMs
as an actual copilot in robot navigation, enabling the integration of a broader
range of informational inputs while maintaining the robustness of traditional
robotic navigation systems. Our code, demo, map, experiment results can be
accessed at
https://github.com/xiexiexiaoxiexie/Intelligent-LiDAR-Navigation-LLM-as-Copilot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Aware Combinatorial Assembly Sequence Planning using Data-free
  Action Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10162v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10162v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixuan Liu, Alan Chen, Weiye Zhao, Changliu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial assembly uses standardized unit primitives to build objects
that satisfy user specifications. This paper studies assembly sequence planning
(ASP) for physical combinatorial assembly. Given the shape of the desired
object, the goal is to find a sequence of actions for placing unit primitives
to build the target object. In particular, we aim to ensure the planned
assembly sequence is physically executable. However, ASP for combinatorial
assembly is particularly challenging due to its combinatorial nature. To
address the challenge, we employ deep reinforcement learning to learn a
construction policy for placing unit primitives sequentially to build the
desired object. Specifically, we design an online physics-aware action mask
that filters out invalid actions, which effectively guides policy learning and
ensures violation-free deployment. In the end, we apply the proposed method to
Lego assembly with more than 250 3D structures. The experiment results
demonstrate that the proposed method plans physically valid assembly sequences
to build all structures, achieving a $100\%$ success rate, whereas the best
comparable baseline fails more than $40$ structures. Our implementation is
available at
\url{https://github.com/intelligent-control-lab/PhysicsAwareCombinatorialASP}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Diffusion</span> Transformer Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15959v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15959v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu, Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai, Yuntao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large vision-language-action models pretrained on diverse robot
datasets have demonstrated the potential for generalizing to new environments
with a few in-domain data. However, those approaches usually predict individual
discretized or continuous action by a small action head, which limits the
ability in handling diverse action spaces. In contrast, we model the continuous
action sequence with a large multi-modal diffusion transformer, dubbed as
Diffusion Transformer Policy, in which we directly denoise action chunks by a
large transformer model rather than a small action head for action embedding.
By leveraging the scaling capability of transformers, the proposed approach can
effectively model continuous end-effector actions across large diverse robot
datasets, and achieve better generalization performance. Extensive experiments
demonstrate the effectiveness and generalization of Diffusion Transformer
Policy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world
Franka arm, achieving consistent better performance on Real-to-Sim benchmark
SimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo.
Specifically, without bells and whistles, the proposed approach achieves
state-of-the-art performance with only a single third-view camera stream in the
Calvin task ABC->D, improving the average number of tasks completed in a row of
5 to 3.6, and the pretraining stage significantly facilitates the success
sequence length on the Calvin by over 1.2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint; New Project Page: https://robodita.github.io; revert
  unsuitable replacement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intent Prediction-Driven Model Predictive Control for UAV Planning and
  Navigation in Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhefan Xu, Hanyu Jin, Xinming Han, Haoyu Shen, Kenji Shimada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerial robots can enhance construction site productivity by autonomously
handling inspection and mapping tasks. However, ensuring safe navigation near
human workers remains challenging. While navigation in static environments has
been well studied, navigating dynamic environments remains open due to
challenges in perception and planning. Payload limitations restrict the robots
to using cameras with limited fields of view, resulting in unreliable
perception and tracking during collision avoidance. Moreover, the rapidly
changing conditions of dynamic environments can quickly make the generated
optimal trajectory outdated.To address these challenges, this paper presents a
comprehensive navigation framework that integrates perception, intent
prediction, and planning. Our perception module detects and tracks dynamic
obstacles efficiently and handles tracking loss and occlusion during collision
avoidance. The proposed intent prediction module employs a Markov Decision
Process (MDP) to forecast potential actions of dynamic obstacles with the
possible future trajectories. Finally, a novel intent-based planning algorithm,
leveraging model predictive control (MPC), is applied to generate navigation
trajectories. Simulation and physical experiments demonstrate that our method
improves the safety of navigation by achieving the fewest collisions compared
to benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 2 tables, experiment video:
  https://youtu.be/4xsEeMB9WPY, GitHub: https://github.com/Zhefan-Xu/Intent-MPC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> DexDiffuser: Interaction-aware <span class="highlight-title">Diffusion</span> Planning for Adaptive Dexterous
  <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18562v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18562v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Liang, <span class="highlight-author">Yao Mu</span>, Yixiao Wang, Tianxing Chen, Wenqi Shao, Wei Zhan, Masayoshi Tomizuka, Ping Luo, Mingyu Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dexterous manipulation with contact-rich interactions is crucial for advanced
robotics. While recent diffusion-based planning approaches show promise for
simple manipulation tasks, they often produce unrealistic ghost states (e.g.,
the object automatically moves without hand contact) or lack adaptability when
handling complex sequential interactions. In this work, we introduce
DexDiffuser, an interaction-aware diffusion planning framework for adaptive
dexterous manipulation. DexDiffuser models joint state-action dynamics through
a dual-phase diffusion process which consists of pre-interaction contact
alignment and post-contact goal-directed control, enabling goal-adaptive
generalizable dexterous manipulation. Additionally, we incorporate dynamics
model-based dual guidance and leverage large language models for automated
guidance function generation, enhancing generalizability for physical
interactions and facilitating diverse goal adaptation through language cues.
Experiments on physical interaction tasks such as door opening, pen and block
re-orientation, object relocation, and hammer striking demonstrate
DexDiffuser's effectiveness on goals outside training distributions, achieving
over twice the average success rate (59.2% vs. 29.5%) compared to existing
methods. Our framework achieves an average of 70.7% success rate on goal
adaptive dexterous tasks, highlighting its robustness and flexibility in
contact-rich manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version. 27 pages. Project page:
  https://dexdiffuser.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-22T00:00:00Z">2025-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Smart Ankleband for Plug-and-Play Hand-Prosthetic Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dean Zadok, Oren Salzman, Alon Wolf, Alex M. Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building robotic prostheses requires the creation of a sensor-based interface
designed to provide the robotic hand with the control required to perform hand
gestures. Traditional Electromyography (EMG) based prosthetics and emerging
alternatives often face limitations such as muscle-activation limitations, high
cost, and complex-calibration procedures. In this paper, we present a low-cost
robotic system composed of a smart ankleband for intuitive, calibration-free
control of a robotic hand, and a robotic prosthetic hand that executes actions
corresponding to leg gestures. The ankleband integrates an Inertial Measurement
Unit (IMU) sensor with a lightweight temporal neural network to infer
user-intended leg gestures from motion data. Our system represents a
significant step towards higher adoption rates of robotic prostheses among arm
amputees, as it enables one to operate a prosthetic hand using a low-cost,
low-power, and calibration-free solution. To evaluate our work, we collected
data from 10 subjects and tested our prototype ankleband with a robotic hand on
an individual with upper-limb amputations. Our results demonstrate that this
system empowers users to perform daily tasks more efficiently, requiring few
compensatory movements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GS-LTS: 3D Gaussian Splatting-Based Adaptive Modeling for Long-Term
  Service <span class="highlight-title">Robo</span>ts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Fu, Jialin Li, Bin Zhang, Ruiping Wang, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has garnered significant attention in robotics
for its explicit, high fidelity dense scene representation, demonstrating
strong potential for robotic applications. However, 3DGS-based methods in
robotics primarily focus on static scenes, with limited attention to the
dynamic scene changes essential for long-term service robots. These robots
demand sustained task execution and efficient scene updates-challenges current
approaches fail to meet. To address these limitations, we propose GS-LTS
(Gaussian Splatting for Long-Term Service), a 3DGS-based system enabling indoor
robots to manage diverse tasks in dynamic environments over time. GS-LTS
detects scene changes (e.g., object addition or removal) via single-image
change detection, employs a rule-based policy to autonomously collect
multi-view observations, and efficiently updates the scene representation
through Gaussian editing. Additionally, we propose a simulation-based benchmark
that automatically generates scene change data as compact configuration
scripts, providing a standardized, user-friendly evaluation benchmark.
Experimental results demonstrate GS-LTS's advantages in reconstruction,
navigation, and superior scene updates-faster and higher quality than the image
training baseline-advancing 3DGS for long-term robotic operations. Code and
benchmark are available at: https://vipl-vsu.github.io/3DGS-LTS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aportes para el cumplimiento del Reglamento (UE) 2024/1689 en robótica
  y sistemas autónomos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco J. Rodríguez Lera, Yoana Pita Lorenzo, David Sobrín Hidalgo, Laura Fernández Becerra, Irene González Fernández, Jose Miguel Guerrero Hernández
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cybersecurity in robotics stands out as a key aspect within Regulation (EU)
2024/1689, also known as the Artificial Intelligence Act, which establishes
specific guidelines for intelligent and automated systems. A fundamental
distinction in this regulatory framework is the difference between robots with
Artificial Intelligence (AI) and those that operate through automation systems
without AI, since the former are subject to stricter security requirements due
to their learning and autonomy capabilities. This work analyzes cybersecurity
tools applicable to advanced robotic systems, with special emphasis on the
protection of knowledge bases in cognitive architectures. Furthermore, a list
of basic tools is proposed to guarantee the security, integrity, and resilience
of these systems, and a practical case is presented, focused on the analysis of
robot knowledge management, where ten evaluation criteria are defined to ensure
compliance with the regulation and reduce risks in human-robot interaction
(HRI) environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figure, in Spanish</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Perching and <span class="highlight-title">Grasp</span>ing by Aerial <span class="highlight-title">Robo</span>t with Light-weight and
  High Grip-force Tendon-driven Three-fingered Hand using Single Actuator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hisaaki Iida, Junichiro Sugihara, Kazuki Sugihara, Haruki Kozuka, Jinjie Li, Keisuke Nagato, Moju Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In previous research, various types of aerial robots equipped with perching
mechanisms have been developed to extend operational time. However, most
existing perching methods adopt either an upward or downward approach, making
it difficult to perch near walls with surrounding obstacles. Additionally,
perching hands are typically designed solely for attachment to objects and lack
additional functionality, imposing a payload burden during flight. To address
these issues, this paper proposes a lightweight robotic hand, the "Tri-force
hand", capable of both perching and object grasping, as well as a new perching
method called "Pendulum-perching". The Tri-force hand is a tendon-driven,
three-fingered hand utilizing a spherical joint and a two-dimensional
differential plate, enabling passive actuation with a single actuator. Each
finger module, designed with controllable semi-tendon drive, can conform to
arbitrary shapes within its operating range, allowing both perching and
adaptive object grasping. By integrating this hand into a fully actuated aerial
robot, the system can perform multi-directional approaches from the side and
landing using gravity. This approach is similar to Crush-perching seen in
researches with fixed-wing aerial robots, but it differs in its superior
control over approach speed and direction, as well as its ability to achieve
stable detachment and re-launch. In experiments, the fabricated Tri-force hand
demonstrated the ability to withstand a total weight of up to 27.5 kg, grasp
various objects ranging from simple to complex-shaped tools, and achieve a high
success rate in both perching and takeoff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAIDER: Tool-Equipped Large Language Model Agent for <span class="highlight-title">Robo</span>tic Action
  Issue Detection, Explanation and Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvia Izquierdo-Badiola, Carlos Rizzo, Guillem Alenyà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robots increasingly operate in dynamic human-centric environments,
improving their ability to detect, explain, and recover from action-related
issues becomes crucial. Traditional model-based and data-driven techniques lack
adaptability, while more flexible generative AI methods struggle with grounding
extracted information to real-world constraints. We introduce RAIDER, a novel
agent that integrates Large Language Models (LLMs) with grounded tools for
adaptable and efficient issue detection and explanation. Using a unique
"Ground, Ask& Answer, Issue" procedure, RAIDER dynamically generates
context-aware precondition questions and selects appropriate tools for
resolution, achieving targeted information gathering. Our results within a
simulated household environment surpass methods relying on predefined models,
full scene descriptions, or standalone trained models. Additionally, RAIDER's
explanations enhance recovery success, including cases requiring human
interaction. Its modular architecture, featuring self-correction mechanisms,
enables straightforward adaptation to diverse scenarios, as demonstrated in a
real-world human-assistive task. This showcases RAIDER's potential as a
versatile agentic AI solution for robotic issue detection and explanation,
while addressing the problem of grounding generative AI for its effective
application in embodied agents. Project website:
https://raider-llmagent.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sense4FL: Vehicular Crowdsensing Enhanced Federated Learning for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Ma, Senkang Hu, Zhengru Fang, Yun Ji, Yiqin Deng, Yuguang Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To accommodate constantly changing road conditions, real-time model training
is essential for autonomous driving (AD). Federated learning (FL) serves as a
promising paradigm to enable autonomous vehicles to train models
collaboratively with their onboard computing resources. However, existing
vehicle selection schemes for FL all assume predetermined and
location-independent vehicles' datasets, neglecting the fact that vehicles
collect training data along their routes, thereby resulting in suboptimal
vehicle selection. To improve the perception quality in AD for a region, we
propose Sense4FL, a vehicular crowdsensing-enhanced FL framework featuring
trajectory-dependent vehicular training data collection. To this end, we first
derive the convergence bound of FL by considering the impact of both vehicles'
uncertain trajectories and uploading probabilities, from which we discover that
minimizing the training loss is equivalent to minimizing a weighted sum of
local and global earth mover's distance (EMD) between vehicles' collected data
distribution and global data distribution. Based on this observation, we
formulate the trajectory-dependent vehicle selection and data collection
problem for FL in AD. Given that the problem is NP-hard, we develop an
efficient algorithm to find the solution with an approximation guarantee.
Extensive simulation results have demonstrated the effectiveness of our
approach in improving object detection performance compared with existing
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computationally and Sample Efficient Safe <span class="highlight-title">Reinforcement Learning</span> Using
  Adaptive Conformal Prediction <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhou, Yanze Zhang, Wenhao Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety is a critical concern in learning-enabled autonomous systems
especially when deploying these systems in real-world scenarios. An important
challenge is accurately quantifying the uncertainty of unknown models to
generate provably safe control policies that facilitate the gathering of
informative data, thereby achieving both safe and optimal policies.
Additionally, the selection of the data-driven model can significantly impact
both the real-time implementation and the uncertainty quantification process.
In this paper, we propose a provably sample efficient episodic safe learning
framework that remains robust across various model choices with quantified
uncertainty for online control tasks. Specifically, we first employ Quadrature
Fourier Features (QFF) for kernel function approximation of Gaussian Processes
(GPs) to enable efficient approximation of unknown dynamics. Then the Adaptive
Conformal Prediction (ACP) is used to quantify the uncertainty from online
observations and combined with the Control Barrier Functions (CBF) to
characterize the uncertainty-aware safe control constraints under learned
dynamics. Finally, an optimism-based exploration strategy is integrated with
ACP-based CBFs for safe exploration and near-optimal safe nonlinear control.
Theoretical proofs and simulation results are provided to demonstrate the
effectiveness and efficiency of the proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transferable Latent-to-Latent Locomotion Policy for Efficient and
  Versatile Motion Control of Diverse Legged <span class="highlight-title">Robo</span>ts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziang Zheng, Guojian Zhan, Bin Shuai, Shengtao Qin, Jiangtao Li, Tao Zhang, Shengbo Eben Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has demonstrated remarkable capability in
acquiring robot skills, but learning each new skill still requires substantial
data collection for training. The pretrain-and-finetune paradigm offers a
promising approach for efficiently adapting to new robot entities and tasks.
Inspired by the idea that acquired knowledge can accelerate learning new tasks
with the same robot and help a new robot master a trained task, we propose a
latent training framework where a transferable latent-to-latent locomotion
policy is pretrained alongside diverse task-specific observation encoders and
action decoders. This policy in latent space processes encoded latent
observations to generate latent actions to be decoded, with the potential to
learn general abstract motion skills. To retain essential information for
decision-making and control, we introduce a diffusion recovery module that
minimizes information reconstruction loss during pretrain stage. During
fine-tune stage, the pretrained latent-to-latent locomotion policy remains
fixed, while only the lightweight task-specific encoder and decoder are
optimized for efficient adaptation. Our method allows a robot to leverage its
own prior experience across different tasks as well as the experience of other
morphologically diverse robots to accelerate adaptation. We validate our
approach through extensive simulations and real-world experiments,
demonstrating that the pretrained latent-to-latent locomotion policy
effectively generalizes to new robot entities and tasks with improved
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Selection Based on <span class="highlight-title">Reinforcement Learning</span> and Hazard State
  Classification for Magnetic Adhesion Wall-Climbing <span class="highlight-title">Robo</span>ts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Ma, He Xu, Jielong Dou, Yi Qin, Xueyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic adhesion tracked wall-climbing robots face potential risks of
overturning during high-altitude operations, making their stability crucial for
ensuring safety. This study presents a dynamic feature selection method based
on Proximal Policy Optimization (PPO) reinforcement learning, combined with
typical machine learning models, aimed at improving the classification accuracy
of hazardous states under complex operating conditions. Firstly, this work
innovatively employs a fiber rod-based MEMS attitude sensor to collect
vibration data from the robot and extract high-dimensional feature vectors in
both time and frequency domains. Then, a reinforcement learning model is used
to dynamically select the optimal feature subset, reducing feature redundancy
and enhancing classification accuracy. Finally, a CNN-LSTM deep learning model
is employed for classification and recognition. Experimental results
demonstrate that the proposed method significantly improves the robot's ability
to assess hazardous states across various operational scenarios, providing
reliable technical support for robotic safety monitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 11 figures, manuscript for Journal of Autonomous Robots</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending First-order Motion Planners to Second-order Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayur Sawant, Abdelhamid Tayebi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper extends first-order motion planners to robots governed by
second-order dynamics. Two control schemes are proposed based on the knowledge
of a scalar function whose negative gradient aligns with a given first-order
motion planner. When such a function is known, the first-order motion planner
is combined with a damping velocity vector with a dynamic gain to extend the
safety and convergence guarantees of the first-order motion planner to
second-order systems. If no such function is available, we propose an
alternative control scheme ensuring that the error between the robot's velocity
and the first-order motion planner converges to zero. The theoretical
developments are supported by simulation results demonstrating the
effectiveness of the proposed approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MERLION: Marine ExploRation with Language guIded Online iNformative
  <span class="highlight-title">Visual</span> Sampling and Enhancement <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrutika Vishal Thengane, Marcel Bartholomeus Prasetyo, Yu Xiang Tan, Malika Meghjani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous and targeted underwater visual monitoring and exploration using
Autonomous Underwater Vehicles (AUVs) can be a challenging task due to both
online and offline constraints. The online constraints comprise limited onboard
storage capacity and communication bandwidth to the surface, whereas the
offline constraints entail the time and effort required for the selection of
desired key frames from the video data. An example use case of targeted
underwater visual monitoring is finding the most interesting visual frames of
fish in a long sequence of an AUV's visual experience. This challenge of
targeted informative sampling is further aggravated in murky waters with poor
visibility. In this paper, we present MERLION, a novel framework that provides
semantically aligned and visually enhanced summaries for murky underwater
marine environment monitoring and exploration. Specifically, our framework
integrates (a) an image-text model for semantically aligning the visual samples
to the users' needs, (b) an image enhancement model for murky water visual data
and (c) an informative sampler for summarizing the monitoring experience. We
validate our proposed MERLION framework on real-world data with user studies
and present qualitative and quantitative results using our evaluation metric
and show improved results compared to the state-of-the-art approaches. We have
open-sourced the code for MERLION at the following link
https://github.com/MARVL-Lab/MERLION.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of IEEE ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning framework for action prediction reveals multi-timescale
  locomotor control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Chen Wang, Antoine De Comite, Alexandra Voloshina, Monica Daley, Nidhi Seethapathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling movement in real-world tasks is a fundamental scientific goal for
motor control, biomechanics, and rehabilitation engineering. However, existing
models and their simplifying assumptions such as linear and fixed timescale
mappings do not generalize to real-world contexts. Here, we develop a deep
learning-based framework for action prediction with architecture-dependent
trial embedding, outperforming traditional models across multiple contexts
(walking and running, treadmill and overground, varying terrains) and input
modalities (multiple body states, gaze). We find that neural network
architectures with flexible input history-dependence like GRU and Transformer
perform best overall. By quantifying the model's predictions relative to an
autoregressive baseline, we identify context- and modality-dependent
timescales. There is greater reliance on fast-timescale predictions in complex
terrain, gaze predictions precede body state predictions, and full-body state
predictions precede center-of-mass-relevant predictions. This deep learning
framework for action prediction provides quantifiable insights into the control
of complex movements and can be extended to other actions, contexts, and
populations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NavCoT: Boosting LLM-Based <span class="highlight-title">Vision</span>-and-Language Navigation via Learning
  Disentangled Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN), as a crucial research problem of
Embodied AI, requires an embodied agent to navigate through complex 3D
environments following natural language instructions. Recent research has
highlighted the promising capacity of large language models (LLMs) in VLN by
improving navigational reasoning accuracy and interpretability. However, their
predominant use in an offline manner usually suffers from substantial domain
gap between the VLN task and the LLM training corpus. This paper introduces a
novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill
parameter-efficient in-domain training to enable self-guided navigational
decision, leading to a significant mitigation of the domain gap in a
cost-effective manner. Specifically, at each timestep, the LLM is prompted to
forecast the navigational chain-of-thought by: 1) acting as a world model to
imagine the next observation according to the instruction, 2) selecting the
candidate observation that best aligns with the imagination, and 3) determining
the action based on the reasoning from the prior steps. Through constructing
formalized labels for training, the LLM can learn to generate desired and
reasonable chain-of-thought outputs for improving the action decision.
Experimental results across various training settings and popular VLN
benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room
(R4R)) show the significant superiority of NavCoT over the direct action
prediction variants. Through simple parameter-efficient finetuning, our NavCoT
outperforms a recent GPT4-based approach with ~7% relative improvement on the
R2R dataset. We believe that NavCoT will help unlock more task-adaptive and
scalable LLM-based embodied agents, which are helpful for developing real-world
robotics applications. Code is available at
https://github.com/expectorlin/NavCoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TPAMI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEVA: A high-mobility logistic vehicle with legged suspension <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10028v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10028v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Arnold, Lukas Hildebrandt, Kaspar Janssen, Efe Ongan, Pascal Bürge, Ádám Gyula Gábriel, James Kennedy, Rishi Lolla, Quanisha Oppliger, Micha Schaaf, Joseph Church, Michael Fritsche, Victor Klemm, Turcan Tuna, Giorgio Valsecchi, Cedric Weibel, Michael Wüthrich, Marco Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The autonomous transportation of materials over challenging terrain is a
challenge with major economic implications and remains unsolved. This paper
introduces LEVA, a high-payload, high-mobility robot designed for autonomous
logistics across varied terrains, including those typical in agriculture,
construction, and search and rescue operations. LEVA uniquely integrates an
advanced legged suspension system using parallel kinematics. It is capable of
traversing stairs using a rl controller, has steerable wheels, and includes a
specialized box pickup mechanism that enables autonomous payload loading as
well as precise and reliable cargo transportation of up to 85 kg across uneven
surfaces, steps and inclines while maintaining a cot of as low as 0.15. Through
extensive experimental validation, LEVA demonstrates its off-road capabilities
and reliability regarding payload loading and transport.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2025 IEEE International Conference on
  Robotics and Automation (ICRA). This is the author's preprint version. 6
  pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closing the Intent-to-Behavior Gap via Fulfillment Priority Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.05818v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.05818v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bassel El Mabsout, Abdelrahman AbdelGawad, Renato Mancuso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practitioners designing reinforcement learning policies face a fundamental
challenge: translating intended behavioral objectives into representative
reward functions. This challenge stems from behavioral intent requiring
simultaneous achievement of multiple competing objectives, typically addressed
through labor-intensive linear reward composition that yields brittle results.
Consider the ubiquitous robotics scenario where performance maximization
directly conflicts with energy conservation. Such competitive dynamics are
resistant to simple linear reward combinations. In this paper, we present the
concept of objective fulfillment upon which we build Fulfillment Priority Logic
(FPL). FPL allows practitioners to define logical formula representing their
intentions and priorities within multi-objective reinforcement learning. Our
novel Balanced Policy Gradient algorithm leverages FPL specifications to
achieve up to 500\% better sample efficiency compared to Soft Actor Critic.
Notably, this work constitutes the first implementation of non-linear utility
scalarization design, specifically for continuous control problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liewen Liao, Weihao Yan, Ming Yang, Songan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based 3D reconstruction has emerged as a transformative technique in
autonomous driving, enabling precise modeling of both dynamic and static
environments through advanced neural representations. Despite data
augmentation, 3D reconstruction inspires pioneering solution for vital tasks in
the field of autonomous driving, such as scene understanding and closed-loop
simulation. We investigates the details of 3D reconstruction and conducts a
multi-perspective, in-depth analysis of recent advancements. Specifically, we
first provide a systematic introduction of preliminaries, including data
modalities, benchmarks and technical preliminaries of learning-based 3D
reconstruction, facilitating instant identification of suitable methods
according to sensor suites. Then, we systematically review learning-based 3D
reconstruction methods in autonomous driving, categorizing approaches by
subtasks and conducting multi-dimensional analysis and summary to establish a
comprehensive technical reference. The development trends and existing
challenges are summarized in the context of learning-based 3D reconstruction in
autonomous driving. We hope that our review will inspire future researches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GDRNPP: A Geometry-guided and Fully Learning-based Object Pose Estimator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.12145v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.12145v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Liu, Ruida Zhang, Chenyangguang Zhang, Gu Wang, Jiwen Tang, Zhigang Li, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  6D pose estimation of rigid objects is a long-standing and challenging task
in computer vision. Recently, the emergence of deep learning reveals the
potential of Convolutional Neural Networks (CNNs) to predict reliable 6D poses.
Given that direct pose regression networks currently exhibit suboptimal
performance, most methods still resort to traditional techniques to varying
degrees. For example, top-performing methods often adopt an indirect strategy
by first establishing 2D-3D or 3D-3D correspondences followed by applying the
RANSAC-based PnP or Kabsch algorithms, and further employing ICP for
refinement. Despite the performance enhancement, the integration of traditional
techniques makes the networks time-consuming and not end-to-end trainable.
Orthogonal to them, this paper introduces a fully learning-based object pose
estimator. In this work, we first perform an in-depth investigation of both
direct and indirect methods and propose a simple yet effective Geometry-guided
Direct Regression Network (GDRN) to learn the 6D pose from monocular images in
an end-to-end manner. Afterwards, we introduce a geometry-guided pose
refinement module, enhancing pose accuracy when extra depth data is available.
Guided by the predicted coordinate map, we build an end-to-end differentiable
architecture that establishes robust and accurate 3D-3D correspondences between
the observed and rendered RGB-D images to refine the pose. Our enhanced pose
estimation pipeline GDRNPP (GDRN Plus Plus) conquered the leaderboard of the
BOP Challenge for two consecutive years, becoming the first to surpass all
prior methods that relied on traditional techniques in both accuracy and speed.
The code and models are available at
https://github.com/shanice-l/gdrnpp_bop2022.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI), code: https://github.com/shanice-l/gdrnpp_bop2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-21T00:00:00Z">2025-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">53</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM+MAP: Bimanual <span class="highlight-title">Robo</span>t Task Planning using Large Language Models and
  Planning Domain Definition Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Chu, Xufeng Zhao, Cornelius Weber, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bimanual robotic manipulation provides significant versatility, but also
presents an inherent challenge due to the complexity involved in the spatial
and temporal coordination between two hands. Existing works predominantly focus
on attaining human-level manipulation skills for robotic hands, yet little
attention has been paid to task planning on long-horizon timescales. With their
outstanding in-context learning and zero-shot generation abilities, Large
Language Models (LLMs) have been applied and grounded in diverse robotic
embodiments to facilitate task planning. However, LLMs still suffer from errors
in long-horizon reasoning and from hallucinations in complex robotic tasks,
lacking a guarantee of logical correctness when generating the plan. Previous
works, such as LLM+P, extended LLMs with symbolic planners. However, none have
been successfully applied to bimanual robots. New challenges inevitably arise
in bimanual manipulation, necessitating not only effective task decomposition
but also efficient task allocation. To address these challenges, this paper
introduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning
and multi-agent planning, automating effective and efficient bimanual task
planning. We conduct simulated experiments on various long-horizon manipulation
tasks of differing complexity. Our method is built using GPT-4o as the backend,
and we compare its performance against plans generated directly by LLMs,
including GPT-4o, V3 and also recent strong reasoning models o1 and R1. By
analyzing metrics such as planning time, success rate, group debits, and
planning-step reduction rate, we demonstrate the superior performance of
LLM+MAP, while also providing insights into robotic reasoning. Code is
available at https://github.com/Kchu/LLM-MAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at https://github.com/Kchu/LLM-MAP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control the Soft <span class="highlight-title">Robo</span>t Arm with its Physical Twin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinghua Guan, Hung Hon Cheng, Benhui Dai, Josie Hughes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To exploit the compliant capabilities of soft robot arms we require
controller which can exploit their physical capabilities. Teleoperation,
leveraging a human in the loop, is a key step towards achieving more complex
control strategies. Whilst teleoperation is widely used for rigid robots, for
soft robots we require teleoperation methods where the configuration of the
whole body is considered. We propose a method of using an identical 'physical
twin', or demonstrator of the robot. This tendon robot can be back-driven, with
the tendon lengths providing configuration perception, and enabling a direct
mapping of tendon lengths for the execture. We demonstrate how this
teleoperation across the entire configuration of the robot enables complex
interactions with exploit the envrionment, such as squeezing into gaps. We also
show how this method can generalize to robots which are a larger scale that the
physical twin, and how, tuneability of the stiffness properties of the physical
twin simplify its use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Language Models for Out-of-Distribution Recovery in
  <span class="highlight-title">Reinforcement Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Kim, Seung-Woo Seo, Seong-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) has demonstrated strong performance in
robotic control but remains susceptible to out-of-distribution (OOD) states,
often resulting in unreliable actions and task failure. While previous methods
have focused on minimizing or preventing OOD occurrences, they largely neglect
recovery once an agent encounters such states. Although the latest research has
attempted to address this by guiding agents back to in-distribution states,
their reliance on uncertainty estimation hinders scalability in complex
environments. To overcome this limitation, we introduce Language Models for
Out-of-Distribution Recovery (LaMOuR), which enables recovery learning without
relying on uncertainty estimation. LaMOuR generates dense reward codes that
guide the agent back to a state where it can successfully perform its original
task, leveraging the capabilities of LVLMs in image description, logical
reasoning, and code generation. Experimental results show that LaMOuR
substantially enhances recovery efficiency across diverse locomotion tasks and
even generalizes effectively to complex environments, including humanoid
locomotion and mobile manipulation, where existing methods struggle. The code
and supplementary materials are available at
\href{https://lamour-rl.github.io/}{https://lamour-rl.github.io/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAA-TSO: Geometry-Aware Assisted Depth Completion for Transparent and
  Specular Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Liu, Tong Jia, Da Cai, Hao Wang, Dongyue Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transparent and specular objects are frequently encountered in daily life,
factories, and laboratories. However, due to the unique optical properties, the
depth information on these objects is usually incomplete and inaccurate, which
poses significant challenges for downstream robotics tasks. Therefore, it is
crucial to accurately restore the depth information of transparent and specular
objects. Previous depth completion methods for these objects usually use RGB
information as an additional channel of the depth image to perform depth
prediction. Due to the poor-texture characteristics of transparent and specular
objects, these methods that rely heavily on color information tend to generate
structure-less depth predictions. Moreover, these 2D methods cannot effectively
explore the 3D structure hidden in the depth channel, resulting in depth
ambiguity. To this end, we propose a geometry-aware assisted depth completion
method for transparent and specular objects, which focuses on exploring the 3D
structural cues of the scene. Specifically, besides extracting 2D features from
RGB-D input, we back-project the input depth to a point cloud and build the 3D
branch to extract hierarchical scene-level 3D structural features. To exploit
3D geometric information, we design several gated cross-modal fusion modules to
effectively propagate multi-level 3D geometric features to the image branch. In
addition, we propose an adaptive correlation aggregation strategy to
appropriately assign 3D features to the corresponding 2D features. Extensive
experiments on ClearGrasp, OOD, TransCG, and STD datasets show that our method
outperforms other state-of-the-art methods. We further demonstrate that our
method significantly enhances the performance of downstream robotic grasping
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring psychophysiological methods for human-<span class="highlight-title">robo</span>t collaboration in
  construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saika Wong, Zhentao Chen, Mi Pan, Miroslaw J. Skibniewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Psychophysiological methods present a promising approach to fostering
enhanced mutual communication and collaboration between human workers and
robots. Despite their potential, there is still limited understanding of how to
effectively integrate psychophysiological methods to improve human-robot
collaboration (HRC) in construction. This paper addresses this gap by
critically reviewing the use of psychophysiological methods for HRC within
construction environments, employing a concept-methodology-value philosophical
framework. The analysis reveals that measuring brain activity using
electroencephalography is the most widely used method, while most of the works
are still at the proof of concept stage and lack empirical evidence. Three
potential research directions were proposed: the integration of multi-modal
psychophysiological signals, enriching the existing experimental settings for
better generalizability, and leveraging advanced biocompatible or contactless
technologies for effective signal detection. The findings should benefit
subsequent exploration and practical applications of psychophysiological
methods to enable better implementation of robots and support HRC in
construction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HAPI: A Model for Learning <span class="highlight-title">Robo</span>t Facial Expressions from Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsheng Yang, Qianying Liu, Wataru Sato, Takashi Minato, Chaoran Liu, Shin'ya Nishida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic robotic facial expression generation is crucial for human-robot
interaction, as handcrafted methods based on fixed joint configurations often
yield rigid and unnatural behaviors. Although recent automated techniques
reduce the need for manual tuning, they tend to fall short by not adequately
bridging the gap between human preferences and model predictions-resulting in a
deficiency of nuanced and realistic expressions due to limited degrees of
freedom and insufficient perceptual integration. In this work, we propose a
novel learning-to-rank framework that leverages human feedback to address this
discrepancy and enhanced the expressiveness of robotic faces. Specifically, we
conduct pairwise comparison annotations to collect human preference data and
develop the Human Affective Pairwise Impressions (HAPI) model, a Siamese
RankNet-based approach that refines expression evaluation. Results obtained via
Bayesian Optimization and online expression survey on a 35-DOF android platform
demonstrate that our approach produces significantly more realistic and
socially resonant expressions of Anger, Happiness, and Surprise than those
generated by baseline and expert-designed methods. This confirms that our
framework effectively bridges the gap between human preferences and model
predictions while robustly aligning robotic expression generation with human
affective responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Behavioral Conflict Avoidance Between Humans and Quadruped <span class="highlight-title">Robo</span>ts in
  Shared Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Wei, Muhua Zhang, Yun Gan, Deqing Huang, Lei Ma, Chenguang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, robots are increasingly operated in environments shared with
humans, where conflicts between human and robot behaviors may compromise
safety. This paper presents a proactive behavioral conflict avoidance framework
based on the principle of adaptation to trends for quadruped robots that not
only ensures the robot's safety but also minimizes interference with human
activities. It can proactively avoid potential conflicts with approaching
humans or other dynamic objects, whether the robot is stationary or in motion,
then swiftly resume its tasks once the conflict subsides. An enhanced approach
is proposed to achieve precise human detection and tracking on vibratory robot
platform equipped with low-cost hybrid solid-state LiDAR. When potential
conflict detected, the robot selects an avoidance point and executes an evasion
maneuver before resuming its task. This approach contrasts with conventional
methods that remain goal-driven, often resulting in aggressive behaviors, such
as forcibly bypassing obstacles and causing conflicts or becoming stuck in
deadlock scenarios. The selection of avoidance points is achieved by
integrating static and dynamic obstacle to generate a potential field map. The
robot then searches for feasible regions within this map and determines the
optimal avoidance point using an evaluation function. Experimental results
demonstrate that the framework significantly reduces interference with human
activities, enhances the safety of both robots and persons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures. This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Exploration-Based Precise Mapping for Mobile <span class="highlight-title">Robo</span>ts through
  Stepwise and Consistent Motions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhua Zhang, Lei Ma, Ying Wu, Kai Shen, Yongkui Sun, Henry Leung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an autonomous exploration framework. It is designed for
indoor ground mobile robots that utilize laser Simultaneous Localization and
Mapping (SLAM), ensuring process completeness and precise mapping results. For
frontier search, the local-global sampling architecture based on multiple
Rapidly Exploring Random Trees (RRTs) is employed. Traversability checks during
RRT expansion and global RRT pruning upon map updates eliminate unreachable
frontiers, reducing potential collisions and deadlocks. Adaptive sampling
density adjustments, informed by obstacle distribution, enhance exploration
coverage potential. For frontier point navigation, a stepwise consistent motion
strategy is adopted, wherein the robot strictly drives straight on
approximately equidistant line segments in the polyline path and rotates in
place at segment junctions. This simplified, decoupled motion pattern improves
scan-matching stability and mitigates map drift. For process control, the
framework serializes frontier point selection and navigation, avoiding
oscillation caused by frequent goal changes in conventional parallelized
processes. The waypoint retracing mechanism is introduced to generate repeated
observations, triggering loop closure detection and backend optimization in
graph-based SLAM, thereby improving map consistency and precision. Experiments
in both simulation and real-world scenarios validate the effectiveness of the
framework. It achieves improved mapping coverage and precision in more
challenging environments compared to baseline 2D exploration algorithms. It
also shows robustness in supporting resource-constrained robot platforms and
maintaining mapping consistency across various LiDAR field-of-view (FoV)
configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures. This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Targetless 6DoF Calibration of <span class="highlight-title">LiDAR</span> and 2D Scanning Radar Based on
  Cylindrical Occupancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weimin Wang, Yu Du, Ting Yang, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to the capability for reliable and all-weather long-range sensing, the
fusion of LiDAR and Radar has been widely applied to autonomous vehicles for
robust perception. In practical operation, well manually calibrated extrinsic
parameters, which are crucial for the fusion of multi-modal sensors, may drift
due to the vibration. To address this issue, we present a novel targetless
calibration approach, termed LiRaCo, for the extrinsic 6DoF calibration of
LiDAR and Radar sensors. Although both types of sensors can obtain geometric
information, bridging the geometric correspondences between multi-modal data
without any clues of explicit artificial markers is nontrivial, mainly due to
the low vertical resolution of scanning Radar. To achieve the targetless
calibration, LiRaCo leverages a spatial occupancy consistency between LiDAR
point clouds and Radar scans in a common cylindrical representation,
considering the increasing data sparsity with distance for both sensors.
Specifically, LiRaCo expands the valid Radar scanned pixels into 3D occupancy
grids to constrain LiDAR point clouds based on spatial consistency.
Consequently, a cost function involving extrinsic calibration parameters is
formulated based on the spatial overlap of 3D grids and LiDAR points. Extrinsic
parameters are finally estimated by optimizing the cost function. Comprehensive
quantitative and qualitative experiments on two real outdoor datasets with
different LiDAR sensors demonstrate the feasibility and accuracy of the
proposed method. The source code will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending Behavior Trees for <span class="highlight-title">Robo</span>tic Missions with Quality Requirements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razan Ghzouli, Rebekka Wohlrab, Jennifer Horkoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context and motivation: In recent years, behavior trees have gained growing
interest within the robotics community as a specification and control switching
mechanism for the different tasks that form a robotics mission. Problem: Given
the rising complexity and prevalence of robotic systems, it is increasingly
challenging and important for practitioners to design high-quality missions
that meet certain qualities, for instance, to consider potential failures or
mitigate safety risks. In software requirements engineering, quality or
non-functional requirements have long been recognized as a key factor in system
success. Currently, qualities are not represented in behavior tree models,
which capture a robotic mission, making it difficult to assess the extent to
which different mission components comply with those qualities. Principal
ideas: In this paper, we propose an extension for behavior trees to have
qualities and quality requirements explicitly represented in robotics missions.
We provide a meta-model for the extension, develop a domain-specific language
(DSL), and describe how we integrated our DSL in one of the most used languages
in robotics for developing behavior trees, BehaviorTree.CPP. A preliminary
evaluation of the implemented DSL shows promising results for the feasibility
of our approach and the need for similar DSLs. Contribution: Our approach paves
the way for incorporating qualities into the behavior model of robotics
missions. This promotes early expression of qualities in robotics missions, and
a better overview of missions components and their contribution to the
satisfaction of quality concerns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, Requirements Engineering: Foundation for
  Software Quality (REFSQ) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Somatic Safety: An Embodied Approach Towards Safe Human-<span class="highlight-title">Robo</span>t
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steve Benford, Eike Schneiders, Juan Pablo Martinez Avila, Praminda Caleb-Solly, Patrick Robert Brundell, Simon Castle-Green, Feng Zhou, Rachael Garrett, Kristina Höök, Sarah Whatley, Kate Marsh, Paul Tennent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robots enter the messy human world so the vital matter of safety takes on
a fresh complexion with physical contact becoming inevitable and even
desirable. We report on an artistic-exploration of how dancers, working as part
of a multidisciplinary team, engaged in contact improvisation exercises to
explore the opportunities and challenges of dancing with cobots. We reveal how
they employed their honed bodily senses and physical skills to engage with the
robots aesthetically and yet safely, interleaving improvised physical
manipulations with reflections to grow their knowledge of how the robots
behaved and felt. We introduce somatic safety, a holistic mind-body approach in
which safety is learned, felt and enacted through bodily contact with robots in
addition to being reasoned about. We conclude that robots need to be better
designed for people to hold them and might recognise tacit safety cues among
people.We propose that safety should be learned through iterative bodily
experience interleaved with reflection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM/IEEE International Conference on Human-Robot Interaction (HRI'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reachability-Guaranteed Optimal Control for the Interception of Dynamic
  Targets under Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso Faraci, Roberto Lampariello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intercepting dynamic objects in uncertain environments involves a significant
unresolved challenge in modern robotic systems. Current control approaches rely
solely on estimated information, and results lack guarantees of robustness and
feasibility. In this work, we introduce a novel method to tackle the
interception of targets whose motion is affected by known and bounded
uncertainty. Our approach introduces new techniques of reachability analysis
for rigid bodies, leveraged to guarantee feasibility of interception under
uncertain conditions. We then propose a Reachability-Guaranteed Optimal Control
Problem, ensuring robustness and guaranteed reachability to a target set of
configurations. We demonstrate the methodology in the case study of an
interception maneuver of a tumbling target in space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rude Humans and Vengeful <span class="highlight-title">Robo</span>ts: Examining Human Perceptions of <span class="highlight-title">Robo</span>t
  Retaliatory Intentions in Professional Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Letheren, Nicole Robinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans and robots are increasingly working in personal and professional
settings. In workplace settings, humans and robots may work together as
colleagues, potentially leading to social expectations, or violation thereof.
Extant research has primarily sought to understand social interactions and
expectations in personal rather than professional settings, and none of these
studies have examined negative outcomes arising from violations of social
expectations. This paper reports the results of a 2x3 online experiment that
used a unique first-person perspective video to immerse participants in a
collaborative workplace setting. The results are nuanced and reveal that while
robots are expected to act in accordance with social expectations despite human
behavior, there are benefits for robots perceived as being the bigger person in
the face of human rudeness. Theoretical and practical implications are provided
which discuss the import of these findings for the design of social robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the author version of the manuscript submitted to ACM
  Transactions on Human-Robot Interaction. The final version, if accepted, will
  be published by ACM and available via the ACM Digital Library. 12 pages, 1
  figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Human Locomotion Analysis in Lower-Limb Exoskeletons:
  A Comparative Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Coser, Christian Tamantini, Matteo Tortora, Leonardo Furia, Rosa Sicilia, Loredana Zollo, Paolo Soda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wearable robotics for lower-limb assistance have become a pivotal area of
research, aiming to enhance mobility for individuals with physical impairments
or augment the performance of able-bodied users. Accurate and adaptive control
systems are essential to ensure seamless interaction between the wearer and the
robotic device, particularly when navigating diverse and dynamic terrains.
Despite the recent advances in neural networks for time series analysis, no
attempts have been directed towards the classification of ground conditions,
categorized into five classes and subsequently determining the ramp's slope and
stair's height. In this respect, this paper presents an experimental comparison
between eight deep neural network backbones to predict high-level locomotion
parameters across diverse terrains.
  All the models are trained on the publicly available CAMARGO 2021 dataset.
IMU-only data equally or outperformed IMU+EMG inputs, promoting a
cost-effective and efficient design. Indeeds, using three IMU sensors, the LSTM
achieved high terrain classification accuracy (0.94 +- 0.04) and precise ramp
slope (1.95 +- 0.58{\deg}) and the CNN-LSTM a stair height (15.65 +- 7.40 mm)
estimations. As a further contribution, SHAP analysis justified sensor
reduction without performance loss, ensuring a lightweight setup. The system
operates with ~2 ms inference time, supporting real-time applications. The code
is code available at
https://github.com/cosbidev/Human-Locomotion-Identification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe On-Orbit Dislodging of Deployable Structures via Robust Adaptive
  MPC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longsen Gao, Claus Danielson, Andrew Kwas, Rafael Fierro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel robust adaptive model predictive controller for
on-orbit dislodging. We consider the scenario where a servicer, equipped with a
robot arm, must dislodge a client, a time-varying system composed of an
underpowered jammed solar panel with a hybrid hinge system on a space station.
Our approach leverages online set-membership identification to reduce the
uncertainty to provide robust safety guarantees during dislodging despite
bounded disturbances while balancing exploration and exploitation effectively
in the parameter space. The feasibility of the developed robust adaptive MPC
method is also examined through dislodging simulations and hardware experiments
in zero-gravity and gravity environments, respectively. In addition, the
advantages of our method are shown through comparison experiments with several
state-of-the-art control schemes for both accuracy of parameter estimation and
control performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to IEEE Transactions on Control Systems
  Technology and is being reviewed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyue Guo, Jiarui Hu, Junjie Hu, Hujun Bao, Guofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, camera-based solutions have been extensively explored for scene
semantic completion (SSC). Despite their success in visible areas, existing
methods struggle to capture complete scene semantics due to frequent visual
occlusions. To address this limitation, this paper presents the first
satellite-ground cooperative SSC framework, i.e., SGFormer, exploring the
potential of satellite-ground image pairs in the SSC task. Specifically, we
propose a dual-branch architecture that encodes orthogonal satellite and ground
views in parallel, unifying them into a common domain. Additionally, we design
a ground-view guidance strategy that corrects satellite image biases during
feature encoding, addressing misalignment between satellite and ground views.
Moreover, we develop an adaptive weighting strategy that balances contributions
from satellite and ground views. Experiments demonstrate that SGFormer
outperforms the state of the art on SemanticKITTI and SSCBench-KITTI-360
datasets. Our code is available on https://github.com/gxytcrc/SGFormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DyWA: Dynamics-adaptive World Action Model for Generalizable
  Non-prehensile <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangran Lyu, Ziming Li, Xuesong Shi, Chaoyi Xu, Yizhou Wang, <span class="highlight-author">He Wang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonprehensile manipulation is crucial for handling objects that are too thin,
large, or otherwise ungraspable in unstructured environments. While
conventional planning-based approaches struggle with complex contact modeling,
learning-based methods have recently emerged as a promising alternative.
However, existing learning-based approaches face two major limitations: they
heavily rely on multi-view cameras and precise pose tracking, and they fail to
generalize across varying physical conditions, such as changes in object mass
and table friction. To address these challenges, we propose the
Dynamics-Adaptive World Action Model (DyWA), a novel framework that enhances
action learning by jointly predicting future states while adapting to dynamics
variations based on historical trajectories. By unifying the modeling of
geometry, state, physics, and robot actions, DyWA enables more robust policy
learning under partial observability. Compared to baselines, our method
improves the success rate by 31.5% using only single-view point cloud
observations in the simulation. Furthermore, DyWA achieves an average success
rate of 68% in real-world experiments, demonstrating its ability to generalize
across diverse object geometries, adapt to varying table friction, and
robustness in challenging scenarios such as half-filled water bottles and
slippery surfaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page:https://pku-epic.github.io/DyWA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BEAC: Imitating Complex Exploration and Task-oriented Behaviors for
  Invisible Object Nonprehensile <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hirotaka Tahara, Takamitsu Matsubara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying imitation learning (IL) is challenging to nonprehensile manipulation
tasks of invisible objects with partial observations, such as excavating buried
rocks. The demonstrator must make such complex action decisions as exploring to
find the object and task-oriented actions to complete the task while estimating
its hidden state, perhaps causing inconsistent action demonstration and high
cognitive load problems. For these problems, work in human cognitive science
suggests that promoting the use of pre-designed, simple exploration rules for
the demonstrator may alleviate the problems of action inconsistency and high
cognitive load. Therefore, when performing imitation learning from
demonstrations using such exploration rules, it is important to accurately
imitate not only the demonstrator's task-oriented behavior but also his/her
mode-switching behavior (exploratory or task-oriented behavior) under partial
observation. Based on the above considerations, this paper proposes a novel
imitation learning framework called Belief Exploration-Action Cloning (BEAC),
which has a switching policy structure between a pre-designed exploration
policy and a task-oriented action policy trained on the estimated belief states
based on past history. In simulation and real robot experiments, we confirmed
that our proposed method achieved the best task performance, higher mode and
action prediction accuracies, while reducing the cognitive load in the
demonstration indicated by a user study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Displacement-Actuated Continuum <span class="highlight-title">Robo</span>ts: A Joint Space Abstraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reinhard M. Grassmann, Jessica Burgner-Kahrs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The displacement-actuated continuum robot as an abstraction has been shown as
a key abstraction to significantly simplify and improve approaches due to its
relation to the Clarke transform. To highlight further potentials, we revisit
and extend this abstraction that features an increasingly popular length
extension and an underutilized twisting. For each extension, the corresponding
mapping from the joint values to the local coordinates of the manifold embedded
in the joint spaces is provided. Each mapping is characterized by its
compactness and linearity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Drone: Aerial Additive Manufacturing with Drones Planned Using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Raman, Chad Merrill, Abraham George, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Additive manufacturing (AM) has transformed the production landscape by
enabling the precision creation of complex geometries. However, AM faces
limitations when applied to challenging environments, such as elevated surfaces
and remote locations. Aerial additive manufacturing, facilitated by drones,
presents a solution to these challenges. However, despite advances in methods
for the planning, control, and localization of drones, the accuracy of these
methods is insufficient to run traditional feedforward extrusion-based additive
manufacturing processes (such as Fused Deposition Manufacturing). Recently, the
emergence of LLMs has revolutionized various fields by introducing advanced
semantic reasoning and real-time planning capabilities. This paper proposes the
integration of LLMs with aerial additive manufacturing to assist with the
planning and execution of construction tasks, granting greater flexibility and
enabling a feed-back based design and construction system. Using the semantic
understanding and adaptability of LLMs, we can overcome the limitations of
drone based systems by dynamically generating and adapting building plans on
site, ensuring efficient and accurate construction even in constrained
environments. Our system is able to design and build structures given only a
semantic prompt and has shown success in understanding the spatial environment
despite tight planning constraints. Our method's feedback system enables
replanning using the LLM if the manufacturing process encounters unforeseen
errors, without requiring complicated heuristics or evaluation functions.
Combining the semantic planning with automatic error correction, our system
achieved a 90% build accuracy, converting simple text prompts to build
structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 Pages, 6 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shear-based <span class="highlight-title">Grasp</span> Control for Multi-fingered Underactuated Tactile
  <span class="highlight-title">Robo</span>tic Hands 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher J. Ford, Haoran Li, Manuel G. Catalano, Matteo Bianchi, Efi Psomopoulou, Nathan F. Lepora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a shear-based control scheme for grasping and
manipulating delicate objects with a Pisa/IIT anthropomorphic SoftHand equipped
with soft biomimetic tactile sensors on all five fingertips. These `microTac'
tactile sensors are miniature versions of the TacTip vision-based tactile
sensor, and can extract precise contact geometry and force information at each
fingertip for use as feedback into a controller to modulate the grasp while a
held object is manipulated. Using a parallel processing pipeline, we
asynchronously capture tactile images and predict contact pose and force from
multiple tactile sensors. Consistent pose and force models across all sensors
are developed using supervised deep learning with transfer learning techniques.
We then develop a grasp control framework that uses contact force feedback from
all fingertip sensors simultaneously, allowing the hand to safely handle
delicate objects even under external disturbances. This control framework is
applied to several grasp-manipulation experiments: first, retaining a flexible
cup in a grasp without crushing it under changes in object weight; second, a
pouring task where the center of mass of the cup changes dynamically; and
third, a tactile-driven leader-follower task where a human guides a held
object. These manipulation tasks demonstrate more human-like dexterity with
underactuated robotic hands by using fast reflexive control from tactile
sensing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages. 10 figures. Accepted in IEEE Transactions on Robotics,
  Special Section on Tactile Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Splat-LOAM: Gaussian Splatting <span class="highlight-title">LiDAR</span> Odometry and Mapping <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Giacomini, Luca Di Giammarino, Lorenzo De Rebotti, Giorgio Grisetti, Martin R. Oswald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDARs provide accurate geometric measurements, making them valuable for
ego-motion estimation and reconstruction tasks. Although its success, managing
an accurate and lightweight representation of the environment still poses
challenges. Both classic and NeRF-based solutions have to trade off accuracy
over memory and processing times. In this work, we build on recent advancements
in Gaussian Splatting methods to develop a novel LiDAR odometry and mapping
pipeline that exclusively relies on Gaussian primitives for its scene
representation. Leveraging spherical projection, we drive the refinement of the
primitives uniquely from LiDAR measurements. Experiments show that our approach
matches the current registration performance, while achieving SOTA results for
mapping tasks with minimal GPU requirements. This efficiency makes it a strong
candidate for further exploration and potential adoption in real-time robotics
estimation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TamedPUMA: safe and stable imitation learning with geometric fabrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saray Bakker, Rodrigo Pérez-Dattari, Cosimo Della Santina, Wendelin Böhmer, Javier Alonso-Mora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using the language of dynamical systems, Imitation learning (IL) provides an
intuitive and effective way of teaching stable task-space motions to robots
with goal convergence. Yet, IL techniques are affected by serious limitations
when it comes to ensuring safety and fulfillment of physical constraints. With
this work, we solve this challenge via TamedPUMA, an IL algorithm augmented
with a recent development in motion generation called geometric fabrics. As
both the IL policy and geometric fabrics describe motions as artificial
second-order dynamical systems, we propose two variations where IL provides a
navigation policy for geometric fabrics. The result is a stable imitation
learning strategy within which we can seamlessly blend geometrical constraints
like collision avoidance and joint limits. Beyond providing a theoretical
analysis, we demonstrate TamedPUMA with simulated and real-world tasks,
including a 7-DoF manipulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages (10+4), 1+3*5 figures, 1 table, preprint version of accepted
  paper at L4DC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiNash-PF: A Particle Filtering Approach for Computing Multiple Local
  Generalized Nash Equilibria in Trajectory Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maulik Bhatt, Iman Askari, Yue Yu, Ufuk Topcu, Huazhen Fang, Negar Mehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern robotic systems frequently engage in complex multi-agent interactions,
many of which are inherently multi-modal, meaning they can lead to multiple
distinct outcomes. To interact effectively, robots must recognize the possible
interaction modes and adapt to the one preferred by other agents. In this work,
we propose an efficient algorithm for capturing the multimodality in
multi-agent interactions. We leverage a game-theoretic planner to model
interaction outcomes as equilibria where \emph{each equilibrium} corresponds to
a distinct interaction \emph{mode}. We then develop an efficient algorithm to
identify all the equilibria, allowing robots to reason about multiple
interaction modes. More specifically, we formulate interactive planning as
Constrained Potential Trajectory Games (CPTGs) and model interaction outcomes
by local Generalized Nash equilibria (GNEs) of the game. CPTGs are a class of
games for which a local GNE can be found by solving a single constrained
optimal control problem where a potential function is minimized. We propose to
integrate the potential game approach with implicit particle filtering, a
sample-efficient method for non-convex trajectory optimization. We utilize
implicit particle filtering to identify the coarse estimates of multiple local
minimizers of the game's potential function. MultiNash-PF then refines these
estimates with optimization solvers, obtaining different local GNEs. We show
through numerical simulations that MultiNash-PF reduces computation time by up
to 50\% compared to a baseline. We further demonstrate the effectiveness of our
algorithm in real-world human-robot interaction scenarios, where it
successfully accounts for the multi-modal nature of interactions and resolves
potential conflicts in real-time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Strategic Decision-Making in Multi-Agent Domains: A Weighted Constrained
  Potential Dynamic Game Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05876v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05876v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maulik Bhatt, Yixuan Jia, Negar Mehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In interactive multi-agent settings, decision-making and planning are
challenging mainly due to the agents' interconnected objectives. Dynamic game
theory offers a formal framework for analyzing such intricacies. Yet, solving
constrained dynamic games and determining the interaction outcome in the form
of generalized Nash Equilibria (GNE) pose computational challenges due to the
need for solving constrained coupled optimal control problems. In this paper,
we address this challenge by proposing to leverage the special structure of
many real-world multi-agent interactions. More specifically, our key idea is to
leverage constrained dynamic potential games, which are games for which GNE can
be found by solving a single constrained optimal control problem associated
with minimizing the potential function. We argue that constrained dynamic
potential games can effectively facilitate interactive decision-making in many
multi-agent interactions. We will identify structures in realistic multi-agent
interactive scenarios that can be transformed into weighted constrained
potential dynamic games (WCPDGs). We will show that the GNE of the resulting
WCPDG can be obtained by solving a single constrained optimal control problem.
We will demonstrate the effectiveness of the proposed method through various
simulation studies and show that we achieve significant improvements in solve
time compared to state-of-the-art game solvers. We further provide experimental
validation of our proposed method in a navigation setup involving two
quadrotors carrying a rigid object while avoiding collisions with two humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in IEEE Transactions on Robotics 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOUS VIDE: Cooking <span class="highlight-title">Visual</span> Drone Navigation Policies in a Gaussian
  Splatting Vacuum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JunEn Low, Maximilian Adang, Javier Yu, Keiko Nagami, Mac Schwager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new simulator, training approach, and policy architecture,
collectively called SOUS VIDE, for end-to-end visual drone navigation. Our
trained policies exhibit zero-shot sim-to-real transfer with robust real-world
performance using only onboard perception and computation. Our simulator,
called FiGS, couples a computationally simple drone dynamics model with a high
visual fidelity Gaussian Splatting scene reconstruction. FiGS can quickly
simulate drone flights producing photorealistic images at up to 130 fps. We use
FiGS to collect 100k-300k image/state-action pairs from an expert MPC with
privileged state and dynamics information, randomized over dynamics parameters
and spatial disturbances. We then distill this expert MPC into an end-to-end
visuomotor policy with a lightweight neural architecture, called SV-Net. SV-Net
processes color image, optical flow and IMU data streams into low-level thrust
and body rate commands at 20 Hz onboard a drone. Crucially, SV-Net includes a
learned module for low-level control that adapts at runtime to variations in
drone dynamics. In a campaign of 105 hardware experiments, we show SOUS VIDE
policies to be robust to 30% mass variations, 40 m/s wind gusts, 60% changes in
ambient brightness, shifting or removing objects from the scene, and people
moving aggressively through the drone's visual field. Code, data, and
experiment videos can be found on our project page:
https://stanfordmsl.github.io/SousVide/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Social Gesture Recognition in spHRI: Leveraging Fabric-Based Tactile
  Sensing on Humanoid <span class="highlight-title">Robo</span>ts <span class="chip">ICRA 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03234v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03234v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dakarai Crowder, Kojo Vandyck, Xiping Sun, James McCann, Wenzhen Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans are able to convey different messages using only touch. Equipping
robots with the ability to understand social touch adds another modality in
which humans and robots can communicate. In this paper, we present a social
gesture recognition system using a fabric-based, large-scale tactile sensor
placed onto the arms of a humanoid robot. We built a social gesture dataset
using multiple participants and extracted temporal features for classification.
By collecting tactile data on a humanoid robot, our system provides insights
into human-robot social touch, and displays that the use of fabric based
sensors could be a potential way of advancing the development of spHRI systems
for more natural and effective communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 25. 8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Policy: Learning Disentangled Action Space for Multi-Task
  <span class="highlight-title">Robo</span>tic <span class="highlight-title">Manipulation</span> <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18707v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18707v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Wu, Yichen Zhu, Jinming Li, Junjie Wen, Ning Liu, Zhiyuan Xu, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning visuomotor policy for multi-task robotic manipulation has been a
long-standing challenge for the robotics community. The difficulty lies in the
diversity of action space: typically, a goal can be accomplished in multiple
ways, resulting in a multimodal action distribution for a single task. The
complexity of action distribution escalates as the number of tasks increases.
In this work, we propose \textbf{Discrete Policy}, a robot learning method for
training universal agents capable of multi-task manipulation skills. Discrete
Policy employs vector quantization to map action sequences into a discrete
latent space, facilitating the learning of task-specific codes. These codes are
then reconstructed into the action space conditioned on observations and
language instruction. We evaluate our method on both simulation and multiple
real-world embodiments, including both single-arm and bimanual robot settings.
We demonstrate that our proposed Discrete Policy outperforms a well-established
Diffusion Policy baseline and many state-of-the-art approaches, including ACT,
Octo, and OpenVLA. For example, in a real-world multi-task training setting
with five tasks, Discrete Policy achieves an average success rate that is 26\%
higher than Diffusion Policy and 15\% higher than OpenVLA. As the number of
tasks increases to 12, the performance gap between Discrete Policy and
Diffusion Policy widens to 32.5\%, further showcasing the advantages of our
approach. Our work empirically demonstrates that learning multi-task policies
within the latent space is a vital step toward achieving general-purpose
agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SuperPC: A Single <span class="highlight-title">Diffusion</span> Model for Point Cloud Completion,
  Upsampling, Denoising, and Colorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Du, Zhipeng Zhao, Shaoshu Su, Sharath Golluri, Haoze Zheng, Runmao Yao, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud (PC) processing tasks-such as completion, upsampling, denoising,
and colorization-are crucial in applications like autonomous driving and 3D
reconstruction. Despite substantial advancements, prior approaches often
address each of these tasks independently, with separate models focused on
individual issues. However, this isolated approach fails to account for the
fact that defects like incompleteness, low resolution, noise, and lack of color
frequently coexist, with each defect influencing and correlating with the
others. Simply applying these models sequentially can lead to error
accumulation from each model, along with increased computational costs. To
address these challenges, we introduce SuperPC, the first unified diffusion
model capable of concurrently handling all four tasks. Our approach employs a
three-level-conditioned diffusion framework, enhanced by a novel
spatial-mix-fusion strategy, to leverage the correlations among these four
defects for simultaneous, efficient processing. We show that SuperPC
outperforms the state-of-the-art specialized models as well as their
combination on all four individual tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping Object-level Planning with Large Language Models <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12262v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12262v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Paulius, Alejandro Agostini, Benedict Quartey, George Konidaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new method that extracts knowledge from a large language model
(LLM) to produce object-level plans, which describe high-level changes to
object state, and uses them to bootstrap task and motion planning (TAMP).
Existing work uses LLMs to directly output task plans or generate goals in
representations like PDDL. However, these methods fall short because they rely
on the LLM to do the actual planning or output a hard-to-satisfy goal. Our
approach instead extracts knowledge from an LLM in the form of plan schemas as
an object-level representation called functional object-oriented networks
(FOON), from which we automatically generate PDDL subgoals. Our method markedly
outperforms alternative planning strategies in completing several
pick-and-place tasks in simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2025; 11 pages (6 pages + 1 page references + 4
  pages appendix); for demo videos, please see
  https://davidpaulius.github.io/olp_llm/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bayesian Modeling Framework for Estimation and Ground Segmentation of
  Cluttered Staircases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasanna Sriganesh, Burhanuddin Shirose, Matthew Travers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous robot navigation in complex environments requires robust
perception as well as high-level scene understanding due to perceptual
challenges, such as occlusions, and uncertainty introduced by robot movement.
For example, a robot climbing a cluttered staircase can misinterpret clutter as
a step, misrepresenting the state and compromising safety. This requires robust
state estimation methods capable of inferring the underlying structure of the
environment even from incomplete sensor data. In this paper, we introduce a
novel method for robust state estimation of staircases. To address the
challenge of perceiving occluded staircases extending beyond the robot's
field-of-view, our approach combines an infinite-width staircase representation
with a finite endpoint state to capture the overall staircase structure. This
representation is integrated into a Bayesian inference framework to fuse noisy
measurements enabling accurate estimation of staircase location even with
partial observations and occlusions. Additionally, we present a segmentation
algorithm that works in conjunction with the staircase estimation pipeline to
accurately identify clutter-free regions on a staircase. Our method is
extensively evaluated on real robot across diverse staircases, demonstrating
significant improvements in estimation accuracy and segmentation performance
compared to baseline approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contraction Theory for Nonlinear Stability Analysis and Learning-based
  Control: A Tutorial <span class="highlight-title">Overview</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.00675v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.00675v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroyasu Tsukamoto, Soon-Jo Chung, Jean-Jacques E. Slotine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contraction theory is an analytical tool to study differential dynamics of a
non-autonomous (i.e., time-varying) nonlinear system under a contraction metric
defined with a uniformly positive definite matrix, the existence of which
results in a necessary and sufficient characterization of incremental
exponential stability of multiple solution trajectories with respect to each
other. By using a squared differential length as a Lyapunov-like function, its
nonlinear stability analysis boils down to finding a suitable contraction
metric that satisfies a stability condition expressed as a linear matrix
inequality, indicating that many parallels can be drawn between well-known
linear systems theory and contraction theory for nonlinear systems.
Furthermore, contraction theory takes advantage of a superior robustness
property of exponential stability used in conjunction with the comparison
lemma. This yields much-needed safety and stability guarantees for neural
network-based control and estimation schemes, without resorting to a more
involved method of using uniform asymptotic stability for input-to-state
stability. Such distinctive features permit the systematic construction of a
contraction metric via convex optimization, thereby obtaining an explicit
exponential bound on the distance between a time-varying target trajectory and
solution trajectories perturbed externally due to disturbances and learning
errors. The objective of this paper is, therefore, to present a tutorial
overview of contraction theory and its advantages in nonlinear stability
analysis of deterministic and stochastic systems, with an emphasis on deriving
formal robustness and stability guarantees for various learning-based and
data-driven automatic control methods. In particular, we provide a detailed
review of techniques for finding contraction metrics and associated control and
estimation laws using deep neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Annual Reviews in Control, Preprint Version, Accepted, Oct. 1st</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Code-as-Monitor: Constraint-aware <span class="highlight-title">Visual</span> Programming for Reactive and
  Proactive <span class="highlight-title">Robo</span>tic Failure Detection <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04455v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04455v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, <span class="highlight-author">He Wang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic detection and prevention of open-set failures are crucial in
closed-loop robotic systems. Recent studies often struggle to simultaneously
identify unexpected failures reactively after they occur and prevent
foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a
novel paradigm leveraging the vision-language model (VLM) for both open-set
reactive and proactive failure detection. The core of our method is to
formulate both tasks as a unified set of spatio-temporal constraint
satisfaction problems and use VLM-generated code to evaluate them for real-time
monitoring. To enhance the accuracy and efficiency of monitoring, we further
introduce constraint elements that abstract constraint-related entities or
their parts into compact geometric elements. This approach offers greater
generality, simplifies tracking, and facilitates constraint-aware visual
programming by leveraging these elements as visual prompts. Experiments show
that CaM achieves a 28.7% higher success rate and reduces execution time by
31.8% under severe disturbances compared to baselines across three simulators
and a real-world setting. Moreover, CaM can be integrated with open-loop
control policies to form closed-loop systems, enabling long-horizon tasks in
cluttered scenes with dynamic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025. Project page:
  https://zhoues.github.io/Code-as-Monitor/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven <span class="highlight-title">Camera</span> and <span class="highlight-title">Lidar</span> Simulation Models for Autonomous Driving: A
  <span class="highlight-title">Review</span> from Generative Models to Volume Renderers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Haghighi, Xiaomeng Wang, Hao Jing, Mehrdad Dianati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perception sensors, particularly camera and Lidar, are key elements of
Autonomous Driving Systems (ADS) that enable them to comprehend their
surroundings to informed driving and control decisions. Therefore, developing
realistic simulation models for these sensors is essential for conducting
effective simulation-based testing of ADS. Moreover, the rise of deep
learning-based perception models has increased the utility of sensor simulation
models for synthesising diverse training datasets. The traditional sensor
simulation models rely on computationally expensive physics-based algorithms,
specifically in complex systems such as ADS. Hence, the current potential
resides in data-driven approaches, fuelled by the exceptional performance of
deep generative models in capturing high-dimensional data distribution and
volume renderers in accurately representing scenes. This paper reviews the
current state-of-the-art data-driven camera and Lidar simulation models and
their evaluation methods. It explores a spectrum of models from the novel
perspective of generative models and volume renderers. Generative models are
discussed in terms of their input-output types, while volume renderers are
categorised based on their input encoding. Finally, the paper illustrates
commonly used evaluation techniques for assessing sensor simulation models and
highlights the existing research gaps in the area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in IEEE Transactions on Intelligent Vehicles</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global SLAM Using 5G ToA Integration: Performance Analysis with Unknown
  Base Stations and Loop Closure Alternatives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12406v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12406v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meisam Kabiri, Holger Voos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach that integrates 5G Time of Arrival (ToA)
measurements into ORB-SLAM3 to enable global localization and enhance mapping
capabilities for indoor drone navigation. We extend ORB-SLAM3's optimization
pipeline to jointly process ToA data from 5G base stations alongside visual and
inertial measurements while estimating system biases. This integration
transforms the inherently local SLAM estimates into globally referenced
trajectories and effectively resolves scale ambiguity in monocular
configurations. Our method is evaluated using both Aerolab indoor datasets with
RGB-D cameras and the EuRoC MAV benchmark, complemented by simulated 5G ToA
measurements at 28 GHz and 78 GHz frequencies using MATLAB and QuaDRiGa.
Extensive experiments across multiple SLAM configurations demonstrate that ToA
integration enables consistent global positioning across all modes while
maintaining local accuracy. For monocular configurations, ToA integration
successfully resolves scale ambiguity and improves consistency. We further
investigate scenarios with unknown base station positions and demonstrate that
ToA measurements can effectively serve as an alternative to loop closure for
drift correction. We also analyze how different geometric arrangements of base
stations impact SLAM performance. Comparative analysis with state-of-the-art
methods, including UWB-VO, confirms our approach's robustness even with lower
measurement frequencies and sequential base station operation. The results
validate that 5G ToA integration provides substantial benefits for global SLAM
applications, particularly in challenging indoor environments where accurate
positioning is critical.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesizing multi-log <span class="highlight-title">grasp</span> poses in cluttered environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvid Fälldin, Tommy Löfstedt, Tobias Semberg, Erik Wallin, Martin Servin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-object grasping is a challenging task. It is important for energy and
cost-efficient operation of industrial crane manipulators, such as those used
to collect tree logs from the forest floor and on forest machines. In this
work, we used synthetic data from physics simulations to explore how
data-driven modeling can be used to infer multi-object grasp poses from images.
We showed that convolutional neural networks can be trained specifically for
synthesizing multi-object grasps. Using RGB-Depth images and instance
segmentation masks as input, a U-Net model outputs grasp maps with the
corresponding grapple orientation and opening width. Given an observation of a
pile of logs, the model can be used to synthesize and rate the possible grasp
poses and select the most suitable one, with the possibility to respect
changing operational constraints such as lift capacity and reach. When tested
on previously unseen data, the proposed model found successful grasp poses with
an accuracy up to 96%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Set-membership target search and tracking within an unknown cluttered
  area using cooperating UAVs equipped with <span class="highlight-title">vision</span> systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15113v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15113v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Zagar, Luc Meyer, Michel Kieffer, Hélène Piet-Lahanier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of target search and tracking using a fleet
of cooperating UAVs evolving in some unknown region of interest containing an a
priori unknown number of moving ground targets. Each drone is equipped with an
embedded Computer Vision System (CVS), providing an image with labeled pixels
and a depth map of the observed part of its environment. Moreover, a box
containing the corresponding pixels in the image frame is available when a UAV
identifies a target. Hypotheses regarding information provided by the pixel
classification, depth map construction, and target identification algorithms
are proposed to allow its exploitation by set-membership approaches. A
set-membership target location estimator is developed using the information
provided by the CVS. Each UAV evaluates sets guaranteed to contain the location
of the identified targets and a set possibly containing the locations of
targets still to be identified. Then, each UAV uses these sets to search and
track targets cooperatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to Elsevier / ScienceDirect for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Training of Generalizable Visuomotor Policies via
  Control-Aware Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinuo Zhao, Kun Wu, Tianjiao Yi, Zhiyuan Xu, Xiaozhu Ju, Zhengping Che, Chi Harold Liu, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving generalization is one key challenge in embodied AI, where obtaining
large-scale datasets across diverse scenarios is costly. Traditional weak
augmentations, such as cropping and flipping, are insufficient for improving a
model's performance in new environments. Existing data augmentation methods
often disrupt task-relevant information in images, potentially degrading
performance. To overcome these challenges, we introduce EAGLE, an efficient
training framework for generalizable visuomotor policies that improves upon
existing methods by (1) enhancing generalization by applying augmentation only
to control-related regions identified through a self-supervised control-aware
mask and (2) improving training stability and efficiency by distilling
knowledge from an expert to a visuomotor student policy, which is then deployed
to unseen environments without further fine-tuning. Comprehensive experiments
on three domains, including the DMControl Generalization Benchmark, the
enhanced Robot Manipulation Distraction Benchmark, and a long-sequential
drawer-opening task, validate the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Framework for Real-Time Failure Handling in <span class="highlight-title">Robo</span>tics Using
  <span class="highlight-title">Vision</span>-Language Models, Reactive Planner and Behavior Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faseeh Ahmad, Hashim Ismail, Jonathan Styrud, Maj Stenmark, Volker Krueger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic systems often face execution failures due to unexpected obstacles,
sensor errors, or environmental changes. Traditional failure recovery methods
rely on predefined strategies or human intervention, making them less
adaptable. This paper presents a unified failure recovery framework that
combines Vision-Language Models (VLMs), a reactive planner, and Behavior Trees
(BTs) to enable real-time failure handling. Our approach includes pre-execution
verification, which checks for potential failures before execution, and
reactive failure handling, which detects and corrects failures during execution
by verifying existing BT conditions, adding missing preconditions and, when
necessary, generating new skills. The framework uses a scene graph for
structured environmental perception and an execution history for continuous
monitoring, enabling context-aware and adaptive failure handling. We evaluate
our framework through real-world experiments with an ABB YuMi robot on tasks
like peg insertion, object sorting, and drawer placement, as well as in
AI2-THOR simulator. Compared to using pre-execution and reactive methods
separately, our approach achieves higher task success rates and greater
adaptability. Ablation studies highlight the importance of VLM-based reasoning,
structured scene representation, and execution history tracking for effective
failure recovery in robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuromorphic Attitude Estimation and Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stein Stroobants, Christophe de Wagter, Guido C. H. E. De Croon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The real-world application of small drones is mostly hampered by energy
limitations. Neuromorphic computing promises extremely energy-efficient AI for
autonomous flight but is still challenging to train and deploy on real robots.
To reap the maximal benefits from neuromorphic computing, it is necessary to
perform all autonomy functions end-to-end on a single neuromorphic chip, from
low-level attitude control to high-level navigation. This research presents the
first neuromorphic control system using a spiking neural network (SNN) to
effectively map a drone's raw sensory input directly to motor commands. We
apply this method to low-level attitude estimation and control for a quadrotor,
deploying the SNN on a tiny Crazyflie. We propose a modular SNN, separately
training and then merging estimation and control sub-networks. The SNN is
trained with imitation learning, using a flight dataset of sensory-motor pairs.
Post-training, the network is deployed on the Crazyflie, issuing control
commands from sensor inputs at 500Hz. Furthermore, for the training procedure
we augmented training data by flying a controller with additional excitation
and time-shifting the target data to enhance the predictive capabilities of the
SNN. On the real drone, the perception-to-control SNN tracks attitude commands
with an average error of 3.0 degrees, compared to 2.7 degrees for the regular
flight stack. We also show the benefits of the proposed learning modifications
for reducing the average tracking error and reducing oscillations. Our work
shows the feasibility of performing neuromorphic end-to-end control, laying the
basis for highly energy-efficient and low-latency neuromorphic autopilots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> GAPartManip: A Large-scale Part-centric <span class="highlight-title">Dataset</span> for Material-Agnostic
  <span class="highlight-title">Articulate</span>d Object <span class="highlight-title">Manipulation</span> <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Cui, Chengyang Zhao, Songlin Wei, Jiazhao Zhang, Haoran Geng, Yaran Chen, Haoran Li, <span class="highlight-author">He Wang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively manipulating articulated objects in household scenarios is a
crucial step toward achieving general embodied artificial intelligence.
Mainstream research in 3D vision has primarily focused on manipulation through
depth perception and pose detection. However, in real-world environments, these
methods often face challenges due to imperfect depth perception, such as with
transparent lids and reflective handles. Moreover, they generally lack the
diversity in part-based interactions required for flexible and adaptable
manipulation. To address these challenges, we introduced a large-scale
part-centric dataset for articulated object manipulation that features both
photo-realistic material randomization and detailed annotations of
part-oriented, scene-level actionable interaction poses. We evaluated the
effectiveness of our dataset by integrating it with several state-of-the-art
methods for depth estimation and interaction pose prediction. Additionally, we
proposed a novel modular framework that delivers superior and robust
performance for generalizable articulated object manipulation. Our extensive
experiments demonstrate that our dataset significantly improves the performance
of depth perception and actionable interaction pose prediction in both
simulation and real-world scenarios. More information and demos can be found
at: https://pku-epic.github.io/GAPartManip/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025. Project page:
  https://pku-epic.github.io/GAPartManip/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incremental Learning for <span class="highlight-title">Robo</span>t Shared Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06315v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06315v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Tao, Guixiu Qiao, Dan Ding, Zackory Erickson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shared autonomy holds promise for improving the usability and accessibility
of assistive robotic arms, but current methods often rely on costly expert
demonstrations and remain static after pretraining, limiting their ability to
handle real-world variations. Even with extensive training data, unforeseen
challenges--especially those that fundamentally alter task dynamics, such as
unexpected obstacles or spatial constraints--can cause assistive policies to
break down, leading to ineffective or unreliable assistance. To address this,
we propose ILSA, an Incrementally Learned Shared Autonomy framework that
continuously refines its assistive policy through user interactions, adapting
to real-world challenges beyond the scope of pre-collected data. At the core of
ILSA is a structured fine-tuning mechanism that enables continual improvement
with each interaction by effectively integrating limited new interaction data
while preserving prior knowledge, ensuring a balance between adaptation and
generalization. A user study with 20 participants demonstrates ILSA's
effectiveness, showing faster task completion and improved user experience
compared to static alternatives. Code and videos are available at
https://ilsa-robo.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HOTFormerLoc: Hierarchical Octree Transformer for Versatile <span class="highlight-title">Lidar</span> Place
  Recognition Across Ground and Aerial Views <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Griffiths, Maryam Haghighat, Simon Denman, Clinton Fookes, Milad Ramezani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HOTFormerLoc, a novel and versatile Hierarchical Octree-based
TransFormer, for large-scale 3D place recognition in both ground-to-ground and
ground-to-aerial scenarios across urban and forest environments. We propose an
octree-based multi-scale attention mechanism that captures spatial and semantic
features across granularities. To address the variable density of point
distributions from spinning lidar, we present cylindrical octree attention
windows to reflect the underlying distribution during attention. We introduce
relay tokens to enable efficient global-local interactions and multi-scale
representation learning at reduced computational cost. Our pyramid attentional
pooling then synthesises a robust global descriptor for end-to-end place
recognition in challenging environments. In addition, we introduce
CS-Wild-Places, a novel 3D cross-source dataset featuring point cloud data from
aerial and ground lidar scans captured in dense forests. Point clouds in
CS-Wild-Places contain representational gaps and distinctive attributes such as
varying point densities and noise patterns, making it a challenging benchmark
for cross-view localisation in the wild. HOTFormerLoc achieves a top-1 average
recall improvement of 5.5% - 11.5% on the CS-Wild-Places benchmark.
Furthermore, it consistently outperforms SOTA 3D place recognition methods,
with an average performance gain of 4.9% on well-established urban and forest
datasets. The code and CS-Wild-Places benchmark is available at
https://csiro-robotics.github.io/HOTFormerLoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 13 figures, 10 tables, Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AlignBot: Aligning VLM-powered Customized Task Planning with User
  Reminders Through Fine-Tuning for Household <span class="highlight-title">Robo</span>ts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaxizhuoma Zhaxizhuoma, Pengan Chen, Ziniu Wu, Jiawei Sun, Dong Wang, Peng Zhou, Nieqing Cao, Yan Ding, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents AlignBot, a novel framework designed to optimize
VLM-powered customized task planning for household robots by effectively
aligning with user reminders. In domestic settings, aligning task planning with
user reminders poses significant challenges due to the limited quantity,
diversity, and multimodal nature of the reminders. To address these challenges,
AlignBot employs a fine-tuned LLaVA-7B model, functioning as an adapter for
GPT-4o. This adapter model internalizes diverse forms of user reminders-such as
personalized preferences, corrective guidance, and contextual assistance-into
structured instruction-formatted cues that prompt GPT-4o in generating
customized task plans. Additionally, AlignBot integrates a dynamic retrieval
mechanism that selects task-relevant historical successes as prompts for
GPT-4o, further enhancing task planning accuracy. To validate the effectiveness
of AlignBot, experiments are conducted in real-world household environments,
which are constructed within the laboratory to replicate typical household
settings. A multimodal dataset with over 1,500 entries derived from volunteer
reminders is used for training and evaluation. The results demonstrate that
AlignBot significantly improves customized task planning, outperforming
existing LLM- and VLM-powered planners by interpreting and aligning with user
reminders, achieving 86.8% success rate compared to the vanilla GPT-4o baseline
at 21.6%, reflecting a 65% improvement and over four times greater
effectiveness. Supplementary materials are available at:
https://yding25.com/AlignBot/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Integrated Approach to <span class="highlight-title">Robo</span>tic Object <span class="highlight-title">Grasp</span>ing and <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Owais Ahmed, M Huzaifa, M Areeb, Hamza Ali Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to the growing challenges of manual labor and efficiency in
warehouse operations, Amazon has embarked on a significant transformation by
incorporating robotics to assist with various tasks. While a substantial number
of robots have been successfully deployed for tasks such as item transportation
within warehouses, the complex process of object picking from shelves remains a
significant challenge. This project addresses the issue by developing an
innovative robotic system capable of autonomously fulfilling a simulated order
by efficiently selecting specific items from shelves. A distinguishing feature
of the proposed robotic system is its capacity to navigate the challenge of
uncertain object positions within each bin of the shelf. The system is
engineered to autonomously adapt its approach, employing strategies that enable
it to efficiently locate and retrieve the desired items, even in the absence of
pre-established knowledge about their placements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I am making big changes in the paper and continuing its further
  development with other instituition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Wang, Bo Yu, Junzhe Zhao, Wen<span class="highlight-author">hao Su</span>n, Sai Hou, Shuai Liang, Xing Hu, Yinhe Han, Yiming Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied AI agents responsible for executing interconnected, long-sequence
household tasks often face difficulties with in-context memory, leading to
inefficiencies and errors in task execution. To address this issue, we
introduce KARMA, an innovative memory system that integrates long-term and
short-term memory modules, enhancing large language models (LLMs) for planning
in embodied agents through memory-augmented prompting. KARMA distinguishes
between long-term and short-term memory, with long-term memory capturing
comprehensive 3D scene graphs as representations of the environment, while
short-term memory dynamically records changes in objects' positions and states.
This dual-memory structure allows agents to retrieve relevant past scene
experiences, thereby improving the accuracy and efficiency of task planning.
Short-term memory employs strategies for effective and adaptive memory
replacement, ensuring the retention of critical information while discarding
less pertinent data. Compared to state-of-the-art embodied agents enhanced with
memory, our memory-augmented embodied AI agent improves success rates by 1.3x
and 2.3x in Composite Tasks and Complex Tasks within the AI2-THOR simulator,
respectively, and enhances task execution efficiency by 3.4x and 62.7x.
Furthermore, we demonstrate that KARMA's plug-and-play capability allows for
seamless deployment on real-world robotic systems, such as mobile manipulation
platforms.Through this plug-and-play memory system, KARMA significantly
enhances the ability of embodied agents to generate coherent and contextually
appropriate plans, making the execution of complex household tasks more
efficient. The experimental videos from the work can be found at
https://youtu.be/4BT7fnw9ehs. Our code is available at
https://github.com/WZX0Swarm0Robotics/KARMA/tree/master.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moto: Latent Motion Token as the Bridging Language for Learning <span class="highlight-title">Robo</span>t
  <span class="highlight-title">Manipulation</span> from Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04445v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04445v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Chen, Yuying Ge, Weiliang Tang, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in Large Language Models pre-trained on extensive corpora
have shown significant success in various natural language processing tasks
with minimal fine-tuning. This success offers new promise for robotics, which
has long been constrained by the high cost of action-labeled data. We ask:
given the abundant video data containing interaction-related knowledge
available as a rich "corpus", can a similar generative pre-training approach be
effectively applied to enhance robot learning? The key challenge is to identify
an effective representation for autoregressive pre-training that benefits robot
manipulation tasks. Inspired by the way humans learn new skills through
observing dynamic environments, we propose that effective robotic learning
should emphasize motion-related knowledge, which is closely tied to low-level
actions and is hardware-agnostic, facilitating the transfer of learned motions
to actual robot actions. To this end, we introduce Moto, which converts video
content into latent Motion Token sequences by a Latent Motion Tokenizer,
learning a bridging "language" of motion from videos in an unsupervised manner.
We pre-train Moto-GPT through motion token autoregression, enabling it to
capture diverse visual motion knowledge. After pre-training, Moto-GPT
demonstrates the promising ability to produce semantically interpretable motion
tokens, predict plausible motion trajectories, and assess trajectory
rationality through output likelihood. To transfer learned motion priors to
real robot actions, we implement a co-fine-tuning strategy that seamlessly
bridges latent motion token prediction and real robot control. Extensive
experiments show that the fine-tuned Moto-GPT exhibits superior robustness and
efficiency on robot manipulation benchmarks, underscoring its effectiveness in
transferring knowledge from video data to downstream visual manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project released at: https://chenyi99.github.io/moto/ Code released
  at: https://github.com/TencentARC/Moto Update: Added content related to
  real-world robot experiments and learning from human videos; Modified author
  information</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPINE: Online Semantic Planning for Missions with Incomplete Natural
  Language Specifications in Unstructured Environments <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03035v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03035v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Ravichandran, Varun Murali, Mariliza Tzes, George J. Pappas, Vijay Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robots become increasingly capable, users will want to describe high-level
missions and have robots infer the relevant details. Because pre-built maps are
difficult to obtain in many realistic settings, accomplishing such missions
will require the robot to map and plan online. While many semantic planning
methods operate online, they are typically designed for well specified missions
such as object search or exploration. Recently, Large Language Models (LLMs)
have demonstrated powerful contextual reasoning abilities over a range of
robotic tasks described in natural language. However, existing LLM-enabled
planners typically do not consider online planning or complex missions; rather,
relevant subtasks and semantics are provided by a pre-built map or a user. We
address these limitations via SPINE, an online planner for missions with
incomplete mission specifications provided in natural language. The planner
uses an LLM to reason about subtasks implied by the mission specification and
then realizes these subtasks in a receding horizon framework. Tasks are
automatically validated for safety and refined online with new map
observations. We evaluate SPINE in simulation and real-world settings with
missions that require multiple steps of semantic reasoning and exploration in
cluttered outdoor environments of over 20,000m$^2$. Compared to baselines that
use existing LLM-enabled planning approaches, our method is over twice as
efficient in terms of time and distance, requires less user interactions, and
does not require a full map. Additional resources are provided at
https://zacravichandran.github.io/SPINE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the International Conference on Robotics and Automation
  (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robo</span>tic In-Hand <span class="highlight-title">Manipulation</span> for Large-Range Precise Object Movement:
  The RGMC Champion Solution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Yu, Yongpeng Jiang, Chen Chen, Yongyi Jia, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-hand manipulation using multiple dexterous fingers is a critical robotic
skill that can reduce the reliance on large arm motions, thereby saving space
and energy. This letter focuses on in-grasp object movement, which refers to
manipulating an object to a desired pose through only finger motions within a
stable grasp. The key challenge lies in simultaneously achieving high precision
and large-range movements while maintaining a constant stable grasp. To address
this problem, we propose a simple and practical approach based on kinematic
trajectory optimization with no need for pretraining or object geometries,
which can be easily applied to novel objects in real-world scenarios. Adopting
this approach, we won the championship for the in-hand manipulation track at
the 9th Robotic Grasping and Manipulation Competition (RGMC) held at ICRA 2024.
Implementation details, discussion, and further quantitative experimental
results are presented in this letter, which aims to comprehensively evaluate
our approach and share our key takeaways from the competition. Supplementary
materials including video and code are available at
https://rgmc-xl-team.github.io/ingrasp_manipulation .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by RA-L. Project website:
  https://rgmc-xl-team.github.io/ingrasp_manipulation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adver-City: Open-Source Multi-Modal <span class="highlight-title">Dataset</span> for Collaborative Perception
  Under Adverse Weather Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateus Karvat, Sidney Givigi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adverse weather conditions pose a significant challenge to the widespread
adoption of Autonomous Vehicles (AVs) by impacting sensors like LiDARs and
cameras. Even though Collaborative Perception (CP) improves AV perception in
difficult conditions, existing CP datasets lack adverse weather conditions. To
address this, we introduce Adver-City, the first open-source synthetic CP
dataset focused on adverse weather conditions. Simulated in CARLA with OpenCDA,
it contains over 24 thousand frames, over 890 thousand annotations, and 110
unique scenarios across six different weather conditions: clear weather, soft
rain, heavy rain, fog, foggy heavy rain and, for the first time in a synthetic
CP dataset, glare. It has six object categories including pedestrians and
cyclists, and uses data from vehicles and roadside units featuring LiDARs, RGB
and semantic segmentation cameras, GNSS, and IMUs. Its scenarios, based on real
crash reports, depict the most relevant road configurations for adverse weather
and poor visibility conditions, varying in object density, with both dense and
sparse scenes, allowing for novel testing conditions of CP models. Benchmarks
run on the dataset show that weather conditions created challenging conditions
for perception models, with CoBEVT scoring 58.30/52.44/38.90 (AP@30/50/70). The
dataset, code and documentation are available at
https://labs.cs.queensu.ca/quarrg/datasets/adver-city/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Electrostatic Clutches Enable Simultaneous Mechanical Multiplexing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy E. Amish, Jeffrey T. Auletta, Chad C. Kessens, Joshua R. Smith, Jeffrey I. Lipton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Actuating robotic systems with multiple degrees of freedom (DoF)
traditionally requires numerous motors, leading to increased size, weight,
cost, and power consumption. Mechanical multiplexing offers a solution by
enabling a single actuator to control multiple DoF. However, existing
multiplexers have either been limited to electrically controlled time-based
multiplexing that control one DoF at a time or have relied on mechanical
switching to control multiple DoF simultaneously. There is a strong need for a
system that can perform electrically controlled multiplexing for both
time-based and simultaneous control of multiple DoF. This study introduces a
novel electrostatic capstan clutch-based mechanical multiplexer that enables
high-force, single-motor control of multiple DoF. Here, we show that our system
achieves both single-input-single-output (SISO) and single-input-multipleoutput
(SIMO) actuation, allowing bidirectional control and position holding with
minimal power consumption. Each output can actuate a 22.24 N load, limited by
clutch performance, up to 5 cm. The number of outputs and actuation length is
currently limited by the length of the drive shaft. We demonstrate the
integration of our system into a 4-DoF commercial robotic hand using a single
motor. These findings show that electrostatic clutchbased multiplexing provides
a scalable and energy-efficient design solution for high-DoF robotic platforms,
opening new possibilities for lightweight and power-efficient actuation in
robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physically-Feasible Reactive Synthesis for Terrain-Adaptive Locomotion
  via Trajectory Optimization and Symbolic Repair 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Zhou, Qian Meng, Hadas Kress-Gazit, Ye Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an integrated planning framework for quadrupedal locomotion over
dynamically changing, unforeseen terrains. Existing approaches either rely on
heuristics for instantaneous foothold selection--compromising safety and
versatility--or solve expensive trajectory optimization problems with complex
terrain features and long time horizons. In contrast, our framework leverages
reactive synthesis to generate correct-by-construction controllers at the
symbolic level, and mixed-integer convex programming (MICP) for dynamic and
physically feasible footstep planning for each symbolic transition. We use a
high-level manager to reduce the large state space in synthesis by
incorporating local environment information, improving synthesis scalability.
To handle specifications that cannot be met due to dynamic infeasibility, and
to minimize costly MICP solves, we leverage a symbolic repair process to
generate only necessary symbolic transitions. During online execution,
re-running the MICP with real-world terrain data, along with runtime symbolic
repair, bridges the gap between offline synthesis and online execution. We
demonstrate, in simulation, our framework's capabilities to discover missing
locomotion skills and react promptly in safety-critical environments, such as
scattered stepping stones and rebars.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding and Imitating Human-<span class="highlight-title">Robo</span>t Motion with Restricted <span class="highlight-title">Visual</span>
  Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maulik Bhatt, HongHao Zhen, Monroe Kennedy III, Negar Mehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When working around humans, it is important to model their perception
limitations in order to predict their behavior more accurately. In this work,
we consider agents with a limited field of view, viewing range, and ability to
miss objects within the viewing range (e.g., transparency). By considering the
observation model independently from the motion policy, we can better predict
the agent's behavior by considering these limitations and approximating them.
We perform a user study where human operators navigate a cluttered scene while
scanning the region for obstacles with a limited field of view and range. Using
imitation learning, we show that a robot can adopt a human's strategy for
observing an environment with limitations on observation and navigate with
minimal collision with dynamic and static obstacles. We also show that this
learned model helps it successfully navigate a physical hardware vehicle in
real-time.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position: Interactive Generative Video as Next-Generation Game Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern game development faces significant challenges in creativity and cost
due to predetermined content in traditional game engines. Recent breakthroughs
in video generation models, capable of synthesizing realistic and interactive
virtual environments, present an opportunity to revolutionize game creation. In
this position paper, we propose Interactive Generative Video (IGV) as the
foundation for Generative Game Engines (GGE), enabling unlimited novel content
generation in next-generation gaming. GGE leverages IGV's unique strengths in
unlimited high-quality content synthesis, physics-aware world modeling,
user-controlled interactivity, long-term memory capabilities, and causal
reasoning. We present a comprehensive framework detailing GGE's core modules
and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work
charts a new course for game development in the AI era, envisioning a future
where AI-powered generative systems fundamentally reshape how games are created
and experienced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image as an IMU: Estimating <span class="highlight-title">Camera</span> Motion from a Single Motion-Blurred
  Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerred Chen, Ronald Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many robotics and VR/AR applications, fast camera motions cause a high
level of motion blur, causing existing camera pose estimation methods to fail.
In this work, we propose a novel framework that leverages motion blur as a rich
cue for motion estimation rather than treating it as an unwanted artifact. Our
approach works by predicting a dense motion flow field and a monocular depth
map directly from a single motion-blurred image. We then recover the
instantaneous camera velocity by solving a linear least squares problem under
the small motion assumption. In essence, our method produces an IMU-like
measurement that robustly captures fast and aggressive camera movements. To
train our model, we construct a large-scale dataset with realistic synthetic
motion blur derived from ScanNet++v2 and further refine our model by training
end-to-end on real data using our fully differentiable pipeline. Extensive
evaluations on real-world benchmarks demonstrate that our method achieves
state-of-the-art angular and translational velocity estimates, outperforming
current methods like MASt3R and COLMAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jerredchen.github.io/image-as-imu/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenVLThinker: An Early Exploration to Complex <span class="highlight-title">Vision</span>-Language Reasoning
  via Iterative Self-Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, Kai-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements demonstrated by DeepSeek-R1 have shown that complex
reasoning abilities in large language models (LLMs), including sophisticated
behaviors such as self-verification and self-correction, can be achieved by RL
with verifiable rewards and significantly improves model performance on
challenging tasks such as AIME. Motivated by these findings, our study
investigates whether similar reasoning capabilities can be successfully
integrated into large vision-language models (LVLMs) and assesses their impact
on challenging multimodal reasoning tasks. We consider an approach that
iteratively leverages supervised fine-tuning (SFT) on lightweight training data
and Reinforcement Learning (RL) to further improve model generalization.
Initially, reasoning capabilities were distilled from pure-text R1 models by
generating reasoning steps using high-quality captions of the images sourced
from diverse visual datasets. Subsequently, iterative RL training further
enhance reasoning skills, with each iteration's RL-improved model generating
refined SFT datasets for the next round. This iterative process yielded
OpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on
challenging benchmarks such as MathVista, MathVerse, and MathVision,
demonstrating the potential of our strategy for robust vision-language
reasoning. The code, model and data are held at
https://github.com/yihedeng9/OpenVLThinker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time-Series U-Net with Recurrence for Noise-Robust Imaging
  Photoplethysmography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vineet R. Shenoy, Shaoju Wu, Armand Comas, Tim K. Marks, Suhas Lohit, Hassan Mansour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote estimation of vital signs enables health monitoring for situations in
which contact-based devices are either not available, too intrusive, or too
expensive. In this paper, we present a modular, interpretable pipeline for
pulse signal estimation from video of the face that achieves state-of-the-art
results on publicly available datasets.Our imaging photoplethysmography (iPPG)
system consists of three modules: face and landmark detection, time-series
extraction, and pulse signal/pulse rate estimation. Unlike many deep learning
methods that make use of a single black-box model that maps directly from input
video to output signal or heart rate, our modular approach enables each of the
three parts of the pipeline to be interpreted individually. The pulse signal
estimation module, which we call TURNIP (Time-Series U-Net with Recurrence for
Noise-Robust Imaging Photoplethysmography), allows the system to faithfully
reconstruct the underlying pulse signal waveform and uses it to measure heart
rate and pulse rate variability metrics, even in the presence of motion. When
parts of the face are occluded due to extreme head poses, our system explicitly
detects such "self-occluded" regions and maintains estimation robustness
despite the missing information. Our algorithm provides reliable heart rate
estimates without the need for specialized sensors or contact with the skin,
outperforming previous iPPG methods on both color (RGB) and near-infrared (NIR)
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 Pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decouple and Track: Benchmarking and Improving Video <span class="highlight-title">Diffusion</span>
  Transformers for Motion Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Shi, Jianzong Wu, Jinbin Bai, Jiangning Zhang, Lu Qi, Xiangtai Li, Yunhai Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The motion transfer task involves transferring motion from a source video to
newly generated videos, requiring the model to decouple motion from appearance.
Previous diffusion-based methods primarily rely on separate spatial and
temporal attention mechanisms within 3D U-Net. In contrast, state-of-the-art
video Diffusion Transformers (DiT) models use 3D full attention, which does not
explicitly separate temporal and spatial information. Thus, the interaction
between spatial and temporal dimensions makes decoupling motion and appearance
more challenging for DiT models. In this paper, we propose DeT, a method that
adapts DiT models to improve motion transfer ability. Our approach introduces a
simple yet effective temporal kernel to smooth DiT features along the temporal
dimension, facilitating the decoupling of foreground motion from background
appearance. Meanwhile, the temporal kernel effectively captures temporal
variations in DiT features, which are closely related to motion. Moreover, we
introduce explicit supervision along dense trajectories in the latent feature
space to further enhance motion consistency. Additionally, we present MTBench,
a general and challenging benchmark for motion transfer. We also introduce a
hybrid motion fidelity metric that considers both the global and local motion
similarity. Therefore, our work provides a more comprehensive evaluation than
previous works. Extensive experiments on MTBench demonstrate that DeT achieves
the best trade-off between motion fidelity and edit fidelity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Semantics: Rediscovering Spatial Awareness in <span class="highlight-title">Vision</span>-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianing Qi, Jiawei Liu, Hao Tang, Zhigang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) excel at identifying and describing objects but
struggle with spatial reasoning such as accurately understanding the relative
positions of objects. Inspired by the dual-pathway (ventral-dorsal) model of
human vision, we investigate why VLMs fail spatial tasks despite strong object
recognition capabilities. Our interpretability-driven analysis reveals a
critical underlying cause: vision embeddings in VLMs are treated primarily as
semantic ``bag-of-tokens," overshadowing subtle yet crucial positional cues due
to their disproportionately large embedding norms. We validate this insight
through extensive diagnostic experiments, demonstrating minimal performance
impact when token orders or fine-grained spatial details are removed. Guided by
these findings, we propose simple, interpretable interventions, including
normalizing vision embedding norms and extracting mid-layer spatially rich
features, to restore spatial awareness. Empirical results on both our synthetic
data and standard benchmarks demonstrate improved spatial reasoning
capabilities, highlighting the value of interpretability-informed design
choices. Our study not only uncovers fundamental limitations in current VLM
architectures but also provides actionable insights for enhancing structured
perception of visual scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dereflection Any Image with <span class="highlight-title">Diffusion</span> Priors and Diversified Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jichen Hu, Chen Yang, Zanwei Zhou, Jiemin Fang, Xiaokang Yang, Qi Tian, Wei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reflection removal of a single image remains a highly challenging task due to
the complex entanglement between target scenes and unwanted reflections.
Despite significant progress, existing methods are hindered by the scarcity of
high-quality, diverse data and insufficient restoration priors, resulting in
limited generalization across various real-world scenarios. In this paper, we
propose Dereflection Any Image, a comprehensive solution with an efficient data
preparation pipeline and a generalizable model for robust reflection removal.
First, we introduce a dataset named Diverse Reflection Removal (DRR) created by
randomly rotating reflective mediums in target scenes, enabling variation of
reflection angles and intensities, and setting a new benchmark in scale,
quality, and diversity. Second, we propose a diffusion-based framework with
one-step diffusion for deterministic outputs and fast inference. To ensure
stable learning, we design a three-stage progressive training strategy,
including reflection-invariant finetuning to encourage consistent outputs
across varying reflection patterns that characterize our dataset. Extensive
experiments show that our method achieves SOTA performance on both common
benchmarks and challenging in-the-wild images, showing superior generalization
across diverse real-world scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Align Your Rhythm: Generating Highly Aligned Dance Poses with
  Gating-Enhanced Rhythm-Aware Feature Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Congyi Fan, Jian Guan, Xuanjia Zhao, Dongli Xu, Youtian Lin, Tong Ye, Pengming Feng, Haiwei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically generating natural, diverse and rhythmic human dance movements
driven by music is vital for virtual reality and film industries. However,
generating dance that naturally follows music remains a challenge, as existing
methods lack proper beat alignment and exhibit unnatural motion dynamics. In
this paper, we propose Danceba, a novel framework that leverages gating
mechanism to enhance rhythm-aware feature representation for music-driven dance
generation, which achieves highly aligned dance poses with enhanced rhythmic
sensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to
precisely extract rhythmic information from musical phase data, capitalizing on
the intrinsic periodicity and temporal structures of music. Additionally, we
propose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic
features, ensuring that dance movements closely follow the musical rhythm. We
also introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately
model upper and lower body motions along with musical features, thereby
improving the naturalness and diversity of generated dance movements. Extensive
experiments confirm that Danceba outperforms state-of-the-art methods,
achieving significantly better rhythmic alignment and motion diversity. Project
page: https://danceba.github.io/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Topological Data Analysis Framework for Quantifying Necrosis in
  Glioblastomas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Tellez, Enrique Torres-Giese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a shape descriptor that we call "interior
function". This is a Topological Data Analysis (TDA) based descriptor that
refines previous descriptors for image analysis. Using this concept, we define
subcomplex lacunarity, a new index that quantifies geometric characteristics of
necrosis in tumors such as conglomeration. Building on this framework, we
propose a set of indices to analyze necrotic morphology and construct a diagram
that captures the distinct structural and geometric properties of necrotic
regions in tumors. We present an application of this framework in the study of
MRIs of Glioblastomas (GB). Using cluster analysis, we identify four distinct
subtypes of Glioblastomas that reflect geometric properties of necrotic
regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pow3R: Empowering Unconstrained 3D Reconstruction with <span class="highlight-title">Camera</span> and Scene
  Priors <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonbong Jang, Philippe Weinzaepfel, Vincent Leroy, Lourdes Agapito, Jerome Revaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Pow3r, a novel large 3D vision regression model that is highly
versatile in the input modalities it accepts. Unlike previous feed-forward
models that lack any mechanism to exploit known camera or scene priors at test
time, Pow3r incorporates any combination of auxiliary information such as
intrinsics, relative pose, dense or sparse depth, alongside input images,
within a single network. Building upon the recent DUSt3R paradigm, a
transformer-based architecture that leverages powerful pre-training, our
lightweight and versatile conditioning acts as additional guidance for the
network to predict more accurate estimates when auxiliary information is
available. During training we feed the model with random subsets of modalities
at each iteration, which enables the model to operate under different levels of
known priors at test time. This in turn opens up new capabilities, such as
performing inference in native image resolution, or point-cloud completion. Our
experiments on 3D reconstruction, depth completion, multi-view depth
prediction, multi-view stereo, and multi-view pose estimation tasks yield
state-of-the-art results and confirm the effectiveness of Pow3r at exploiting
all available information. The project webpage is
https://europe.naverlabs.com/pow3r.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring a Principled Framework for Deep Subspace Clustering <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianghan Meng, Zhiyuan Huang, Wei He, Xianbiao Qi, Rong Xiao, Chun-Guang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subspace clustering is a classical unsupervised learning task, built on a
basic assumption that high-dimensional data can be approximated by a union of
subspaces (UoS). Nevertheless, the real-world data are often deviating from the
UoS assumption. To address this challenge, state-of-the-art deep subspace
clustering algorithms attempt to jointly learn UoS representations and
self-expressive coefficients. However, the general framework of the existing
algorithms suffers from a catastrophic feature collapse and lacks a theoretical
guarantee to learn desired UoS representation. In this paper, we present a
Principled fRamewOrk for Deep Subspace Clustering (PRO-DSC), which is designed
to learn structured representations and self-expressive coefficients in a
unified manner. Specifically, in PRO-DSC, we incorporate an effective
regularization on the learned representations into the self-expressive model,
prove that the regularized self-expressive model is able to prevent feature
space collapse, and demonstrate that the learned optimal representations under
certain condition lie on a union of orthogonal subspaces. Moreover, we provide
a scalable and efficient approach to implement our PRO-DSC and conduct
extensive experiments to verify our theoretical findings and demonstrate the
superior performance of our proposed deep subspace clustering approach. The
code is available at https://github.com/mengxianghan123/PRO-DSC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is accepted by ICLR 2025. The first two authors are equally
  contributed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Iterative Feedback Mechanism for Improving Natural Language Class
  Descriptions in Open-Vocabulary Object Detection <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Y. Kim, Michelle Karker, Victoria Valledor, Seiyoung C. Lee, Karl F. Brzoska, Margaret Duff, Anthony Palladino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in open-vocabulary object detection models will enable
Automatic Target Recognition systems to be sustainable and repurposed by
non-technical end-users for a variety of applications or missions. New, and
potentially nuanced, classes can be defined with natural language text
descriptions in the field, immediately before runtime, without needing to
retrain the model. We present an approach for improving non-technical users'
natural language text descriptions of their desired targets of interest, using
a combination of analysis techniques on the text embeddings, and proper
combinations of embeddings for contrastive examples. We quantify the
improvement that our feedback mechanism provides by demonstrating performance
with multiple publicly-available open-vocabulary object detection models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of SPIE 13463 Automatic Target
  Recognition XXXV, Orlando, FL, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperNVD: Accelerating Neural Video Decomposition via Hypernetworks <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Pilligua, Danna Xue, Javier Vazquez-Corral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decomposing a video into a layer-based representation is crucial for easy
video editing for the creative industries, as it enables independent editing of
specific layers. Existing video-layer decomposition models rely on implicit
neural representations (INRs) trained independently for each video, making the
process time-consuming when applied to new videos. Noticing this limitation, we
propose a meta-learning strategy to learn a generic video decomposition model
to speed up the training on new videos. Our model is based on a hypernetwork
architecture which, given a video-encoder embedding, generates the parameters
for a compact INR-based neural video decomposition model. Our strategy
mitigates the problem of single-video overfitting and, importantly, shortens
the convergence of video decomposition on new, unseen videos. Our code is
available at: https://hypernvd.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025, project page: https://hypernvd.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Vision</span> Transformer Based Semantic Communications for Next Generation
  Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ahmed Mohsin, Muhammad Jazib, Zeeshan Alam, Muhmmad Farhan Khan, Muhammad Saad, Muhammad Ali Jamshed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the evolving landscape of 6G networks, semantic communications are poised
to revolutionize data transmission by prioritizing the transmission of semantic
meaning over raw data accuracy. This paper presents a Vision Transformer
(ViT)-based semantic communication framework that has been deliberately
designed to achieve high semantic similarity during image transmission while
simultaneously minimizing the demand for bandwidth. By equipping ViT as the
encoder-decoder framework, the proposed architecture can proficiently encode
images into a high semantic content at the transmitter and precisely
reconstruct the images, considering real-world fading and noise consideration
at the receiver. Building on the attention mechanisms inherent to ViTs, our
model outperforms Convolution Neural Network (CNNs) and Generative Adversarial
Networks (GANs) tailored for generating such images. The architecture based on
the proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38
dB, which is higher than other Deep Learning (DL) approaches in maintaining
semantic similarity across different communication environments. These findings
establish our ViT-based approach as a significant breakthrough in semantic
communications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted @ ICC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recovering Pulse Waves from Video Using Deep Unrolling and Deep
  Equilibrium Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vineet R Shenoy, Suhas Lohit, Hassan Mansour, Rama Chellappa, Tim K. Marks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera-based monitoring of vital signs, also known as imaging
photoplethysmography (iPPG), has seen applications in driver-monitoring,
perfusion assessment in surgical settings, affective computing, and more. iPPG
involves sensing the underlying cardiac pulse from video of the skin and
estimating vital signs such as the heart rate or a full pulse waveform. Some
previous iPPG methods impose model-based sparse priors on the pulse signals and
use iterative optimization for pulse wave recovery, while others use end-to-end
black-box deep learning methods. In contrast, we introduce methods that combine
signal processing and deep learning methods in an inverse problem framework.
Our methods estimate the underlying pulse signal and heart rate from facial
video by learning deep-network-based denoising operators that leverage deep
algorithm unfolding and deep equilibrium models. Experiments show that our
methods can denoise an acquired signal from the face and infer the correct
underlying pulse rate, achieving state-of-the-art heart rate estimation
performance on well-known benchmarks, all with less than one-fifth the number
of learnable parameters as the closest competing method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physical Plausibility-aware Trajectory Prediction via Locomotion
  Embodiment <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiromu Taketsugu, Takeru Oba, Takahiro Maeda, Shohei Nobuhara, Norimichi Ukita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can predict future human trajectories even from momentary observations
by using human pose-related cues. However, previous Human Trajectory Prediction
(HTP) methods leverage the pose cues implicitly, resulting in implausible
predictions. To address this, we propose Locomotion Embodiment, a framework
that explicitly evaluates the physical plausibility of the predicted trajectory
by locomotion generation under the laws of physics. While the plausibility of
locomotion is learned with an indifferentiable physics simulator, it is
replaced by our differentiable Locomotion Value function to train an HTP
network in a data-driven manner. In particular, our proposed Embodied
Locomotion loss is beneficial for efficiently training a stochastic HTP network
using multiple heads. Furthermore, the Locomotion Value filter is proposed to
filter out implausible trajectories at inference. Experiments demonstrate that
our method enhances even the state-of-the-art HTP methods across diverse
datasets and problem settings. Our code is available at:
https://github.com/ImIntheMiddle/EmLoco.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2025. Project page: https://iminthemiddle.github.io/EmLoco-Page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Joint Learning of Optical Flow and Intensity with Event
  <span class="highlight-title">Camera</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Guo, Friedhelm Hamann, Guillermo Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras rely on motion to obtain information about scene appearance. In
other words, for event cameras, motion and appearance are seen both or neither,
which are encoded in the output event stream. Previous works consider
recovering these two visual quantities as separate tasks, which does not fit
with the nature of event cameras and neglects the inherent relations between
both tasks. In this paper, we propose an unsupervised learning framework that
jointly estimates optical flow (motion) and image intensity (appearance), with
a single network. Starting from the event generation model, we newly derive the
event-based photometric error as a function of optical flow and image
intensity, which is further combined with the contrast maximization framework,
yielding a comprehensive loss function that provides proper constraints for
both flow and intensity estimation. Exhaustive experiments show that our model
achieves state-of-the-art performance for both optical flow (achieves 20% and
25% improvement in EPE and AE respectively in the unsupervised learning
category) and intensity estimation (produces competitive results with other
baselines, particularly in high dynamic range scenarios). Last but not least,
our model achieves shorter inference time than all the other optical flow
models and many of the image reconstruction models, while they output only one
quantity. Project page: https://github.com/tub-rip/e2fai
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 page, 8 figures, 9 tables. Project page:
  https://github.com/tub-rip/e2fai</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Interactive Perception Network with Mamba for Lung Tumor
  Segmentation in PET-CT Images <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Mei, Chenyu Lin, Yu Qiu, Yaonan Wang, Hui Zhang, Ziyang Wang, Dong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lung cancer is a leading cause of cancer-related deaths globally. PET-CT is
crucial for imaging lung tumors, providing essential metabolic and anatomical
information, while it faces challenges such as poor image quality, motion
artifacts, and complex tumor morphology. Deep learning-based models are
expected to address these problems, however, existing small-scale and private
datasets limit significant performance improvements for these methods. Hence,
we introduce a large-scale PET-CT lung tumor segmentation dataset, termed
PCLT20K, which comprises 21,930 pairs of PET-CT images from 605 patients.
Furthermore, we propose a cross-modal interactive perception network with Mamba
(CIPA) for lung tumor segmentation in PET-CT images. Specifically, we design a
channel-wise rectification module (CRM) that implements a channel state space
block across multi-modal features to learn correlated representations and helps
filter out modality-specific noise. A dynamic cross-modality interaction module
(DCIM) is designed to effectively integrate position and context information,
which employs PET images to learn regional position information and serves as a
bridge to assist in modeling the relationships between local features of CT
images. Extensive experiments on a comprehensive benchmark demonstrate the
effectiveness of our CIPA compared to the current state-of-the-art segmentation
methods. We hope our research can provide more exploration opportunities for
medical image segmentation. The dataset and code are available at
https://github.com/mj129/CIPA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep End-to-End Posterior ENergy (DEEPEN) for image recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jyothi Rikhab Chand, Mathews Jacob
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current end-to-end (E2E) and plug-and-play (PnP) image reconstruction
algorithms approximate the maximum a posteriori (MAP) estimate but cannot offer
sampling from the posterior distribution, like diffusion models. By contrast,
it is challenging for diffusion models to be trained in an E2E fashion. This
paper introduces a Deep End-to-End Posterior ENergy (DEEPEN) framework, which
enables MAP estimation as well as sampling. We learn the parameters of the
posterior, which is the sum of the data consistency error and the negative
log-prior distribution, using maximum likelihood optimization in an E2E
fashion. The proposed approach does not require algorithm unrolling, and hence
has a smaller computational and memory footprint than current E2E methods,
while it does not require contraction constraints typically needed by current
PnP methods. Our results demonstrate that DEEPEN offers improved performance
than current E2E and PnP models in the MAP setting, while it also offers faster
sampling compared to diffusion models. In addition, the learned energy-based
model is observed to be more robust to changes in image acquisition settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Slide-Level Prompt Learning with <span class="highlight-title">Vision</span> Language Models for Few-Shot
  Multiple Instance Learning in Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devavrat Tomar, Guillaume Vray, Dwarikanath Mahapatra, Sudipta Roy, Jean-Philippe Thiran, Behzad Bozorgtabar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of few-shot classification in
histopathology whole slide images (WSIs) by utilizing foundational
vision-language models (VLMs) and slide-level prompt learning. Given the
gigapixel scale of WSIs, conventional multiple instance learning (MIL) methods
rely on aggregation functions to derive slide-level (bag-level) predictions
from patch representations, which require extensive bag-level labels for
training. In contrast, VLM-based approaches excel at aligning visual embeddings
of patches with candidate class text prompts but lack essential pathological
prior knowledge. Our method distinguishes itself by utilizing pathological
prior knowledge from language models to identify crucial local tissue types
(patches) for WSI classification, integrating this within a VLM-based MIL
framework. Our approach effectively aligns patch images with tissue types, and
we fine-tune our model via prompt learning using only a few labeled WSIs per
category. Experimentation on real-world pathological WSI datasets and ablation
studies highlight our method's superior performance over existing MIL- and
VLM-based methods in few-shot WSI classification tasks. Our code is publicly
available at https://github.com/LTS5/SLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISBI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Hsi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal
infrared video is inherently challenging due to low contrast, environmental
noise, and small target sizes. This paper provides a straightforward approach
to address multi-UAV tracking in thermal infrared video, leveraging recent
advances in detection and tracking. Instead of relying on the YOLOv5 with the
DeepSORT pipeline, we present a tracking framework built on YOLOv12 and
BoT-SORT, enhanced with tailored training and inference strategies. We evaluate
our approach following the metrics from the 4th Anti-UAV Challenge and
demonstrate competitive performance. Notably, we achieve strong results without
using contrast enhancement or temporal information fusion to enrich UAV
features, highlighting our approach as a "Strong Baseline" for the multi-UAV
tracking task. We provide implementation details, in-depth experimental
analysis, and a discussion of potential improvements. The code is available at
https://github.com/wish44165/YOLOv12-BoT-SORT-ReID .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Text-to-Image Generation for Handling Spurious Correlation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryan Yazdan Parast, Basim Azam, Naveed Akhtar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks trained with Empirical Risk Minimization (ERM) perform
well when both training and test data come from the same domain, but they often
fail to generalize to out-of-distribution samples. In image classification,
these models may rely on spurious correlations that often exist between labels
and irrelevant features of images, making predictions unreliable when those
features do not exist. We propose a technique to generate training samples with
text-to-image (T2I) diffusion models for addressing the spurious correlation
problem. First, we compute the best describing token for the visual features
pertaining to the causal components of samples by a textual inversion
mechanism. Then, leveraging a language segmentation method and a diffusion
model, we generate new samples by combining the causal component with the
elements from other classes. We also meticulously prune the generated samples
based on the prediction probabilities and attribution scores of the ERM model
to ensure their correct composition for our objective. Finally, we retrain the
ERM model on our augmented dataset. This process reduces the model's reliance
on spurious correlations by learning from carefully crafted samples for in
which this correlation does not exist. Our experiments show that across
different benchmarks, our technique achieves better worst-group accuracy than
the existing state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuro-Symbolic Scene Graph Conditioning for Synthetic Image <span class="highlight-title">Dataset</span>
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Savazzi, Eugenio Lomurno, Cristian Sbrolli, Agnese Chiatti, Matteo Matteucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning models increase in scale and complexity, obtaining
sufficient training data has become a critical bottleneck due to acquisition
costs, privacy constraints, and data scarcity in specialised domains. While
synthetic data generation has emerged as a promising alternative, a notable
performance gap remains compared to models trained on real data, particularly
as task complexity grows. Concurrently, Neuro-Symbolic methods, which combine
neural networks' learning strengths with symbolic reasoning's structured
representations, have demonstrated significant potential across various
cognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning
for synthetic image dataset generation, focusing specifically on improving the
performance of Scene Graph Generation models. The research investigates whether
structured symbolic representations in the form of scene graphs can enhance
synthetic data quality through explicit encoding of relational constraints. The
results demonstrate that Neuro-Symbolic conditioning yields significant
improvements of up to +2.59% in standard Recall metrics and +2.83% in No Graph
Constraint Recall metrics when used for dataset augmentation. These findings
establish that merging Neuro-Symbolic and generative approaches produces
synthetic data with complementary structural information that enhances model
performance when combined with real data, providing a novel approach to
overcome data scarcity limitations even for complex visual reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> UniCon: Unidirectional Information Flow for Effective Control of
  Large-Scale <span class="highlight-title">Diffusion</span> Models <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanghua Yu, Jinjin Gu, Jinfan Hu, Zheyuan Li, C<span class="highlight-author">hao Dong</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce UniCon, a novel architecture designed to enhance control and
efficiency in training adapters for large-scale diffusion models. Unlike
existing methods that rely on bidirectional interaction between the diffusion
model and control adapter, UniCon implements a unidirectional flow from the
diffusion network to the adapter, allowing the adapter alone to generate the
final output. UniCon reduces computational demands by eliminating the need for
the diffusion model to compute and store gradients during adapter training. Our
results indicate that UniCon reduces GPU memory usage by one-third and
increases training speed by 2.3 times, while maintaining the same adapter
parameter size. Additionally, without requiring extra computational resources,
UniCon enables the training of adapters with double the parameter volume of
existing ControlNets. In a series of image conditional generation tasks, UniCon
has demonstrated precise responsiveness to control inputs and exceptional
generation capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication at the International
  Conference on Learning Representations (ICLR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PP-DocLayout: A Unified Document Layout Detection Model to Accelerate
  Large-Scale Data Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting Sun, Cheng Cui, Yuning Du, Yi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document layout analysis is a critical preprocessing step in document
intelligence, enabling the detection and localization of structural elements
such as titles, text blocks, tables, and formulas. Despite its importance,
existing layout detection models face significant challenges in generalizing
across diverse document types, handling complex layouts, and achieving
real-time performance for large-scale data processing. To address these
limitations, we present PP-DocLayout, which achieves high precision and
efficiency in recognizing 23 types of layout regions across diverse document
formats. To meet different needs, we offer three models of varying scales.
PP-DocLayout-L is a high-precision model based on the RT-DETR-L detector,
achieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on
a T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an
inference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a
high-efficiency model designed for resource-constrained environments and
real-time applications, with an inference time of 8.1 ms per page on a T4 GPU
and 14.5 ms on a CPU. This work not only advances the state of the art in
document layout analysis but also provides a robust solution for constructing
high-quality training data, enabling advancements in document intelligence and
multimodal AI systems. Code and models are available at
https://github.com/PaddlePaddle/PaddleX .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github Repo: https://github.com/PaddlePaddle/PaddleX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Learning Framework for <span class="highlight-title">Visual</span> Attention Prediction and Analysis
  of News Interfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Kenely, Dylan Seychell, Carl James Debono, Chris Porter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News outlets' competition for attention in news interfaces has highlighted
the need for demographically-aware saliency prediction models. Despite recent
advancements in saliency detection applied to user interfaces (UI), existing
datasets are limited in size and demographic representation. We present a deep
learning framework that enhances the SaRa (Saliency Ranking) model with
DeepGaze IIE, improving Salient Object Ranking (SOR) performance by 10.7%. Our
framework optimizes three key components: saliency map generation, grid segment
scoring, and map normalization. Through a two-fold experiment using
eye-tracking (30 participants) and mouse-tracking (375 participants aged
13--70), we analyze attention patterns across demographic groups. Statistical
analysis reveals significant age-based variations (p < 0.05, {\epsilon^2} =
0.042), with older users (36--70) engaging more with textual content and
younger users (13--35) interacting more with images. Mouse-tracking data
closely approximates eye-tracking behavior (sAUC = 0.86) and identifies UI
elements that immediately stand out, validating its use in large-scale studies.
We conclude that saliency studies should prioritize gathering data from a
larger, demographically representative sample and report exact demographic
distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint submitted to the 2025 IEEE Conference on
  Artificial Intelligence (CAI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Language Anchor-Guided Method for Robust Noisy Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilin Dai, Lehong Wang, Fangzhou Lin, Yidong Wang, Zhigang Li, Kazunori D Yamada, Ziming Zhang, Wang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world machine learning applications often struggle with two major
challenges: distribution shift and label noise. Models tend to overfit by
focusing on redundant and uninformative features in the training data, which
makes it hard for them to generalize to the target domain. Noisy data worsens
this problem by causing further overfitting to the noise, meaning that existing
methods often fail to tell the difference between true, invariant features and
misleading, spurious ones. To tackle these issues, we introduce Anchor
Alignment and Adaptive Weighting (A3W). This new algorithm uses sample
reweighting guided by natural language processing (NLP) anchors to extract more
representative features. In simple terms, A3W leverages semantic
representations from natural language models as a source of domain-invariant
prior knowledge. Additionally, it employs a weighted loss function that adjusts
each sample's contribution based on its similarity to the corresponding NLP
anchor. This adjustment makes the model more robust to noisy labels. Extensive
experiments on standard benchmark datasets show that A3W consistently
outperforms state-of-the-art domain generalization methods, offering
significant improvements in both accuracy and robustness across different
datasets and noise levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongli Xiang, Ziming Hong, Lina Yao, Dadong Wang, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-transferable learning (NTL) has been proposed to protect model
intellectual property (IP) by creating a "non-transferable barrier" to restrict
generalization from authorized to unauthorized domains. Recently, well-designed
attack, which restores the unauthorized-domain performance by fine-tuning NTL
models on few authorized samples, highlights the security risks of NTL-based
applications. However, such attack requires modifying model weights, thus being
invalid in the black-box scenario. This raises a critical question: can we
trust the security of NTL models deployed as black-box systems? In this work,
we reveal the first loophole of black-box NTL models by proposing a novel
attack method (dubbed as JailNTL) to jailbreak the non-transferable barrier
through test-time data disguising. The main idea of JailNTL is to disguise
unauthorized data so it can be identified as authorized by the NTL model,
thereby bypassing the non-transferable barrier without modifying the NTL model
weights. Specifically, JailNTL encourages unauthorized-domain disguising in two
levels, including: (i) data-intrinsic disguising (DID) for eliminating domain
discrepancy and preserving class-related content at the input-level, and (ii)
model-guided disguising (MGD) for mitigating output-level statistics difference
of the NTL model. Empirically, when attacking state-of-the-art (SOTA) NTL
models in the black-box scenario, JailNTL achieves an accuracy increase of up
to 55.7% in the unauthorized domain by using only 1% authorized samples,
largely exceeding existing SOTA white-box attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is released at https://github.com/tmllab/2025_CVPR_JailNTL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via
  Cross-Assembly Inference Strategy <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchao Yang, Takafumi Taketomi, Yuki Endo, Yoshihiro Kanamori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering high-quality 3D facial textures from single-view 2D images is a
challenging task, especially under constraints of limited data and complex
facial details such as makeup, wrinkles, and occlusions. In this paper, we
introduce FreeUV, a novel ground-truth-free UV texture recovery framework that
eliminates the need for annotated or synthetic UV data. FreeUV leverages
pre-trained stable diffusion model alongside a Cross-Assembly inference
strategy to fulfill this objective. In FreeUV, separate networks are trained
independently to focus on realistic appearance and structural consistency, and
these networks are combined during inference to generate coherent textures. Our
approach accurately captures intricate facial features and demonstrates robust
performance across diverse poses and occlusions. Extensive experiments validate
FreeUV's effectiveness, with results surpassing state-of-the-art methods in
both quantitative and qualitative metrics. Additionally, FreeUV enables new
applications, including local editing, facial feature interpolation, and
multi-view texture recovery. By reducing data requirements, FreeUV offers a
scalable solution for generating high-fidelity 3D facial textures suitable for
real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Project: https://yangxingchao.github.io/FreeUV-page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSCA-Net:Multi-Scale Context Aggregation Network for Infrared Small
  Target Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojin Lu, Taoran yue, Jiaxi cai, Shibing Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting infrared small targets in complex backgrounds remains a challenging
task because of the low contrast and high noise levels inherent in infrared
images. These factors often lead to the loss of crucial details during feature
extraction. Moreover, existing detection methods have limitations in adequately
integrating global and local information, which constrains the efficiency and
accuracy of infrared small target detection. To address these challenges, this
paper proposes a novel network architecture named MSCA-Net, which integrates
three key components: Multi-Scale Enhanced Detection Attention
mechanism(MSEDA), Positional Convolutional Block Attention Module (PCBAM), and
Channel Aggregation Block (CAB). Specifically, MSEDA employs a multi-scale
feature fusion attention mechanism to adaptively aggregate information across
different scales, enriching feature representation. PCBAM captures the
correlation between global and local features through a correlation
matrix-based strategy, enabling deep feature interaction. Moreover, CAB
redistributes input feature channels, facilitating the efficient transmission
of beneficial features and further enhancing the model detection capability in
complex backgrounds. The experimental results demonstrate that MSCA-Net
achieves outstanding small target detection performance in complex backgrounds.
Specifically, it attains mIoU scores of 78.43\%, 94.56\%, and 67.08\% on the
NUAA-SIRST, NUDT-SIRST, and IRTSD-1K datasets, respectively, underscoring its
effectiveness and strong potential for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueqi Qiu, Xingyu Miao, Fan Wan, Haoran Duan, Tejal Shah, Varun Ojhab, Yang Longa, Rajiv Ranjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfake detection is crucial for curbing the harm it causes to society.
However, current Deepfake detection methods fail to thoroughly explore artifact
information across different domains due to insufficient intrinsic
interactions. These interactions refer to the fusion and coordination after
feature extraction processes across different domains, which are crucial for
recognizing complex forgery clues. Focusing on more generalized Deepfake
detection, in this work, we introduce a novel bi-directional attention module
to capture the local positional information of artifact clues from the spatial
domain. This enables accurate artifact localization, thus addressing the coarse
processing with artifact features. To further address the limitation that the
proposed bi-directional attention module may not well capture global subtle
forgery information in the artifact feature (e.g., textures or edges), we
employ a fine-grained frequency attention module in the frequency domain. By
doing so, we can obtain high-frequency information in the fine-grained
features, which contains the global and subtle forgery information. Although
these features from the diverse domains can be effectively and independently
improved, fusing them directly does not effectively improve the detection
performance. Therefore, we propose a feature superposition strategy that
complements information from spatial and frequency domains. This strategy turns
the feature components into the form of wave-like tokens, which are updated
based on their phase, such that the distinctions between authentic and artifact
features can be amplified. Our method demonstrates significant improvements
over state-of-the-art (SOTA) methods on five public Deepfake datasets in
capturing abnormalities across different manipulated operations and real-life.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Radar-Guided Polynomial Fitting for Metric Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Rim, Hyoungseob Park, Vadim Ezhov, Jeffrey Moon, Alex Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose PolyRad, a novel radar-guided depth estimation method that
introduces polynomial fitting to transform scaleless depth predictions from
pretrained monocular depth estimation (MDE) models into metric depth maps.
Unlike existing approaches that rely on complex architectures or expensive
sensors, our method is grounded in a simple yet fundamental insight: using
polynomial coefficients predicted from cheap, ubiquitous radar data to
adaptively adjust depth predictions non-uniformly across depth ranges. Although
MDE models often infer reasonably accurate local depth structure within each
object or local region, they may misalign these regions relative to one
another, making a linear scale-and-shift transformation insufficient given
three or more of these regions. In contrast, PolyRad generalizes beyond linear
transformations and is able to correct such misalignments by introducing
inflection points. Importantly, our polynomial fitting framework preserves
structural consistency through a novel training objective that enforces
monotonicity via first-derivative regularization. PolyRad achieves
state-of-the-art performance on the nuScenes, ZJU-4DRadarCam, and View-of-Delft
datasets, outperforming existing methods by 30.3% in MAE and 37.2% in RMSE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Which2comm: An Efficient Collaborative Perception Framework for 3D
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duanrui Yu, Jing You, Xin Pei, Anqi Qu, Dingyu Wang, Shaocheng Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative perception allows real-time inter-agent information exchange
and thus offers invaluable opportunities to enhance the perception capabilities
of individual agents. However, limited communication bandwidth in practical
scenarios restricts the inter-agent data transmission volume, consequently
resulting in performance declines in collaborative perception systems. This
implies a trade-off between perception performance and communication cost. To
address this issue, we propose Which2comm, a novel multi-agent 3D object
detection framework leveraging object-level sparse features. By integrating
semantic information of objects into 3D object detection boxes, we introduce
semantic detection boxes (SemDBs). Innovatively transmitting these
information-rich object-level sparse features among agents not only
significantly reduces the demanding communication volume, but also improves 3D
object detection performance. Specifically, a fully sparse network is
constructed to extract SemDBs from individual agents; a temporal fusion
approach with a relative temporal encoding mechanism is utilized to obtain the
comprehensive spatiotemporal features. Extensive experiments on the V2XSet and
OPV2V datasets demonstrate that Which2comm consistently outperforms other
state-of-the-art methods on both perception performance and communication cost,
exhibiting better robustness to real-world latency. These results present that
for multi-agent collaborative 3D object detection, transmitting only
object-level sparse features is sufficient to achieve high-precision and robust
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hi-ALPS -- An Experimental Robustness Quantification of Six <span class="highlight-title">LiDAR</span>-based
  Object Detection Systems for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Arzberger, Ramin Tavakoli Kolagari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Light Detection and Ranging (LiDAR) is an essential sensor technology for
autonomous driving as it can capture high-resolution 3D data. As 3D object
detection systems (OD) can interpret such point cloud data, they play a key
role in the driving decisions of autonomous vehicles. Consequently, such 3D OD
must be robust against all types of perturbations and must therefore be
extensively tested. One approach is the use of adversarial examples, which are
small, sometimes sophisticated perturbations in the input data that change,
i.e., falsify, the prediction of the OD. These perturbations are carefully
designed based on the weaknesses of the OD. The robustness of the OD cannot be
quantified with adversarial examples in general, because if the OD is
vulnerable to a given attack, it is unclear whether this is due to the
robustness of the OD or whether the attack algorithm produces particularly
strong adversarial examples. The contribution of this work is Hi-ALPS --
Hierarchical Adversarial-example-based LiDAR Perturbation Level System, where
higher robustness of the OD is required to withstand the perturbations as the
perturbation levels increase. In doing so, the Hi-ALPS levels successively
implement a heuristic followed by established adversarial example approaches.
In a series of comprehensive experiments using Hi-ALPS, we quantify the
robustness of six state-of-the-art 3D OD under different types of
perturbations. The results of the experiments show that none of the OD is
robust against all Hi-ALPS levels; an important factor for the ranking is that
human observers can still correctly recognize the perturbed objects, as the
respective perturbations are small. To increase the robustness of the OD, we
discuss the applicability of state-of-the-art countermeasures. In addition, we
derive further suggestions for countermeasures based on our experimental
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoRLD: Contrastive Representation Learning Of Deformable Shapes In
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tonmoy Hossain ana Miaomiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable shape representations, parameterized by deformations relative to a
given template, have proven effective for improved image analysis tasks.
However, their broader applicability is hindered by two major challenges.
First, existing methods mainly rely on a known template during testing, which
is impractical and limits flexibility. Second, they often struggle to capture
fine-grained, voxel-level distinctions between similar shapes (e.g., anatomical
variations among healthy individuals, those with mild cognitive impairment, and
diseased states). To address these limitations, we propose a novel framework -
Contrastive Representation Learning of Deformable shapes (CoRLD) in learned
deformation spaces and demonstrate its effectiveness in the context of image
classification. Our CoRLD leverages a class-aware contrastive supervised
learning objective in latent deformation spaces, promoting proximity among
representations of similar classes while ensuring separation of dissimilar
groups. In contrast to previous deep learning networks that require a reference
image as input to predict deformation changes, our approach eliminates this
dependency. Instead, template images are utilized solely as ground truth in the
loss function during the training process, making our model more flexible and
generalizable to a wide range of medical applications. We validate CoRLD on
diverse datasets, including real brain magnetic resonance imaging (MRIs) and
adrenal shapes derived from computed tomography (CT) scans. Experimental
results show that our model effectively extracts deformable shape features,
which can be easily integrated with existing classifiers to substantially boost
the classification accuracy. Our code is available at GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D2C: Unlocking the Potential of Continuous Autoregressive Image
  Generation with Discrete Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panpan Wang, Liqiang Niu, Fandong Meng, Jinan Xu, Yufeng Chen, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of image generation, latent-based generative models occupy a
dominant status; however, these models rely heavily on image tokenizer. To meet
modeling requirements, autoregressive models possessing the characteristics of
scalability and flexibility embrace a discrete-valued tokenizer, but face the
challenge of poor image generation quality. In contrast, diffusion models take
advantage of the continuous-valued tokenizer to achieve better generation
quality but are subject to low efficiency and complexity. The existing hybrid
models are mainly to compensate for information loss and simplify the diffusion
learning process. The potential of merging discrete-valued and
continuous-valued tokens in the field of image generation has not yet been
explored. In this paper, we propose D2C, a novel two-stage method to enhance
model generation capacity. In the first stage, the discrete-valued tokens
representing coarse-grained image features are sampled by employing a small
discrete-valued generator. Then in the second stage, the continuous-valued
tokens representing fine-grained image features are learned conditioned on the
discrete token sequence. In addition, we design two kinds of fusion modules for
seamless interaction. On the ImageNet-256 benchmark, extensive experiment
results validate that our model achieves superior performance compared with
several continuous-valued and discrete-valued generative models on the
class-conditional image generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Steering Estimation with Semantic-Aware GNNs <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fouad Makiyeh, Huy-Dung Nguyen, Patrick Chareyre, Ramin Hasani, Marc Blanchon, Daniela Rus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steering estimation is a critical task in autonomous driving, traditionally
relying on 2D image-based models. In this work, we explore the advantages of
incorporating 3D spatial information through hybrid architectures that combine
3D neural network models with recurrent neural networks (RNNs) for temporal
modeling, using LiDAR-based point clouds as input. We systematically evaluate
four hybrid 3D models, all of which outperform the 2D-only baseline, with the
Graph Neural Network (GNN) - RNN model yielding the best results.
  To reduce reliance on LiDAR, we leverage a pretrained unified model to
estimate depth from monocular images, reconstructing pseudo-3D point clouds. We
then adapt the GNN-RNN model, originally designed for LiDAR-based point clouds,
to work with these pseudo-3D representations, achieving comparable or even
superior performance compared to the LiDAR-based model. Additionally, the
unified model provides semantic labels for each point, enabling a more
structured scene representation. To further optimize graph construction, we
introduce an efficient connectivity strategy where connections are
predominantly formed between points of the same semantic class, with only 20\%
of inter-class connections retained. This targeted approach reduces graph
complexity and computational cost while preserving critical spatial
relationships.
  Finally, we validate our approach on the KITTI dataset, achieving a 71%
improvement over 2D-only models. Our findings highlight the advantages of 3D
spatial information and efficient graph construction for steering estimation,
while maintaining the cost-effectiveness of monocular images and avoiding the
expense of LiDAR-based systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not Only Text: Exploring Compositionality of <span class="highlight-title">Visual</span> Representations in
  <span class="highlight-title">Vision</span>-Language Models <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Berasi, Matteo Farina, Massimiliano Mancini, Elisa Ricci, Nicola Strisciuglio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) learn a shared feature space for text and
images, enabling the comparison of inputs of different modalities. While prior
works demonstrated that VLMs organize natural language representations into
regular structures encoding composite meanings, it remains unclear if
compositional patterns also emerge in the visual embedding space. In this work,
we investigate compositionality in the image domain, where the analysis of
compositional properties is challenged by noise and sparsity of visual data. We
address these problems and propose a framework, called Geodesically
Decomposable Embeddings (GDE), that approximates image representations with
geometry-aware compositional structures in the latent space. We demonstrate
that visual embeddings of pre-trained VLMs exhibit a compositional arrangement,
and evaluate the effectiveness of this property in the tasks of compositional
classification and group robustness. GDE achieves stronger performance in
compositional classification compared to its counterpart method that assumes
linear geometry of the latent space. Notably, it is particularly effective for
group robustness, where we achieve higher results than task-specific solutions.
Our results indicate that VLMs can automatically develop a human-like form of
compositional reasoning in the visual domain, making their underlying processes
more interpretable. Code is available at
https://github.com/BerasiDavide/vlm_image_compositionality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for CVPR 2025 (with Supp.Mat.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal-Guided Spiking Neural Networks for Event-Based Human Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Yang, Shilin Lu, Shizheng Wang, Meng Hwa Er, Zengwei Zheng, Alex C. Kot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the promising interplay between spiking neural networks
(SNNs) and event-based cameras for privacy-preserving human action recognition
(HAR). The unique feature of event cameras in capturing only the outlines of
motion, combined with SNNs' proficiency in processing spatiotemporal data
through spikes, establishes a highly synergistic compatibility for event-based
HAR. Previous studies, however, have been limited by SNNs' ability to process
long-term temporal information, essential for precise HAR. In this paper, we
introduce two novel frameworks to address this: temporal segment-based SNN
(\textit{TS-SNN}) and 3D convolutional SNN (\textit{3D-SNN}). The
\textit{TS-SNN} extracts long-term temporal information by dividing actions
into shorter segments, while the \textit{3D-SNN} replaces 2D spatial elements
with 3D components to facilitate the transmission of temporal information. To
promote further research in event-based HAR, we create a dataset,
\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V
event camera $(1280 \times 800)$, comprising 7 distinct actions. Extensive
experimental results show that our proposed frameworks surpass state-of-the-art
SNN methods on our newly collected dataset and three other neuromorphic
datasets, showcasing their effectiveness in handling long-range temporal
information for event-based HAR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R-LiViT: A <span class="highlight-title">LiDAR</span>-<span class="highlight-title">Visual</span>-Thermal <span class="highlight-title">Dataset</span> Enabling Vulnerable Road User
  Focused Roadside Perception <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Mirlach, Lei Wan, Andreas Wiedholz, Hannan Ejaz Keen, Andreas Eich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, the integration of roadside perception systems is
essential for overcoming occlusion challenges and enhancing the safety of
Vulnerable Road Users (VRUs). While LiDAR and visual (RGB) sensors are commonly
used, thermal imaging remains underrepresented in datasets, despite its
acknowledged advantages for VRU detection in extreme lighting conditions. In
this paper, we present R-LiViT, the first dataset to combine LiDAR, RGB, and
thermal imaging from a roadside perspective, with a strong focus on VRUs.
R-LiViT captures three intersections during both day and night, ensuring a
diverse dataset. It includes 10,000 LiDAR frames and 2,400 temporally and
spatially aligned RGB and thermal images across over 150 traffic scenarios,
with 6 and 8 annotated classes respectively, providing a comprehensive resource
for tasks such as object detection and tracking. The dataset1 and the code for
reproducing our evaluation results2 are made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, submitted to ICCV2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New Statistical Model of Star Speckles for Learning to Detect and
  Characterize Exoplanets in Direct Imaging Observations <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Théo Bodrito, Olivier Flasseur, Julien Mairal, Jean Ponce, Maud Langlois, Anne-Marie Lagrange
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The search for exoplanets is an active field in astronomy, with direct
imaging as one of the most challenging methods due to faint exoplanet signals
buried within stronger residual starlight. Successful detection requires
advanced image processing to separate the exoplanet signal from this nuisance
component. This paper presents a novel statistical model that captures nuisance
fluctuations using a multi-scale approach, leveraging problem symmetries and a
joint spectral channel representation grounded in physical principles. Our
model integrates into an interpretable, end-to-end learnable framework for
simultaneous exoplanet detection and flux estimation. The proposed algorithm is
evaluated against the state of the art using datasets from the SPHERE
instrument operating at the Very Large Telescope (VLT). It significantly
improves the precision-recall trade-off, notably on challenging datasets that
are otherwise unusable by astronomers. The proposed approach is computationally
efficient, robust to varying data quality, and well suited for large-scale
observational surveys.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The CASTLE 2024 <span class="highlight-title">Dataset</span>: Advancing the Art of Multimodal Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Rossetto, Werner Bailer, Duc-Tien Dang-Nguyen, Graham Healy, Björn Þór Jónsson, Onanong Kongmeesub, Hoang-Bao Le, Stevan Rudinac, Klaus Schöffmann, Florian Spiess, Allie Tran, Minh-Triet Tran, Quang-Linh Tran, Cathal Gurrin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Egocentric video has seen increased interest in recent years, as it is used
in a range of areas. However, most existing datasets are limited to a single
perspective. In this paper, we present the CASTLE 2024 dataset, a multimodal
collection containing ego- and exo-centric (i.e., first- and third-person
perspective) video and audio from 15 time-aligned sources, as well as other
sensor streams and auxiliary data. The dataset was recorded by volunteer
participants over four days in a fixed location and includes the point of view
of 10 participants, with an additional 5 fixed cameras providing an exocentric
perspective. The entire dataset contains over 600 hours of UHD video recorded
at 50 frames per second. In contrast to other datasets, CASTLE 2024 does not
contain any partial censoring, such as blurred faces or distorted audio. The
dataset is available via https://castle-dataset.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, dataset available via
  https://castle-dataset.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Accuracy: What Matters in Designing Well-Behaved Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Hesse, Doğukan Bağcı, Bernt Schiele, Simone Schaub-Meyer, Stefan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has become an essential part of computer vision, with deep
neural networks (DNNs) excelling in predictive performance. However, they often
fall short in other critical quality dimensions, such as robustness,
calibration, or fairness. While existing studies have focused on a subset of
these quality dimensions, none have explored a more general form of
"well-behavedness" of DNNs. With this work, we address this gap by
simultaneously studying nine different quality dimensions for image
classification. Through a large-scale study, we provide a bird's-eye view by
analyzing 326 backbone models and how different training paradigms and model
architectures affect the quality dimensions. We reveal various new insights
such that (i) vision-language models exhibit high fairness on ImageNet-1k
classification and strong robustness against domain changes; (ii)
self-supervised learning is an effective training paradigm to improve almost
all considered quality dimensions; and (iii) the training dataset size is a
major driver for most of the quality dimensions. We conclude our study by
introducing the QUBA score (Quality Understanding Beyond Accuracy), a novel
metric that ranks models across multiple dimensions of quality, enabling
tailored recommendations based on specific user needs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/visinf/beyond-accuracy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Missing Target-Relevant Information Prediction with World Model for
  Accurate Zero-Shot Composed Image Retrieval <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanmin Tang, Jing Yu, Keke Gai, Jiamin Zhuang, Gang Xiong, Gaopeng Gou, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-Shot Composed Image Retrieval (ZS-CIR) involves diverse tasks with a
broad range of visual content manipulation intent across domain, scene, object,
and attribute. The key challenge for ZS-CIR tasks is to modify a reference
image according to manipulation text to accurately retrieve a target image,
especially when the reference image is missing essential target content. In
this paper, we propose a novel prediction-based mapping network, named
PrediCIR, to adaptively predict the missing target visual content in reference
images in the latent space before mapping for accurate ZS-CIR. Specifically, a
world view generation module first constructs a source view by omitting certain
visual content of a target view, coupled with an action that includes the
manipulation intent derived from existing image-caption pairs. Then, a target
content prediction module trains a world model as a predictor to adaptively
predict the missing visual information guided by user intention in manipulating
text at the latent space. The two modules map an image with the predicted
relevant information to a pseudo-word token without extra supervision. Our
model shows strong generalization ability on six ZS-CIR tasks. It obtains
consistent and significant performance boosts ranging from 1.73% to 4.45% over
the best methods and achieves new state-of-the-art results on ZS-CIR. Our code
is available at https://github.com/Pter61/predicir.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Few-Shot Object Detection on Blood Smear Images: A Case Study
  of Leukocytes and Schistocytes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Antonio Mura, Michela Pinna, Lorenzo Putzu, Andrea Loddo, Alessandra Perniciano, Olga Mulas, Cecilia Di Ruberto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of blood disorders often hinges upon the quantification of
specific blood cell types. Variations in cell counts may indicate the presence
of pathological conditions. Thus, the significance of developing precise
automatic systems for blood cell enumeration is underscored. The investigation
focuses on a novel approach termed DE-ViT. This methodology is employed in a
Few-Shot paradigm, wherein training relies on a limited number of images. Two
distinct datasets are utilised for experimental purposes: the Raabin-WBC
dataset for Leukocyte detection and a local dataset for Schistocyte
identification. In addition to the DE-ViT model, two baseline models, Faster
R-CNN 50 and Faster R-CNN X 101, are employed, with their outcomes being
compared against those of the proposed model. While DE-ViT has demonstrated
state-of-the-art performance on the COCO and LVIS datasets, both baseline
models surpassed its performance on the Raabin-WBC dataset. Moreover, only
Faster R-CNN X 101 yielded satisfactory results on the SC-IDB. The observed
disparities in performance may possibly be attributed to domain shift
phenomena.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAA-TSO: Geometry-Aware Assisted Depth Completion for Transparent and
  Specular Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Liu, Tong Jia, Da Cai, Hao Wang, Dongyue Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transparent and specular objects are frequently encountered in daily life,
factories, and laboratories. However, due to the unique optical properties, the
depth information on these objects is usually incomplete and inaccurate, which
poses significant challenges for downstream robotics tasks. Therefore, it is
crucial to accurately restore the depth information of transparent and specular
objects. Previous depth completion methods for these objects usually use RGB
information as an additional channel of the depth image to perform depth
prediction. Due to the poor-texture characteristics of transparent and specular
objects, these methods that rely heavily on color information tend to generate
structure-less depth predictions. Moreover, these 2D methods cannot effectively
explore the 3D structure hidden in the depth channel, resulting in depth
ambiguity. To this end, we propose a geometry-aware assisted depth completion
method for transparent and specular objects, which focuses on exploring the 3D
structural cues of the scene. Specifically, besides extracting 2D features from
RGB-D input, we back-project the input depth to a point cloud and build the 3D
branch to extract hierarchical scene-level 3D structural features. To exploit
3D geometric information, we design several gated cross-modal fusion modules to
effectively propagate multi-level 3D geometric features to the image branch. In
addition, we propose an adaptive correlation aggregation strategy to
appropriately assign 3D features to the corresponding 2D features. Extensive
experiments on ClearGrasp, OOD, TransCG, and STD datasets show that our method
outperforms other state-of-the-art methods. We further demonstrate that our
method significantly enhances the performance of downstream robotic grasping
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Analysis of Image <span class="highlight-title">Descriptor</span>s for Histopathological
  Classification of Gastric Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Usai, Andrea Loddo, Alessandra Perniciano, Maurizio Atzori, Cecilia Di Ruberto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gastric cancer ranks as the fifth most common and fourth most lethal cancer
globally, with a dismal 5-year survival rate of approximately 20%. Despite
extensive research on its pathobiology, the prognostic predictability remains
inadequate, compounded by pathologists' high workload and potential diagnostic
errors. Thus, automated, accurate histopathological diagnosis tools are
crucial. This study employs Machine Learning and Deep Learning techniques to
classify histopathological images into healthy and cancerous categories. Using
handcrafted and deep features with shallow learning classifiers on the
GasHisSDB dataset, we offer a comparative analysis and insights into the most
robust and high-performing combinations of features and classifiers for
distinguishing between normal and abnormal histopathological images without
fine-tuning strategies. With the RF classifier, our approach can reach F1 of
93.4%, demonstrating its validity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging
  <span class="highlight-title">Diffusion</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Zheng, Shouyi Lu, Renbo Huang, Minqing Huang, Fan Lu, Wei Tian, Guirong Zhuo, Lu Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce R2LDM, an innovative approach for generating dense and accurate
4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of
utilizing range images or bird's eye view (BEV) images, we represent both LiDAR
and 4D radar point clouds using voxel features, which more effectively capture
3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model
(LVDM), which performs the diffusion process in the latent space. Additionally,
a novel Latent Point Cloud Reconstruction (LPCR) module is utilized to
reconstruct point clouds from high-dimensional latent voxel features. As a
result, R2LDM effectively generates LiDAR-like point clouds from paired raw
radar data. We evaluate our approach on two different datasets, and the
experimental results demonstrate that our model achieves 6- to 10-fold
densification of radar point clouds, outperforming state-of-the-art baselines
in 4D radar point cloud super-resolution. Furthermore, the enhanced radar point
clouds generated by our method significantly improve downstream tasks,
achieving up to 31.7% improvement in point cloud registration recall rate and
24.9% improvement in object detection accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal Multi-platform Person Re-Identification: Benchmark and
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyang Ha, Songyi Jiang, Bin Li, Bikang Pan, Yihang Zhu, Junjie Zhang, Xiatian Zhu, Shaogang Gong, Jingya Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional person re-identification (ReID) research is often limited to
single-modality sensor data from static cameras, which fails to address the
complexities of real-world scenarios where multi-modal signals are increasingly
prevalent. For instance, consider an urban ReID system integrating stationary
RGB cameras, nighttime infrared sensors, and UAVs equipped with dynamic
tracking capabilities. Such systems face significant challenges due to
variations in camera perspectives, lighting conditions, and sensor modalities,
hindering effective person ReID. To address these challenges, we introduce the
MP-ReID benchmark, a novel dataset designed specifically for multi-modality and
multi-platform ReID. This benchmark uniquely compiles data from 1,930
identities across diverse modalities, including RGB, infrared, and thermal
imaging, captured by both UAVs and ground-based cameras in indoor and outdoor
environments. Building on this benchmark, we introduce Uni-Prompt ReID, a
framework with specific-designed prompts, tailored for cross-modality and
cross-platform scenarios. Our method consistently outperforms state-of-the-art
approaches, establishing a robust foundation for future research in complex and
dynamic ReID environments. Our dataset are available
at:https://mp-reid.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwan Yun, Chaelin Kim, Hangyeul Shin, Junyong Noh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent 3D face editing methods using masks have produced high-quality edited
images by leveraging Neural Radiance Fields (NeRF). Despite their impressive
performance, existing methods often provide limited user control due to the use
of pre-trained segmentation masks. To utilize masks with a desired layout, an
extensive training dataset is required, which is challenging to gather. We
present FFaceNeRF, a NeRF-based face editing technique that can overcome the
challenge of limited user control due to the use of fixed mask layouts. Our
method employs a geometry adapter with feature injection, allowing for
effective manipulation of geometry attributes. Additionally, we adopt latent
mixing for tri-plane augmentation, which enables training with a few samples.
This facilitates rapid model adaptation to desired mask layouts, crucial for
applications in fields like personalized medical imaging or creative face
editing. Our comparative evaluations demonstrate that FFaceNeRF surpasses
existing mask based face editing methods in terms of flexibility, control, and
generated image quality, paving the way for future advancements in customized
and high-fidelity 3D face editing. The code is available on the
{\href{https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2025, 11 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ColabSfM: Collaborative Structure-from-Motion by Point Cloud
  Registration <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Edstedt, André Mateus, Alberto Jaenal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structure-from-Motion (SfM) is the task of estimating 3D structure and camera
poses from images. We define Collaborative SfM (ColabSfM) as sharing
distributed SfM reconstructions. Sharing maps requires estimating a joint
reference frame, which is typically referred to as registration. However, there
is a lack of scalable methods and training datasets for registering SfM
reconstructions. In this paper, we tackle this challenge by proposing the
scalable task of point cloud registration for SfM reconstructions. We find that
current registration methods cannot register SfM point clouds when trained on
existing datasets. To this end, we propose a SfM registration dataset
generation pipeline, leveraging partial reconstructions from synthetically
generated camera trajectories for each scene. Finally, we propose a simple but
impactful neural refiner on top of the SotA registration method RoITr that
yields significant improvements, which we call RefineRoITr. Our extensive
experimental evaluation shows that our proposed pipeline and model enables
ColabSfM. Code is available at https://github.com/EricssonResearch/ColabSfM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does a Rising Tide Lift All Boats? Bias Mitigation for AI-based CMR
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiarna Lee, Esther Puyol-Antón, Bram Ruijsink, Miaojing Shi, Andrew P. King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) is increasingly being used for medical imaging
tasks. However, there can be biases in the resulting models, particularly when
they were trained using imbalanced training datasets. One such example has been
the strong race bias effect in cardiac magnetic resonance (CMR) image
segmentation models. Although this phenomenon has been reported in a number of
publications, little is known about the effectiveness of bias mitigation
algorithms in this domain. We aim to investigate the impact of common bias
mitigation methods to address bias between Black and White subjects in AI-based
CMR segmentation models. Specifically, we use oversampling, importance
reweighing and Group DRO as well as combinations of these techniques to
mitigate the race bias. Furthermore, motivated by recent findings on the root
causes of AI-based CMR segmentation bias, we evaluate the same methods using
models trained and evaluated on cropped CMR images. We find that bias can be
mitigated using oversampling, significantly improving performance for the
underrepresented Black subjects whilst not significantly reducing the majority
White subjects' performance. Group DRO also improves performance for Black
subjects but not significantly, while reweighing decreases performance for
Black subjects. Using a combination of oversampling and Group DRO also improves
performance for Black subjects but not significantly. Using cropped images
increases performance for both races and reduces the bias, whilst adding
oversampling as a bias mitigation technique with cropped images reduces the
bias further.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gensheng Pei, Tao Chen, Yujia Wang, Xinhao Cai, Xiangbo Shu, Tianfei Zhou, Yazhou Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The CLIP model has demonstrated significant advancements in aligning visual
and language modalities through large-scale pre-training on image-text pairs,
enabling strong zero-shot classification and retrieval capabilities on various
domains. However, CLIP's training remains computationally intensive, with high
demands on both data processing and memory. To address these challenges, recent
masking strategies have emerged, focusing on the selective removal of image
patches to improve training efficiency. Although effective, these methods often
compromise key semantic information, resulting in suboptimal alignment between
visual features and text descriptions. In this work, we present a concise yet
effective approach called Patch Generation-to-Selection to enhance CLIP's
training efficiency while preserving critical semantic content. Our method
introduces a gradual masking process in which a small set of candidate patches
is first pre-selected as potential mask regions. Then, we apply Sobel edge
detection across the entire image to generate an edge mask that prioritizes the
retention of the primary object areas. Finally, similarity scores between the
candidate mask patches and their neighboring patches are computed, with optimal
transport normalization refining the selection process to ensure a balanced
similarity matrix. Our approach, CLIP-PGS, sets new state-of-the-art results in
zero-shot classification and retrieval tasks, achieving superior performance in
robustness evaluation and language compositionality benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Halton Scheduler For Masked Generative Image Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Besnier, Mickael Chen, David Hurych, Eduardo Valle, Matthieu Cord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Generative Image Transformers (MaskGIT) have emerged as a scalable and
efficient image generation framework, able to deliver high-quality visuals with
low inference costs. However, MaskGIT's token unmasking scheduler, an essential
component of the framework, has not received the attention it deserves. We
analyze the sampling objective in MaskGIT, based on the mutual information
between tokens, and elucidate its shortcomings. We then propose a new sampling
strategy based on our Halton scheduler instead of the original Confidence
scheduler. More precisely, our method selects the token's position according to
a quasi-random, low-discrepancy Halton sequence. Intuitively, that method
spreads the tokens spatially, progressively covering the image uniformly at
each step. Our analysis shows that it allows reducing non-recoverable sampling
errors, leading to simpler hyper-parameters tuning and better quality images.
Our scheduler does not require retraining or noise injection and may serve as a
simple drop-in replacement for the original sampling strategy. Evaluation of
both class-to-image synthesis on ImageNet and text-to-image generation on the
COCO dataset demonstrates that the Halton scheduler outperforms the Confidence
scheduler quantitatively by reducing the FID and qualitatively by generating
more diverse and more detailed images. Our code is at
https://github.com/valeoai/Halton-MaskGIT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Styled Text Image Generation, but Make It Autoregressive <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vittorio Pippi, Fabio Quattrini, Silvia Cascianelli, Alessio Tonioni, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Styled Handwritten Text Generation (HTG) has recently received attention from
the computer vision and document analysis communities, which have developed
several solutions, either GAN- or diffusion-based, that achieved promising
results. Nonetheless, these strategies fail to generalize to novel styles and
have technical constraints, particularly in terms of maximum output length and
training efficiency. To overcome these limitations, in this work, we propose a
novel framework for text image generation, dubbed Emuru. Our approach leverages
a powerful text image representation model (a variational autoencoder) combined
with an autoregressive Transformer. Our approach enables the generation of
styled text images conditioned on textual content and style examples, such as
specific fonts or handwriting styles. We train our model solely on a diverse,
synthetic dataset of English text rendered in over 100,000 typewritten and
calligraphy fonts, which gives it the capability to reproduce unseen styles
(both fonts and users' handwriting) in zero-shot. To the best of our knowledge,
Emuru is the first autoregressive model for HTG, and the first designed
specifically for generalization to novel styles. Moreover, our model generates
images without background artifacts, which are easier to use for downstream
applications. Extensive evaluation on both typewritten and handwritten,
any-length text image generation scenarios demonstrates the effectiveness of
our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superpowering Open-Vocabulary Object Detectors for X-ray <span class="highlight-title">Vision</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Garcia-Fernandez, Lorenzo Vaquero, Mingxuan Liu, Feng Xue, Daniel Cores, Nicu Sebe, Manuel Mucientes, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary object detection (OvOD) is set to revolutionize security
screening by enabling systems to recognize any item in X-ray scans. However,
developing effective OvOD models for X-ray imaging presents unique challenges
due to data scarcity and the modality gap that prevents direct adoption of
RGB-based solutions. To overcome these limitations, we propose RAXO, a
training-free framework that repurposes off-the-shelf RGB OvOD detectors for
robust X-ray detection. RAXO builds high-quality X-ray class descriptors using
a dual-source retrieval strategy. It gathers relevant RGB images from the web
and enriches them via a novel X-ray material transfer mechanism, eliminating
the need for labeled databases. These visual descriptors replace text-based
classification in OvOD, leveraging intra-modal feature distances for robust
detection. Extensive experiments demonstrate that RAXO consistently improves
OvOD performance, providing an average mAP increase of up to 17.0 points over
base detectors. To further support research in this emerging field, we also
introduce DET-COMPASS, a new benchmark featuring bounding box annotations for
over 300 object categories, enabling large-scale evaluation of OvOD in X-ray.
Code and dataset available at: https://github.com/PAGF188/RAXO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PVChat: Personalized Video Chat with One-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Shi, Weilong Yan, Gang Xu, Yumeng Li, Yuchen Li, Zhenxi Li, Fei Richard Yu, Ming Li, Si Yong Yeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video large language models (ViLLMs) excel in general video understanding,
e.g., recognizing activities like talking and eating, but struggle with
identity-aware comprehension, such as "Wilson is receiving chemotherapy" or
"Tom is discussing with Sarah", limiting their applicability in smart
healthcare and smart home environments. To address this limitation, we propose
a one-shot learning framework PVChat, the first personalized ViLLM that enables
subject-aware question answering (QA) from a single video for each subject. Our
approach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically
augmented video-QA dataset, leveraging a progressive image-to-video learning
strategy. Specifically, we introduce an automated augmentation pipeline that
synthesizes identity-preserving positive samples and retrieves hard negatives
from existing video corpora, generating a diverse training dataset with four QA
types: existence, appearance, action, and location inquiries. To enhance
subject-specific learning, we propose a ReLU Routing MoH attention mechanism,
alongside two novel objectives: (1) Smooth Proximity Regularization for
progressive learning through exponential distance scaling and (2) Head
Activation Enhancement for balanced attention routing. Finally, we adopt a
two-stage training strategy, transitioning from image pre-training to video
fine-tuning, enabling a gradual learning process from static attributes to
dynamic representations. We evaluate PVChat on diverse datasets covering
medical scenarios, TV series, anime, and real-world footage, demonstrating its
superiority in personalized feature understanding after learning from a single
video, compared to state-of-the-art ViLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIDiffGes: Decoupled Semi-Implicit <span class="highlight-title">Diffusion</span> Models for Real-time
  Gesture Generation from Speech <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongkang Cheng, Shaoli Huang, Xuelin Chen, Jifeng Ning, Mingming Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable synthesis quality and diversity
in generating co-speech gestures. However, the computationally intensive
sampling steps associated with diffusion models hinder their practicality in
real-world applications. Hence, we present DIDiffGes, for a Decoupled
Semi-Implicit Diffusion model-based framework, that can synthesize
high-quality, expressive gestures from speech using only a few sampling steps.
Our approach leverages Generative Adversarial Networks (GANs) to enable
large-step sampling for diffusion model. We decouple gesture data into body and
hands distributions and further decompose them into marginal and conditional
distributions. GANs model the marginal distribution implicitly, while L2
reconstruction loss learns the conditional distributions exciplictly. This
strategy enhances GAN training stability and ensures expressiveness of
generated full-body gestures. Our framework also learns to denoise root noise
conditioned on local body representation, guaranteeing stability and realism.
DIDiffGes can generate gestures from speech with just 10 sampling steps,
without compromising quality and expressiveness, reducing the number of
sampling steps by a factor of 100 compared to existing methods. Our user study
reveals that our method outperforms state-of-the-art approaches in human
likeness, appropriateness, and style correctness. Project is
https://cyk990422.github.io/DIDiffGes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework
  for Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyijie Wang, Kathleen M. Curran, Guénolé Silvestre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of ultrasound (US) images of the cervical muscles is
crucial for precision healthcare. The demand for automatic computer-assisted
methods is high. However, the scarcity of labeled data hinders the development
of these methods. Advanced semi-supervised learning approaches have displayed
promise in overcoming this challenge by utilizing labeled and unlabeled data.
This study introduces a novel semi-supervised learning (SSL) framework that
integrates dual neural networks. This SSL framework utilizes both networks to
generate pseudo-labels and cross-supervise each other at the pixel level.
Additionally, a self-supervised contrastive learning strategy is introduced,
which employs a pair of deep representations to enhance feature learning
capabilities, particularly on unlabeled data. Our framework demonstrates
competitive performance in cervical segmentation tasks. Our codes are publicly
available on https://github.com/13204942/SSL\_Cervical\_Segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for an oral presentation at ISBI 2025 Fetal Ultrasound Grand
  Challenge: Semi-Supervised Cervical Segmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scoring, Remember, and Reference: Catching Camouflaged Objects in Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Feng, Shuyong Gao, Fuzhen Yan, Yicheng Song, Lingyi Hong, Junjie Hu, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Camouflaged Object Detection (VCOD) aims to segment objects whose
appearances closely resemble their surroundings, posing a challenging and
emerging task. Existing vision models often struggle in such scenarios due to
the indistinguishable appearance of camouflaged objects and the insufficient
exploitation of dynamic information in videos. To address these challenges, we
propose an end-to-end VCOD framework inspired by human memory-recognition,
which leverages historical video information by integrating memory reference
frames for camouflaged sequence processing. Specifically, we design a
dual-purpose decoder that simultaneously generates predicted masks and scores,
enabling reference frame selection based on scores while introducing auxiliary
supervision to enhance feature extraction.Furthermore, this study introduces a
novel reference-guided multilevel asymmetric attention mechanism, effectively
integrating long-term reference information with short-term motion cues for
comprehensive feature extraction. By combining these modules, we develop the
Scoring, Remember, and Reference (SRR) framework, which efficiently extracts
information to locate targets and employs memory guidance to improve subsequent
processing. With its optimized module design and effective utilization of video
data, our model achieves significant performance improvements, surpassing
existing approaches by 10% on benchmark datasets while requiring fewer
parameters (54M) and only a single pass through the video. The code will be
made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HAPI: A Model for Learning <span class="highlight-title">Robo</span>t Facial Expressions from Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsheng Yang, Qianying Liu, Wataru Sato, Takashi Minato, Chaoran Liu, Shin'ya Nishida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic robotic facial expression generation is crucial for human-robot
interaction, as handcrafted methods based on fixed joint configurations often
yield rigid and unnatural behaviors. Although recent automated techniques
reduce the need for manual tuning, they tend to fall short by not adequately
bridging the gap between human preferences and model predictions-resulting in a
deficiency of nuanced and realistic expressions due to limited degrees of
freedom and insufficient perceptual integration. In this work, we propose a
novel learning-to-rank framework that leverages human feedback to address this
discrepancy and enhanced the expressiveness of robotic faces. Specifically, we
conduct pairwise comparison annotations to collect human preference data and
develop the Human Affective Pairwise Impressions (HAPI) model, a Siamese
RankNet-based approach that refines expression evaluation. Results obtained via
Bayesian Optimization and online expression survey on a 35-DOF android platform
demonstrate that our approach produces significantly more realistic and
socially resonant expressions of Anger, Happiness, and Surprise than those
generated by baseline and expert-designed methods. This confirms that our
framework effectively bridges the gap between human preferences and model
predictions while robustly aligning robotic expression generation with human
affective responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExCap3D: Expressive 3D Scene Understanding via Object Captioning with
  Varying Detail 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chandan Yeshwanth, David Rozenberszki, Angela Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating text descriptions of objects in 3D indoor scenes is an important
building block of embodied understanding. Existing methods do this by
describing objects at a single level of detail, which often does not capture
fine-grained details such as varying textures, materials, and shapes of the
parts of objects. We propose the task of expressive 3D captioning: given an
input 3D scene, describe objects at multiple levels of detail: a high-level
object description, and a low-level description of the properties of its parts.
To produce such captions, we present ExCap3D, an expressive 3D captioning model
which takes as input a 3D scan, and for each detected object in the scan,
generates a fine-grained collective description of the parts of the object,
along with an object-level description conditioned on the part-level
description. We design ExCap3D to encourage semantic consistency between the
generated text descriptions, as well as textual similarity in the latent space,
to further increase the quality of the generated captions. To enable this task,
we generated the ExCap3D Dataset by leveraging a visual-language model (VLM)
for multi-view captioning. The ExCap3D Dataset contains captions on the
ScanNet++ dataset with varying levels of detail, comprising 190k text
descriptions of 34k 3D objects in 947 indoor scenes. Our experiments show that
the object- and part-level of detail captions generated by ExCap3D are of
higher quality than those produced by state-of-the-art methods, with a Cider
score improvement of 17% and 124% for object- and part-level details
respectively. Our code, dataset and models will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://cy94.github.io/excap3d/, Video:
  https://www.youtube.com/watch?v=SQRV1l_0oY0</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Attentive Representative Sample Selection Strategy Combined with
  Balanced Batch Training for Skin Lesion Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Lloyd-Brown, Susan Francis, Caroline Hoad, Penny Gowland, Karen Mullinger, Andrew French, Xin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An often overlooked problem in medical image segmentation research is the
effective selection of training subsets to annotate from a complete set of
unlabelled data. Many studies select their training sets at random, which may
lead to suboptimal model performance, especially in the minimal supervision
setting where each training image has a profound effect on performance
outcomes. This work aims to address this issue. We use prototypical contrasting
learning and clustering to extract representative and diverse samples for
annotation. We improve upon prior works with a bespoke cluster-based image
selection process. Additionally, we introduce the concept of unsupervised
balanced batch dataloading to medical image segmentation, which aims to improve
model learning with minimally annotated data. We evaluated our method on a
public skin lesion dataset (ISIC 2018) and compared it to another
state-of-the-art data sampling method. Our method achieved superior performance
in a low annotation budget scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISBI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented
  Reality via 3D Gaussian Splatting <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianchuan Chen, Jingchuan Hu, Gaige Wang, Zhonghua Jiang, Tiansong Zhou, Zhiwen Chen, Chengfei Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realistic 3D full-body talking avatars hold great potential in AR, with
applications ranging from e-commerce live streaming to holographic
communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike
avatar creation, existing methods struggle with fine-grained control of facial
expressions and body movements in full-body talking tasks. Additionally, they
often lack sufficient details and cannot run in real-time on mobile devices. We
present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking
avatar driven by various signals. Our approach starts by creating a
personalized clothed human parametric template that binds Gaussians to
represent appearances. We then pre-train a StyleUnet-based network to handle
complex pose-dependent non-rigid deformation, which can capture high-frequency
appearance details but is too resource-intensive for mobile devices. To
overcome this, we "bake" the non-rigid deformations into a lightweight
MLP-based network using a distillation technique and develop blend shapes to
compensate for details. Extensive experiments show that TaoAvatar achieves
state-of-the-art rendering quality while running in real-time across various
devices, maintaining 90 FPS on high-definition stereo devices such as the Apple
Vision Pro.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025, project page:
  https://PixelAI-Team.github.io/TaoAvatar</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Efficacy of Partial Denoising Using Bit Plane Slicing for
  Enhanced Fracture Identification: A Comparative Study of Deep Learning-Based
  Approaches and Handcrafted Feature Extraction Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Snigdha Paul, Sambit Mallick, Anindya Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer vision has transformed medical diagnosis, treatment, and research
through advanced image processing and machine learning techniques. Fracture
classification, a critical area in healthcare, has greatly benefited from these
advancements, yet accurate detection is challenged by complex patterns and
image noise. Bit plane slicing enhances medical images by reducing noise
interference and extracting informative features. This research explores
partial denoising techniques to provide practical solutions for improved
fracture analysis, ultimately enhancing patient care. The study explores deep
learning model DenseNet and handcrafted feature extraction. Decision Tree and
Random Forest, were employed to train and evaluate distinct image
representations. These include the original image, the concatenation of the
four bit planes from the LSB as well as MSB, the fully denoised image, and an
image consisting of 6 bit planes from MSB and 2 denoised bit planes from LSB.
The purpose of forming these diverse image representations is to analyze SNR as
well as classification accuracy and identify the bit planes that contain the
most informative features. Moreover, the study delves into the significance of
partial denoising techniques in preserving crucial features, leading to
improvements in classification results. Notably, this study shows that
employing the Random Forest classifier, the partially denoised image
representation exhibited a testing accuracy of 95.61% surpassing the
performance of other image representations. The outcomes of this research
provide valuable insights into the development of efficient preprocessing,
feature extraction and classification approaches for fracture identification.
By enhancing diagnostic accuracy, these advancements hold the potential to
positively impact patient care and overall medical outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnimatePainter: A Self-Supervised Rendering Framework for Reconstructing
  Painting Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Hu, Shuyong Gao, Qianyu Guo, Yan Wang, Qishan Wang, Yuang Feng, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can intuitively decompose an image into a sequence of strokes to
create a painting, yet existing methods for generating drawing processes are
limited to specific data types and often rely on expensive human-annotated
datasets. We propose a novel self-supervised framework for generating drawing
processes from any type of image, treating the task as a video generation
problem. Our approach reverses the drawing process by progressively removing
strokes from a reference image, simulating a human-like creation sequence.
Crucially, our method does not require costly datasets of real human drawing
processes; instead, we leverage depth estimation and stroke rendering to
construct a self-supervised dataset. We model human drawings as "refinement"
and "layering" processes and introduce depth fusion layers to enable video
generation models to learn and replicate human drawing behavior. Extensive
experiments validate the effectiveness of our approach, demonstrating its
ability to generate realistic drawings without the need for real drawing
process data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAW-Adapter: Adapting Pre-trained <span class="highlight-title">Visual</span> Model to <span class="highlight-title">Camera</span> RAW Images and
  A Benchmark <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziteng Cui, Jianfei Yang, Tatsuya Harada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the computer vision community, the preference for pre-training visual
models has largely shifted toward sRGB images due to their ease of acquisition
and compact storage. However, camera RAW images preserve abundant physical
details across diverse real-world scenarios. Despite this, most existing visual
perception methods that utilize RAW data directly integrate image signal
processing (ISP) stages with subsequent network modules, often overlooking
potential synergies at the model level. Building on recent advances in
adapter-based methodologies in both NLP and computer vision, we propose
RAW-Adapter, a novel framework that incorporates learnable ISP modules as
input-level adapters to adjust RAW inputs. At the same time, it employs
model-level adapters to seamlessly bridge ISP processing with high-level
downstream architectures. Moreover, RAW-Adapter serves as a general framework
applicable to various computer vision frameworks.
  Furthermore, we introduce RAW-Bench, which incorporates 17 types of RAW-based
common corruptions, including lightness degradations, weather effects,
blurriness, camera imaging degradations, and variations in camera color
response. Using this benchmark, we systematically compare the performance of
RAW-Adapter with state-of-the-art (SOTA) ISP methods and other RAW-based
high-level vision algorithms. Additionally, we propose a RAW-based data
augmentation strategy to further enhance RAW-Adapter's performance and improve
its out-of-domain (OOD) generalization ability. Extensive experiments
substantiate the effectiveness and efficiency of RAW-Adapter, highlighting its
robust performance across diverse scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 17 figures, extension of ECCV 2024 work: arXiv:2408.14802</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Tale of Two Classes: Adapting Supervised Contrastive Learning to
  Binary Imbalanced <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Mildenberger, Paul Hager, Daniel Rueckert, Martin J Menten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised contrastive learning (SupCon) has proven to be a powerful
alternative to the standard cross-entropy loss for classification of
multi-class balanced datasets. However, it struggles to learn well-conditioned
representations of datasets with long-tailed class distributions. This problem
is potentially exacerbated for binary imbalanced distributions, which are
commonly encountered during many real-world problems such as medical diagnosis.
In experiments on seven binary datasets of natural and medical images, we show
that the performance of SupCon decreases with increasing class imbalance. To
substantiate these findings, we introduce two novel metrics that evaluate the
quality of the learned representation space. By measuring the class
distribution in local neighborhoods, we are able to uncover structural
deficiencies of the representation space that classical metrics cannot detect.
Informed by these insights, we propose two new supervised contrastive learning
strategies tailored to binary imbalanced datasets that improve the structure of
the representation space and increase downstream classification accuracy over
standard SupCon by up to 35%. We make our code available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Specifying What You Know or Not for Multi-Label Class-Incremental
  Learning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoting Zhang, Dongbao Yang, Chang Liu, Xiaopeng Hong, Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing class incremental learning is mainly designed for single-label
classification task, which is ill-equipped for multi-label scenarios due to the
inherent contradiction of learning objectives for samples with incomplete
labels. We argue that the main challenge to overcome this contradiction in
multi-label class-incremental learning (MLCIL) lies in the model's inability to
clearly distinguish between known and unknown knowledge. This ambiguity hinders
the model's ability to retain historical knowledge, master current classes, and
prepare for future learning simultaneously. In this paper, we target at
specifying what is known or not to accommodate Historical, Current, and
Prospective knowledge for MLCIL and propose a novel framework termed as HCP.
Specifically, (i) we clarify the known classes by dynamic feature purification
and recall enhancement with distribution prior, enhancing the precision and
retention of known information. (ii) We design prospective knowledge mining to
probe the unknown, preparing the model for future learning. Extensive
experiments validate that our method effectively alleviates catastrophic
forgetting in MLCIL, surpassing the previous state-of-the-art by 3.3% on
average accuracy for MS-COCO B0-C10 setting without replay buffers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steady Progress Beats Stagnation: Mutual Aid of Foundation and
  Conventional Models in Mixed Domain Semi-Supervised Medical Image
  Segmentation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinghe Ma, Jian Zhang, Zekun Li, Lei Qi, Qian Yu, Yinghuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pretrained visual foundation models exhibit impressive general
capabilities. However, the extensive prior knowledge inherent in these models
can sometimes be a double-edged sword when adapting them to downstream tasks in
specific domains. In the context of semi-supervised medical image segmentation
with domain shift, foundation models like MedSAM tend to make overconfident
predictions, some of which are incorrect. The error accumulation hinders the
effective utilization of unlabeled data and limits further improvements. In
this paper, we introduce a Synergistic training framework for Foundation and
Conventional models (SynFoC) to address the issue. We observe that a
conventional model trained from scratch has the ability to correct the
high-confidence mispredictions of the foundation model, while the foundation
model can supervise it with high-quality pseudo-labels in the early training
stages. Furthermore, to enhance the collaborative training effectiveness of
both models and promote reliable convergence towards optimization, the
consensus-divergence consistency regularization is proposed. We demonstrate the
superiority of our method across four public multi-domain datasets. In
particular, our method improves the Dice score by 10.31\% on the Prostate
dataset. Our code is available at https://github.com/MQinghe/SynFoC .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High Accuracy Pulmonary Vessel Segmentation for Contrast and
  Non-contrast CT Images and Its Clinical Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Ming, Shaoze Luo, Longfei Zhao, Qiqi Xu, Wei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of pulmonary vessels plays a very critical role in
diagnosing and assessing various lung diseases. In clinical practice, diagnosis
is typically carried out using CTPA images. However, there is a lack of
high-precision pulmonary vessel segmentation algorithms for CTPA, and pulmonary
vessel segmentation for NCCT poses an even greater challenge. In this study, we
propose a 3D image segmentation algorithm for automated pulmonary vessel
segmentation from both contrast and non-contrast CT images. In the network, we
designed a Vessel Lumen Structure Optimization Module (VLSOM), which extracts
the centerline of vessels and adjusts the weights based on the positional
information and adds a Cl-Dice-Loss to supervise the stability of the vessels
structure. In addition, we designed a method for generating vessel GT from CTPA
to NCCT for training models that support both CTPA and NCCT. In this work, we
used 427 sets of high-precision annotated CT data from multiple vendors and
countries. Finally, our experimental model achieved Cl-Recall, Cl-DICE and
Recall values of 0.879, 0.909, 0.934 (CTPA) and 0.928, 0.936, 0.955 (NCCT)
respectively. This shows that our model has achieved good performance in both
accuracy and completeness of pulmonary vessel segmentation. In clinical visual
evaluation, our model also had good segmentation performance on various disease
types and can assist doctors in medical diagnosis, verifying the great
potential of this method in clinical application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Versatile Controls for Video <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Zhang, Hao Zhou, Haoming Qin, Xiaobin Lu, Jiaxing Yan, Guanzhong Wang, Zeyu Chen, Yi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite substantial progress in text-to-video generation, achieving precise
and flexible control over fine-grained spatiotemporal attributes remains a
significant unresolved challenge in video generation research. To address these
limitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework
designed to enable fine-grained control over pre-trained video diffusion models
in a unified manner. VCtrl integrates diverse user-specified control
signals-such as Canny edges, segmentation masks, and human keypoints-into
pretrained video diffusion models via a generalizable conditional module
capable of uniformly encoding multiple types of auxiliary signals without
modifying the underlying generator. Additionally, we design a unified control
signal encoding pipeline and a sparse residual connection mechanism to
efficiently incorporate control representations. Comprehensive experiments and
human evaluations demonstrate that VCtrl effectively enhances controllability
and generation quality. The source code and pre-trained models are publicly
available and implemented using the PaddlePaddle framework at
http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes and Supplementary Material:
  http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Dynamics: Towards Efficient and Dynamic Video Token Representation
  for Video Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Zhang, Zhuowei Li, Dimitris Metaxas, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Token-based video representation has emerged as a promising approach for
enabling large language models to interpret video content. However, existing
token reduction techniques, such as token pruning and token merging, often
disrupt essential spatial-temporal positional embeddings, failing to adequately
balance computational efficiency with fewer tokens. Consequently, these methods
result in relatively lengthy token sequences, limiting their applicability in
scenarios requiring extreme token compression, such as video large language
models. In this paper, we introduce the novel task of extreme short token
reduction, aiming to represent extensive video sequences with a minimal number
of tokens. To address this challenge, we propose Token Dynamics, a new video
representation framework that dynamically reduces token count while preserving
spatial-temporal coherence. Specifically, we disentangle video representations
by separating visual embeddings from grid-level motion information, structuring
them into: 1. a concise token base, created by clustering tokens that describe
object-level content; 2. a token dynamics map, capturing detailed
spatial-temporal motion patterns across grids. Furthermore, we introduce a
cross-dynamics attention mechanism that integrates motion features into the
token base without increasing token length, thereby maintaining compactness and
spatial-temporal integrity. The experiments demonstrate a reduction of token
count to merely 0.07% of the original tokens, with only a minor performance
drop of 1.13%. Additionally, we propose two novel subtasks within extreme token
reduction (fixed-length and adaptive-length compression), both effectively
representing long token sequences for video-language tasks. Our method offers
significantly lower theoretical complexity, fewer tokens, and enhanced
throughput, thus providing an efficient solution for video LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic
  Scene Reconstruction via Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinbo Yan, Rui Peng, Zhiyan Wang, Luyang Tang, Jiayu Yang, Jie Liang, Jiahao Wu, Ronggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building Free-Viewpoint Videos in a streaming manner offers the advantage of
rapid responsiveness compared to offline training methods, greatly enhancing
user experience. However, current streaming approaches face challenges of high
per-frame reconstruction time (10s+) and error accumulation, limiting their
broader application. In this paper, we propose Instant Gaussian Stream (IGS), a
fast and generalizable streaming framework, to address these issues. First, we
introduce a generalized Anchor-driven Gaussian Motion Network, which projects
multi-view 2D motion features into 3D space, using anchor points to drive the
motion of all Gaussians. This generalized Network generates the motion of
Gaussians for each target frame in the time required for a single inference.
Second, we propose a Key-frame-guided Streaming Strategy that refines each key
frame, enabling accurate reconstruction of temporally complex scenes while
mitigating error accumulation. We conducted extensive in-domain and
cross-domain evaluations, demonstrating that our approach can achieve streaming
with a average per-frame reconstruction time of 2s+, alongside a enhancement in
view synthesis quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoT: Geometry-guided Instance-dependent Transition Matrix for
  Semi-supervised Tooth Point Cloud Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Yu, Xiaoqing Guo, Chenxin Li, Yifan Liu, Yixuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving meticulous segmentation of tooth point clouds from intra-oral scans
stands as an indispensable prerequisite for various orthodontic applications.
Given the labor-intensive nature of dental annotation, a significant amount of
data remains unlabeled, driving increasing interest in semi-supervised
approaches. One primary challenge of existing semi-supervised medical
segmentation methods lies in noisy pseudo labels generated for unlabeled data.
To address this challenge, we propose GeoT, the first framework that employs
instance-dependent transition matrix (IDTM) to explicitly model noise in pseudo
labels for semi-supervised dental segmentation. Specifically, to handle the
extensive solution space of IDTM arising from tens of thousands of dental
points, we introduce tooth geometric priors through two key components:
point-level geometric regularization (PLGR) to enhance consistency between
point adjacency relationships in 3D and IDTM spaces, and class-level geometric
smoothing (CLGS) to leverage the fixed spatial distribution of tooth categories
for optimal IDTM estimation. Extensive experiments performed on the public
Teeth3DS dataset and private dataset demonstrate that our method can make full
utilization of unlabeled data to facilitate segmentation, achieving performance
comparable to fully supervised methods with only $20\%$ of the labeled data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IPMI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and
  Generalized <span class="highlight-title">Vision</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Mao, Yuefeng Chen, Rong Zhang, Hui Xue, Zhao Li, Hang Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) has shown great promise in computer vision tasks.
However, machine vision achieved by DNNs cannot be as robust as human
perception. Adversarial attacks and data distribution shifts have been known as
two major scenarios which degrade machine performance and obstacle the wide
deployment of machines "in the wild". In order to break these obstructions and
facilitate the research of model robustness, we develop EasyRobust, a
comprehensive and easy-to-use toolkit for training, evaluation and analysis of
robust vision models. EasyRobust targets at two types of robustness: 1)
Adversarial robustness enables the model to defense against malicious inputs
crafted by worst-case perturbations, also known as adversarial examples; 2)
Non-adversarial robustness enhances the model performance on natural test
images with corruptions or distribution shifts. Thorough benchmarks on image
classification enable EasyRobust to provide an accurate robustness evaluation
on vision models. We wish our EasyRobust can help for training
practically-robust models and promote academic and industrial progress in
closing the gap between human and machine vision. Codes and models of
EasyRobust have been open-sourced in https://github.com/alibaba/easyrobust.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARFlow: Human Action-Reaction Flow Matching with Physical Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Jiang, Jingya Wang, Haotao Lu, Kaiyang Ji, Baoxiong Jia, Siyuan Huang, Ye Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human action-reaction synthesis, a fundamental challenge in modeling causal
human interactions, plays a critical role in applications ranging from virtual
reality to social robotics. While diffusion-based models have demonstrated
promising performance, they exhibit two key limitations for interaction
synthesis: reliance on complex noise-to-reaction generators with intricate
conditional mechanisms, and frequent physical violations in generated motions.
To address these issues, we propose Action-Reaction Flow Matching (ARFlow), a
novel framework that establishes direct action-to-reaction mappings,
eliminating the need for complex conditional mechanisms. Our approach
introduces two key innovations: an x1-prediction method that directly outputs
human motions instead of velocity fields, enabling explicit constraint
enforcement; and a training-free, gradient-based physical guidance mechanism
that effectively prevents body penetration artifacts during sampling. Extensive
experiments on NTU120 and Chi3D datasets demonstrate that ARFlow not only
outperforms existing methods in terms of Fr\'echet Inception Distance and
motion diversity but also significantly reduces body collisions, as measured by
our new Intersection Volume and Intersection Frequency metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computer Vision and Pattern Recognition (cs.CV); Artificial
  Intelligence (cs.AI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Monocular Foundation Model for Fine-grained Depth Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingping Liang, Yutao Hu, Wenqi Shao, Ying Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth completion involves predicting dense depth maps from sparse LiDAR
inputs. However, sparse depth annotations from sensors limit the availability
of dense supervision, which is necessary for learning detailed geometric
features. In this paper, we propose a two-stage knowledge distillation
framework that leverages powerful monocular foundation models to provide dense
supervision for depth completion. In the first stage, we introduce a
pre-training strategy that generates diverse training data from natural images,
which distills geometric knowledge to depth completion. Specifically, we
simulate LiDAR scans by utilizing monocular depth and mesh reconstruction,
thereby creating training data without requiring ground-truth depth. Besides,
monocular depth estimation suffers from inherent scale ambiguity in real-world
settings. To address this, in the second stage, we employ a scale- and
shift-invariant loss (SSI Loss) to learn real-world scales when fine-tuning on
real-world datasets. Our two-stage distillation framework enables depth
completion models to harness the strengths of monocular foundation models.
Experimental results demonstrate that models trained with our two-stage
distillation framework achieve state-of-the-art performance, ranking
\textbf{first place} on the KITTI benchmark. Code is available at
https://github.com/Sharpiless/DMD3C
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Words Outperform <span class="highlight-title">Vision</span>: VLMs Can Self-Improve Via Text-Only
  Training For Human-Centered Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Hu, Jing Li, Yu Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied decision-making is fundamental for AI agents operating in real-world
environments. While Visual Language Models (VLMs) have advanced this
capability, they still struggle with complex decisions, particularly in
human-centered situations that require deep reasoning about human needs and
values. In this study, we systematically evaluate open-sourced VLMs on
multimodal human-centered decision-making tasks. We find that LLMs receiving
only textual descriptions unexpectedly outperform their VLM counterparts of
similar scale that process actual images, suggesting that visual alignment may
hinder VLM abilities. To address this challenge, we propose a novel text-only
training approach with synthesized textual data. This method strengthens VLMs'
language components and transfers the learned abilities to multimodal
inference, eliminating the need for expensive image-text paired data.
Furthermore, we show that VLMs can achieve substantial performance gains
through self-improvement, using training data generated by their LLM
counterparts rather than relying on larger teacher models like GPT-4. Our
findings establish a more efficient and scalable approach to enhancing VLMs'
human-centered decision-making capabilities, opening new avenues for optimizing
VLMs through self-improvement mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from
  In-the-Wild Drone Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiadong Tang, Yu Gao, Dianyi Yang, Liqi Yan, Yufeng Yue, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drones have become essential tools for reconstructing wild scenes due to
their outstanding maneuverability. Recent advances in radiance field methods
have achieved remarkable rendering quality, providing a new avenue for 3D
reconstruction from drone imagery. However, dynamic distractors in wild
environments challenge the static scene assumption in radiance fields, while
limited view constraints hinder the accurate capture of underlying scene
geometry. To address these challenges, we introduce DroneSplat, a novel
framework designed for robust 3D reconstruction from in-the-wild drone imagery.
Our method adaptively adjusts masking thresholds by integrating local-global
segmentation heuristics with statistical approaches, enabling precise
identification and elimination of dynamic distractors in static scenes. We
enhance 3D Gaussian Splatting with multi-view stereo predictions and a
voxel-guided optimization strategy, supporting high-quality rendering under
limited view constraints. For comprehensive evaluation, we provide a
drone-captured 3D reconstruction dataset encompassing both dynamic and static
scenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS
and NeRF baselines in handling in-the-wild drone imagery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Center-guided Classifier for Semantic Segmentation of Remote Sensing
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhang, Mengting Ma, Yizhen Jiang, Rongrong Lian, Zhenkai Wu, Kangning Cui, Xiaowen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared with natural images, remote sensing images (RSIs) have the unique
characteristic. i.e., larger intraclass variance, which makes semantic
segmentation for remote sensing images more challenging. Moreover, existing
semantic segmentation models for remote sensing images usually employ a vanilla
softmax classifier, which has three drawbacks: (1) non-direct supervision for
the pixel representations during training; (2) inadequate modeling ability of
parametric softmax classifiers under large intraclass variance; and (3) opaque
process of classification decision. In this paper, we propose a novel
classifier (called CenterSeg) customized for RSI semantic segmentation, which
solves the abovementioned problems with multiple prototypes, direct supervision
under Grassmann manifold, and interpretability strategy. Specifically, for each
class, our CenterSeg obtains local class centers by aggregating corresponding
pixel features based on ground-truth masks, and generates multiple prototypes
through hard attention assignment and momentum updating. In addition, we
introduce the Grassmann manifold and constrain the joint embedding space of
pixel features and prototypes based on two additional regularization terms.
Especially, during the inference, CenterSeg can further provide
interpretability to the model by restricting the prototype as a sample of the
training set. Experimental results on three remote sensing segmentation
datasets validate the effectiveness of the model. Besides the superior
performance, CenterSeg has the advantages of simplicity, lightweight,
compatibility, and interpretability. Code is available at
https://github.com/xwmaxwma/rssegmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Faces to Voices: Learning Hierarchical Representations for
  High-quality Video-to-Speech <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji-Hoon Kim, Jeongsoo Choi, Jaehun Kim, Chaeyoung Jung, Joon Son Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of this study is to generate high-quality speech from silent
talking face videos, a task also known as video-to-speech synthesis. A
significant challenge in video-to-speech synthesis lies in the substantial
modality gap between silent video and multi-faceted speech. In this paper, we
propose a novel video-to-speech system that effectively bridges this modality
gap, significantly enhancing the quality of synthesized speech. This is
achieved by learning of hierarchical representations from video to speech.
Specifically, we gradually transform silent video into acoustic feature spaces
through three sequential stages -- content, timbre, and prosody modeling. In
each stage, we align visual factors -- lip movements, face identity, and facial
expressions -- with corresponding acoustic counterparts to ensure the seamless
transformation. Additionally, to generate realistic and coherent speech from
the visual representations, we employ a flow matching model that estimates
direct trajectories from a simple prior distribution to the target speech
distribution. Extensive experiments demonstrate that our method achieves
exceptional generation quality comparable to real utterances, outperforming
existing methods by a significant margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025, demo page: https://mm.kaist.ac.kr/projects/faces2voices/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MagicColor: Multi-Instance Sketch Colorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinhan Zhang, Yue Ma, Bingyuan Wang, Qifeng Chen, Zeyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present \textit{MagicColor}, a diffusion-based framework for
multi-instance sketch colorization. The production of multi-instance 2D line
art colorization adheres to an industry-standard workflow, which consists of
three crucial stages: the design of line art characters, the coloring of
individual objects, and the refinement process. The artists are required to
repeat the process of coloring each instance one by one, which is inaccurate
and inefficient. Meanwhile, current generative methods fail to solve this task
due to the challenge of multi-instance pair data collection. To tackle these
challenges, we incorporate three technical designs to ensure precise character
detail transcription and achieve multi-instance sketch colorization in a single
forward. Specifically, we first propose the self-play training strategy to
solve the lack of training data. Then we introduce an instance guider to feed
the color of the instance. To achieve accurate color matching, we present
fine-grained color matching with edge loss to enhance visual quality. Equipped
with the proposed modules, MagicColor enables automatically transforming
sketches into vividly-colored images with accurate consistency and
multi-instance control. Experiments on our collected datasets show that our
model outperforms existing methods regarding chromatic precision. Specifically,
our model critically automates the colorization process with zero manual
adjustments, so novice users can produce stylistically consistent artwork by
providing reference instances and the original line art. Our code and
additional details are available at https://yinhan-zhang.github.io/color
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PE-CLIP: A Parameter-Efficient Fine-Tuning of <span class="highlight-title">Vision</span> Language Models for
  Dynamic Facial Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibtissam Saadi, Abdenour Hadid, Douglas W. Cunningham, Abdelmalik Taleb-Ahmed, Yassin El Hillali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) like CLIP offer promising solutions for Dynamic
Facial Expression Recognition (DFER) but face challenges such as inefficient
full fine-tuning, high complexity, and poor alignment between textual and
visual representations. Additionally, existing methods struggle with
ineffective temporal modeling. To address these issues, we propose PE-CLIP, a
parameter-efficient fine-tuning (PEFT) framework that adapts CLIP for DFER
while significantly reducing trainable parameters while maintaining high
accuracy. PE-CLIP introduces two specialized adapters: a Temporal Dynamic
Adapter (TDA) and a Shared Adapter (ShA). The TDA is a GRU-based module with
dynamic scaling that captures sequential dependencies while emphasizing
informative temporal features and suppressing irrelevant variations. The ShA is
a lightweight adapter that refines representations within both textual and
visual encoders, ensuring consistency and efficiency. Additionally, we
integrate Multi-modal Prompt Learning (MaPLe), introducing learnable prompts
for visual and action unit-based textual inputs, enhancing semantic alignment
between modalities and enabling efficient CLIP adaptation for dynamic tasks. We
evaluate PE-CLIP on two benchmark datasets, DFEW and FERV39K, achieving
competitive performance compared to state-of-the-art methods while requiring
fewer trainable parameters. By balancing efficiency and accuracy, PE-CLIP sets
a new benchmark in resource-efficient DFER. The source code of the proposed
PE-CLIP will be publicly available at https://github.com/Ibtissam-SAADI/PE-CLIP .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengtian Li, Jinshu Chen, Wanquan Feng, Bingchuan Li, Fei Dai, Songtao Zhao, Qian He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized portrait synthesis, essential in domains like social
entertainment, has recently made significant progress. Person-wise fine-tuning
based methods, such as LoRA and DreamBooth, can produce photorealistic outputs
but need training on individual samples, consuming time and resources and
posing an unstable risk. Adapter based techniques such as IP-Adapter freeze the
foundational model parameters and employ a plug-in architecture to enable
zero-shot inference, but they often exhibit a lack of naturalness and
authenticity, which are not to be overlooked in portrait synthesis tasks. In
this paper, we introduce a parameter-efficient adaptive generation method,
namely HyperLoRA, that uses an adaptive plug-in network to generate LoRA
weights, merging the superior performance of LoRA with the zero-shot capability
of adapter scheme. Through our carefully designed network structure and
training strategy, we achieve zero-shot personalized portrait generation
(supporting both single and multiple image inputs) with high photorealism,
fidelity, and editability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-HOLD: Video Hand Object Interaction Reenactment via adaptive
  Layout-instructed <span class="highlight-title">Diffusion</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingying Fan, Quanwei Yang, Kaisiyuan Wang, Hang Zhou, Yingying Li, Haocheng Feng, Yu Wu, Jingdong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current digital human studies focusing on lip-syncing and body movement are
no longer sufficient to meet the growing industrial demand, while human video
generation techniques that support interacting with real-world environments
(e.g., objects) have not been well investigated. Despite human hand synthesis
already being an intricate problem, generating objects in contact with hands
and their interactions presents an even more challenging task, especially when
the objects exhibit obvious variations in size and shape. To cope with these
issues, we present a novel video Reenactment framework focusing on Human-Object
Interaction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD).
Our key insight is to employ specialized layout representation for hands and
objects, respectively. Such representations enable effective disentanglement of
hand modeling and object adaptation to diverse motion sequences. To further
improve the generation quality of HOI, we have designed an interactive textural
enhancement module for both hands and objects by introducing two independent
memory banks. We also propose a layout-adjusting strategy for the cross-object
reenactment scenario to adaptively adjust unreasonable layouts caused by
diverse object sizes during inference. Comprehensive qualitative and
quantitative evaluations demonstrate that our proposed framework significantly
outperforms existing methods. Project page: https://fyycs.github.io/Re-HOLD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URLOST: Unsupervised Representation Learning without Stationarity or
  Topology <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Yun, Juexiao Zhang, Yann LeCun, Yubei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised representation learning has seen tremendous progress. However,
it is constrained by its reliance on domain specific stationarity and topology,
a limitation not found in biological intelligence systems. For instance, unlike
computer vision, human vision can process visual signals sampled from highly
irregular and non-stationary sensors. We introduce a novel framework that
learns from high-dimensional data without prior knowledge of stationarity and
topology. Our model, abbreviated as URLOST, combines a learnable
self-organizing layer, spectral clustering, and a masked autoencoder (MAE). We
evaluate its effectiveness on three diverse data modalities including simulated
biological vision data, neural recordings from the primary visual cortex, and
gene expressions. Compared to state-of-the-art unsupervised learning methods
like SimCLR and MAE, our model excels at learning meaningful representations
across diverse modalities without knowing their stationarity or topology. It
also outperforms other methods that are not dependent on these factors, setting
a new benchmark in the field. We position this work as a step toward
unsupervised learning methods capable of generalizing across diverse
high-dimensional data modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025; Code will be available at this
  https://github.com/zeyuyun1/URLOST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topo<span class="highlight-title">Diffusion</span>Net: A Topology-aware <span class="highlight-title">Diffusion</span> Model <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saumya Gupta, Dimitris Samaras, Chao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models excel at creating visually impressive images but often
struggle to generate images with a specified topology. The Betti number, which
represents the number of structures in an image, is a fundamental measure in
topology. Yet, diffusion models fail to satisfy even this basic constraint.
This limitation restricts their utility in applications requiring exact
control, like robotics and environmental modeling. To address this, we propose
TopoDiffusionNet (TDN), a novel approach that enforces diffusion models to
maintain the desired topology. We leverage tools from topological data
analysis, particularly persistent homology, to extract the topological
structures within an image. We then design a topology-based objective function
to guide the denoising process, preserving intended structures while
suppressing noisy ones. Our experiments across four datasets demonstrate
significant improvements in topological accuracy. TDN is the first to integrate
topology with diffusion models, opening new avenues of research in this area.
Code available at https://github.com/Saumya-Gupta-26/TopoDiffusionNet
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025 (Poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting to the Unknown: Training-Free Audio-<span class="highlight-title">Visual</span> Event Perception
  with Dynamic Thresholds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eitan Shaar, Ariel Shaulov, Gal Chechik, Lior Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of audio-visual event perception, which focuses on the temporal
localization and classification of events across distinct modalities (audio and
visual), existing approaches are constrained by the vocabulary available in
their training data. This limitation significantly impedes their capacity to
generalize to novel, unseen event categories. Furthermore, the annotation
process for this task is labor-intensive, requiring extensive manual labeling
across modalities and temporal segments, limiting the scalability of current
methods. Current state-of-the-art models ignore the shifts in event
distributions over time, reducing their ability to adjust to changing video
dynamics. Additionally, previous methods rely on late fusion to combine audio
and visual information. While straightforward, this approach results in a
significant loss of multimodal interactions. To address these challenges, we
propose Audio-Visual Adaptive Video Analysis ($\text{AV}^2\text{A}$), a
model-agnostic approach that requires no further training and integrates a
score-level fusion technique to retain richer multimodal interactions.
$\text{AV}^2\text{A}$ also includes a within-video label shift algorithm,
leveraging input video data and predictions from prior frames to dynamically
adjust event distributions for subsequent frames. Moreover, we present the
first training-free, open-vocabulary baseline for audio-visual event
perception, demonstrating that $\text{AV}^2\text{A}$ achieves substantial
improvements over naive training-free baselines. We demonstrate the
effectiveness of $\text{AV}^2\text{A}$ on both zero-shot and weakly-supervised
state-of-the-art methods, achieving notable improvements in performance metrics
over existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOUS VIDE: Cooking <span class="highlight-title">Visual</span> Drone Navigation Policies in a Gaussian
  Splatting Vacuum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JunEn Low, Maximilian Adang, Javier Yu, Keiko Nagami, Mac Schwager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new simulator, training approach, and policy architecture,
collectively called SOUS VIDE, for end-to-end visual drone navigation. Our
trained policies exhibit zero-shot sim-to-real transfer with robust real-world
performance using only onboard perception and computation. Our simulator,
called FiGS, couples a computationally simple drone dynamics model with a high
visual fidelity Gaussian Splatting scene reconstruction. FiGS can quickly
simulate drone flights producing photorealistic images at up to 130 fps. We use
FiGS to collect 100k-300k image/state-action pairs from an expert MPC with
privileged state and dynamics information, randomized over dynamics parameters
and spatial disturbances. We then distill this expert MPC into an end-to-end
visuomotor policy with a lightweight neural architecture, called SV-Net. SV-Net
processes color image, optical flow and IMU data streams into low-level thrust
and body rate commands at 20 Hz onboard a drone. Crucially, SV-Net includes a
learned module for low-level control that adapts at runtime to variations in
drone dynamics. In a campaign of 105 hardware experiments, we show SOUS VIDE
policies to be robust to 30% mass variations, 40 m/s wind gusts, 60% changes in
ambient brightness, shifting or removing objects from the scene, and people
moving aggressively through the drone's visual field. Code, data, and
experiment videos can be found on our project page:
https://stanfordmsl.github.io/SousVide/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DINO-LG: A Task-Specific DINO Model for Coronary Calcium Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07976v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07976v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmut S. Gokmen, Caner Ozcan, Moneera N. Haque, Steve W. Leung, C. Seth Parker, W. Brent Seales, Cody Bumgardner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coronary artery disease (CAD), one of the leading causes of mortality
worldwide, necessitates effective risk assessment strategies, with coronary
artery calcium (CAC) scoring via computed tomography (CT) being a key method
for prevention. Traditional methods, primarily based on UNET architectures
implemented on pre-built models, face challenges like the scarcity of annotated
CT scans containing CAC and imbalanced datasets, leading to reduced performance
in segmentation and scoring tasks. In this study, we address these limitations
by incorporating the self-supervised learning (SSL) technique of DINO
(self-distillation with no labels), which trains without requiring CAC-specific
annotations, enhancing its robustness in generating distinct features. The
DINO-LG model, which leverages label guidance to focus on calcified areas,
achieves significant improvements, with a sensitivity of 89% and specificity of
90% for detecting CAC-containing CT slices, compared to the standard DINO
model's sensitivity of 79% and specificity of 77%. Additionally, false-negative
and false-positive rates are reduced by 49% and 59%, respectively, instilling
greater confidence in clinicians when ruling out calcification in low-risk
patients and minimizing unnecessary imaging reviews by radiologists. Further,
CAC scoring and segmentation tasks are conducted using a basic UNET
architecture, applied specifically to CT slices identified by the DINO-LG model
as containing calcified areas. This targeted approach enhances CAC scoring
accuracy by feeding the UNET model with relevant slices, significantly
improving diagnostic precision, reducing both false positives and false
negatives, and ultimately lowering overall healthcare costs by minimizing
unnecessary tests and treatments, presenting a valuable advancement in CAD risk
assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Developed by Center for Applied Artificial Intelligence (CAAI),
  University of Kentucky</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MANTA: <span class="highlight-title">Diffusion</span> Mamba for Efficient and Effective Stochastic Long-Term
  Dense Anticipation <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Zatsarynna, Emad Bahrami, Yazan Abu Farha, Gianpiero Francesca, Juergen Gall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term dense action anticipation is very challenging since it requires
predicting actions and their durations several minutes into the future based on
provided video observations. To model the uncertainty of future outcomes,
stochastic models predict several potential future action sequences for the
same observation. Recent work has further proposed to incorporate uncertainty
modelling for observed frames by simultaneously predicting per-frame past and
future actions in a unified manner. While such joint modelling of actions is
beneficial, it requires long-range temporal capabilities to connect events
across distant past and future time points. However, the previous work
struggles to achieve such a long-range understanding due to its limited and/or
sparse receptive field. To alleviate this issue, we propose a novel MANTA
(MAmba for ANTicipation) network. Our model enables effective long-term
temporal modelling even for very long sequences while maintaining linear
complexity in sequence length. We demonstrate that our approach achieves
state-of-the-art results on three datasets - Breakfast, 50Salads, and
Assembly101 - while also significantly improving computational and memory
efficiency. Our code is available at https://github.com/olga-zats/DIFF_MANTA .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ United we stand, Divided we fall: Handling Weak Complementary
  Relationships for Audio-<span class="highlight-title">Visual</span> Emotion Recognition in Valence-Arousal Space <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Gnana Praveen, Jahangir Alam, Eric Charton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio and visual modalities are two predominant contact-free channels in
videos, which are often expected to carry a complementary relationship with
each other. However, they may not always complement each other, resulting in
poor audio-visual feature representations. In this paper, we introduce Gated
Recursive Joint Cross Attention (GRJCA) using a gating mechanism that can
adaptively choose the most relevant features to effectively capture the
synergic relationships across audio and visual modalities. Specifically, we
improve the performance of Recursive Joint Cross-Attention (RJCA) by
introducing a gating mechanism to control the flow of information between the
input features and the attended features of multiple iterations depending on
the strength of their complementary relationship. For instance, if the
modalities exhibit strong complementary relationships, the gating mechanism
emphasizes cross-attended features, otherwise non-attended features. To further
improve the performance of the system, we also explored a hierarchical gating
approach by introducing a gating mechanism at every iteration, followed by
high-level gating across the gated outputs of each iteration. The proposed
approach improves the performance of RJCA model by adding more flexibility to
deal with weak complementary relationships across audio and visual modalities.
Extensive experiments are conducted on the challenging Affwild2 dataset to
demonstrate the robustness of the proposed approach. By effectively handling
the weak complementary relationships across the audio and visual modalities,
the proposed model achieves a Concordance Correlation Coefficient (CCC) of
0.561 (0.623) and 0.620 (0.660) for valence and arousal respectively on the
test set (validation set).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieved 2nd place in valence arousal challenge Submission to
  CVPR2025 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FALCON: Fairness Learning via Contrastive Attention Approach to
  Continual Semantic Scene Understanding <span class="chip">CVPR'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15965v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15965v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh-Dat Truong, Utsav Prabhu, Bhiksha Raj, Jackson Cothren, Khoa Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning in semantic scene segmentation aims to continually learn
new unseen classes in dynamic environments while maintaining previously learned
knowledge. Prior studies focused on modeling the catastrophic forgetting and
background shift challenges in continual learning. However, fairness, another
major challenge that causes unfair predictions leading to low performance among
major and minor classes, still needs to be well addressed. In addition, prior
methods have yet to model the unknown classes well, thus resulting in producing
non-discriminative features among unknown classes. This work presents a novel
Fairness Learning via Contrastive Attention Approach to continual learning in
semantic scene understanding. In particular, we first introduce a new Fairness
Contrastive Clustering loss to address the problems of catastrophic forgetting
and fairness. Then, we propose an attention-based visual grammar approach to
effectively model the background shift problem and unknown classes, producing
better feature representations for different unknown classes. Through our
experiments, our proposed approach achieves State-of-the-Art (SoTA) performance
on different continual learning benchmarks, i.e., ADE20K, Cityscapes, and
Pascal VOC. It promotes the fairness of the continual semantic segmentation
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gamal Elghazaly, Raphael Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-Definition (HD) maps play a crucial role in autonomous vehicle
navigation, complementing onboard perception sensors for improved accuracy and
safety. Traditional HD map generation relies on dedicated mapping vehicles,
which are costly and fail to capture real-time infrastructure changes. This
paper presents HDMapLaneNet, a novel framework leveraging V2X communication and
Scene Graph Generation to collaboratively construct a localized geometric layer
of HD maps. The approach extracts lane centerlines from front-facing camera
images, represents them as graphs, and transmits the data for global
aggregation to the cloud via V2X. Preliminary results on the nuScenes dataset
demonstrate superior association prediction performance compared to a
state-of-the-art method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Karyotype AI for Precision Oncology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14312v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14312v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Shamsi, Isaac Reid, Drew Bryant, Jacob Wilson, Xiaoyu Qu, Avinava Dubey, Konik Kothari, Mostafa Dehghani, Mariya Chavarha, Valerii Likhosherstov, Brian Williams, Michael Frumkin, Fred Appelbaum, Krzysztof Choromanski, Ali Bashir, Min Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a machine learning method capable of accurately detecting
chromosome abnormalities that cause blood cancers directly from microscope
images of the metaphase stage of cell division. The pipeline is built on a
series of fine-tuned Vision Transformers. Current state of the art (and
standard clinical practice) requires expensive, manual expert analysis, whereas
our pipeline takes only 15 seconds per metaphase image. Using a novel
pretraining-finetuning strategy to mitigate the challenge of data scarcity, we
achieve a high precision-recall score of 94% AUC for the clinically significant
del(5q) and t(9;22) anomalies. Our method also unlocks zero-shot detection of
rare aberrations based on model latent embeddings. The ability to quickly,
accurately, and scalably diagnose genetic abnormalities directly from metaphase
images could transform karyotyping practice and improve patient outcomes. We
will make code publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-end Adaptive Dynamic Subsampling and Reconstruction for Cardiac
  MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Yiasemis, Jan-Jakob Sonke, Jonas Teuwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $\textbf{Background:}$ Accelerating dynamic MRI is vital for advancing
clinical applications and improving patient comfort. Commonly, deep learning
(DL) methods for accelerated dynamic MRI reconstruction typically rely on
uniformly applying non-adaptive predetermined or random subsampling patterns
across all temporal frames of the dynamic acquisition. This approach fails to
exploit temporal correlations or optimize subsampling on a case-by-case basis.
  $\textbf{Purpose:}$ To develop an end-to-end approach for adaptive dynamic
MRI subsampling and reconstruction, capable of generating customized sampling
patterns maximizing at the same time reconstruction quality.
  $\textbf{Methods:}$ We introduce the End-to-end Adaptive Dynamic Sampling and
Reconstruction (E2E-ADS-Recon) for MRI framework, which integrates an adaptive
dynamic sampler (ADS) that adapts the acquisition trajectory to each case for a
given acceleration factor with a state-of-the-art dynamic reconstruction
network, vSHARP, for reconstructing the adaptively sampled data into a dynamic
image. The ADS can produce either frame-specific patterns or unified patterns
applied to all temporal frames. E2E-ADS-Recon is evaluated under both
frame-specific and unified 1D or 2D sampling settings, using dynamic cine
cardiac MRI data and compared with vSHARP models employing standard subsampling
trajectories, as well as pipelines where ADS was replaced by parameterized
samplers optimized for dataset-specific schemes.
  $\textbf{Results:}$ E2E-ADS-Recon exhibited superior reconstruction quality,
especially at high accelerations, in terms of standard quantitative metrics
(SSIM, pSNR, NMSE).
  $\textbf{Conclusion:}$ The proposed framework improves reconstruction
quality, highlighting the importance of case-specific subsampling optimization
in dynamic MRI applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 26 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SuperPC: A Single <span class="highlight-title">Diffusion</span> Model for Point Cloud Completion,
  Upsampling, Denoising, and Colorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Du, Zhipeng Zhao, Shaoshu Su, Sharath Golluri, Haoze Zheng, Runmao Yao, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud (PC) processing tasks-such as completion, upsampling, denoising,
and colorization-are crucial in applications like autonomous driving and 3D
reconstruction. Despite substantial advancements, prior approaches often
address each of these tasks independently, with separate models focused on
individual issues. However, this isolated approach fails to account for the
fact that defects like incompleteness, low resolution, noise, and lack of color
frequently coexist, with each defect influencing and correlating with the
others. Simply applying these models sequentially can lead to error
accumulation from each model, along with increased computational costs. To
address these challenges, we introduce SuperPC, the first unified diffusion
model capable of concurrently handling all four tasks. Our approach employs a
three-level-conditioned diffusion framework, enhanced by a novel
spatial-mix-fusion strategy, to leverage the correlations among these four
defects for simultaneous, efficient processing. We show that SuperPC
outperforms the state-of-the-art specialized models as well as their
combination on all four individual tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjie Li, Hong-Xing Yu, Jiaman Li, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-scene interaction (HSI) generation is crucial for applications in
embodied AI, virtual reality, and robotics. Yet, existing methods cannot
synthesize interactions in unseen environments such as in-the-wild scenes or
reconstructed scenes, as they rely on paired 3D scenes and captured human
motion data for training, which are unavailable for unseen environments. We
present ZeroHSI, a novel approach that enables zero-shot 4D human-scene
interaction synthesis, eliminating the need for training on any MoCap data. Our
key insight is to distill human-scene interactions from state-of-the-art video
generation models, which have been trained on vast amounts of natural human
movements and interactions, and use differentiable rendering to reconstruct
human-scene interactions. ZeroHSI can synthesize realistic human motions in
both static scenes and environments with dynamic objects, without requiring any
ground-truth motion data. We evaluate ZeroHSI on a curated dataset of different
types of various indoor and outdoor scenes with different interaction prompts,
demonstrating its ability to generate diverse and contextually appropriate
human-scene interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://awfuact.github.io/zerohsi/ The first two
  authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> TruthPrInt: Mitigating LVLM Object Hallucination Via Latent
  Truthful-Guided Pre-Intervention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhao Duan, Fei Kong, Hao Cheng, James Diffenderfer, Bhavya Kailkhura, Lic<span class="highlight-author">hao Su</span>n, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object Hallucination (OH) has been acknowledged as one of the major
trustworthy challenges in Large Vision-Language Models (LVLMs). Recent
advancements in Large Language Models (LLMs) indicate that internal states,
such as hidden states, encode the "overall truthfulness" of generated
responses. However, it remains under-explored how internal states in LVLMs
function and whether they could serve as "per-token" hallucination indicators,
which is essential for mitigating OH. In this paper, we first conduct an
in-depth exploration of LVLM internal states in relation to OH issues and
discover that (1) LVLM internal states are high-specificity per-token
indicators of hallucination behaviors. Moreover, (2) different LVLMs encode
universal patterns of hallucinations in common latent subspaces, indicating
that there exist "generic truthful directions" shared by various LVLMs. Based
on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)
that first learns the truthful direction of LVLM decoding and then applies
truthful-guided inference-time intervention during LVLM decoding. We further
propose ComnHallu to enhance both cross-LVLM and cross-data hallucination
detection transferability by constructing and aligning hallucination latent
subspaces. We evaluate TruthPrInt in extensive experimental settings, including
in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.
Experimental results indicate that TruthPrInt significantly outperforms
state-of-the-art methods. Codes will be available at
https://github.com/jinhaoduan/TruthPrInt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, the first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RadioActive: 3D Radiological Interactive Segmentation Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07885v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07885v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constantin Ulrich, Tassilo Wald, Emily Tempus, Maximilian Rokuss, Paul F. Jaeger, Klaus Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effortless and precise segmentation with minimal clinician effort could
greatly streamline clinical workflows. Recent interactive segmentation models,
inspired by METAs Segment Anything, have made significant progress but face
critical limitations in 3D radiology. These include impractical human
interaction requirements such as slice-by-slice operations for 2D models on 3D
data and a lack of iterative refinement. Prior studies have been hindered by
inadequate evaluation protocols, resulting in unreliable performance
assessments and inconsistent findings across studies. The RadioActive benchmark
addresses these challenges by providing a rigorous and reproducible evaluation
framework for interactive segmentation methods in clinically relevant
scenarios. It features diverse datasets, a wide range of target structures, and
the most impactful 2D and 3D interactive segmentation methods, all within a
flexible and extensible codebase. We also introduce advanced prompting
techniques that reduce interaction steps, enabling fair comparisons between 2D
and 3D models. Surprisingly, SAM2 outperforms all specialized medical 2D and 3D
models in a setting requiring only a few interactions to generate prompts for a
3D volume. This challenges prevailing assumptions and demonstrates that
general-purpose models surpass specialized medical approaches. By open-sourcing
RadioActive, we invite researchers to integrate their models and prompting
techniques, ensuring continuous and transparent evaluation of 3D medical
interactive models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Undergoing Peer-Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniCoRN: Latent <span class="highlight-title">Diffusion</span>-based Unified Controllable Image Restoration
  Network across Multiple Degradations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debabrata Mandal, Soumitri Chattopadhyay, Guansen Tong, Praneeth Chakravarthula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image restoration is essential for enhancing degraded images across computer
vision tasks. However, most existing methods address only a single type of
degradation (e.g., blur, noise, or haze) at a time, limiting their real-world
applicability where multiple degradations often occur simultaneously. In this
paper, we propose UniCoRN, a unified image restoration approach capable of
handling multiple degradation types simultaneously using a multi-head diffusion
model. Specifically, we uncover the potential of low-level visual cues
extracted from images in guiding a controllable diffusion model for real-world
image restoration and we design a multi-head control network adaptable via a
mixture-of-experts strategy. We train our model without any prior assumption of
specific degradations, through a smartly designed curriculum learning recipe.
Additionally, we also introduce MetaRestore, a metalens imaging benchmark
containing images with multiple degradations and artifacts. Extensive
evaluations on several challenging datasets, including our benchmark,
demonstrate that our method achieves significant performance gains and can
robustly restore images with severe degradations. Project page:
https://codejaeger.github.io/unicorn-gh
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Matrix3D: Large Photogrammetry Model All-in-One <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao, Shiwei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Matrix3D, a unified model that performs several photogrammetry
subtasks, including pose estimation, depth prediction, and novel view synthesis
using just the same model. Matrix3D utilizes a multi-modal diffusion
transformer (DiT) to integrate transformations across several modalities, such
as images, camera parameters, and depth maps. The key to Matrix3D's large-scale
multi-modal training lies in the incorporation of a mask learning strategy.
This enables full-modality model training even with partially complete data,
such as bi-modality data of image-pose and image-depth pairs, thus
significantly increases the pool of available training data. Matrix3D
demonstrates state-of-the-art performance in pose estimation and novel view
synthesis tasks. Additionally, it offers fine-grained control through
multi-round interactions, making it an innovative tool for 3D content creation.
Project page: https://nju-3dv.github.io/projects/matrix3d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025 camera ready. Project Page:
  https://nju-3dv.github.io/projects/matrix3d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mono2D: A Trainable Monogenic Layer for Robust Knee Cartilage
  Segmentation on Out-of-Distribution 2D Ultrasound Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09050v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09050v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvin Kimbowa, Arjun Parmar, Maziar Badii, David Liu, Matthew Harkey, Ilker Hacihaliloglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated knee cartilage segmentation using point-of-care ultrasound devices
and deep-learning networks has the potential to enhance the management of knee
osteoarthritis. However, segmentation algorithms often struggle with domain
shifts caused by variations in ultrasound devices and acquisition parameters,
limiting their generalizability. In this paper, we propose Mono2D, a monogenic
layer that extracts multi-scale, contrast- and intensity-invariant local phase
features using trainable bandpass quadrature filters. This layer mitigates
domain shifts, improving generalization to out-of-distribution domains. Mono2D
is integrated before the first layer of a segmentation network, and its
parameters jointly trained alongside the network's parameters. We evaluated
Mono2D on a multi-domain 2D ultrasound knee cartilage dataset for single-source
domain generalization (SSDG). Our results demonstrate that Mono2D outperforms
other SSDG methods in terms of Dice score and mean average surface distance. To
further assess its generalizability, we evaluate Mono2D on a multi-site
prostate MRI dataset, where it continues to outperform other SSDG methods,
highlighting its potential to improve domain generalization in medical imaging.
Nevertheless, further evaluation on diverse datasets is still necessary to
assess its clinical utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, removed unrelated LaTeX template figure from last page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty modeling for fine-tuned implicit functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12082v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12082v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Susmelj, Mael Macuglia, Nataša Tagasovska, Reto Sutter, Sebastiano Caprara, Jean-Philippe Thiran, Ender Konukoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit functions such as Neural Radiance Fields (NeRFs), occupancy
networks, and signed distance functions (SDFs) have become pivotal in computer
vision for reconstructing detailed object shapes from sparse views. Achieving
optimal performance with these models can be challenging due to the extreme
sparsity of inputs and distribution shifts induced by data corruptions. To this
end, large, noise-free synthetic datasets can serve as shape priors to help
models fill in gaps, but the resulting reconstructions must be approached with
caution. Uncertainty estimation is crucial for assessing the quality of these
reconstructions, particularly in identifying areas where the model is uncertain
about the parts it has inferred from the prior. In this paper, we introduce
Dropsembles, a novel method for uncertainty estimation in tuned implicit
functions. We demonstrate the efficacy of our approach through a series of
experiments, starting with toy examples and progressing to a real-world
scenario. Specifically, we train a Convolutional Occupancy Network on synthetic
anatomical data and test it on low-resolution MRI segmentations of the lumbar
spine. Our results show that Dropsembles achieve the accuracy and calibration
levels of deep ensembles but with significantly less computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep End-to-end Adaptive k-Space Sampling, Reconstruction, and
  Registration for Dynamic MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Yiasemis, Jan-Jakob Sonke, Jonas Teuwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic MRI enables a range of clinical applications, including cardiac
function assessment, organ motion tracking, and radiotherapy guidance. However,
fully sampling the dynamic k-space data is often infeasible due to time
constraints and physiological motion such as respiratory and cardiac motion.
This necessitates undersampling, which degrades the quality of reconstructed
images. Poor image quality not only hinders visualization but also impairs the
estimation of deformation fields, crucial for registering dynamic (moving)
images to a static reference image. This registration enables tasks such as
motion correction, treatment planning, and quantitative analysis in
applications like cardiac imaging and MR-guided radiotherapy. To overcome the
challenges posed by undersampling and motion, we introduce an end-to-end deep
learning (DL) framework that integrates adaptive dynamic k-space sampling,
reconstruction, and registration. Our approach begins with a DL-based adaptive
sampling strategy, optimizing dynamic k-space acquisition to capture the most
relevant data for each specific case. This is followed by a DL-based
reconstruction module that produces images optimized for accurate deformation
field estimation from the undersampled moving data. Finally, a registration
module estimates the deformation fields aligning the reconstructed dynamic
images with a static reference. The proposed framework is independent of
specific reconstruction and registration modules allowing for plug-and-play
integration of these components. The entire framework is jointly trained using
a combination of supervised and unsupervised loss functions, enabling
end-to-end optimization for improved performance across all components. Through
controlled experiments and ablation studies, we validate each component,
demonstrating that each choice contributes to robust motion estimation from
undersampled dynamic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 23 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Code-as-Monitor: Constraint-aware <span class="highlight-title">Visual</span> Programming for Reactive and
  Proactive <span class="highlight-title">Robo</span>tic Failure Detection <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04455v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04455v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, <span class="highlight-author">He Wang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic detection and prevention of open-set failures are crucial in
closed-loop robotic systems. Recent studies often struggle to simultaneously
identify unexpected failures reactively after they occur and prevent
foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a
novel paradigm leveraging the vision-language model (VLM) for both open-set
reactive and proactive failure detection. The core of our method is to
formulate both tasks as a unified set of spatio-temporal constraint
satisfaction problems and use VLM-generated code to evaluate them for real-time
monitoring. To enhance the accuracy and efficiency of monitoring, we further
introduce constraint elements that abstract constraint-related entities or
their parts into compact geometric elements. This approach offers greater
generality, simplifies tracking, and facilitates constraint-aware visual
programming by leveraging these elements as visual prompts. Experiments show
that CaM achieves a 28.7% higher success rate and reduces execution time by
31.8% under severe disturbances compared to baselines across three simulators
and a real-world setting. Moreover, CaM can be integrated with open-loop
control policies to form closed-loop systems, enabling long-horizon tasks in
cluttered scenes with dynamic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025. Project page:
  https://zhoues.github.io/Code-as-Monitor/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VASparse: Towards Efficient <span class="highlight-title">Visual</span> Hallucination Mitigation via
  <span class="highlight-title">Visual</span>-Aware Token Sparsification <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianwei Zhuang, Zhihong Zhu, Yuxin Xie, Liming Liang, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) may produce outputs that are unfaithful
to reality, also known as visual hallucinations (VH), which significantly
impedes their real-world usage. To alleviate VH, various decoding strategies
have been proposed to enhance visual information. However, many of these
methods may require secondary decoding and rollback, which significantly
reduces inference speed. In this work, we propose an efficient plug-and-play
decoding algorithm via Visual-Aware Sparsification (VASparse) from the
perspective of token sparsity for mitigating VH. VASparse is inspired by
empirical observations: (1) the sparse activation of attention in LVLMs, and
(2) visual-agnostic tokens sparsification exacerbates VH. Based on these
insights, we propose a novel token sparsification strategy that balances
efficiency and trustworthiness. Specifically, VASparse implements a
visual-aware token selection strategy during decoding to reduce redundant
tokens while preserving visual context effectively. Additionally, we
innovatively introduce a sparse-based visual contrastive decoding method to
recalibrate the distribution of hallucinated outputs without the time overhead
associated with secondary decoding. Subsequently, VASparse recalibrates
attention scores to penalize attention sinking of LVLMs towards text tokens.
Extensive experiments across four popular benchmarks confirm the effectiveness
of VASparse in mitigating VH across different LVLM families without requiring
additional training or post-processing. Impressively, VASparse achieves
state-of-the-art performance for mitigating VH while maintaining competitive
decoding speed. Code is available at
https://github.com/mengchuang123/VASparse-github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GiVE: Guiding <span class="highlight-title">Visual</span> Encoder to Perceive Overlooked Information <span class="chip">ICME 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Li, Jianghong Ma, Xiaofeng Zhang, Yuhang Li, Jianyang Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models have advanced AI in applications like
text-to-video generation and visual question answering. These models rely on
visual encoders to convert non-text data into vectors, but current encoders
either lack semantic alignment or overlook non-salient objects. We propose the
Guiding Visual Encoder to Perceive Overlooked Information (GiVE) approach. GiVE
enhances visual representation with an Attention-Guided Adapter (AG-Adapter)
module and an Object-focused Visual Semantic Learning module. These incorporate
three novel loss terms: Object-focused Image-Text Contrast (OITC) loss,
Object-focused Image-Image Contrast (OIIC) loss, and Object-focused Image
Discrimination (OID) loss, improving object consideration, retrieval accuracy,
and comprehensiveness. Our contributions include dynamic visual focus
adjustment, novel loss functions to enhance object retrieval, and the
Multi-Object Instruction (MOInst) dataset. Experiments show our approach
achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by ICME 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat-Seng Chua, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EgoTextVQA, a novel and rigorously constructed benchmark for
egocentric QA assistance involving scene text. EgoTextVQA contains 1.5K
ego-view videos and 7K scene-text aware questions that reflect real user needs
in outdoor driving and indoor house-keeping activities. The questions are
designed to elicit identification and reasoning on scene text in an egocentric
and dynamic environment. With EgoTextVQA, we comprehensively evaluate 10
prominent multimodal large language models. Currently, all models struggle, and
the best results (Gemini 1.5 Pro) are around 33\% accuracy, highlighting the
severe deficiency of these techniques in egocentric QA assistance. Our further
investigations suggest that precise temporal grounding and multi-frame
reasoning, along with high resolution and auxiliary scene-text inputs, are key
for better performance. With thorough analyses and heuristic suggestions, we
hope EgoTextVQA can serve as a solid testbed for research in egocentric
scene-text QA assistance. Our dataset is released at:
https://github.com/zhousheng97/EgoTextVQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Domain Generalization meets Generalized Category Discovery: An
  Adaptive Task-Arithmetic Driven Approach <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Rathore, Shubhranil B, Saikat Dutta, Sarthak Mehrotra, Zsolt Kira, Biplab Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized Class Discovery (GCD) clusters base and novel classes in a target
domain using supervision from a source domain with only base classes. Current
methods often falter with distribution shifts and typically require access to
target data during training, which can sometimes be impractical. To address
this issue, we introduce the novel paradigm of Domain Generalization in GCD
(DG-GCD), where only source data is available for training, while the target
domain, with a distinct data distribution, remains unseen until inference. To
this end, our solution, DG2CD-Net, aims to construct a domain-independent,
discriminative embedding space for GCD. The core innovation is an episodic
training strategy that enhances cross-domain generalization by adapting a base
model on tasks derived from source and synthetic domains generated by a
foundation model. Each episode focuses on a cross-domain GCD task, diversifying
task setups over episodes and combining open-set domain adaptation with a novel
margin loss and representation learning for optimizing the feature space
progressively. To capture the effects of fine-tuning on the base model, we
extend task arithmetic by adaptively weighting the local task vectors
concerning the fine-tuned models based on their GCD performance on a validation
distribution. This episodic update mechanism boosts the adaptability of the
base model to unseen targets. Experiments across three datasets confirm that
DG2CD-Net outperforms existing GCD methods customized for DG-GCD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2025 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven <span class="highlight-title">Camera</span> and <span class="highlight-title">Lidar</span> Simulation Models for Autonomous Driving: A
  <span class="highlight-title">Review</span> from Generative Models to Volume Renderers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Haghighi, Xiaomeng Wang, Hao Jing, Mehrdad Dianati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perception sensors, particularly camera and Lidar, are key elements of
Autonomous Driving Systems (ADS) that enable them to comprehend their
surroundings to informed driving and control decisions. Therefore, developing
realistic simulation models for these sensors is essential for conducting
effective simulation-based testing of ADS. Moreover, the rise of deep
learning-based perception models has increased the utility of sensor simulation
models for synthesising diverse training datasets. The traditional sensor
simulation models rely on computationally expensive physics-based algorithms,
specifically in complex systems such as ADS. Hence, the current potential
resides in data-driven approaches, fuelled by the exceptional performance of
deep generative models in capturing high-dimensional data distribution and
volume renderers in accurately representing scenes. This paper reviews the
current state-of-the-art data-driven camera and Lidar simulation models and
their evaluation methods. It explores a spectrum of models from the novel
perspective of generative models and volume renderers. Generative models are
discussed in terms of their input-output types, while volume renderers are
categorised based on their input encoding. Finally, the paper illustrates
commonly used evaluation techniques for assessing sensor simulation models and
highlights the existing research gaps in the area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in IEEE Transactions on Intelligent Vehicles</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instant Adversarial Purification with Adversarial Consistency
  Distillation <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17064v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17064v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun Tong Lei, Hon Ming Yam, Zhongliang Guo, Yifei Qian, Chun Pong Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks have revolutionized numerous fields with their exceptional
performance, yet they remain susceptible to adversarial attacks through subtle
perturbations. While diffusion-based purification methods like DiffPure offer
promising defense mechanisms, their computational overhead presents a
significant practical limitation. In this paper, we introduce One Step Control
Purification (OSCP), a novel defense framework that achieves robust adversarial
purification in a single Neural Function Evaluation (NFE) within diffusion
models. We propose Gaussian Adversarial Noise Distillation (GAND) as the
distillation objective and Controlled Adversarial Purification (CAP) as the
inference pipeline, which makes OSCP demonstrate remarkable efficiency while
maintaining defense efficacy. Our proposed GAND addresses a fundamental tension
between consistency distillation and adversarial perturbation, bridging the gap
between natural and adversarial manifolds in the latent space, while remaining
computationally efficient through Parameter-Efficient Fine-Tuning (PEFT)
methods such as LoRA, eliminating the high computational budget request from
full parameter fine-tuning. The CAP guides the purification process through the
unlearnable edge detection operator calculated by the input image as an extra
prompt, effectively preventing the purified images from deviating from their
original appearance when large purification steps are used. Our experimental
results on ImageNet showcase OSCP's superior performance, achieving a 74.19%
defense success rate with merely 0.1s per purification -- a 100-fold speedup
compared to conventional approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiao Gao, Liangliang Nan, Hugo Ledoux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation in urban scene analysis has mainly focused on images or
point clouds, while textured meshes - offering richer spatial representation -
remain underexplored. This paper introduces SUM Parts, the first large-scale
dataset for urban textured meshes with part-level semantic labels, covering
about 2.5 km2 with 21 classes. The dataset was created using our own annotation
tool, which supports both face- and texture-based annotations with efficient
interactive selection. We also provide a comprehensive evaluation of 3D
semantic segmentation and interactive annotation methods on this dataset. Our
project page is available at https://tudelft3d.github.io/SUMParts/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanxin Liu, Rui Zhu, Shuhuai Ren, Jiacong Wang, Haoyuan Guo, Xu Sun, Lu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of video generative models (VGMs), it is essential to
develop reliable and comprehensive automatic metrics for AI-generated videos
(AIGVs). Existing methods either use off-the-shelf models optimized for other
tasks or rely on human assessment data to train specialized evaluators. These
approaches are constrained to specific evaluation aspects and are difficult to
scale with the increasing demands for finer-grained and more comprehensive
evaluations. To address this issue, this work investigates the feasibility of
using multimodal large language models (MLLMs) as a unified evaluator for
AIGVs, leveraging their strong visual perception and language understanding
capabilities. To evaluate the performance of automatic metrics in unified AIGV
evaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects
videos generated by state-of-the-art VGMs and provides pairwise human
preference annotations across 15 evaluation aspects. Using UVE-Bench, we
extensively evaluate 16 MLLMs. Our empirical results suggest that while
advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human
evaluators, they demonstrate promising ability in unified AIGV evaluation,
significantly surpassing existing specialized evaluation methods. Additionally,
we conduct an in-depth analysis of key design choices that impact the
performance of MLLM-driven evaluators, offering valuable insights for future
research on AIGV evaluation. The code is available at
https://github.com/bytedance/UVE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DLEN: Dual Branch of Transformer for Low-Light Image Enhancement in Dual
  Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12235v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12235v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Xia, Jiesong Bai, Yihang Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light image enhancement (LLE) aims to improve the visual quality of
images captured in poorly lit conditions, which often suffer from low
brightness, low contrast, noise, and color distortions. These issues hinder the
performance of computer vision tasks such as object detection, facial
recognition, and autonomous driving.Traditional enhancement techniques, such as
multi-scale fusion and histogram equalization, fail to preserve fine details
and often struggle with maintaining the natural appearance of enhanced images
under complex lighting conditions. Although the Retinex theory provides a
foundation for image decomposition, it often amplifies noise, leading to
suboptimal image quality. In this paper, we propose the Dual Light Enhance
Network (DLEN), a novel architecture that incorporates two distinct attention
mechanisms, considering both spatial and frequency domains. Our model
introduces a learnable wavelet transform module in the illumination estimation
phase, preserving high- and low-frequency components to enhance edge and
texture details. Additionally, we design a dual-branch structure that leverages
the power of the Transformer architecture to enhance both the illumination and
structural components of the image.Through extensive experiments, our model
outperforms state-of-the-art methods on standard benchmarks.Code is available
here: https://github.com/LaLaLoXX/DLEN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages and 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral State Space Model for Rotation-Invariant <span class="highlight-title">Visual</span> Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06369v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06369v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Dastani, Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, David Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Milad Cheraghalikhani, Arnab Kumar Mondal, Herve Lombaert, Christian Desrosiers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State Space Models (SSMs) have recently emerged as an alternative to Vision
Transformers (ViTs) due to their unique ability of modeling global
relationships with linear complexity. SSMs are specifically designed to capture
spatially proximate relationships of image patches. However, they fail to
identify relationships between conceptually related yet not adjacent patches.
This limitation arises from the non-causal nature of image data, which lacks
inherent directional relationships. Additionally, current vision-based SSMs are
highly sensitive to transformations such as rotation. Their predefined scanning
directions depend on the original image orientation, which can cause the model
to produce inconsistent patch-processing sequences after rotation. To address
these limitations, we introduce Spectral VMamba, a novel approach that
effectively captures the global structure within an image by leveraging
spectral information derived from the graph Laplacian of image patches. Through
spectral decomposition, our approach encodes patch relationships independently
of image orientation, achieving rotation invariance with the aid of our
Rotational Feature Normalizer (RFN) module. Our experiments on classification
tasks show that Spectral VMamba outperforms the leading SSM models in vision,
such as VMamba, while maintaining invariance to rotations and a providing a
similar runtime efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embedded <span class="highlight-title">Visual</span> Prompt Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01003v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01003v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiang Zu, Shenghao Xie, Qing Zhao, Guoqi Li, Lei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models pre-trained on large-scale data have been widely witnessed
to achieve success in various natural imaging downstream tasks.
Parameter-efficient fine-tuning (PEFT) methods aim to adapt foundation models
to new domains by updating only a small portion of parameters in order to
reduce computational overhead. However, the effectiveness of these PEFT
methods, especially in cross-domain few-shot scenarios, e.g., medical image
analysis, has not been fully explored. In this work, we facilitate the study of
the performance of PEFT when adapting foundation models to medical image
classification tasks. Furthermore, to alleviate the limitations of prompt
introducing ways and approximation capabilities on Transformer architectures of
mainstream prompt tuning methods, we propose the Embedded Prompt Tuning (EPT)
method by embedding prompt tokens into the expanded channels. We also find that
there are anomalies in the feature space distribution of foundation models
during pre-training process, and prompt tuning can help mitigate this negative
impact. To explain this phenomenon, we also introduce a novel perspective to
understand prompt tuning: Prompt tuning is a distribution calibrator. And we
support it by analyzing patch-wise scaling and feature separation operations
contained in EPT. Our experiments show that EPT outperforms several
state-of-the-art fine-tuning methods by a significant margin on few-shot
medical image classification tasks, and completes the fine-tuning process
within highly competitive time, indicating EPT is an effective PEFT method. The
source code is available at github.com/zuwenqiang/EPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic
  <span class="highlight-title">Vision</span>-language Context Sparsification <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00876v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00876v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaosheng Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, Yao Hu, Shaohui Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have achieved remarkable success in
vision understanding, reasoning, and interaction. However, the inference
computation and memory increase progressively with the generation of output
tokens during decoding, directly affecting the efficacy of MLLMs. Existing
methods attempt to reduce the vision context redundancy to achieve efficient
MLLMs. Unfortunately, the efficiency benefits of the vision context reduction
in the prefill stage gradually diminish during the decoding stage. To address
this problem, we proposed a dynamic vision-language context sparsification
framework Dynamic-LLaVA, which dynamically reduces the redundancy of vision
context in the prefill stage and decreases the memory and computation overhead
of the generated language context during decoding. Dynamic-LLaVA designs a
tailored sparsification inference scheme for different inference modes, i.e.,
prefill, decoding with and without KV cache, to achieve efficient inference of
MLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by
$\sim$75\% in the prefill stage. Meanwhile, throughout the entire generation
process of MLLMs, Dynamic-LLaVA reduces the $\sim$50\% computation consumption
under decoding without KV cache, while saving $\sim$50\% GPU memory overhead
when decoding with KV cache, due to the vision-language context sparsification.
Extensive experiments also demonstrate that Dynamic-LLaVA achieves efficient
inference for MLLMs with negligible understanding and generation ability
degradation or even performance gains compared to the full-context inference
baselines. Code is available at https://github.com/Osilly/dynamic_llava .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Code is available at
  https://github.com/Osilly/dynamic_llava</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-supervised Monocular Depth Estimation Based on Hierarchical
  Feature-Guided <span class="highlight-title">Diffusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runze Liu, Dongchen Zhu, Guanghui Zhang, Lei Wang, Jiamao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised monocular depth estimation has received widespread attention
because of its capability to train without ground truth. In real-world
scenarios, the images may be blurry or noisy due to the influence of weather
conditions and inherent limitations of the camera. Therefore, it is
particularly important to develop a robust depth estimation model. Benefiting
from the training strategies of generative networks, generative-based methods
often exhibit enhanced robustness. In light of this, we employ the
generative-based diffusion model with a unique denoising training process for
self-supervised monocular depth estimation. Additionally, to further enhance
the robustness of the diffusion model, we probe into the influence of
perturbations on image features and propose a hierarchical feature-guided
denoising module. Furthermore, we explore the implicit depth within
reprojection and design an implicit depth consistency loss. This loss function
is not interfered by the other subnetwork, which can be targeted to constrain
the depth estimation network and ensure the scale consistency of depth within a
video sequence. We conduct experiments on the KITTI and Make3D datasets. The
results indicate that our approach stands out among generative-based models,
while also showcasing remarkable robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoMA: Singular Value Decomposed Minor Components Adaptation for Domain
  Generalizable Representation Learning <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokju Yun, Seunghye Chae, Dongheon Lee, Youngmin Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) aims to adapt a model using one or multiple source
domains to ensure robust performance in unseen target domains. Recently,
Parameter-Efficient Fine-Tuning (PEFT) of foundation models has shown promising
results in the context of DG problem. Nevertheless, existing PEFT methods still
struggle to strike a balance between preserving generalizable components of the
pre-trained model and learning task-specific features. To gain insights into
the distribution of generalizable components, we begin by analyzing the
pre-trained weights through the lens of singular value decomposition. Building
on these insights, we introduce Singular Value Decomposed Minor Components
Adaptation (SoMA), an approach that selectively tunes minor singular components
while keeping the residual parts frozen. SoMA effectively retains the
generalization ability of the pre-trained model while efficiently acquiring
task-specific skills. Moreover, we freeze domain-generalizable blocks and
employ an annealing weight decay strategy, thereby achieving an optimal balance
in the delicate trade-off between generalizability and discriminability. SoMA
attains state-of-the-art results on multiple benchmarks that span both domain
generalized semantic segmentation to domain generalized object detection. In
addition, our methods introduce no additional inference overhead or
regularization loss, maintain compatibility with any backbone or head, and are
designed to be versatile, allowing easy integration into a wide range of tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025 Project page: https://ysj9909.github.io/SoRA.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse autoencoders reveal selective remapping of <span class="highlight-title">visual</span> concepts during
  adaptation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyesu Lim, Jinho Choi, Jaegul Choo, Steffen Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting foundation models for specific purposes has become a standard
approach to build machine learning systems for downstream applications. Yet, it
is an open question which mechanisms take place during adaptation. Here we
develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named
PatchSAE, to extract interpretable concepts at granular levels (e.g., shape,
color, or semantics of an object) and their patch-wise spatial attributions. We
explore how these concepts influence the model output in downstream image
classification tasks and investigate how recent state-of-the-art prompt-based
adaptation techniques change the association of model inputs to these concepts.
While activations of concepts slightly change between adapted and non-adapted
models, we find that the majority of gains on common adaptation tasks can be
explained with the existing concepts already present in the non-adapted
foundation model. This work provides a concrete framework to train and use SAEs
for Vision Transformers and provides insights into explaining adaptation
mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at the Thirteenth International
  Conference on Learning Representations (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Qian, Zhongliang Guo, Bowen Deng, Chun Tong Lei, Shuai Zhao, Chun Pong Lau, Xiaopeng Hong, Michael P. Pound
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot object counting aims to count instances of arbitrary object
categories specified by text descriptions. Existing methods typically rely on
vision-language models like CLIP, but often exhibit limited sensitivity to text
prompts. We present T2ICount, a diffusion-based framework that leverages rich
prior knowledge and fine-grained visual understanding from pretrained diffusion
models. While one-step denoising ensures efficiency, it leads to weakened text
sensitivity. To address this challenge, we propose a Hierarchical Semantic
Correction Module that progressively refines text-image feature alignment, and
a Representational Regional Coherence Loss that provides reliable supervision
signals by leveraging the cross-attention maps extracted from the denosing
U-Net. Furthermore, we observe that current benchmarks mainly focus on majority
objects in images, potentially masking models' text sensitivity. To address
this, we contribute a challenging re-annotated subset of FSC147 for better
evaluation of text-guided counting ability. Extensive experiments demonstrate
that our method achieves superior performance across different benchmarks. Code
is available at https://github.com/cha15yq/T2ICount.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Adaptive Dual-level Weighting Mechanism for Remote Sensing
  Pansharpening <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13214v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13214v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Huang, Haorui Chen, Jiaxuan Ren, Siran Peng, Liangjian Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, deep learning-based methods for remote sensing pansharpening have
advanced rapidly. However, many existing methods struggle to fully leverage
feature heterogeneity and redundancy, thereby limiting their effectiveness. We
use the covariance matrix to model the feature heterogeneity and redundancy and
propose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW
captures these correlations through the covariance matrix, which is then
processed by a nonlinear function to generate weights for adjustment. Building
upon CACW, we introduce a general adaptive dual-level weighting mechanism
(ADWM) to address these challenges from two key perspectives, enhancing a wide
range of existing deep-learning methods. First, Intra-Feature Weighting (IFW)
evaluates correlations among channels within each feature to reduce redundancy
and enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts
contributions across layers based on inter-layer correlations, refining the
final output. Extensive experiments demonstrate the superior performance of
ADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we
validate the effectiveness of our approach through generality experiments,
redundancy visualization, comparison experiments, key variables and complexity
analysis, and ablation studies. Our code is available at
https://github.com/Jie-1203/ADWM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at the CVPR Conference on Computer Vision and
  Pattern Recognition 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Number it: Temporal Grounding Videos like Flipping Manga <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10332v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10332v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongliang Wu, Xinting Hu, Yuyang Sun, Yizhou Zhou, Wenbo Zhu, Fengyun Rao, Bernt Schiele, Xu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Large Language Models (Vid-LLMs) have made remarkable advancements in
comprehending video content for QA dialogue. However, they struggle to extend
this visual understanding to tasks requiring precise temporal localization,
known as Video Temporal Grounding (VTG). To address this gap, we introduce
Number-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual
comprehension with temporal grounding by adding unique numerical identifiers to
each video frame. Treating a video as a sequence of numbered frame images,
NumPro transforms VTG into an intuitive process: flipping through manga panels
in sequence. This allows Vid-LLMs to "read" event timelines, accurately linking
visual content with corresponding temporal information. Our experiments
demonstrate that NumPro significantly boosts VTG performance of top-tier
Vid-LLMs without additional computational cost. Furthermore, fine-tuning on a
NumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing
previous top-performing methods by up to 6.9\% in mIoU for moment retrieval and
8.5\% in mAP for highlight detection. The code will be available at
https://github.com/yongliang-wu/NumPro.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PUGS: Zero-shot Physical Understanding with Gaussian Splatting <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Shuai, Ran Yu, Yuantao Chen, Zijian Jiang, Xiaowei Song, Nan Wang, Jv Zheng, Jianzhu Ma, Meng Yang, Zhicheng Wang, Wenbo Ding, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current robotic systems can understand the categories and poses of objects
well. But understanding physical properties like mass, friction, and hardness,
in the wild, remains challenging. We propose a new method that reconstructs 3D
objects using the Gaussian splatting representation and predicts various
physical properties in a zero-shot manner. We propose two techniques during the
reconstruction phase: a geometry-aware regularization loss function to improve
the shape quality and a region-aware feature contrastive loss function to
promote region affinity. Two other new techniques are designed during
inference: a feature-based property propagation module and a volume integration
module tailored for the Gaussian representation. Our framework is named as
zero-shot physical understanding with Gaussian splatting, or PUGS. PUGS
achieves new state-of-the-art results on the standard benchmark of ABO-500 mass
prediction. We provide extensive quantitative ablations and qualitative
visualization to demonstrate the mechanism of our designs. We show the proposed
methodology can help address challenging real-world grasping tasks. Our codes,
data, and models are available at https://github.com/EverNorif/PUGS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025, Project page: https://evernorif.github.io/PUGS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inverting Transformer-based <span class="highlight-title">Vision</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Rathjens, Shirin Reyhanian, David Kappel, Laurenz Wiskott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the mechanisms underlying deep neural networks in computer
vision remains a fundamental challenge. While many previous approaches have
focused on visualizing intermediate representations within deep neural
networks, particularly convolutional neural networks, these techniques have yet
to be thoroughly explored in transformer-based vision models. In this study, we
apply a modular approach of training inverse models to reconstruct input images
from intermediate layers within a Detection Transformer and a Vision
Transformer, showing that this approach is efficient and feasible. Through
qualitative and quantitative evaluations of reconstructed images, we generate
insights into the underlying mechanisms of these architectures, highlighting
their similarities and differences in terms of contextual shape and
preservation of image details, inter-layer correlation, and robustness to color
perturbations. Our analysis illustrates how these properties emerge within the
models, contributing to a deeper understanding of transformer-based vision
models. The code for reproducing our experiments is available at
github.com/wiskott-lab/inverse-detection-transformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WAIT: Feature Warping for Animation to Illustration video Translation
  using GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samet Hicsonmez, Nermin Samet, Fidan Samet, Oguz Bakir, Emre Akbas, Pinar Duygulu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore a new domain for video-to-video translation.
Motivated by the availability of animation movies that are adopted from
illustrated books for children, we aim to stylize these videos with the style
of the original illustrations. Current state-of-the-art video-to-video
translation models rely on having a video sequence or a single style image to
stylize an input video. We introduce a new problem for video stylizing where an
unordered set of images are used. This is a challenging task for two reasons:
i) we do not have the advantage of temporal consistency as in video sequences;
ii) it is more difficult to obtain consistent styles for video frames from a
set of unordered images compared to using a single image. Most of the
video-to-video translation methods are built on an image-to-image translation
model, and integrate additional networks such as optical flow, or temporal
predictors to capture temporal relations. These additional networks make the
model training and inference complicated and slow down the process. To ensure
temporal coherency in video-to-video style transfer, we propose a new generator
network with feature warping layers which overcomes the limitations of the
previous methods. We show the effectiveness of our method on three datasets
both qualitatively and quantitatively. Code and pretrained models are available
at https://github.com/giddyyupp/wait.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurocomputing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining Human Activity Recognition with SHAP: Validating Insights
  with Perturbation and Quantitative Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03714v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03714v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Tempel, Espen Alexander F. Ihlen, Lars Adde, Inga Strümke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Human Activity Recognition (HAR), understanding the intricacy of body
movements within high-risk applications is essential. This study uses SHapley
Additive exPlanations (SHAP) to explain the decision-making process of Graph
Convolution Networks (GCNs) when classifying activities with skeleton data. We
employ SHAP to explain two real-world datasets: one for cerebral palsy (CP)
classification and the widely used NTU RGB+D 60 action recognition dataset. To
test the explanation, we introduce a novel perturbation approach that modifies
the model's edge importance matrix, allowing us to evaluate the impact of
specific body key points on prediction outcomes. To assess the fidelity of our
explanations, we employ informed perturbation, targeting body key points
identified as important by SHAP and comparing them against random perturbation
as a control condition. This perturbation enables a judgment on whether the
body key points are truly influential or non-influential based on the SHAP
values. Results on both datasets show that body key points identified as
important through SHAP have the largest influence on the accuracy, specificity,
and sensitivity metrics. Our findings highlight that SHAP can provide granular
insights into the input feature contribution to the prediction outcome of GCNs
in HAR tasks. This demonstrates the potential for more interpretable and
trustworthy models in high-stakes applications like healthcare or
rehabilitation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Computers in Biology and Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic infant 2D pose estimation from videos: comparing seven deep
  neural network methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17382v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17382v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filipe Gama, Matej Misar, Lukas Navara, Sergiu T. Popescu, Matej Hoffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic markerless estimation of infant posture and motion from ordinary
videos carries great potential for movement studies "in the wild", facilitating
understanding of motor development and massively increasing the chances of
early diagnosis of disorders. There is rapid development of human pose
estimation methods in computer vision thanks to advances in deep learning and
machine learning. However, these methods are trained on datasets that feature
adults in different contexts. This work tests and compares seven popular
methods (AlphaPose, DeepLabCut/DeeperCut, Detectron2, HRNet,
MediaPipe/BlazePose, OpenPose, and ViTPose) on videos of infants in supine
position and in more complex settings. Surprisingly, all methods except
DeepLabCut and MediaPipe have competitive performance without additional
finetuning, with ViTPose performing best. Next to standard performance metrics
(average precision and recall), we introduce errors expressed in the
neck-mid-hip (torso length) ratio and additionally study missed and redundant
detections, and the reliability of the internal confidence ratings of the
different methods, which are relevant for downstream tasks. Among the networks
with competitive performance, only AlphaPose could run close to real time (27
fps) on our machine. We provide documented Docker containers or instructions
for all the methods we used, our analysis scripts, and the processed data at
https://hub.docker.com/u/humanoidsctu and https://osf.io/x465b/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 7 figures, 20 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Babel: A Scalable Pre-trained Model for Multi-Modal Sensing via
  Expandable Modality Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghong Dai, Shiqi Jiang, Yifan Yang, Ting Cao, Mo Li, Suman Banerjee, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Babel, the expandable modality alignment model, specially
designed for multi-modal sensing. While there has been considerable work on
multi-modality alignment, they all struggle to effectively incorporate multiple
sensing modalities due to the data scarcity constraints. How to utilize
multi-modal data with partial pairings in sensing remains an unresolved
challenge. Babel tackles this challenge by introducing the concept of
expandable modality alignment. The key idea involves transforming the
N-modality alignment into a series of binary-modality alignments. Novel
techniques are also proposed to further mitigate data scarcity issue and
balance the contribution of the newly incorporated modality with the previously
established modality alignment during the expandable alignment process. We
provide the comprehensive implementation. In the pre-training phase, Babel
currently aligns 6 sensing modalities, namely Wi-Fi, mmWave, IMU, LiDAR, video,
and depth. For the deployment phase, as a foundation model, any single or
combination of aligned modalities could be selected from Babel and applied to
downstream tasks. Evaluation demonstrates Babel's outstanding performance on
eight human activity recognition datasets, compared to a broad range of
baselines e.g., the SOTA single-modal sensing networks, multi-modal sensing
framework, and multi-modal large language models. Babel not only improves the
performance of individual modality sensing (12% averaged accuracy improvement),
but also effectively fuses multiple available modalities (up to 22% accuracy
increase). Case studies also highlight emerging application scenarios empowered
by Babel, including cross-modality retrieval (i.e., sensing imaging), and
bridging LLM for sensing comprehension.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SenSys'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Diffusion</span> Beats Autoregressive: An Evaluation of Compositional
  Generation in Text-to-Image Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22775v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22775v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arash Marioriyad, Parham Rezaei, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) generative models, such as Stable Diffusion and DALL-E,
have shown remarkable proficiency in producing high-quality, realistic, and
natural images from textual descriptions. However, these models sometimes fail
to accurately capture all the details specified in the input prompts,
particularly concerning entities, attributes, and spatial relationships. This
issue becomes more pronounced when the prompt contains novel or complex
compositions, leading to what are known as compositional generation failure
modes. Recently, a new open-source diffusion-based T2I model, FLUX, has been
introduced, demonstrating strong performance in high-quality image generation.
Additionally, autoregressive T2I models like LlamaGen have claimed competitive
visual quality performance compared to diffusion-based models. In this study,
we evaluate the compositional generation capabilities of these newly introduced
models against established models using the T2I-CompBench benchmark. Our
findings reveal that LlamaGen, as a vanilla autoregressive model, is not yet on
par with state-of-the-art diffusion models for compositional generation tasks
under the same criteria, such as model size and inference time. On the other
hand, the open-source diffusion-based model FLUX exhibits compositional
generation capabilities comparable to the state-of-the-art closed-source model
DALL-E3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Workshop on Compositional Learning: Perspectives,
  Methods, and Paths Forward</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval
  and Routing in Long-Form Video Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Kim, Hyunjun Kim, Hosu Lee, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in Large Multi-modal Models, applying them to long and
untrimmed video content remains challenging due to limitations in context
length and substantial memory overhead. These constraints often lead to
significant information loss and reduced relevance in the model responses. With
the exponential growth of video data across web platforms, understanding
long-form video is crucial for advancing generalized intelligence. In this
paper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel
video-LLM framework designed to enhance the comprehension of lengthy video
content through targeted retrieval process. We address two main challenges to
achieve it: (i) We present the SceneWalk dataset, a high-quality collection of
87.8K long videos, each densely captioned at the segment level to enable models
to capture scene continuity and maintain rich descriptive context. (ii) We
develop robust architectural designs integrating dynamic routing mechanism and
spatio-temporal projector to efficiently retrieve and process relevant video
segments based on user queries. Our framework mitigates the limitations of
current video-LMMs by allowing for precise identification and retrieval of
relevant video segments in response to queries, thereby improving the
contextual relevance of the generated responses. Through extensive experiments,
SALOVA demonstrates enhanced capability in processing complex long-form videos,
showing significant capability to maintain contextual integrity across extended
sequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ivy-lvlm.github.io/SALOVA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Part-Informed <span class="highlight-title">Visual</span>-Language Learning for Person
  Re-Identification <span class="chip">ICME 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Lin, Yehansen Chen, Baocai Yin, Jinshui Hu, Bing Yin, Cong Liu, Zengfu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, visual-language learning (VLL) has shown great potential in
enhancing visual-based person re-identification (ReID). Existing VLL-based ReID
methods typically focus on image-text feature alignment at the whole-body
level, while neglecting supervision on fine-grained part features, thus lacking
constraints for local feature semantic consistency. To this end, we propose
Part-Informed Visual-language Learning ($\pi$-VL) to enhance fine-grained
visual features with part-informed language supervisions for ReID tasks.
Specifically, $\pi$-VL introduces a human parsing-guided prompt tuning strategy
and a hierarchical visual-language alignment paradigm to ensure within-part
feature semantic consistency. The former combines both identity labels and
human parsing maps to constitute pixel-level text prompts, and the latter fuses
multi-scale visual features with a light-weight auxiliary head to perform
fine-grained image-text alignment. As a plug-and-play and inference-free
solution, our $\pi$-VL achieves performance comparable to or better than
state-of-the-art methods on four commonly used ReID benchmarks. Notably, it
reports 91.0% Rank-1 and 76.9% mAP on the challenging MSMT17 database, without
bells and whistles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, ICME 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UrbanGS: Semantic-Guided Gaussian Splatting for Urban Scene
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwen Li, Jiaxin Huang, Runnan Chen, Yunlong Che, Yandong Guo, Tongliang Liu, Fakhri Karray, Mingming Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing urban scenes is challenging due to their complex geometries
and the presence of potentially dynamic objects. 3D Gaussian Splatting
(3DGS)-based methods have shown strong performance, but existing approaches
often incorporate manual 3D annotations to improve dynamic object modeling,
which is impractical due to high labeling costs. Some methods leverage 4D
Gaussian Splatting (4DGS) to represent the entire scene, but they treat static
and dynamic objects uniformly, leading to unnecessary updates for static
elements and ultimately degrading reconstruction quality. To address these
issues, we propose UrbanGS, which leverages 2D semantic maps and an existing
dynamic Gaussian approach to distinguish static objects from the scene,
enabling separate processing of definite static and potentially dynamic
elements. Specifically, for definite static regions, we enforce global
consistency to prevent unintended changes in dynamic Gaussian and introduce a
K-nearest neighbor (KNN)-based regularization to improve local coherence on
low-textured ground surfaces. Notably, for potentially dynamic objects, we
aggregate temporal information using learnable time embeddings, allowing each
Gaussian to model deformations over time. Extensive experiments on real-world
datasets demonstrate that our approach outperforms state-of-the-art methods in
reconstruction quality and efficiency, accurately preserving static content
while capturing dynamic elements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Neural Networks on RAW and HDR Images for Restoration Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Yanzhe Ke, Lei Luo, Alexandre Chapiro, Xiaoyu Xiang, Yuchen Fan, Rakesh Ranjan, Rafal Mantiuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vast majority of standard image and video content available online is
represented in display-encoded color spaces, in which pixel values are
conveniently scaled to a limited range (0-1) and the color distribution is
approximately perceptually uniform. In contrast, both camera RAW and high
dynamic range (HDR) images are often represented in linear color spaces, in
which color values are linearly related to colorimetric quantities of light.
While training on commonly available display-encoded images is a
well-established practice, there is no consensus on how neural networks should
be trained for tasks on RAW and HDR images in linear color spaces. In this
work, we test several approaches on three popular image restoration
applications: denoising, deblurring, and single-image super-resolution. We
examine whether HDR/RAW images need to be display-encoded using popular
transfer functions (PQ, PU21, and mu-law), or whether it is better to train in
linear color spaces, but use loss functions that correct for perceptual
non-uniformity. Our results indicate that neural networks train significantly
better on HDR and RAW images represented in display-encoded color spaces, which
offer better perceptual uniformity than linear spaces. This small change to the
training strategy can bring a very substantial gain in performance, between 2
and 9 dB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large
  Language Models <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05935v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05935v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Hongsheng Li, Yu Qiao, Peng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)
series developed upon SPHINX. To improve the architecture and training
efficiency, we modify the SPHINX framework by removing redundant visual
encoders, bypassing fully-padded sub-images with skip tokens, and simplifying
multi-stage training into a one-stage all-in-one paradigm. To fully unleash the
potential of MLLMs, we assemble a comprehensive multi-domain and multimodal
dataset covering publicly available resources in language, vision, and
vision-language tasks. We further enrich this collection with our curated OCR
intensive and Set-of-Mark datasets, extending the diversity and generality. By
training over different base LLMs including TinyLlama1.1B, InternLM2-7B,
LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in
parameter size and multilingual capabilities. Comprehensive benchmarking
reveals a strong correlation between the multi-modal performance with the data
and parameter scales. Code and models are released at
https://github.com/Alpha-VLLM/LLaMA2-Accessory
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2024. Code and models are released at
  https://github.com/Alpha-VLLM/LLaMA2-Accessory</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Reloc3r: Large-Scale Training of Relative <span class="highlight-title">Camera</span> Pose Regression for
  Generalizable, Fast, and Accurate <span class="highlight-title">Visual</span> Localization <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyan Dong, Shuz<span class="highlight-author">he Wang</span>, Shaohui Liu, Lulu Cai, Qingnan Fan, Juho Kannala, Yanchao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual localization aims to determine the camera pose of a query image
relative to a database of posed images. In recent years, deep neural networks
that directly regress camera poses have gained popularity due to their fast
inference capabilities. However, existing methods struggle to either generalize
well to new scenes or provide accurate camera pose estimates. To address these
issues, we present Reloc3r, a simple yet effective visual localization
framework. It consists of an elegantly designed relative pose regression
network, and a minimalist motion averaging module for absolute pose estimation.
Trained on approximately eight million posed image pairs, Reloc3r achieves
surprisingly good performance and generalization ability. We conduct extensive
experiments on six public datasets, consistently demonstrating the
effectiveness and efficiency of the proposed method. It provides high-quality
camera pose estimates in real time and generalizes to novel scenes. Code:
https://github.com/ffrivera0/reloc3r.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutArch: An AI-assisted workflow for object detection and automated
  recording in archaeological catalogues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17978v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17978v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Klein, Antoine Muller, Alyssa Wohde, Alexander V. Gorelik, Volker Heyd, Ralf Lämmel, Yoan Diekmann, Maxime Brami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The context of this paper is the creation of large uniform archaeological
datasets from heterogeneous published resources, such as find catalogues - with
the help of AI and Big Data. The paper is concerned with the challenge of
consistent assemblages of archaeological data. We cannot simply combine
existing records, as they differ in terms of quality and recording standards.
Thus, records have to be recreated from published archaeological illustrations.
This is only a viable path with the help of automation. The contribution of
this paper is a new workflow for collecting data from archaeological find
catalogues available as legacy resources, such as archaeological drawings and
photographs in large unsorted PDF files; the workflow relies on custom software
(AutArch) supporting image processing, object detection, and interactive means
of validating and adjusting automatically retrieved data. We integrate
artificial intelligence (AI) in terms of neural networks for object detection
and classification into the workflow, thereby speeding up, automating, and
standardising data collection. Objects commonly found in archaeological
catalogues - such as graves, skeletons, ceramics, ornaments, stone tools and
maps - are detected. Those objects are spatially related and analysed to
extract real-life attributes, such as the size and orientation of graves based
on the north arrow and the scale. We also automate recording of geometric
whole-outlines through contour detection, as an alternative to landmark-based
geometric morphometrics. Detected objects, contours, and other automatically
retrieved data can be manually validated and adjusted. We use third millennium
BC Europe (encompassing cultures such as 'Corded Ware' and 'Bell Beaker', and
their burial practices) as a 'testing ground' and for evaluation purposes; this
includes a user study for the workflow and the AutArch software.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surgical Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09230v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09230v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinedu Innocent Nwoye, Rupak Bose, Kareem Elgohary, Lorenzo Arboit, Giorgio Carlino, Joël L. Lavanchy, Pietro Mascagni, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquiring surgical data for research and development is significantly
hindered by high annotation costs and practical and ethical constraints.
Utilizing synthetically generated images could offer a valuable alternative. In
this work, we explore adapting text-to-image generative models for the surgical
domain using the CholecT50 dataset, which provides surgical images annotated
with action triplets (instrument, verb, target). We investigate several
language models and find T5 to offer more distinct features for differentiating
surgical actions on triplet-based textual inputs, and showcasing stronger
alignment between long and triplet-based captions. To address challenges in
training text-to-image models solely on triplet-based captions without
additional inputs and supervisory signals, we discover that triplet text
embeddings are instrument-centric in the latent space. Leveraging this insight,
we design an instrument-based class balancing technique to counteract data
imbalance and skewness, improving training convergence. Extending Imagen, a
diffusion-based generative model, we develop Surgical Imagen to generate
photorealistic and activity-aligned surgical images from triplet-based textual
prompts. We assess the model on quality, alignment, reasoning, and knowledge,
achieving FID and CLIP scores of 3.7 and 26.8% respectively. Human expert
survey shows that participants were highly challenged by the realistic
characteristics of the generated samples, demonstrating Surgical Imagen's
effectiveness as a practical alternative to real data collection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 13 figures, 3 tables, published in Pattern Recognition
  Letters 2025, project page at https://camma-public.github.io/endogen/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Modal Consistency Learning for Sign Language Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kepeng Wu, Zecheng Li, Hezhen Hu, Wengang Zhou, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training has been proven to be effective in boosting the performance of
Isolated Sign Language Recognition (ISLR). Existing pre-training methods solely
focus on the compact pose data, which eliminates background perturbation but
inevitably suffers from insufficient semantic cues compared to raw RGB videos.
Nevertheless, learning representation directly from RGB videos remains
challenging due to the presence of sign-independent visual features. To address
this dilemma, we propose a Cross-modal Consistency Learning framework
(CCL-SLR), which leverages the cross-modal consistency from both RGB and pose
modalities based on self-supervised pre-training. First, CCL-SLR employs
contrastive learning for instance discrimination within and across modalities.
Through the single-modal and cross-modal contrastive learning, CCL-SLR
gradually aligns the feature spaces of RGB and pose modalities, thereby
extracting consistent sign representations. Second, we further introduce
Motion-Preserving Masking (MPM) and Semantic Positive Mining (SPM) techniques
to improve cross-modal consistency from the perspective of data augmentation
and sample similarity, respectively. Extensive experiments on four ISLR
benchmarks show that CCL-SLR achieves impressive performance, demonstrating its
effectiveness. The code will be released to the public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Design of an Expression Recognition Solution Based on the Global
  Channel-Spatial Attention Mechanism and Proportional Criterion Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11935v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11935v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yu, Yang Zheng, Lei Wang, Yongqi Wang, Shengfan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression recognition is a challenging classification task that holds
broad application prospects in the field of human-computer interaction. This
paper aims to introduce the method we will adopt in the 8th Affective and
Behavioral Analysis in the Wild (ABAW) Competition, which will be held during
the Conference on Computer Vision and Pattern Recognition (CVPR) in 2025.First
of all, we apply the frequency masking technique and the method of extracting
data at equal time intervals to conduct targeted processing on the original
videos. Then, based on the residual hybrid convolutional neural network and the
multi-branch convolutional neural network respectively, we design feature
extraction models for image and audio sequences. In particular, we propose a
global channel-spatial attention mechanism to enhance the features initially
extracted from both the audio and image modalities respectively.Finally, we
adopt a decision fusion strategy based on the proportional criterion to fuse
the classification results of the two single modalities, obtain an emotion
probability vector, and output the final emotional classification. We also
design a coarse - fine granularity loss function to optimize the performance of
the entire network, which effectively improves the accuracy of facial
expression recognition.In the facial expression recognition task of the 8th
ABAW Competition, our method ranked third on the official validation set. This
result fully confirms the effectiveness and competitiveness of the method we
have proposed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morphing Tokens Draw Strong Masked Image Models <span class="chip">ICLR'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00254v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00254v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taekyung Kim, Byeongho Heo, Dongyoon Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked image modeling (MIM) has emerged as a promising approach for
pre-training Vision Transformers (ViTs). MIMs predict masked tokens token-wise
to recover target signals that are tokenized from images or generated by
pre-trained models like vision-language models. While using tokenizers or
pre-trained models is viable, they often offer spatially inconsistent
supervision even for neighboring tokens, hindering models from learning
discriminative representations. Our pilot study identifies spatial
inconsistency in supervisory signals and suggests that addressing it can
improve representation learning. Building upon this insight, we introduce
Dynamic Token Morphing (DTM), a novel method that dynamically aggregates tokens
while preserving context to generate contextualized targets, thereby likely
reducing spatial inconsistency. DTM is compatible with various SSL frameworks;
we showcase significantly improved MIM results, barely introducing extra
training costs. Our method facilitates MIM training by using more spatially
consistent targets, resulting in improved training trends as evidenced by lower
losses. Experiments on ImageNet-1K and ADE20K demonstrate DTM's superiority,
which surpasses complex state-of-the-art MIM methods. Furthermore, the
evaluation of transfer learning on downstream tasks like iNaturalist, along
with extensive empirical studies, supports DTM's effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 16 tables, 8 figures. To be presented at ICLR'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Continual Learning of <span class="highlight-title">Vision</span>-Language Models with Model Fusion <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Gao, Zicong Zhang, Yuqi Wei, Linglan Zhao, Guilin Li, Yexin Li, Linghe Kong, Weiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) represent a breakthrough in artificial
intelligence by integrating visual and textual modalities to achieve impressive
zero-shot capabilities. However, VLMs are susceptible to catastrophic
forgetting when sequentially fine-tuned on multiple downstream tasks. Existing
continual learning methods for VLMs often rely heavily on additional reference
datasets, compromise zero-shot performance, or are limited to
parameter-efficient fine-tuning scenarios. In this paper, we propose Continual
Decoupling-Unifying (ConDU), a novel approach, by introducing model fusion into
continual learning for VLMs. ConDU maintains a unified model along with task
triggers and prototype sets, employing an iterative process of decoupling
task-specific models for previous tasks and unifying them with the model for
the newly learned task. Additionally, we introduce an inference strategy for
zero-shot scenarios by aggregating predictions from multiple decoupled
task-specific models. Extensive experiments across various settings show that
ConDU achieves up to a 2\% improvement in average performance across all seen
tasks compared to state-of-the-art baselines, while also enhancing zero-shot
capabilities relative to the original VLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FATE: Full-head Gaussian Avatar with Textural Editing from Monocular
  Video <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhang, Zijian Wu, Zhiyang Liang, Yicheng Gong, Dongfang Hu, Yao Yao, Xun Cao, Hao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing high-fidelity, animatable 3D head avatars from effortlessly
captured monocular videos is a pivotal yet formidable challenge. Although
significant progress has been made in rendering performance and manipulation
capabilities, notable challenges remain, including incomplete reconstruction
and inefficient Gaussian representation. To address these challenges, we
introduce FATE, a novel method for reconstructing an editable full-head avatar
from a single monocular video. FATE integrates a sampling-based densification
strategy to ensure optimal positional distribution of points, improving
rendering efficiency. A neural baking technique is introduced to convert
discrete Gaussian representations into continuous attribute maps, facilitating
intuitive appearance editing. Furthermore, we propose a universal completion
framework to recover non-frontal appearance, culminating in a
360$^\circ$-renderable 3D head avatar. FATE outperforms previous approaches in
both qualitative and quantitative evaluations, achieving state-of-the-art
performance. To the best of our knowledge, FATE is the first animatable and
360$^\circ$ full-head monocular reconstruction method for a 3D head avatar.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ You See it, You Got it: Learning 3D Creation on Pose-Free Videos at
  Scale <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06699v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06699v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent 3D generation models typically rely on limited-scale 3D `gold-labels'
or 2D diffusion priors for 3D content creation. However, their performance is
upper-bounded by constrained 3D priors due to the lack of scalable learning
paradigms. In this work, we present See3D, a visual-conditional multi-view
diffusion model trained on large-scale Internet videos for open-world 3D
creation. The model aims to Get 3D knowledge by solely Seeing the visual
contents from the vast and rapidly growing video data -- You See it, You Got
it. To achieve this, we first scale up the training data using a proposed data
curation pipeline that automatically filters out multi-view inconsistencies and
insufficient observations from source videos. This results in a high-quality,
richly diverse, large-scale dataset of multi-view images, termed WebVi3D,
containing 320M frames from 16M video clips. Nevertheless, learning generic 3D
priors from videos without explicit 3D geometry or camera pose annotations is
nontrivial, and annotating poses for web-scale videos is prohibitively
expensive. To eliminate the need for pose conditions, we introduce an
innovative visual-condition - a purely 2D-inductive visual signal generated by
adding time-dependent noise to the masked video data. Finally, we introduce a
novel visual-conditional 3D generation framework by integrating See3D into a
warping-based pipeline for high-fidelity 3D generation. Our numerical and
visual comparisons on single and sparse reconstruction benchmarks show that
See3D, trained on cost-effective and scalable video data, achieves notable
zero-shot and open-world generation capabilities, markedly outperforming models
trained on costly and constrained 3D datasets. Please refer to our project page
at: https://vision.baai.ac.cn/see3d
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025, Project Page: https://vision.baai.ac.cn/see3d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Free-Lunch Color-Texture Disentanglement for Stylized Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Qin, Senmao Li, Alexandra Gomez-Villa, Shiqi Yang, Yaxing Wang, Kai Wang, Joost van de Weijer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Text-to-Image (T2I) diffusion models have transformed
image generation, enabling significant progress in stylized generation using
only a few style reference images. However, current diffusion-based methods
struggle with fine-grained style customization due to challenges in controlling
multiple style attributes, such as color and texture. This paper introduces the
first tuning-free approach to achieve free-lunch color-texture disentanglement
in stylized T2I generation, addressing the need for independently controlled
style elements for the Disentangled Stylized Image Generation (DisIG) problem.
Our approach leverages the Image-Prompt Additivity property in the CLIP image
embedding space to develop techniques for separating and extracting
Color-Texture Embeddings (CTE) from individual color and texture reference
images. To ensure that the color palette of the generated image aligns closely
with the color reference, we apply a whitening and coloring transformation to
enhance color consistency. Additionally, to prevent texture loss due to the
signal-leak bias inherent in diffusion training, we introduce a noise term that
preserves textural fidelity during the Regularized Whitening and Coloring
Transformation (RegWCT). Through these methods, our Style Attributes
Disentanglement approach (SADis) delivers a more precise and customizable
solution for stylized image generation. Experiments on images from the WikiArt
and StyleDrop datasets demonstrate that, both qualitatively and quantitatively,
SADis surpasses state-of-the-art stylization methods in the DisIG task.Code
will be released at https://deepffff.github.io/sadis.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be available at https://deepffff.github.io/sadis.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised Segmentation of Hyper-Reflective Foci with Compact
  Convolutional Transformers and SAM2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Morelle, Justus Bisten, Maximilian W. M. Wintergerst, Robert P. Finger, Thomas Schultz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly supervised segmentation has the potential to greatly reduce the
annotation effort for training segmentation models for small structures such as
hyper-reflective foci (HRF) in optical coherence tomography (OCT). However,
most weakly supervised methods either involve a strong downsampling of input
images, or only achieve localization at a coarse resolution, both of which are
unsatisfactory for small structures. We propose a novel framework that
increases the spatial resolution of a traditional attention-based Multiple
Instance Learning (MIL) approach by using Layer-wise Relevance Propagation
(LRP) to prompt the Segment Anything Model (SAM~2), and increases recall with
iterative inference. Moreover, we demonstrate that replacing MIL with a Compact
Convolutional Transformer (CCT), which adds a positional encoding, and permits
an exchange of information between different regions of the OCT image, leads to
a further and substantial increase in segmentation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure, accepted at German Conference on Medical Image
  Computing 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Lighting Deceives: Exposing <span class="highlight-title">Vision</span>-Language Models' Illumination
  Vulnerability Through Illumination Transformation Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06903v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06903v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Liu, Shouwei Ruan, Yao Huang, Shiji Zhao, Xingxing Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have achieved remarkable success in various
tasks, yet their robustness to real-world illumination variations remains
largely unexplored. To bridge this gap, we propose \textbf{I}llumination
\textbf{T}ransformation \textbf{A}ttack (\textbf{ITA}), the first framework to
systematically assess VLMs' robustness against illumination changes. However,
there still exist two key challenges: (1) how to model global illumination with
fine-grained control to achieve diverse lighting conditions and (2) how to
ensure adversarial effectiveness while maintaining naturalness. To address the
first challenge, we innovatively decompose global illumination into multiple
parameterized point light sources based on the illumination rendering equation.
This design enables us to model more diverse lighting variations that previous
methods could not capture. Then, by integrating these parameterized lighting
variations with physics-based lighting reconstruction techniques, we could
precisely render such light interactions in the original scenes, finally
meeting the goal of fine-grained lighting control. For the second challenge, by
controlling illumination through the lighting reconstrution model's latent
space rather than direct pixel manipulation, we inherently preserve physical
lighting priors. Furthermore, to prevent potential reconstruction artifacts, we
design additional perceptual constraints for maintaining visual consistency
with original images and diversity constraints for avoiding light source
convergence.
  Extensive experiments demonstrate that our ITA could significantly reduce the
performance of advanced VLMs, e.g., LLaVA-1.6, while possessing competitive
naturalness, exposing VLMS' critical illuminiation vulnerabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Training of Generalizable Visuomotor Policies via
  Control-Aware Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinuo Zhao, Kun Wu, Tianjiao Yi, Zhiyuan Xu, Xiaozhu Ju, Zhengping Che, Chi Harold Liu, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving generalization is one key challenge in embodied AI, where obtaining
large-scale datasets across diverse scenarios is costly. Traditional weak
augmentations, such as cropping and flipping, are insufficient for improving a
model's performance in new environments. Existing data augmentation methods
often disrupt task-relevant information in images, potentially degrading
performance. To overcome these challenges, we introduce EAGLE, an efficient
training framework for generalizable visuomotor policies that improves upon
existing methods by (1) enhancing generalization by applying augmentation only
to control-related regions identified through a self-supervised control-aware
mask and (2) improving training stability and efficiency by distilling
knowledge from an expert to a visuomotor student policy, which is then deployed
to unseen environments without further fine-tuning. Comprehensive experiments
on three domains, including the DMControl Generalization Benchmark, the
enhanced Robot Manipulation Distraction Benchmark, and a long-sequential
drawer-opening task, validate the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-03-29T05:29:35.947943251Z">
            2025-03-29 05:29:35 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
